[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "",
    "text": "Preface\nWelcome to the lecture notes for the Statistics for Data Science (DATA11007) course offered by the Master’s Programme in Data Science at the University of Helsinki.\nThese notes are designed to be an accessible companion to the course, providing multiple perspectives on statistical concepts for students with diverse backgrounds.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-these-notes",
    "href": "index.html#about-these-notes",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "About These Notes",
    "text": "About These Notes\nThese lecture notes complement the main course textbook, All of Statistics by Larry Wasserman; Wasserman (2013). While Wasserman’s book provides a comprehensive and rigorous treatment, these notes aim to:\n\nProvide gentler introductions to complex topics\nOffer multiple perspectives (intuitive, practical, mathematical, computational)\nInclude extensive code examples in Python (and occasionally R)\nBridge prerequisite gaps for students from different backgrounds\n\n\nFormats and Feedback\nThese notes are available in two formats:\n\nHTML (recommended): Interactive version with tabs, code folding, and better navigation.\nPDF: Traditional document format for printing or offline reading.\n\nThe content is almost identical in both formats. The PDF version is automatically generated from the source using Quarto’s LaTeX conversion and we are aware it contains some formatting artifacts (e.g., additional page breaks), but this should not affect readability. Interactive code elements and end-of-chapter code blocks are not included in the PDF version. The HTML version of the notes is the officially recommended version.\n\n\n\n\n\n\nYour Feedback is Welcome!\n\n\n\nThese lecture notes are new and under active development. We welcome feedback, corrections, and suggestions! If you spot any errors, unclear explanations, or have ideas for improvements, please let your instructor know. Your feedback will contribute directly to the development of these materials.\n\n\n\n\n\n\n\n\nWork in Progress\n\n\n\nThese lecture notes are being developed throughout the course. While the initial chapters are currently available, additional chapters will be released progressively as they are prepared. Each chapter will be made available before its corresponding lecture, ensuring you have the materials when you need them.\nWe appreciate your patience as we continue to build this comprehensive resource!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-these-notes",
    "href": "index.html#how-to-use-these-notes",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "How to Use These Notes",
    "text": "How to Use These Notes\nThroughout these notes, you’ll encounter sections with multiple perspectives on the same concept. These are presented in tabbed panels to help you explore different viewpoints:\nIntuitiveMathematicalComputationalThis perspective provides conceptual understanding through analogies,\nvisualizations, and real-world examples. It focuses on building\nintuition without heavy mathematical formalism.This perspective explains concepts using mathematical language and\nnotation. It may provide alternative formulations or connect to other\nmathematical ideas that some readers find clearer or more familiar.This perspective shows how to implement and explore concepts through\ncode. It includes simulations, numerical experiments, and practical\nalgorithms.\nImportant: These perspectives often contain more advanced material than required for the course, or teasers for material which will be covered later. You’re not expected to understand everything in every panel! Think of them as:\n\nAdditional resources for deeper exploration\nDifferent entry points to the same concept\nTeasers for the content of future lectures\nOptional enrichment beyond the core material\n\nChoose the perspective that best matches your learning style, or explore all three to build a richer understanding. The core course content is always presented in the main text outside these panels.\n\nExample: Understanding Variance\nHere’s a brief example of how these perspectives work. Consider the concept of variance (\\text{Var}), which measures how spread out data values are:\nIntuitiveMathematicalComputationalVariance measures how far data points typically are from their\naverage.Imagine test scores in two classes:\nClass A: Everyone scores between 75-85 (low variance)\nClass B: Scores range from 40-100 (high variance)\nBoth might have the same average (say, 80), but Class B has much more\nspread. Variance captures this spread numerically.The variance of a random variable\n\\(X\\) is defined as:\\[\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\]For a discrete distribution:\\[\\text{Var}(X) = \\sum_i (x_i - \\mu)^2 \\cdot \\mathbb{P}(X = x_i)\\]Key properties:\n\\(\\text{Var}(aX + b) = a^2\\text{Var}(X)\\)\nIf \\(X, Y\\) are independent:\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Two datasets with same mean but different variance\nclass_a = [75, 78, 80, 82, 85, 77, 83, 79, 81, 80]\nclass_b = [40, 95, 100, 65, 85, 90, 45, 98, 70, 82]\n\nprint(f\"Class A: mean = {np.mean(class_a):.1f}, variance = {np.var(class_a):.1f}\")\nprint(f\"Class B: mean = {np.mean(class_b):.1f}, variance = {np.var(class_b):.1f}\")\n\n# Visualize to see the spread\nplt.figure(figsize=(7, 3))\n\n# Use same bins for both histograms to ensure consistent bar width\nbins = np.linspace(30, 110, 17)  # 16 bins from 30 to 110\n\nplt.hist(class_a, alpha=0.5, label='Class A (low variance)', bins=bins, edgecolor='black')\nplt.hist(class_b, alpha=0.5, label='Class B (high variance)', bins=bins, edgecolor='black')\nplt.xlabel('Test Score')\nplt.ylabel('Count')\nplt.title('Comparison of Test Score Distributions')\nplt.legend(loc='upper left')\n\n# Set y-axis to show only integers\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nClass A: mean = 80.0, variance = 7.8\nClass B: mean = 77.0, variance = 413.8\n\n\n\n\n\nNotice how each perspective offers something different: intuition through examples, mathematical rigor through formulas, and hands-on exploration through code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe course assumes:\n\nBasic programming ability (Python or R)\nCalculus (derivatives and integrals)\nLinear algebra (matrices and vectors)\nBasic probability (sample spaces, events)\n\nDon’t worry if you’re rusty on some topics – we’ll review key concepts as needed.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "Course Structure",
    "text": "Course Structure\n\nFoundations: Probability, random variables, distributions\nInference: Estimation, confidence intervals, hypothesis testing\n\nMethods: Regression, resampling, Bayesian approaches\nAdvanced Topics: Missing data, causal inference, experimental design\n\n\nAvailable Chapters\nThe following chapters are currently available:\n\nProbability Foundations\nExpectation\nConvergence and Statistical Inference\nNonparametric Methods and Bootstrap\nParametric Inference I: Finding Estimators\nParametric Inference II: Properties of Estimators\nHypothesis Testing and p-values\nBayesian Inference and Decision Theory\nLinear and Logistic Regression",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "Programming",
    "text": "Programming\nIn this course, we’ll mainly use Python with occasional usages of R. Python is the main language for modern machine learning, and both Python and R are widely used in data science. Feel free to choose the environment you’re most comfortable with.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Statistics for Data Science: Lecture Notes",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThese notes build upon materials from previous course iterations and incorporate feedback from many students. Special thanks to the teaching assistants and students who have helped improve these materials.\nThe course content is largely adapted from Wasserman (2013), which remains the main course textbook.\nThe course slides and materials in early course iterations (2021-2024) were fully developed by Antti Honkela. Starting from 2025, we introduced the lecture notes that you are currently reading, which have been developed mainly by Luigi Acerbi expanding on the existing course slides and materials.\nThese notes were compiled with the assistance of Gemini 2.5 Pro and the Claude 4 family of AI models, particularly when it comes to the Quarto layout, coding content, and brainstorming examples.\n\nLast updated: September 7, 2025\n\n\n\n\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html",
    "href": "chapters/01-probability-foundations.html",
    "title": "1  Probability Foundations",
    "section": "",
    "text": "1.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#learning-objectives",
    "href": "chapters/01-probability-foundations.html#learning-objectives",
    "title": "1  Probability Foundations",
    "section": "",
    "text": "Explain the role of probability in data science and statistical modeling.\nApply fundamental probability concepts, including sample spaces, events, and the axioms of probability.\nCalculate probabilities using independence, conditional probability, and Bayes’ theorem.\nDistinguish between discrete and continuous random variables and use their distribution functions (PMF, PDF, CDF).\nDescribe and apply key univariate and multivariate distributions (e.g., Binomial, Normal, Multinomial).\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter covers fundamental probability concepts essential for statistical inference. The material is adapted and expanded from Chapters 1 and 2 of Wasserman (2013), which interested readers are encouraged to consult directly for a more rigorous and comprehensive treatment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#why-do-we-need-statistics",
    "href": "chapters/01-probability-foundations.html#why-do-we-need-statistics",
    "title": "1  Probability Foundations",
    "section": "1.2 Why Do We Need Statistics?",
    "text": "1.2 Why Do We Need Statistics?\n\n1.2.1 Statistics and Machine Learning in Data Science\nMachine learning (ML) has revolutionized our ability to make predictions. Given enough training data, modern ML models can achieve remarkable accuracy on unseen data that resembles what they’ve been trained on. But there’s a crucial limitation: these models excel when the new data comes from the same distribution as the training data.\nHow do we move beyond this constraint to make reliable predictions in the real world, where conditions change and data can be messy, incomplete, or collected differently than our training set?\nThis is where statistics becomes essential. Statistics provides the tools to:\n\nUnderstand principles of data collection: How was the data gathered? What biases might exist?\nPlan data collection strategically: Design experiments and surveys that yield meaningful insights\nDeal with missing data: Real-world data is rarely complete - we need principled ways to handle gaps\nUnderstand causality in modeling: Correlation isn’t causation, and confounding variables can mislead us\n\nWithout these statistical foundations, even the most sophisticated ML models can fail spectacularly when deployed in practice.\n\n\n1.2.2 Case Study: IBM Watson Health\nThe story of IBM Watson Health illustrates why statistical thinking is crucial for real-world AI applications.\nIn 2011, IBM Watson made headlines by defeating human champions on the quiz show Jeopardy! This victory showcased the power of natural language processing and knowledge retrieval. IBM saw an opportunity: if Watson could master general knowledge, why not train it to be a doctor?\nWatson Health launched in 2015 with an ambitious goal: use data from top US hospitals to train an AI system that could diagnose and treat patients anywhere in the world. The vision was compelling - bring world-class medical expertise to underserved areas through AI.\nOver the years, IBM:\n\nSpent over $4 billion on acquisitions\nEmployed 7,000 people developing the system\n\nPartnered with prestigious medical institutions\n\nYet by 2022, IBM sold Watson Health’s data and assets for just $1 billion - a massive loss. What went wrong?\nThe fundamental issue was data representativeness. Watson Health was trained on data from elite US hospitals treating specific patient populations. But this data didn’t represent:\n\nDifferent healthcare systems and practices globally\nDiverse patient populations with varying genetics, lifestyles, and environmental factors\nResource constraints in different settings\nVariations in how medical data is recorded and coded\n\nThis failure wasn’t due to inadequate machine learning algorithms - it was a failure to apply statistical thinking about data collection, representation, and generalization. No amount of computational power can overcome fundamentally biased or unrepresentative data.\nRead more in this Slate article.\n\n\n1.2.3 Two Cultures of Statistical Modeling\nIn his influential essay Breiman (2001), statistician Leo Breiman identified two distinct approaches to statistical modeling, each with different goals and philosophies. These are often cast as the two approaches of prediction vs. explanation.\n\n\n\n\n\n\n\n\nFeature\nThe Algorithmic Modeling Culture\nThe Data Modeling Culture\n\n\n\n\nGoal\nAccurate prediction\nUnderstanding mechanisms\n\n\nApproach\nTreat the mapping from inputs to outputs as a black box\nSpecify interpretable models that represent how nature works\n\n\nValidation\nPredictive accuracy on held-out data\nStatistical tests, confidence intervals, model diagnostics\n\n\nPhilosophy\n“It doesn’t matter how it works, as long as it works”\n“We need to understand which factors matter and why”\n\n\nExamples\nDeep learning, random forests, boosting\nLinear regression, causal inference, experimental design\n\n\n\nIntuitiveMathematicalComputationalThink of these two cultures like different approaches to cooking:The algorithmic approach is like following a\ntop-rated recipe - you don’t know why you fold (not stir) the batter or\nwhy ingredients must be room temperature, but following the steps\nprecisely often produces better results than many trained cooks achieve.\nYou can pick 5-star recipes and succeed without any cooking\nknowledge.The data modeling approach is like understanding\nfood science - you know about Maillard reactions, gluten development,\nand emulsification. But translating this into a great dish is slow and\ncomplex. You might spend hours calculating optimal ratios only to\nproduce something inferior to what a simple recipe would have given\nyou.This creates a fundamental tension: The recipe follower often\nproduces better food faster. The food scientist understands why things\nwork and with time might produce a breakthrough - but may struggle to\nmatch the empirical success of well-tested recipes. In machine learning,\nthis same tension exists - a neural network might predict customer\nbehavior better than any theory-based model, even if we don’t understand\nwhy. Sometimes, letting algorithms find patterns empirically works\nbetter than imposing our theoretical understanding. However,\nunderstanding often gives us an edge to build better algorithms and\ngeneralize to novel scenarios.Formally, both cultures can be seen as addressing the problem of\ncharacterizing a mapping:\n\\[X \\rightarrow Y\\]where \\(X\\) represents\ninput features and\n\\(Y\\) represents the\nresponse.Algorithmic approach (example): Find a function\n\\(\\hat{f}\\) that minimizes prediction\nerror. A common approach is to find the function that minimizes the\nmean squared error (MSE),\n\\[\\text{MSE} = \\frac{1}{N} \\sum_{n=1}^N (Y_n - \\hat{f}(X_n))^2\\]that is make the squared difference between the actual outcome\n\\(Y_n\\) and the prediction\n\\(\\hat{f}(X_n)\\) as small as possible\nover the available training data. In practice, we often report the\nroot mean squared error (RMSE) =\n\\(\\sqrt{\\text{MSE}}\\), which has the\nsame units as \\(Y\\). We don’t care\nabout what the function \\(\\hat{f}\\)\nlooks like, as long as it minimizes this error.Data modeling approach (example): Build a\nmechanistic model\n\\(Y = f(X; \\theta) + \\epsilon\\)\nwhere:\n\\(f\\) has a specific, interpretable\nform\n\\(\\theta\\) are parameters with\nscientific, interpretable meaning\n\\(\\epsilon\\) represents random\nerror\nWhile fitting the model to data often still involves optimizing some\nobjective, the goal here is to study the best-fitting parameters\n\\(\\theta\\), or find the best model\n\\(f\\) among a set of competing\nhypotheses.We compare here the two approaches represented by a random forest\n(RF) model and a linear\nregression model. The former represents a traditional machine\nlearning approach, while the latter is a staple of statistical\nmodelling.A trained random forest model is harder to interpret, hence falls in\nthe “algorithmic” camp for the purpose of this example. Conversely, a\nfitted linear regression model yields interpretable weights\nwhich directly tell us how the features linearly affect the response, so\nit represents the data modeling camp.\n# Comparing algorithmic vs data modeling approaches\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport statsmodels.api as sm\n\n# Load synthetic housing price data with complex patterns\n# File available here: \n# https://raw.githubusercontent.com/lacerbi/stats-for-ds-website/refs/heads/main/data/housing_prices.csv\ndf = pd.read_csv('../data/housing_prices.csv')\n\n# Prepare features and target\nfeatures = ['size_sqft', 'bedrooms', 'age_years', 'location_score', \n            'garage_spaces', 'has_pool', 'crime_rate', 'school_rating']\nX = df[features]\ny = df['price']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dataset: {X.shape[0]:,} houses with {X.shape[1]} features\")\nprint(f\"Training on {len(X_train):,} houses, testing on {len(X_test):,}\")\nprint(f\"\\nAverage house price: €{y.mean():,.0f}\")\nprint(f\"Price standard deviation: €{y.std():,.0f}\")\n\n# ALGORITHMIC APPROACH: Random Forest\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\nrf_predictions = rf_model.predict(X_test)\n\n# Calculate metrics\nfrom sklearn.metrics import mean_squared_error\nrf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n\nprint(\"\\n=== ALGORITHMIC APPROACH (Random Forest) ===\")\nprint(f\"Root Mean Squared Error (RMSE): €{rf_rmse:,.0f}\")\nprint(\"\\nFeature Importances:\")\nfor feature, importance in zip(features, rf_model.feature_importances_):\n    print(f\"  {feature}: {importance:.3f}\")\n\nDataset: 5,000 houses with 8 features\nTraining on 4,000 houses, testing on 1,000\n\nAverage house price: €439,750\nPrice standard deviation: €108,720\n\n=== ALGORITHMIC APPROACH (Random Forest) ===\nRoot Mean Squared Error (RMSE): €32,608\n\nFeature Importances:\n  size_sqft: 0.125\n  bedrooms: 0.022\n  age_years: 0.022\n  location_score: 0.047\n  garage_spaces: 0.013\n  has_pool: 0.007\n  crime_rate: 0.018\n  school_rating: 0.746\n\n\n# DATA MODELING APPROACH: Linear Regression\n# Add constant term for intercept\nX_train_lm = sm.add_constant(X_train)\nX_test_lm = sm.add_constant(X_test)\n\n# Fit linear model\nlm_model = sm.OLS(y_train, X_train_lm)\nlm_results = lm_model.fit()\n\n# Make predictions\nlm_predictions = lm_results.predict(X_test_lm)\nlm_rmse = np.sqrt(mean_squared_error(y_test, lm_predictions))\n\nprint(\"\\n=== DATA MODELING APPROACH (Linear Regression) ===\")\nprint(f\"Root Mean Squared Error (RMSE): €{lm_rmse:,.0f}\")\n\n# Show interpretable coefficients\nprint(\"\\nLinear Model Coefficients:\")\ncoef_df = pd.DataFrame({\n    'Feature': ['Intercept'] + features,\n    'Coefficient': lm_results.params,\n    'Std Error': lm_results.bse,\n    'P-value': lm_results.pvalues\n})\ncoef_df['Significant'] = coef_df['P-value'] &lt; 0.05\nprint(coef_df.to_string(index=False))\n\nprint(\"\\n=== INTERPRETATION ===\")\nprint(\"Linear model suggests:\")\nfor i, feature in enumerate(features):\n    coef = lm_results.params[i+1]  # +1 to skip intercept\n    if abs(coef) &gt; 100:\n        print(f\"- Each unit increase in {feature}: €{coef:,.0f} change in price\")\nprint(\"\\nBUT: The model performs (slightly) worse than Random Forest.\")\nprint(f\"RF RMSE: €{rf_rmse:,.0f} vs Linear RMSE: €{lm_rmse:,.0f}\")\nprint(f\"That's €{lm_rmse - rf_rmse:,.0f} worse prediction error.\")\nprint(f\"Should we care more about prediction or understanding?\")\n\n\n=== DATA MODELING APPROACH (Linear Regression) ===\nRoot Mean Squared Error (RMSE): €34,061\n\nLinear Model Coefficients:\n       Feature    Coefficient   Std Error       P-value  Significant\n     Intercept -241058.748914 4060.095405  0.000000e+00         True\n     size_sqft      72.843101    1.113174  0.000000e+00         True\n      bedrooms   17505.976934  561.915079 6.369481e-191         True\n     age_years      66.884992   37.769905  7.666128e-02        False\nlocation_score    6326.172511  191.797597 3.372091e-211         True\n garage_spaces   15599.005835  602.273749 7.547388e-137         True\n      has_pool   29992.244032 1362.847287 2.110344e-101         True\n    crime_rate   -1268.321711  110.805186  7.133961e-30         True\n school_rating   66814.033458  394.636343  0.000000e+00         True\n\n=== INTERPRETATION ===\nLinear model suggests:\n- Each unit increase in bedrooms: €17,506 change in price\n- Each unit increase in location_score: €6,326 change in price\n- Each unit increase in garage_spaces: €15,599 change in price\n- Each unit increase in has_pool: €29,992 change in price\n- Each unit increase in crime_rate: €-1,268 change in price\n- Each unit increase in school_rating: €66,814 change in price\n\nBUT: The model performs (slightly) worse than Random Forest.\nRF RMSE: €32,608 vs Linear RMSE: €34,061\nThat's €1,453 worse prediction error.\nShould we care more about prediction or understanding?\n\n\nBoth approaches have their place in modern data science. The algorithmic culture has driven breakthroughs in areas like computer vision and natural language processing, where prediction accuracy is paramount. For example, large language models (LLMs) are massively large deep neural networks (pre)trained with the extremely simple objective of just “predicting the next word”1 – without any attempt at understanding the underlying process.\nThe data modeling culture remains essential for scientific understanding, policy decisions, and any application where we need to know not just what will happen, but why. For LLMs, and in ML more broadly, this aspect is studied by the field of interpretability or “explainable ML” – trying to understand how modern ML models “think” and reach their conclusions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#foundations-of-probability",
    "href": "chapters/01-probability-foundations.html#foundations-of-probability",
    "title": "1  Probability Foundations",
    "section": "1.3 Foundations of Probability",
    "text": "1.3 Foundations of Probability\nProbability provides the mathematical language for quantifying uncertainty. Before we can make statistical inferences or build predictive models, we need a solid foundation in probability theory.\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nThis course is taught internationally, but for Finnish-speaking students, here’s a reference table of key probability terms you may have encountered in your earlier studies:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nSample space\nPerusjoukko, otosavaruus\nThe set of all possible outcomes\n\n\nEvent\nTapahtuma\nA subset of the sample space\n\n\nProbability distribution\nTodennäköisyysjakauma\nAssignment of probabilities to events\n\n\nProbability measure\nTodennäköisyysmitta\nMathematical function P satisfying axioms\n\n\nIndependent\nRiippumattomat\nEvents that don’t influence each other\n\n\nConditional probability\nEhdollinen todennäköisyys\nProbability given some information\n\n\nBayes’ Theorem\nBayesin kaava\nFormula for updating probabilities\n\n\nRandom variable\nSatunnaismuuttuja\nFunction mapping outcomes to numbers\n\n\nCumulative distribution function (CDF)\nKertymäfunktio\nP(X \\le x)\n\n\nDiscrete\nDiskreetti\nTaking countable values\n\n\nProbability mass function (PMF)\nTodennäköisyysmassafunktio\nP(X = x) for discrete X\n\n\nProbability density function (PDF)\nTiheysfunktio\nDensity for continuous variables\n\n\nQuantile function\nKvantiilifunktio\nInverse of CDF\n\n\nFirst quartile\nEnsimmäinen kvartiili\n25th percentile\n\n\nMedian\nMediaani\n50th percentile\n\n\nJoint density function\nYhteistiheysfunktio\nPDF for multiple variables\n\n\nMarginal density\nReunatiheysfunktio\nPDF of one variable from joint\n\n\nConditional density\nEhdollinen tiheysfunktio\nPDF given another variable’s value\n\n\nRandom vector\nSatunnaisvektori\nVector of random variables\n\n\nIndependent and identically distributed (IID)\nRiippumattomat ja samoin jakautuneet\nCommon assumption for data\n\n\nRandom sample\nSatunnaisotos\nIID observations from population\n\n\nFrequentist probability\nFrekventistinen todennäköisyys\nLong-run frequency interpretation\n\n\nSubjective probability\nSubjektiivinen todennäköisyys\nDegree of belief interpretation\n\n\n\nNote: Some terms have multiple Finnish translations. We report here the most common ones.\n\n\n\n\n1.3.1 Sample Spaces and Events\n\nThe sample space \\Omega is the set of all possible outcomes of an experiment. Individual elements \\omega \\in \\Omega are called sample outcomes, realizations, or just elements. Subsets of \\Omega are called events.\n\nNotation: \\omega and \\Omega are the lowercase (respectively, uppercase) version of the Greek letter omega.\n\n\n\n\n\n\nExample: Coin flips\n\n\n\nIf we flip a coin twice, where each outcome can be head (H) or tails (T), the sample space is: \\Omega = \\{HH, HT, TH, TT\\}\nThe event “first flip is heads” is A = \\{HH, HT\\}.\n\n\n\n\n\n\n\n\nExample: Temperature measurement\n\n\n\nWhen measuring temperature, the sample space might be the full set of real numbers: \\Omega = \\mathbb{R} = (-\\infty, \\infty)\nThe event “temperature between 20°C and 25°C” is the interval A = [20, 25].\nNote that we often take \\Omega to be larger than strictly necessary - in this case for example we are including physically impossible values like -1000°C. This is still mathematically valid. As we will see later, we can assign zero probability to impossible events.\nNotation: [a, b] denotes the interval between a and b (included), whereas (a, b) is the interval between a and b (excluded).\n\n\nSample spaces can be:\n\nFinite: \\Omega = \\{1, 2, 3, 4, 5, 6\\} for a die roll\nCountably infinite: \\Omega = \\{1, 2, 3, ...\\} for “number of flips until first heads”\nUncountably infinite: \\Omega = [0, 1] for “random number between 0 and 1”\n\n\n\n1.3.2 Set Operations for Events\nSince events are sets, we can combine them using standard set operations:\n\n\n\nOperation\nNotation\nMeaning\n\n\n\n\nComplement\nA^c\n“not A” - all outcomes not in A\n\n\nUnion\nA \\cup B\n“A or B” - outcomes in either A or B (or both)\n\n\nIntersection\nA \\cap B\n“A and B” - outcomes in both A and B\n\n\nDifference\nA \\setminus B\nOutcomes in A but not in B\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDisjoint events: Events A and B are disjoint (or mutually exclusive) if A \\cap B = \\emptyset. This means they cannot occur simultaneously. For example, in the case of a standard six-sided die roll, let A = “rolling an even number” = \\{2, 4, 6\\} and B = “rolling a 1” = \\{1\\}. These events are disjoint because you can’t roll both an even number AND a 1 simultaneously.\n\n\n\n\n1.3.3 Probability Axioms\nNow that we have defined the space of possible events, we can define the probability of an event. A probability measure must satisfy three fundamental axioms:\n\nA function \\mathbb{P} that assigns a real number \\mathbb{P}(A) to each event A is a probability measure if:\n\nNon-negativity: \\mathbb{P}(A) \\geq 0 for every event A\nNormalization: \\mathbb{P}(\\Omega) = 1\nCountable additivity: If A_1, A_2, ... are disjoint events, then: \\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} \\mathbb{P}(A_i)\n\n\n\n\n\n\n\n\nWhy These Axioms\n\n\n\n\n\nThese axioms ensure probability respects intuitive laws:\n\nNon-negativity: The probability of rain tomorrow cannot be negative.\nNormalization: When rolling a six-sided die, the probability of getting one of the faces \\{1, 2, 3, 4, 5, 6\\} is 1: 1 represent the total probability.\nCountable additivity: The probability of rolling a 1 or a 2 on a die is the sum of their individual probabilities, as these events cannot happen together.\n\nIt turns out that under some assumptions, it can be shown that these axioms are exactly what you would pick if one wants to quantify the concept of “possibility of an event” with a single number – a result known as Cox’s theorem.\n\n\n\nFrom these axioms, we can derive many useful properties:\n\n\\mathbb{P}(\\emptyset) = 0 (the impossible event has probability 0)\n\\mathbb{P}(A^c) = 1 - \\mathbb{P}(A) (complement rule)\nIf A \\subset B, then \\mathbb{P}(A) \\leq \\mathbb{P}(B) (monotonicity)\n0 \\leq \\mathbb{P}(A) \\leq 1 for any event A\n\n\nFor any events A and B: \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\n\nWhy? This formula accounts for the “double counting” when we add \\mathbb{P}(A) and \\mathbb{P}(B) – the intersection A \\cap B gets counted twice, so we subtract it once.\n\n\n\n\n\n\nNote\n\n\n\nSometimes you will see the notation \\mathbb{P}(A B) to denote \\mathbb{P}(A \\cap B).\n\n\n\n\n1.3.4 Interpretations of Probability\nProbability can be understood from different philosophical perspectives, each leading to the same mathematical framework.\nIntuitiveMathematicalComputationalThere are two main ways to think about what probability means:Frequency interpretation: Probability is the\nlong-run proportion of times an event occurs in repeated experiments. If\nwe flip a fair coin millions of times, we expect heads about 50% of the\ntime.Subjective interpretation: Probability represents a\ndegree of belief. When a weather forecaster says “30% chance of rain,”\nthey’re expressing their confidence based on available information.Both interpretations are useful, and both lead to the same\nmathematical rules.Frequentist probability:\n\\[\\mathbb{P}(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of times A occurs in n trials}}{n}\\]This requires the experiment to be repeatable under identical\nconditions.Subjective/Bayesian probability:\n\\(\\mathbb{P}(A)\\) quantifies an agent’s\ndegree of belief that \\(A\\) is true,\nconstrained by:\nCoherence: beliefs must satisfy the probability axioms\nUpdating: beliefs change rationally when new information arrives\n(via Bayes’ theorem)\nLet’s simulate the frequentist interpretation by\nflipping a fair coin many times and tracking how the proportion of heads\nconverges to the true probability of 0.5. This directly demonstrates the\nmathematical definition: probability as the long-run proportion.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate many coin flips to see frequentist convergence\nnp.random.seed(42)\nn_flips = 10000\nflips = np.random.choice(['H', 'T'], size=n_flips)\n\n# Calculate running proportion of heads\nheads_count = np.cumsum(flips == 'H')\nproportions = heads_count / np.arange(1, n_flips + 1)\n\n# Plot convergence to true probability\nplt.figure(figsize=(7, 4))\nplt.plot(proportions, linewidth=2, alpha=0.8, color='blue')\nplt.axhline(y=0.5, color='red', linestyle='--', label='True probability = 0.5', linewidth=2)\nplt.xlabel('Number of flips')\nplt.ylabel('Proportion of heads')\nplt.title('Frequentist Probability: Long-Run Proportion Converges to True Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.ylim(0.3, 0.7)\nplt.show()\n\n# Print summary\nprint(f\"After {n_flips:,} flips:\")\nprint(f\"Proportion of heads: {proportions[-1]:.4f}\")\nprint(f\"Deviation from 0.5: {abs(proportions[-1] - 0.5):.4f}\")\n\n\n\n\nAfter 10,000 flips:\nProportion of heads: 0.5013\nDeviation from 0.5: 0.0013\n\nThe subjective/Bayesian interpretation involves\nupdating beliefs based on evidence. We’ll explore this computational\napproach in detail when we cover Bayes’ theorem.\n\n\n1.3.5 Finite Sample Spaces and Counting\nWhen \\Omega is finite and all outcomes are equally likely, probability calculations reduce to counting:\n\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\text{number of outcomes in A}}{\\text{total number of outcomes}}\n\n\n\n\n\n\nExample: Rolling two dice\n\n\n\nWhat’s the probability the sum of rolling two six-sided dice equals 7?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\Omega = \\{(i,j) : i,j \\in \\{1,2,3,4,5,6\\}\\}, so |\\Omega| = 36.\nThe event “sum equals 7” is A = \\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\\}.\nTherefore \\mathbb{P}(A) = \\frac{6}{36} = \\frac{1}{6}.\n\n\n\n\n\nKey counting principle - Binomial Coefficient:\nThe binomial coefficient (read as “n choose k”) is:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nWhere n! denotes the factorial, e.g. 4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24.\nThe binomial coefficient2 counts the number of ways to choose k objects from n objects when order doesn’t matter. For example:\n\n\\binom{5}{2} = \\frac{5!}{2!3!} = \\frac{5 \\times 4}{2 \\times 1} = 10 ways to choose 2 items from 5\nChoosing 2 students from a class of 30: \\binom{30}{2} = 435 possible pairs",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#independence-and-conditional-probability",
    "href": "chapters/01-probability-foundations.html#independence-and-conditional-probability",
    "title": "1  Probability Foundations",
    "section": "1.4 Independence and Conditional Probability",
    "text": "1.4 Independence and Conditional Probability\n\n1.4.1 Independent Events\n\nTwo events A and B are independent if: \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)\nWe denote this as A \\perp\\!\\!\\!\\perp B. When events are not independent, we write A \\not\\perp\\!\\!\\!\\perp B.\n\nIndependence means that knowing whether one event occurred tells us nothing about the other event.\n\n\n\n\n\n\nNote\n\n\n\nThe textbook uses non-standard notation for independence and non-independence. We use the standard notation A \\perp\\!\\!\\!\\perp B (and A \\not\\perp\\!\\!\\!\\perp B for dependence), which is widely adopted in probability and statistics literature.\n\n\n\n\n\n\n\n\nExample: Fair coin tosses\n\n\n\nA fair coin is tossed twice. Let H_1 = “first toss is heads” and H_2 = “second toss is heads”. Are these two events independent?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\mathbb{P}(H_1) = \\mathbb{P}(H_2) = \\frac{1}{2}\n\\mathbb{P}(H_1 \\cap H_2) = \\mathbb{P}(\\text{both heads}) = \\frac{1}{4}\nSince \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2}, the events are independent\n\nThis matches the intuition – whether we obtain head on the first flip does not tell us anything about the second flip, and vice versa.\n\n\n\n\n\n\n\n\n\n\n\nExample: Dependent events\n\n\n\nDraw two cards from a deck without replacement.\n\nA = “first card is an ace”\nB = “second card is an ace”\n\nAre these events independent?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\mathbb{P}(A) = \\mathbb{P}(B) = \\frac{4}{52}\nHowever: \\mathbb{P}(A \\cap B) = \\frac{4}{52} \\times \\frac{3}{51} \\neq \\mathbb{P}(A)\\mathbb{P}(B)\n\nThe events are dependent because drawing an ace first changes the probability of drawing an ace second.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon misconception: Disjoint events are NOT independent!\nIf A and B are disjoint with positive probability, then:\n\n\\mathbb{P}(A \\cap B) = 0 (since they are disjoint, they can’t happen together)\n\\mathbb{P}(A)\\mathbb{P}(B) &gt; 0 (if both have positive probability)\n\nSo disjoint events are actually maximally dependent - if one occurs, the other definitely doesn’t!\n\n\n\n\n1.4.2 Conditional Probability\n\nThe conditional probability of A given B is: \\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} provided \\mathbb{P}(B) &gt; 0.\n\nThink of \\mathbb{P}(A|B) as the probability of A in the “new universe” where we know B has occurred.\n\n\n\n\n\n\nCaution\n\n\n\nProsecutor’s Fallacy: Confusing \\mathbb{P}(A|B) with \\mathbb{P}(B|A).\nThese can be vastly different! For example:\n\n\\mathbb{P}(\\text{match} | \\text{guilty}) might be 0.98.\n\\mathbb{P}(\\text{guilty} | \\text{match}) might be 0.04.\n\nThe second depends on the prior probability of guilt and how many innocent people might also match. We will see next how to compute one from the other.\n\n\n\n\n1.4.3 Bayes’ Theorem\nSometimes we know \\mathbb{P}(B|A) but we are really interested in the other way round, \\mathbb{P}(A|B).\nFor example, in the example above, we may know the probability that a test gives a match if the suspect is guilty, \\mathbb{P}(\\text{match} \\mid \\text{guilty}), but what we really want to know is the probability that the suspect is guilty given that we find a match, \\mathbb{P}(\\text{guilty} \\mid \\text{match}).\nSuch “inverse” conditional probabilities can be calculated via Bayes’ theorem.\n\nFor events A and B with \\mathbb{P}(B) &gt; 0: \\mathbb{P}(A|B) = \\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\n\nWhere B is some information (evidence) and A an hypothesis.\n\nIf A_1, ..., A_k partition \\Omega (they’re disjoint and cover all possibilities), then: \\mathbb{P}(B) = \\sum_{i=1}^k \\mathbb{P}(B|A_i)\\mathbb{P}(A_i)\n\nCombining these gives the full form of Bayes’ theorem: \\mathbb{P}(A_i|B) = \\frac{\\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}{\\sum_j \\mathbb{P}(B|A_j)\\mathbb{P}(A_j)}\nTerminology:\n\n\\mathbb{P}(A_i): Prior probability for hypothesis A_i (before seeing evidence B), also known as “base rate”.\n\\mathbb{P}(A_i|B): Posterior probability (after seeing evidence B).\n\\mathbb{P}(B|A_i): Likelihood of hypothesis A_i for fixed evidence B.\n\n\n\n\n\n\n\nExample: Email spam detection\n\n\n\n\nPrior: 70% of emails are spam\n“Free” appears in 90% of spam emails\n“Free” appears in 1% of legitimate emails\n\nIf an email contains “free”, what’s the probability it’s spam?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet S = “email is spam” and F = “email contains ‘free’”.\nGiven:\n\n\\mathbb{P}(S) = 0.7\n\\mathbb{P}(F|S) = 0.9\n\n\\mathbb{P}(F|S^c) = 0.01\n\nBy Bayes’ theorem: \\mathbb{P}(S|F) = \\frac{0.9 \\times 0.7}{0.9 \\times 0.7 + 0.01 \\times 0.3} = \\frac{0.63}{0.633} \\approx 0.995\nSo an email containing “free” has a 99.5% chance of being spam!\n\n\n\n\n\nIntuitiveMathematicalComputationalConditional probability answers questions like:\nWhat’s the probability of rain given that it’s cloudy?\nWhat’s the probability a patient has a disease given a positive test\nresult?\nWhat’s the probability a user clicks an ad given they’re on a mobile\ndevice?\nThe key insight: additional information changes probabilities.\nKnowing that \\(B\\) occurred restricts\nour attention to outcomes where \\(B\\)\nis true, potentially changing how likely\n\\(A\\) becomes.Some conditional probabilities are easier to compute than others. For\nexample, we may know that if the patient has a disease,\nthen the test will return positive with a certain probability. However,\nto compute the “inverse” probability (if the test is positive, what’s\nthe probability of the patient having the disease?) we need Bayes’\ntheorem.For fixed \\(B\\) with\n\\(\\mathbb{P}(B) &gt; 0\\), the conditional\nprobability \\(\\mathbb{P}(\\cdot|B)\\) is\nitself a probability measure:\n\\(\\mathbb{P}(A|B) \\geq 0\\) for all\n\\(A\\)\n\\(\\mathbb{P}(\\Omega|B) = 1\\)\nIf \\(A_1, A_2, ...\\) are disjoint,\nthen\n\\(\\mathbb{P}(\\bigcup_i A_i|B) = \\sum_i \\mathbb{P}(A_i|B)\\)\nKey relationships:\nIf \\(A \\perp\\!\\!\\!\\perp B\\), then\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n(independence means conditioning doesn’t matter)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A|B)\\mathbb{P}(B) = \\mathbb{P}(B|A)\\mathbb{P}(A)\\)\n(multiplication rule)\nConversely, in general\n\\(\\mathbb{P}(A|\\cdot)\\) (the\nlikelihood) is not a probability measure.We will visualize how conditional probability can be\ncounterintuitive. We’ll simulate a medical test scenario to show how\nbase rates affect our interpretation of test results.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Medical test scenario\np_disease = 0.001                   # 0.1% have the disease (base rate)\np_pos_given_disease = 0.99          # 99% sensitivity\np_neg_given_healthy = 0.99          # 99% specificity\n\n# Calculate probability of positive test\np_healthy = 1 - p_disease\np_pos_given_healthy = 1 - p_neg_given_healthy\np_positive = p_pos_given_disease * p_disease + p_pos_given_healthy * p_healthy\n\n# Apply Bayes' theorem: P(disease|positive)\np_disease_given_pos = (p_pos_given_disease * p_disease) / p_positive\n\n# Visualize with different base rates\nbase_rates = np.logspace(-4, -1, 50)  # 0.01% to 10%\nposterior_probs = []\n\nfor base_rate in base_rates:\n    p_pos = p_pos_given_disease * base_rate + p_pos_given_healthy * (1 - base_rate)\n    posterior = (p_pos_given_disease * base_rate) / p_pos\n    posterior_probs.append(posterior)\n\nplt.figure(figsize=(7, 5))\nplt.semilogx(base_rates * 100, np.array(posterior_probs) * 100, linewidth=3)\nplt.axvline(x=0.1, color='red', linestyle='--', label=f'Current: {p_disease_given_pos:.1%} chance')\nplt.xlabel('Disease Prevalence (%)')\nplt.ylabel('P(Disease | Positive Test) (%)')\nplt.title('Impact of Base Rate on Test Interpretation')\nplt.ylim(0, 100)  # Set y-axis range from 0 to 100\nplt.xticks([0.01, 0.1, 1, 10], ['0.01', '0.1', '1', '10'])  # Set custom x-axis ticks\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\nprint(f\"With 99% accurate test and 0.1% base rate:\")\nprint(f\"P(disease | positive test) = {p_disease_given_pos:.1%}\")\nprint(f\"Surprising: A positive test means only ~9% chance of disease!\")\n\n\n\n\nWith 99% accurate test and 0.1% base rate:\nP(disease | positive test) = 9.0%\nSurprising: A positive test means only ~9% chance of disease!\n\n\n\n\n1.4.4 Classic Probability Examples\nLet’s work through some classic examples that illustrate key concepts:\n\n\n\n\n\n\nExample: At least one head in 10 flips\n\n\n\nWhat’s the probability of getting at least one head in 10 coin flips?\nHint: Instead of counting all the ways to get 1, 2, …, or 10 heads, use the complement.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\mathbb{P}(\\text{at least one head}) = 1 - \\mathbb{P}(\\text{no heads}) = 1 - \\mathbb{P}(\\text{all tails})\nSince flips are independent: \\mathbb{P}(\\text{all tails}) = \\left(\\frac{1}{2}\\right)^{10} = \\frac{1}{1024}\nTherefore: \\mathbb{P}(\\text{at least one head}) = 1 - \\frac{1}{1024} \\approx 0.999\n\n\n\n\n\n\n\n\n\n\n\nExample (advanced): Basketball competition\n\n\n\nTwo players take turns shooting. Player A shoots first with probability 1/3 of scoring. Player B shoots second with probability 1/4. First to score wins. What’s the probability A wins?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA wins if:\n\nA scores on first shot: probability 1/3\nBoth miss, then A scores: (2/3)(3/4)(1/3)\nBoth miss twice, then A scores: (2/3)(3/4)(2/3)(3/4)(1/3)\n…\n\nThis is a geometric series: \\mathbb{P}(A \\text{ wins}) = \\frac{1}{3} \\sum_{k=0}^{\\infty} \\left(\\frac{2}{3} \\cdot \\frac{3}{4}\\right)^k = \\frac{1}{3} \\cdot \\frac{1}{1-\\frac{1}{2}} = \\frac{2}{3}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#random-variables",
    "href": "chapters/01-probability-foundations.html#random-variables",
    "title": "1  Probability Foundations",
    "section": "1.5 Random Variables",
    "text": "1.5 Random Variables\nSo far, we’ve worked with events - subsets of the sample space. But in practice, we usually care about numerical quantities associated with random outcomes. This is where random variables come in.\n\n1.5.1 Definition and Intuition\n\nA random variable is a function X: \\Omega \\rightarrow \\mathbb{R} that assigns a real number to each outcome in the sample space.\n\nA random variable is defined by its possible values (real numbers) and their probabilities.\nIn the case of a discrete random variable, the set of values is discrete (finite or infinite), x_1, \\ldots, and each value can be assigned a corresponding point probability p_1, \\ldots with 0 \\le p_i \\le 1, \\sum_{i=1}^\\infty p_i = 1.\nIn the case of a continuous random variable, probabilities are defined by a non-negative probability density function that integrates to 1.\nIntuitiveMathematicalComputationalA random variable is just a way to assign numbers to outcomes. Think\nof it as a measurement or quantity that depends on chance.Examples:\nNumber of heads in 10 coin flips\nTime until next customer arrives\nTemperature at noon tomorrow\nStock price at market close\nThe key insight: once we have numbers, we can do arithmetic,\ncalculate averages, measure spread, and use all the tools of\nmathematics.Warning: The following will likely make sense only if you\nhave taken an advanced course in probability theory or measure theory.\nFeel free to skip it otherwise.Formally, \\(X\\) is a measurable\nfunction from \\((\\Omega, \\mathcal{F})\\)\nto \\((\\mathbb{R}, \\mathcal{B})\\)\nwhere:\n\\(\\mathcal{F}\\) is the\n\\(\\sigma\\)-algebra of events in\n\\(\\Omega\\)\n\\(\\mathcal{B}\\) is the Borel\n\\(\\sigma\\)-algebra on\n\\(\\mathbb{R}\\)\nMeasurability means: for any Borel set\n\\(B \\subset \\mathbb{R}\\), the pre-image\n\\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\)\nis an event in \\(\\mathcal{F}\\).This technical condition ensures we can compute probabilities like\n\\(\\mathbb{P}(X \\in B)\\).Here we demonstrate how random variables map outcomes to numbers,\nallowing us to analyze randomness mathematically. For example, we can\nplot a histogram for the realizations over multiple experiments.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Demonstrate a random variable: X = number of heads in 10 coin flips\nnp.random.seed(42)\n\n# Single experiment\nflips = np.random.choice(['H', 'T'], size=10)\nX = np.sum(flips == 'H')\nprint(f\"Outcomes (single experiment): {flips}\")\nprint(f\"X (number of heads) = {X}\")\n\n# Simulate many experiments to see the distribution\nn_sims = 10000\nX_values = [np.sum(np.random.choice(['H', 'T'], size=10) == 'H') \n            for _ in range(n_sims)]\n\n# Visualize distribution\nplt.figure(figsize=(7, 4))\ncounts, bins, _ = plt.hist(X_values, bins=np.arange(0.5, 11.5, 1), density=True, \n                          alpha=0.7, edgecolor='black')\n\nx = np.arange(0, 11)\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.title(f'Random Variable X: Number of Heads in 10 Flips ({n_sims} experiments)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\nAverage value: {np.mean(X_values):.3f} (theoretical: 5.0)\")\n\nOutcomes (single experiment): ['H' 'T' 'H' 'H' 'H' 'T' 'H' 'H' 'H' 'T']\nX (number of heads) = 7\n\nAverage value: 4.993 (theoretical: 5.0)\n\n\n\n\n\n\n\n\n\n\n\nExample: Coin flips\n\n\n\nWithin the same sample space we can define multiple distinct random variables.\nFor example, let \\Omega = \\{HH, HT, TH, TT\\} (two flips). Define:\n\nX = number of heads\nY = 1 if first flip is heads, 0 otherwise\nZ = 1 if flips match, 0 otherwise\n\nThen:\n\nX(HH) = 2, X(HT) = 1, X(TH) = 1, X(TT) = 0\nY(HH) = 1, Y(HT) = 1, Y(TH) = 0, Y(TT) = 0\n\nZ(HH) = 1, Z(HT) = 0, Z(TH) = 0, Z(TT) = 1\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotation convention:\n\nCapital letters (X, Y, Z) denote random variables\nLowercase letters (x, y, z) denote specific values\n\\{X = x\\} is the event that X takes value x\n\nHowever, do not expect people to strictly follow this convention beyond mathematical and statistical textbooks. In the real world, you will often see “x” used to refer both to a value and to a random variable “X” that happens to take value x.\n\n\n\n\n1.5.2 Cumulative Distribution Functions\nThe Cumulative Distribution Function (CDF) completely characterizes a random variable’s probability distribution.\n\nThe cumulative distribution function (CDF) of a random variable X is the function F_X(x): \\mathbb{R} \\rightarrow [0, 1] defined by F_X(x) = \\mathbb{P}(X \\leq x) for all x \\in \\mathbb{R}.\n\n\n\n\n\n\n\nExample: Two coin flips\n\n\n\nLet X = number of heads for two flips of fair coins.\n\n\\mathbb{P}(X = 0) = 1/4\n\\mathbb{P}(X = 1) = 1/2\n\\mathbb{P}(X = 2) = 1/4\n\nThe CDF is: F_X(x) = \\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n1/4 & \\text{if } 0 \\leq x &lt; 1 \\\\\n3/4 & \\text{if } 1 \\leq x &lt; 2 \\\\\n1 & \\text{if } x \\geq 2\n\\end{cases}\nNote: The CDF is defined for ALL real x, even though X only takes values 0, 1, 2!\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the CDF values\nx_jumps = [0, 1, 2]  # Points where jumps occur\ncdf_values = [0.25, 0.75, 1.0]  # CDF values after jumps\ncdf_values_before = [0, 0.25, 0.75]  # CDF values before jumps\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Plot the step function\n# Left segment (x &lt; 0)\nax.hlines(0, -1, 0, colors='black', linewidth=2)\n\n# Plot each segment\nfor i in range(len(x_jumps)):\n    # Horizontal line segment\n    if i &lt; len(x_jumps) - 1:\n        ax.hlines(cdf_values[i], x_jumps[i], x_jumps[i+1], colors='black', linewidth=2)\n    else:\n        # Last segment extends to the right\n        ax.hlines(cdf_values[i], x_jumps[i], 3, colors='black', linewidth=2)\n    \n    # Open circles (at discontinuities, left endpoints)\n    if i &gt; 0:\n        ax.plot(x_jumps[i], cdf_values_before[i], 'o', color='black', \n                markerfacecolor='white', markersize=8, markeredgewidth=2)\n    \n    # Filled circles (at jump points, right endpoints)\n    ax.plot(x_jumps[i], cdf_values[i], 'o', color='black', \n            markerfacecolor='black', markersize=8)\n\n# Open circle at x=0, y=0\nax.plot(0, 0, 'o', color='black', markerfacecolor='white', \n        markersize=8, markeredgewidth=2)\n\n# Set axis properties\nax.set_xlabel('x', fontsize=12)\nax.set_ylabel('$F_X(x)$', fontsize=12)\nax.set_xlim(-1, 3)\nax.set_ylim(-0.1, 1.1)\n\n# Set tick marks\nax.set_xticks([0, 1, 2])\nax.set_yticks([0.25, 0.50, 0.75, 1.0])\n\n# Add grid\nax.grid(True, alpha=0.3)\n\n# Add arrows to axes\nax.annotate('', xy=(3.2, 0), xytext=(3, 0),\n            arrowprops=dict(arrowstyle='-&gt;', color='black', lw=1))\nax.annotate('', xy=(0, 1.15), xytext=(0, 1.1),\n            arrowprops=dict(arrowstyle='-&gt;', color='black', lw=1))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.1: Cumulative distribution function (CDF) for the number of heads when flipping a coin twice.\n\n\n\n\n\n\n\n\n\n1.5.3 Discrete Random Variables\n\nA random variable X is discrete if it takes countably many values \\{x_1, x_2, ...\\}. Its probability mass function (PMF) (sometimes just probability function) is defined as: f_X(x) = \\mathbb{P}(X = x)\n\nProperties of PMFs:\n\nf_X(x) \\geq 0 for all x\n\\sum_{i} f_X(x_i) = 1 (probabilities sum to 1)\nF_X(x) = \\sum_{x_i \\leq x} f_X(x_i) (CDF is sum of PMF)\n\n\n\nShow code\n# PMF for coin flipping example\nx_values = [0, 1, 2]\npmf_values = [0.25, 0.5, 0.25]\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Plot vertical lines from x-axis to probability values\nfor x, p in zip(x_values, pmf_values):\n    ax.plot([x, x], [0, p], 'k-', linewidth=2)\n    # Add filled circles at the top\n    ax.plot(x, p, 'ko', markersize=8, markerfacecolor='black')\n\n# Set axis properties\nax.set_xlabel('x', fontsize=12)\nax.set_ylabel('$f_X(x)$', fontsize=12)\nax.set_xlim(-0.5, 2.5)\nax.set_ylim(0, 1)\n\n# Set tick marks\nax.set_xticks([0, 1, 2])\nax.set_yticks([0.25, 0.5, 0.75, 1])\n\n# Add grid\nax.grid(True, alpha=0.3, axis='y')\n\n# Add a horizontal line at y=0 for clarity\nax.axhline(y=0, color='black', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.2: Probability mass function (PMF) for the number of heads when flipping a coin twice.\n\n\n\n\n\n\n\n1.5.4 Core Discrete Distributions\n\n\n\n\n\n\nNote\n\n\n\nNotation preview: We’ll use \\mathbb{E}[X] to denote the expected value (mean) of a random variable X, and \\text{Var}(X) or \\sigma^2 for its variance (a measure of spread). These concepts will be covered in detail in Chapter 2 of the lecture notes.\n\n\n\nBernoulli Distribution\nThe Bernoulli distribution is the simplest non-trivial random variable – a single binary outcome with probability p \\in [0, 1] of happening.\n\nX \\sim \\text{Bernoulli}(p) if: f_X(x) = \\begin{cases}\np & \\text{if } x = 1 \\\\\n1-p & \\text{if } x = 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nAn outcome of X = 1 is often referred to as a “hit” or a “success”, while X = 0 is a “miss” or a “failure”.\nUse cases:\n\nCoin flip (heads/tails)\n\nIf p \\neq 0.5, this is known as a biased coin (as opposed to a fair coin with p = 0.5)\nHere what constitutes a “hit” and a “miss” is arbitrary!\n\nSuccess/failure of a single trial\nBinary classification (spam/not spam)\nUser clicks/doesn’t click an ad\n\n\n\nShow code\n# Bernoulli distribution PMF\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n# Parameter\np = 0.3  # probability of success\n\n# PMF visualization\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Plot PMF\nx = [0, 1]\npmf = [1-p, p]\nbars = ax.bar(x, pmf, width=0.4, alpha=0.8, color=['lightcoral', 'lightblue'], \n               edgecolor='black', linewidth=2)\n\n# Add value labels\nfor i, (xi, pi) in enumerate(zip(x, pmf)):\n    ax.text(xi, pi + 0.02, f'{pi:.2f}', ha='center', va='bottom', fontsize=12)\n\nax.set_xticks([0, 1])\nax.set_xticklabels(['Failure (0)', 'Success (1)'])\nax.set_ylabel('Probability')\nax.set_title(f'Bernoulli Distribution PMF (p = {p})')\nax.set_ylim(0, 1)\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"E[X] = p = {p}\")\nprint(f\"Var(X) = p(1-p) = {p*(1-p):.3f}\")\n\n\n\n\n\n\n\n\n\nE[X] = p = 0.3\nVar(X) = p(1-p) = 0.210\n\n\n\n\nBinomial Distribution\nThe binomial distribution counts the number of successes in a fixed number n of independent Bernoulli trials each with probability p.\n\nX \\sim \\text{Binomial}(n, p) if: f_X(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, ..., n\n\nKey properties:\n\nSum of independent Bernoullis: If X_i \\sim \\text{Bernoulli}(p) are independent, then \\sum_{i=1}^n X_i \\sim \\text{Binomial}(n, p)\nAdditivity: If X \\sim \\text{Binomial}(n_1, p) and Y \\sim \\text{Binomial}(n_2, p) are independent, then X + Y \\sim \\text{Binomial}(n_1 + n_2, p)\n\nUse cases:\n\nNumber of heads in n coin flips\nNumber of defective items in a batch\nNumber of customers who make a purchase\nNumber of successful treatments in a clinical trial\n\n\n\n\n\n\n\nWarning\n\n\n\nIndependence assumption: The binomial distribution assumes all trials are independent - each outcome does not affect the probability of subsequent outcomes. This assumption may not hold in practice!\nFor example, if items are defective because a machine has broken (rather than random variation), then finding one defective item suggests all subsequent items might also be defective. In such cases, the binomial distribution would be inappropriate.\n\n\n\n\nShow code\n# Binomial distribution visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Parameters\nn, p = 20, 0.3\nx = np.arange(0, n+1)\n\n# Create figure\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# Plot PMF\npmf = binom.pmf(x, n, p)\nbars = ax.bar(x, pmf, alpha=0.8, color='steelblue', edgecolor='black')\n\n# Highlight mean\nmean = n * p\nax.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean:.1f}')\n\n# Add value labels on significant bars\nfor i, (xi, pi) in enumerate(zip(x, pmf)):\n    if pi &gt; 0.01:  # Only label visible bars\n        ax.text(xi, pi + 0.003, f'{pi:.3f}', ha='center', va='bottom', fontsize=8)\n\nax.set_xlabel('Number of successes (k)')\nax.set_ylabel('P(X = k)')\nax.set_title(f'Binomial Distribution PMF: n={n}, p={p}')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"E[X] = np = {n*p}\")\nprint(f\"Var(X) = np(1-p) = {n*p*(1-p)}\")\nprint(f\"σ = {np.sqrt(n*p*(1-p)):.3f}\")\n\n\n\n\n\n\n\n\n\nE[X] = np = 6.0\nVar(X) = np(1-p) = 4.199999999999999\nσ = 2.049\n\n\n\n\nDiscrete Uniform Distribution\nThe discrete uniform distribution is another simple discrete distribution - every outcome is equally likely.\n\nX \\sim \\text{DiscreteUniform}(a, b) if: f_X(x) = \\frac{1}{b-a+1}, \\quad x \\in \\{a, a+1, ..., b\\} where a and b are integers with a \\leq b.\n\nKey properties:\n\n\\mathbb{E}[X] = \\frac{a+b}{2}\n\\text{Var}(X) = \\frac{(b-a+1)^2 - 1}{12}\n\nUse cases:\n\nFair die roll: \\text{DiscreteUniform}(1, 6)\nAttack roll: \\text{DiscreteUniform}(1, 20)\nRandom selection from a finite set\nLottery number selection\n\n\n\nShow code\n# Discrete Uniform distribution\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint\n\n# Example: fair die\na, b = 1, 6\nx = np.arange(a, b+1)\npmf = [1/(b-a+1)] * len(x)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nbars = ax.bar(x, pmf, width=0.6, alpha=0.8, color='lightgreen', edgecolor='black', linewidth=2)\n\n# Add value labels\nfor i, (xi, pi) in enumerate(zip(x, pmf)):\n    ax.text(xi, pi + 0.01, f'{pi:.3f}', ha='center', va='bottom', fontsize=10)\n\nax.set_xlabel('Outcome')\nax.set_ylabel('Probability')\nax.set_title(f'Discrete Uniform Distribution: Fair Die')\nax.set_ylim(0, 0.3)\nax.set_xticks(x)\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"E[X] = {(a+b)/2}\")\nprint(f\"Var(X) = {((b-a+1)**2 - 1)/12:.3f}\")\n\n\n\n\n\n\n\n\n\nE[X] = 3.5\nVar(X) = 2.917\n\n\n\n\nCategorical Distribution\nThe categorical distribution is a generalization of Bernoulli to multiple categories (also called “Generalized Bernoulli” or “Multinoulli”). You can also see it as a generalization of the discrete uniform distribution to a discrete non-uniform distribution.\n\nX \\sim \\text{Categorical}(p_1, ..., p_k) if: f_X(x) = p_x, \\quad x \\in \\{1, 2, ..., k\\} where p_i \\geq 0 and \\sum_{i=1}^k p_i = 1.\n\nKey properties:\n\nOne-hot encoding: Often represented as a vector with one 1 and rest 0s\nSpecial case: Categorical with k=2 is equivalent to Bernoulli\nSpecial case: If all probabilities are equal, it becomes a discrete uniform\nFoundation for multinomial distribution (multiple categorical trials)\n\nUse cases:\n\nOutcome of rolling a (possibly unfair) die\nClassification into multiple categories\nLanguage modeling (next-token prediction)3\nCustomer choice among products\n\n\n\nShow code\n# Categorical distribution\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Example: Customer choice among 5 products\ncategories = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nprobabilities = [0.30, 0.25, 0.20, 0.15, 0.10]\nx = np.arange(len(categories))\n\nfig, ax = plt.subplots(figsize=(7, 4))\nbars = ax.bar(x, probabilities, alpha=0.8, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'],\n               edgecolor='black', linewidth=2)\n\n# Add value labels\nfor i, p in enumerate(probabilities):\n    ax.text(i, p + 0.01, f'{p:.2f}', ha='center', va='bottom', fontsize=10)\n\nax.set_xlabel('Category')\nax.set_ylabel('Probability')\nax.set_title('Categorical Distribution: Customer Product Choice')\nax.set_xticks(x)\nax.set_xticklabels(categories, rotation=45, ha='right')\nax.set_ylim(0, 0.4)\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Expected value for indicator representation (does it make sense here?)\nprint(\"If we encode categories as 1, 2, 3, 4, 5:\")\nexpected = sum((i+1) * p for i, p in enumerate(probabilities))\nprint(f\"E[X] = {expected:.2f} (does it really make sense here?)\")\n\n\n\n\n\n\n\n\n\nIf we encode categories as 1, 2, 3, 4, 5:\nE[X] = 2.50 (does it really make sense here?)\n\n\n\n\nBrief Catalog: Other Discrete Distributions\nPoisson(\\lambda): The Poisson distribution models count of rare events in fixed intervals:\n\nPMF: f_X(x) = e^{-\\lambda} \\frac{\\lambda^x}{x!} for x = 0, 1, 2, ...\nMean = Variance = \\lambda (lambda)\nUse: Email arrivals, typos per page, customer arrivals\nApproximates Binomial(n,p) when n large, p small: use \\lambda = np\n\nGeometric(p): The geometric distribution represents the number of trials until first success:\n\nPMF: f_X(x) = p(1-p)^{x-1} for x = 1, 2, ...\nUse: Waiting times, number of attempts until success\n\nNegative Binomial(r, p): The negative binomial represents the number of failures before rth success\n\nGeneralization of geometric distribution\nUse: Overdispersed count data, robust alternative to Poisson\n\n\n\n\n1.5.5 Continuous Random Variables\n\nA random variable X is continuous if there exists a function f_X such that:\n\nf_X(x) \\geq 0 for all x\n\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\nFor any a &lt; b: \\mathbb{P}(a &lt; X &lt; b) = \\int_a^b f_X(x) dx\n\nThe function f_X is called the probability density function (PDF).\n\n\n\n\n\n\n\nWarning\n\n\n\nImportant distinctions from discrete case:\n\n\\mathbb{P}(X = x) = 0 for any single point x: in a continuum, there is zero probability of picking one specific point\nPDF can exceed 1 (it’s a density, not a probability!)\nWe get probabilities by integrating densities over an interval, not summing\n\n\n\nRelationship between PDF and CDF:\n\nF_X(x) = \\int_{-\\infty}^x f_X(t) dt\nf_X(x) = F_X'(x) (where the derivative exists)\n\n\n\n1.5.6 Core Continuous Distributions\n\nUniform Distribution\nThe uniform distribution is the continuous analog of “equally likely outcomes.”\n\nX \\sim \\text{Uniform}(a, b) if: f_X(x) = \\begin{cases}\n\\frac{1}{b-a} & \\text{if } a \\leq x \\leq b \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nProperties:\n\nCDF: F_X(x) = \\frac{x-a}{b-a} for a \\leq x \\leq b\nEvery interval of equal length has equal probability\n\nUse cases:\n\nRandom number generation (\\text{Uniform}(0,1) is fundamental in computational statistics)\nModeling complete uncertainty within bounds\nArrival times when “any time is equally likely”\n\n\n\nShow code\n# Uniform distribution visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform\n\n# Example: Uniform(a=2, b=5)\na, b = 2, 5\nx = np.linspace(0, 7, 1000)\n\n# PDF\npdf = uniform.pdf(x, loc=a, scale=b-a)\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Plot the PDF\nax.plot(x, pdf, linewidth=3, color='darkblue', label=f'Uniform({a}, {b})')\nax.fill_between(x, 0, pdf, where=(x &gt;= a) & (x &lt;= b), alpha=0.3, color='lightblue')\n\n# Mark the boundaries\nax.axvline(x=a, color='red', linestyle='--', linewidth=2, alpha=0.7)\nax.axvline(x=b, color='red', linestyle='--', linewidth=2, alpha=0.7)\n\n# Add height label\nax.text((a+b)/2, 1/(b-a) + 0.02, f'height = 1/{b-a} = {1/(b-a):.3f}', \n        ha='center', fontsize=10)\n\nax.set_xlabel('x')\nax.set_ylabel('Density f(x)')\nax.set_title('Uniform Distribution PDF')\nax.set_ylim(0, 0.5)\nax.grid(True, alpha=0.3)\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"E[X] = (a+b)/2 = {(a+b)/2}\")\nprint(f\"Var(X) = (b-a)²/12 = {(b-a)**2/12:.3f}\")\nprint(f\"Total area under curve = {1/(b-a)} × {b-a} = 1 ✓\")\n\n\n\n\n\n\n\n\n\nE[X] = (a+b)/2 = 3.5\nVar(X) = (b-a)²/12 = 0.750\nTotal area under curve = 0.3333333333333333 × 3 = 1 ✓\n\n\n\n\nNormal (Gaussian) Distribution\nThe normal distribution (also called Gaussian distribution) is the most important distribution in statistics, arising from the Central Limit Theorem.\n\nX \\sim \\text{Normal}(\\mu, \\sigma^2) or \\mathcal{N}(\\mu, \\sigma^2) if: f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\nParameters:\n\n\\mu (mu): mean (center of distribution)\n\\sigma^2 (sigma squared): variance (\\sigma is standard deviation - controls spread)\n\nKey properties:\n\nSymmetric bell curve centered at \\mu\nAbout 68% of probability within \\mu \\pm \\sigma\nAbout 95% within \\mu \\pm 2\\sigma\n\nAbout 99.7% within \\mu \\pm 3\\sigma\n\nStandard Normal: Z \\sim \\mathcal{N}(0, 1) has \\mu = 0, \\sigma = 1\n\nAny normal can be standardized: If X \\sim \\mathcal{N}(\\mu, \\sigma^2), then Z = \\frac{X-\\mu}{\\sigma} \\sim \\mathcal{N}(0,1)\nThis allows us to use standard normal tables for any normal distribution\n\nAdditivity: If X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2) are independent, then: \\sum_{i=1}^n X_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma_i^2\\right)\nUse cases:\n\nMeasurement errors\nHeights, weights, test scores in large populations\nSum of many small independent effects (CLT)\nApproximation for many other distributions\n\n\n\nShow code\n# Normal distribution visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n\n# 1. PDF with different parameters\nx = np.linspace(-6, 6, 1000)\n\n# Different means\nfor mu, color in zip([-2, 0, 2], ['red', 'blue', 'green']):\n    ax1.plot(x, norm.pdf(x, loc=mu), linewidth=2, \n             label=f'μ={mu}, σ=1', color=color)\n\n# Different standard deviations\nfor sigma, style in zip([0.5, 1, 2], ['-', '--', ':']):\n    ax1.plot(x, norm.pdf(x, scale=sigma), linewidth=2, \n             label=f'μ=0, σ={sigma}', linestyle=style, color='black')\n\nax1.set_xlabel('x')\nax1.set_ylabel('Density')\nax1.set_title('Normal Distribution PDF')\nax1.legend(loc='upper right')\nax1.grid(True, alpha=0.3)\n\n# 2. 68-95-99.7 Rule\nmu, sigma = 0, 1\nx_range = np.linspace(-4, 4, 1000)\ny = norm.pdf(x_range, mu, sigma)\nax2.plot(x_range, y, 'k-', linewidth=2)\n\n# Fill areas for different sigma ranges\ncolors = ['lightblue', 'lightgreen', 'lightyellow']\nalphas = [0.8, 0.6, 0.4]\nlabels = ['68% (±1σ)', '95% (±2σ)', '99.7% (±3σ)']\n\nfor n_sigma, color, alpha, label in zip([1, 2, 3], colors, alphas, labels):\n    x_fill = x_range[np.abs(x_range - mu) &lt;= n_sigma * sigma]\n    y_fill = norm.pdf(x_fill, mu, sigma)\n    ax2.fill_between(x_fill, 0, y_fill, color=color, alpha=alpha, label=label)\n\nax2.set_xlabel('Standard Deviations from Mean')\nax2.set_ylabel('Density')\nax2.set_title('68-95-99.7 Rule for Standard Normal')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Key probabilities\nprint(\"Standard Normal Probabilities:\")\nprint(f\"P(-1 ≤ Z ≤ 1) = {norm.cdf(1) - norm.cdf(-1):.3f} ≈ 0.68\")\nprint(f\"P(-2 ≤ Z ≤ 2) = {norm.cdf(2) - norm.cdf(-2):.3f} ≈ 0.95\")\nprint(f\"P(-3 ≤ Z ≤ 3) = {norm.cdf(3) - norm.cdf(-3):.3f} ≈ 0.997\")\n\n\n\n\n\n\n\n\n\nStandard Normal Probabilities:\nP(-1 ≤ Z ≤ 1) = 0.683 ≈ 0.68\nP(-2 ≤ Z ≤ 2) = 0.954 ≈ 0.95\nP(-3 ≤ Z ≤ 3) = 0.997 ≈ 0.997\n\n\n\n\nExponential Distribution\nThe exponential distribution models waiting times between events in a Poisson process.\n\nX \\sim \\text{Exponential}(\\beta) if: f_X(x) = \\frac{1}{\\beta} e^{-x/\\beta}, \\quad x &gt; 0\n\nHere \\beta (beta) is the scale parameter, the average time between events.\nYou can also find the exponential distribution parameterized in terms of a rate parameter \\lambda = \\frac{1}{\\beta}.\nKey properties:\n\nMemoryless: \\mathbb{P}(X &gt; s + t | X &gt; s) = \\mathbb{P}(X &gt; t)\n\n“The future doesn’t depend on how long you’ve already waited”\n\nConnection to Poisson: If events occur at rate \\lambda = 1/\\beta, time between events is Exponential(\\beta)\nCDF: F_X(x) = 1 - e^{-x/\\beta} for x &gt; 0\n\nUse cases:\n\nTime between customer arrivals\nLifetime of electronic components\n\nTime until next earthquake\nDuration of phone calls\n\n\n\nShow code\n# Exponential distribution visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon\n\n# Different β values (mean waiting time)\nbetas = [0.5, 1, 2]\ncolors = ['red', 'blue', 'green']\n\nfig, ax = plt.subplots(figsize=(7, 5))\n\nx = np.linspace(0, 6, 1000)\n\nfor beta, color in zip(betas, colors):\n    pdf = expon.pdf(x, scale=beta)\n    ax.plot(x, pdf, linewidth=2, color=color, label=f'β = {beta}')\n    \n    # Show mean as vertical line\n    ax.axvline(beta, color=color, linestyle='--', alpha=0.5, linewidth=1)\n\nax.set_xlabel('Time (x)')\nax.set_ylabel('Density')\nax.set_title('Exponential Distribution PDF')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(0, 6)\nax.set_ylim(0, 2.1)\n\nplt.tight_layout()\nplt.show()\n\n# Example calculations\nbeta = 1\nprint(f\"For β = {beta} (rate λ = {1/beta}):\")\nprint(f\"E[X] = β = {beta}\")\nprint(f\"Var(X) = β² = {beta**2}\")\nprint(f\"P(X &gt; 1) = {1 - expon.cdf(1, scale=beta):.3f}\")\nprint(f\"Median waiting time = {expon.ppf(0.5, scale=beta):.3f}\")\n\n\n\n\n\n\n\n\n\nFor β = 1 (rate λ = 1.0):\nE[X] = β = 1\nVar(X) = β² = 1\nP(X &gt; 1) = 0.368\nMedian waiting time = 0.693\n\n\n\n\nBrief Catalog: Other Continuous Distributions\nGamma(\\alpha, \\beta): The gamma distribution is a generalization of exponential\n\nSum of independent exponentials\nModels waiting time for multiple events\n\nBeta(\\alpha, \\beta): The beta distribution models values between 0 and 1\n\nModels proportions and probabilities\nConjugate prior for binomial parameter\n\nt(\\nu): The t-distribution is a heavy-tailed alternative to normal\n\nMore probability in tails than normal\nUsed when variance is unknown/unstable\n\\nu = 1 gives Cauchy (no finite mean!)\n\n\\chi^2(p): The chi-squared distribution is the sum of squared standard normals\n\nIf Z_1, ..., Z_p \\sim \\mathcal{N}(0,1) independent, then \\sum Z_i^2 \\sim \\chi^2(p)\nUsed in hypothesis testing and confidence intervals",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#multivariate-distributions",
    "href": "chapters/01-probability-foundations.html#multivariate-distributions",
    "title": "1  Probability Foundations",
    "section": "1.6 Multivariate Distributions",
    "text": "1.6 Multivariate Distributions\nSo far we’ve focused on single random variables. But in practice, we often deal with multiple related variables: height and weight, temperature and humidity, stock prices of different companies. This leads us to multivariate distributions.\n\n1.6.1 Joint Distributions\n\nFor random variables X and Y, the joint distribution describes their behavior together:\n\nDiscrete case: Joint PMF f_{X,Y}(x,y) = \\mathbb{P}(X = x, Y = y)\nContinuous case: Joint PDF f_{X,Y}(x,y) where \\mathbb{P}((X,Y) \\in A) = \\iint_A f_{X,Y}(x,y) \\, dx \\, dy\n\n\n\n\n\n\n\n\nExample: Discrete joint distribution\n\n\n\nRoll two fair six-sided dice. Let X = first die, Y = second die.\nThe joint PMF is: f_{X,Y}(x,y) = \\frac{1}{36} \\text{ for } x,y \\in \\{1,2,3,4,5,6\\}\nWe can display this as a 6×6 table with each entry equal to 1/36.\n\n\n\n\n\n\n\n\nExample: Continuous joint distribution\n\n\n\nLet (X,Y) be uniformly distributed on the unit disk.\nf_{X,Y}(x,y) = \\begin{cases}\n\\frac{1}{\\pi} & \\text{if } x^2 + y^2 \\leq 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\nThe normalizing constant 1/\\pi makes the total probability equal to 1 (area of unit disk is \\pi).\n\n\n\n\n1.6.2 Marginal Distributions\nGiven a joint distribution, we can find the distribution of each variable separately.\n\nThe marginal distribution of X is obtained by “summing out” or “integrating out” the other variable:\n\nDiscrete: f_X(x) = \\sum_y f_{X,Y}(x,y)\nContinuous: f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\n\n\n\n\n\n\n\n\nTip\n\n\n\nThink of marginal distributions as projections: if you have points scattered in 2D, the marginal distribution of X is like looking at their shadows on the X-axis.\n\n\n\n\n\n\n\n\nExample: Sum of two dice\n\n\n\nLet X = first die, Y = second die, S = X + Y.\nWhat is \\mathbb{P}(S = 7)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo find \\mathbb{P}(S = 7), we sum over all ways to get 7:\n\n(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\n\nSo \\mathbb{P}(S = 7) = 6 \\times \\frac{1}{36} = \\frac{1}{6}\n\n\n\n\n\n\n\n1.6.3 Independent Random Variables\n\nRandom variables X and Y are independent if: f_{X,Y}(x,y) = f_X(x) \\cdot f_Y(y) for all x, y.\n\nThis means the joint distribution factors into the product of marginals - knowing the value of one variable tells us nothing about the other.\n\n\n\n\n\n\nExample: Independent coin flips\n\n\n\nFlip two fair coins. Let X = 1 if first is heads, 0 otherwise. Same for Y with second coin.\nJoint distribution:\n\n\n\n\nY = 0\nY = 1\n\n\n\n\nX = 0\n1/4\n1/4\n\n\nX = 1\n1/4\n1/4\n\n\n\nSince each entry equals the product of marginal probabilities (e.g., \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2}), X and Y are independent.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon mistake: Assuming uncorrelated means independent.\nIndependence implies zero correlation, but zero correlation does NOT imply independence! We’ll see counterexamples when we study correlation in Chapter 3.\n\n\n\n\n1.6.4 Conditional Distributions\n\nThe conditional distribution of X given Y = y is:\n\nDiscrete: f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)} if f_Y(y) &gt; 0\nContinuous: Same formula, interpreted as densities\n\n\nThis tells us how X behaves when we know Y = y.\n\n\n\n\n\n\nExample: Quality control\n\n\n\nA factory produces items on two machines. Let:\n\nX = quality score (0-100)\nY = machine (1 or 2)\n\nSuppose Machine 1 produces 60% of items with quality \\sim \\mathcal{N}(80, 25), and Machine 2 produces 40% with quality \\sim \\mathcal{N}(70, 100).\nIf we observe a quality score of 75, which machine likely produced it? This requires the conditional distribution \\mathbb{P}(Y|X=75).\n\n\n\n\n1.6.5 Interactive Exploration: Marginal and Conditional Distributions\nLet’s explore how marginal and conditional distributions relate to a joint distribution using an interactive visualization.\nInstructions:\n\nUse the sliders to change the x and y values\nCheck the boxes to switch between marginal distributions (e.g., f_X(x)) and conditional distributions (e.g., f_{X|Y}(x|y))\nWhen showing conditional distributions, red dashed lines appear on the joint distribution showing where we’re conditioning\nThe visualization uses the simpler shorthand notation p(x) for f_X(x) and p(x|y) for f_{X|Y}(x|y) (and analogous formulas for other pdfs)\n\n\n\nShow code\nd3 = require(\"d3@7\")\nhtl = require(\"htl\")\nimport { bivariateDemo } from \"../js/bivariate-demo.js\"\n\n// Initialize the demo\ndemo = bivariateDemo(d3)\n\n// Define interactive controls\nviewof x_value = Inputs.range([-2, 2], {step: 0.1, value: 0, label: \"x value\"})\nviewof y_value = Inputs.range([-2, 4], {step: 0.1, value: 1, label: \"y value\"})\nviewof show_conditionals = Inputs.checkbox([\"p(x|y)\", \"p(y|x)\"], {value: [], label: \"Show conditionals\"})\n\n// This block will be the output of the cell.\n// It lays out ONLY the plot, but it will still react to the controls above.\n{\n  const plot = demo.createVisualization(\n    x_value,\n    y_value,\n    show_conditionals.includes(\"p(x|y)\"),\n    show_conditionals.includes(\"p(y|x)\")\n  );\n\n  // Return ONLY the plot element. The controls will be hidden but still work.\n  return plot;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey insights:\n\nMarginal distributions show the overall distribution of one variable, ignoring the other\nConditional distributions show how one variable is distributed when we fix the other at a specific value\nThe shape of conditional distributions changes as we move the conditioning value\nThis demonstrates how knowing one variable’s value provides information about the other when they’re not independent\n\n\n\n1.6.6 Random Vectors and IID Random Variables\n\nA random vector is a vector \\mathbf{X} = (X_1, X_2, ..., X_n)^T where each component X_i is a random variable. The joint behavior of all components is characterized by their joint distribution.\n\nRandom vectors allow us to study multiple random quantities together, which leads us to an important special case.\nIID Random Variables:\n\nRandom variables X_1, ..., X_n are independent and identically distributed (IID) if:\n\nThey are mutually independent\nThey all have the same distribution\n\nWe write: X_1, ..., X_n \\stackrel{iid}{\\sim} F.\nIf F has density f we also write X_1, ..., X_n \\stackrel{iid}{\\sim} f.\nX_1, ..., X_n is a random sample of size n from F (or f, respectively).\n\nIID assumptions are fundamental in statistics:\n\nRandom sampling: Each observation comes from the same population\nNo interference: One observation doesn’t affect others\nStable conditions: The underlying distribution doesn’t change\n\n\n\n\n\n\n\nExample: Customer arrivals\n\n\n\nTimes between customer arrivals at a stable business might be IID Exponential(\\beta).\nNot IID:\n\nStock prices (today’s price depends on yesterday’s)\nTemperature readings (temporal correlation)\nSurvey responses from same household (likely correlated)\n\n\n\n\n\n1.6.7 Important Multivariate Distributions\n\nMultinomial Distribution\nThe multinomial distribution is a generalization of binomial to multiple categories.\n\nIf we have k categories with probabilities p_1, ..., p_k (summing to 1), and we observe n independent trials, then the counts (X_1, ..., X_k) follow a Multinomial distribution:\nf(x_1, ..., x_k) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k}\nwhere \\sum x_i = n.\n\nUse cases:\n\nDice rolls (6 categories for a standard die – or k for k-sided dice)\nSurvey responses (multiple choice)\nDocument word counts\nGenetic allele frequencies\n\n\n\nMultivariate Normal Distribution\nThe multivariate normal distribution is the multivariate generalization of the normal distribution.\n\nA random vector \\mathbf{X} = (X_1, ..., X_k)^T has a multivariate normal distribution, written \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), if:\nf(\\mathbf{x}) = \\frac{1}{(2\\pi)^{k/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)\nwhere:\n\n\\boldsymbol{\\mu} is the mean vector\n\\boldsymbol{\\Sigma} is the covariance matrix (symmetric, positive definite)\n\n\nKey properties:\n\nMarginals are normal: If \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), then X_i \\sim \\mathcal{N}(\\mu_i, \\Sigma_{ii})\nLinear combinations are normal: \\mathbf{a}^T\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{a}^T\\boldsymbol{\\mu}, \\mathbf{a}^T\\boldsymbol{\\Sigma}\\mathbf{a})\nConditional distributions are normal (with formulas for conditional mean and variance)\n\nSpecial case - Bivariate normal: For two variables with correlation \\rho: \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}\nThe correlation \\rho controls the relationship:\n\n\\rho = 0: independent (for normal variables, uncorrelated = independent!)\n\\rho &gt; 0: positive relationship\n\\rho &lt; 0: negative relationship\n\n\n\nShow code\n# Bivariate normal distribution visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\n# Create figure\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# Bivariate normal with correlation\nmean = [0, 0]\ncov = [[1, 0.7], [0.7, 1]]  # correlation = 0.7\n\n# Create grid\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\npos = np.dstack((X, Y))\n\n# Calculate PDF\nrv = multivariate_normal(mean, cov)\nZ = rv.pdf(pos)\n\n# Contour plot\ncontour = ax.contour(X, Y, Z, levels=10, colors='blue', alpha=0.5)\nax.clabel(contour, inline=True, fontsize=8)\ncontourf = ax.contourf(X, Y, Z, levels=20, cmap='Blues', alpha=0.7)\nfig.colorbar(contourf, ax=ax, label='Density')\n\n# Add marginal indicators\nax.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)\nax.axvline(x=0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('Bivariate Normal Distribution (ρ = 0.7)')\nax.set_aspect('equal')\nax.grid(True, alpha=0.3)\n\nplt.show()\n\n# Example calculations\nprint(\"Bivariate Normal with ρ = 0.7:\")\nprint(f\"Var(X) = Var(Y) = 1\")\nprint(f\"Cov(X,Y) = ρ·σ_X·σ_Y = 0.7\")\nprint(f\"If we observe Y=1, then:\")\nprint(f\"  E[X|Y=1] = ρ·(Y-μ_Y) = 0.7\")\nprint(f\"  Var(X|Y=1) = (1-ρ²) = {1 - 0.7**2:.2f}\")\n\n\n\n\n\n\n\n\n\nBivariate Normal with ρ = 0.7:\nVar(X) = Var(Y) = 1\nCov(X,Y) = ρ·σ_X·σ_Y = 0.7\nIf we observe Y=1, then:\n  E[X|Y=1] = ρ·(Y-μ_Y) = 0.7\n  Var(X|Y=1) = (1-ρ²) = 0.51\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe multivariate normal distribution is central to many statistical methods. We will return to it in more detail in Chapter 2 when we discuss expectations, covariances, and the properties of linear combinations of random variables.\n\n\n\n\n\n\n\n\nAdvanced: Transformations of Random Variables\n\n\n\n\n\nWe often define variables that are transformations g(\\cdot) of other random variables. Assuming we know the distribution of X or (X, Y), how do we find the distribution of Y = g(X) or (U,V) = g(X,Y)?\nMethod 1: CDF technique\n\nFind the CDF: F_Y(y) = \\mathbb{P}(Y \\leq y) = \\mathbb{P}(g(X) \\leq y)\nDifferentiate to get PDF: f_Y(y) = F_Y'(y)\n\nMethod 2: Jacobian method (for bijective transformations)\nIf (U,V) = g(X,Y) is one-to-one with inverse (X,Y) = h(U,V), then: f_{U,V}(u,v) = f_{X,Y}(h(u,v)) \\cdot |J|\nwhere J is the Jacobian determinant: J = \\det\\begin{pmatrix} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{pmatrix}\n\n\n\n\n\n\nExample: Box-Muller transform\n\n\n\nWe can generate Gaussian (normal) distributed random numbers starting from uniform.\nIf U_1, U_2 \\sim \\text{Uniform}(0,1) independently, then:\n\nX = \\sqrt{-2\\ln U_1} \\cos(2\\pi U_2)\nY = \\sqrt{-2\\ln U_1} \\sin(2\\pi U_2)\n\nare independent \\mathcal{N}(0,1) random variables!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#chapter-summary-and-connections",
    "href": "chapters/01-probability-foundations.html#chapter-summary-and-connections",
    "title": "1  Probability Foundations",
    "section": "1.7 Chapter Summary and Connections",
    "text": "1.7 Chapter Summary and Connections\n\n1.7.1 Key Concepts Review\nWe’ve built up probability theory from its foundations:\n\nSample spaces and events provide the basic framework for describing uncertainty\nProbability axioms give us consistent rules for quantifying uncertainty\nIndependence and conditioning let us model relationships between events\nRandom variables connect probability to numerical quantities we can analyze\nDistributions characterize the behavior of random variables\nMultivariate distributions handle multiple related random quantities\n\n\n\n1.7.2 Why These Concepts Matter\nFor Statistical Inference:\n\nRandom variables let us model data mathematically\nDistributions provide templates for common patterns\nIndependence assumptions simplify analysis\nConditioning lets us update beliefs with data\n\nFor Machine Learning:\n\nProbability distributions model uncertainty in predictions\nBayes’ theorem enables Bayesian ML methods\nMultivariate distributions handle high-dimensional data\nIndependence assumptions make computation tractable\n\nFor Data Science Practice:\n\nUnderstanding distributions helps choose appropriate methods\nRecognizing dependence prevents incorrect analyses\nConditional probability quantifies relationships in data\nSimulation using these distributions validates methods\n\n\n\n1.7.3 Common Pitfalls to Avoid\n\nConfusing \\mathbb{P}(A|B) with \\mathbb{P}(B|A) - These can be vastly different!\nAssuming independence without justification - Real-world variables are often dependent\nMisinterpreting PDFs as probabilities - PDFs are densities, not probabilities\nForgetting \\mathbb{P}(X = x) = 0 for continuous variables - Use intervals for continuous RVs\nThinking disjoint means independent - Disjoint events are maximally dependent!\n\n\n\n1.7.4 Chapter Connections\nThe probability foundations from this chapter provide the mathematical language for all of statistics:\n\nNext - Chapter 2 (Expectation): Building on our introduction to random variables, we’ll explore expectation as a fundamental tool for summarizing distributions, including variance and the powerful linearity of expectation property\nChapter 3 (Convergence & Inference): Using the probability framework and IID concept from this chapter, we’ll prove the Law of Large Numbers and Central Limit Theorem—the theoretical foundations that justify using samples to learn about populations\nChapter 4 (Bootstrap): Apply our understanding of empirical distributions to develop computational methods for quantifying uncertainty, providing a modern alternative to traditional parametric approaches\n\n\n\n1.7.5 Self-Test Problems\nTry to answer these questions after reading these lecture notes.\n\nBayes in action: A test for a disease has 95% sensitivity (true positive rate) and 98% specificity (true negative rate). If 0.1% of the population has the disease, what’s the probability someone with a positive test actually has the disease?\nDistribution identification: Times between earthquakes in a region average 50 days. What distribution would you use to model the time until the next earthquake? Why?\nIndependence check: You roll two dice. Let A = “sum is even” and B = “first die shows 3”. Are A and B independent?\nConditional expectation preview: In a factory, Machine 1 makes 70% of products with defect rate 2%. Machine 2 makes 30% with defect rate 5%. If a product is defective, what’s the probability it came from Machine 1?\n\n\n\n1.7.6 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\nThis table maps sections in these lecture notes to the corresponding sections in Wasserman (2013) (“All of Statistics” or AoS).\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding AoS Section(s)\n\n\n\n\nWhy Do We Need Statistics?\nExpanded material from the slides, contextualizing statistics for data science.\n\n\nFoundations of Probability\n\n\n\n↳ Sample Spaces and Events\nAoS §1.2\n\n\n↳ Probability Axioms\nAoS §1.3 (Definition 1.5)\n\n\n↳ Interpretations of Probability\nAoS §1.3\n\n\n↳ Finite Sample Spaces & Counting\nAoS §1.4\n\n\nIndependence and Conditional Probability\n\n\n\n↳ Independent Events\nAoS §1.5 (Definition 1.9)\n\n\n↳ Conditional Probability\nAoS §1.6 (Definition 1.12)\n\n\n↳ Bayes’ Theorem & Law of Total Probability\nAoS §1.7 (Theorems 1.16, 1.17)\n\n\nRandom Variables\n\n\n\n↳ Definition and Intuition\nAoS §2.1 (Definition 2.1)\n\n\n↳ CDF, PMF, and PDF\nAoS §2.2 (Definitions 2.5, 2.9, 2.11)\n\n\n↳ Core Discrete Distributions\nAoS §2.3\n\n\n↳ Core Continuous Distributions\nAoS §2.4\n\n\nMultivariate Distributions\n\n\n\n↳ Joint Distributions\nAoS §2.5\n\n\n↳ Marginal Distributions\nAoS §2.6\n\n\n↳ Independent Random Variables\nAoS §2.7 (Definition 2.29)\n\n\n↳ Conditional Distributions\nAoS §2.8 (Definitions 2.35, 2.36)\n\n\n↳ Random Vectors and IID Samples\nAoS §2.9 (Definition 2.41)\n\n\n↳ Important Multivariate Distributions\nAoS §2.10\n\n\n↳ Transformations of Random Variables\nAoS §2.11, §2.12\n\n\nChapter Summary and Connections\nNew summary material.\n\n\n\n\n\n\n\n\n1.7.7 Further Reading\n\nProbability Theory: Ross, “A First Course in Probability” - accessible introduction\nMathematical Statistics: Casella & Berger, “Statistical Inference” - rigorous treatment\nBayesian Perspective: Gelman et al., “Bayesian Data Analysis” - modern Bayesian view\nComputational Approach: Blitzstein & Hwang, “Introduction to Probability” - simulation-based\n\n\n\n1.7.8 Python and R Reference\nPython CodeR Code# Probability distributions in Python\nfrom scipy import stats\nimport numpy as np\n\n# Discrete distributions\nstats.binom.pmf(x, n=n, p=p)           # Binomial PMF\nstats.binom.cdf(x, n=n, p=p)           # Binomial CDF\nstats.binom.rvs(n=n, p=p, size=size)   # Generate random binomial\n\nstats.poisson.pmf(x, mu=lam)           # Poisson PMF\nstats.poisson.cdf(x, mu=lam)           # Poisson CDF\nstats.poisson.rvs(mu=lam, size=size)   # Generate random Poisson\n\n# Continuous distributions  \nstats.norm.pdf(x, loc=mean, scale=sd)   # Normal PDF\nstats.norm.cdf(x, loc=mean, scale=sd)   # Normal CDF\nstats.norm.rvs(loc=mean, scale=sd, size=size) # Generate random normal\n\nstats.expon.pdf(x, scale=beta)          # Exponential PDF\nstats.expon.cdf(x, scale=beta)          # Exponential CDF\nstats.expon.rvs(scale=beta, size=size)  # Generate random exponential\n\n# Multivariate normal\nstats.multivariate_normal.rvs(mean, cov, size=size)  # Generate\nstats.multivariate_normal.pdf(x, mean, cov)          # Density\n\n\n\n\nNote on lambda parameter: In the Python\ncode, we used lam instead of lambda\n(\\(\\lambda\\)) for the Poisson\ndistribution parameter because lambda is a reserved keyword\nin Python (used for anonymous functions). Using lam (or\nlamb) in Python is a common convention to avoid syntax\nerrors.\n\n# Probability distributions in R\n\n# Discrete distributions\ndbinom(x, size=n, prob=p)     # Binomial PMF\npbinom(x, size=n, prob=p)     # Binomial CDF\nrbinom(n, size, prob)          # Generate random binomial\n\ndpois(x, lambda)               # Poisson PMF\nppois(x, lambda)               # Poisson CDF  \nrpois(n, lambda)               # Generate random Poisson\n\n# Continuous distributions\ndnorm(x, mean=0, sd=1)         # Normal PDF\npnorm(x, mean=0, sd=1)         # Normal CDF\nrnorm(n, mean=0, sd=1)         # Generate random normal\n\ndexp(x, rate=1/beta)           # Exponential PDF\npexp(x, rate=1/beta)           # Exponential CDF\nrexp(n, rate=1/beta)           # Generate random exponential\n\n# Multivariate normal\nlibrary(mvtnorm)\nrmvnorm(n, mean, sigma)        # Generate multivariate normal\ndmvnorm(x, mean, sigma)        # Multivariate normal density\n\nRemember: Probability is the language of uncertainty. Master this language, and you’ll be able to express and analyze uncertainty in any domain.\n\n\n\n\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231.\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/01-probability-foundations.html#footnotes",
    "href": "chapters/01-probability-foundations.html#footnotes",
    "title": "1  Probability Foundations",
    "section": "",
    "text": "More correctly, LLMs predict tokens, which are parts of words and other characters.↩︎\nThe name “binomial” comes from its appearance in the binomial theorem: (x+y)^n = \\sum_{k=0}^{n} \\binom{n}{k} x^k y^{n-k}.↩︎\nIn modern LLMs, the categorical distribution is over tokens (parts of words), not full words. The token vocabulary can be huge - tens of thousands of different tokens like “a”, “aba”, “add”, etc. GPT models typically use vocabularies of 50,000-100,000 tokens.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html",
    "href": "chapters/02-expectation.html",
    "title": "2  Expectation",
    "section": "",
    "text": "2.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#learning-objectives",
    "href": "chapters/02-expectation.html#learning-objectives",
    "title": "2  Expectation",
    "section": "",
    "text": "Explain the concept of expectation and its role in summarizing distributions and machine learning.\nApply key properties of expectation, especially its linearity, to simplify complex calculations.\nCalculate and interpret variance, covariance, and correlation as measures of spread and linear dependence.\nExtend expectation concepts to random vectors, including mean vectors and covariance matrices.\nDefine and apply conditional expectation and understand its key properties.\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter covers expectation, variance, and related concepts essential for statistical inference. The material is adapted and expanded from Chapter 3 of Wasserman (2013), which interested readers are encouraged to consult directly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#introduction-and-motivation",
    "href": "chapters/02-expectation.html#introduction-and-motivation",
    "title": "2  Expectation",
    "section": "2.2 Introduction and Motivation",
    "text": "2.2 Introduction and Motivation\n\n2.2.1 The Essence of Supervised Machine Learning\nThe fundamental goal of supervised machine learning is seemingly simple: train a model that makes successful predictions on new, unseen data. However, this goal hides a deeper challenge that lies at the heart of statistics.\nWhen we train a model, we work with a finite training set: X_1, \\ldots, X_n \\sim F_X\nwhere F_X is the data generating distribution. Our true objective is to find a model that minimizes the expected loss: \\mathbb{E}[L(X)]\nover the entire distribution F_X, where L(\\cdot) is some suitable loss function. But we can only compute the empirical loss: \\frac{1}{n} \\sum_{i=1}^n L(X_i),\nwhich is the loss function summed over the training data.\nThis gap between what we want (expected loss) and what we can calculate (empirical loss) is the central challenge of machine learning. The concept of expectation provides the mathematical framework to understand and bridge this gap.\nIntuitiveMathematicalComputationalGoal: Imagine training a neural network to classify\ncat and dog images.You have 10,000 training images, but your model needs to work on\nmillions of future images it’s never seen. When your model achieves 98%\naccuracy on training data, that’s just the average over your specific\n10,000 images. What you really care about is the accuracy over all\npossible cat and dog images that exist or could exist.This gap—between what we can measure (training performance) and what\nwe want (real-world performance)—is why expectation is central to\nmachine learning. Every loss function is secretly an expectation!The cross-entropy loss used for classification tasks\nmeasures how “surprised” your model is by the true labels. Lower\nsurprise = better predictions. The key insight: we minimize the\naverage surprise over our training data, hoping it approximates\nthe expected surprise over all possible data.Setup: We want to classify images as cats\n(\\(y=1\\)) or dogs\n(\\(y=0\\)).Our model outputs:\n\\[ \\hat{p}(x) = \\text{predicted probability that image } x \\text{ is a cat.} \\]Step 1: Define the loss for one exampleFor a single image-label pair\n\\((x, y)\\), the cross-entropy loss is:\n\\[L(x, y) = -[y \\log(\\hat{p}(x)) + (1-y) \\log(1-\\hat{p}(x))]\\]This penalizes wrong predictions:\nIf \\(y = 1\\) (cat) but\n\\(\\hat{p}(x) \\approx 0\\): large\nloss\nIf \\(y = 0\\) (dog) but\n\\(\\hat{p}(x) \\approx 1\\): large\nloss\nCorrect predictions → small loss\nStep 2: The fundamental problemWhat we want to minimize (expected loss over all possible images):\n\\[R_{\\text{true}} = \\mathbb{E}_{(X,Y)}[L(X, Y)]\\]What we can compute (average loss over training data):\n\\[R_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n L(x_i, y_i)\\]Step 3: The connection to expectationNotice that \\(R_{\\text{train}}\\) is\njust the sample mean of the losses, while\n\\(R_{\\text{true}}\\) is the\nexpectation of the loss. By the Law of Large Numbers:\n\\[R_{\\text{train}} \\xrightarrow{n \\to \\infty} R_{\\text{true}}\\]This is why machine learning is fundamentally about\nexpectation!Let’s see cross-entropy loss in action with a simple cat/dog\nclassifier. We’ll simulate predictions and compute both the loss on a\nsmall training set and the true expected loss over the entire\npopulation.Note that in this example we are not training a model. We\nare given a model, and we want to compute its loss. What we see is how\nclose the empirical loss is to the expected loss as we change the\ndataset size over which we compute the loss.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate a simple \"classifier\" that predicts cat probability based on \n# a single feature (e.g., \"ear pointiness\" from 0 to 1)\nnp.random.seed(42)\n\n# True probabilities: cats have pointier ears\ndef true_cat_probability(ear_pointiness):\n    # Logistic function: more pointy → more likely cat\n    return 1 / (1 + np.exp(-5 * (ear_pointiness - 0.5)))\n\n# Generate population data\nn_population = 10000\near_pointiness = np.random.uniform(0, 1, n_population)\ntrue_probs = true_cat_probability(ear_pointiness)\n# Sample actual labels based on true probabilities\nlabels = (np.random.random(n_population) &lt; true_probs).astype(int)\n\n# Our (imperfect) model's predictions\ndef model_prediction(x):\n    # Slightly wrong sigmoid (shifted and less steep)\n    return 1 / (1 + np.exp(-3 * (x - 0.45)))\n\n# Compute cross-entropy loss\ndef cross_entropy_loss(y_true, y_pred):\n    # Avoid log(0) with small epsilon\n    eps = 1e-7\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n# Show what training sees vs reality\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n\n# Left: Model predictions vs truth\nx_plot = np.linspace(0, 1, 100)\nax1.plot(x_plot, true_cat_probability(x_plot), 'g-', linewidth=3, \n         label='True P(cat|x)')\nax1.plot(x_plot, model_prediction(x_plot), 'b--', linewidth=2, \n         label='Model P̂(cat|x)')\nax1.scatter(ear_pointiness[:100], labels[:100], alpha=0.3, s=20, \n            c=['red' if l == 0 else 'blue' for l in labels[:100]])\nax1.set_xlabel('Ear Pointiness')\nax1.set_ylabel('Probability of Cat')\nax1.set_title('Model vs Reality')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right: Empirical vs Expected Loss\ntraining_sizes = [10, 50, 100, 500, 1000, 5000]\nempirical_losses = []\n\nfor n in training_sizes:\n    # Sample n training examples\n    idx = np.random.choice(n_population, n, replace=False)\n    train_x = ear_pointiness[idx]\n    train_y = labels[idx]\n    train_pred = model_prediction(train_x)\n    \n    # Empirical loss on training set\n    emp_loss = np.mean(cross_entropy_loss(train_y, train_pred))\n    empirical_losses.append(emp_loss)\n\n# True expected loss over entire population\nall_predictions = model_prediction(ear_pointiness)\nexpected_loss = np.mean(cross_entropy_loss(labels, all_predictions))\n\nax2.semilogx(training_sizes, empirical_losses, 'bo-', linewidth=2, \n             markersize=8, label='Empirical Loss (Training)')\nax2.axhline(y=expected_loss, color='red', linestyle='--', linewidth=2,\n            label=f'Expected Loss = {expected_loss:.3f}')\nax2.set_xlabel('Training Set Size')\nax2.set_ylabel('Cross-Entropy Loss')\nax2.set_title('Convergence to Expected Loss')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"With just 10 samples: empirical loss = {empirical_losses[0]:.3f}\")\nprint(f\"With 5000 samples: empirical loss = {empirical_losses[-1]:.3f}\")\nprint(f\"True expected loss: {expected_loss:.3f}\")\nprint(f\"\\nAs we get more training data to calculate the loss, our empirical\")\nprint(f\"loss estimate gets closer to the true expected loss we care about!\")\n\n\n\n\nWith just 10 samples: empirical loss = 0.680\nWith 5000 samples: empirical loss = 0.535\nTrue expected loss: 0.542\n\nAs we get more training data to calculate the loss, our empirical\nloss estimate gets closer to the true expected loss we care about!\n\n\n\n\n2.2.2 Why Expectation Matters in ML and Beyond\nThe concept of expectation appears throughout data science and statistics:\n\nStatistical Inference: Estimating population parameters from samples\nDecision Theory: Maximizing expected utility or minimizing expected risk\nA/B Testing: Measuring expected treatment effects\nFinancial Modeling: Expected returns and risk assessment\nLoss Functions in Deep Learning: Cross-entropy loss and ELBO in VAEs\n\nIn each case, we’re trying to understand average behavior over some distribution, which is precisely what expectation captures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#foundations-of-expectation",
    "href": "chapters/02-expectation.html#foundations-of-expectation",
    "title": "2  Expectation",
    "section": "2.3 Foundations of Expectation",
    "text": "2.3 Foundations of Expectation\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nExpected value/Expectation\nOdotusarvo\nThe mean of a distribution\n\n\nMean\nKeskiarvo\nSame as expectation\n\n\nVariance\nVarianssi\nMeasure of spread\n\n\nStandard deviation\nKeskihajonta\nSquare root of variance\n\n\nCovariance\nKovarianssi\nLinear relationship between variables\n\n\nCorrelation\nKorrelaatio\nStandardized covariance\n\n\nSample mean\nOtoskeskiarvo\nAverage of data points\n\n\nSample variance\nOtosvarianssi\nEmpirical measure of spread\n\n\nConditional expectation\nEhdollinen odotusarvo\nMean given information\n\n\nMoment\nMomentti\nPowers of random variable\n\n\nRandom vector\nSatunnaisvektori\nVector of random variables\n\n\nCovariance matrix\nKovarianssimatriisi\nMatrix of covariances\n\n\nPrecision matrix\nTarkkuusmatriisi\nInverse of covariance matrix\n\n\nMoment generating function\nMomenttigeneroiva funktio\nTransform for finding moments\n\n\nCentral moment\nKeskusmomentti\nMoment about the mean\n\n\n\n\n\n\n\n2.3.1 Definition and Basic Properties\n\nThe expected value, or mean, or first moment of a random variable X is defined to be: \\mathbb{E}(X) = \\begin{cases}\n\\sum_x x \\mathbb{P}(X = x) & \\text{if } X \\text{ is discrete} \\\\\n\\int_{\\mathbb{R}} x f_X(x) \\, dx & \\text{if } X \\text{ is continuous}\n\\end{cases} assuming the sum (or integral) is well defined. We use the following notation interchangeably: \\mathbb{E}(X) = \\mathbb{E} X = \\mu = \\mu_X\n\nThe expectation represents the average value of the distribution – the balance point where the distribution would balance if it were a physical object.\nNotation: The lowercase Greek letter \\mu (mu; pronounced mju) is universally used to denote the mean.\n\n\n\n\n\n\nSimplified Notation\n\n\n\nIn this course, we will write the expectation using the simplified notation:\n\\mathbb{E}(X) = \\int x f_X(x) dx\nwhen the type of random variable is unspecified and could be either continuous or discrete.\nFor a discrete random variable, you would substitute the integral with a sum, and the PDF f_X(x) (probability density function) with the PMF \\mathbb{P}_X(x) (probability mass function), as seen in Chapter 1 of the lecture notes.\nNote that this is an abuse of notation and is not mathematically correct, but we found it to be more intuitive in previous iterations of the course.\n\n\n\n\n\n\n\n\nExample: Simple Expectations\n\n\n\nLet’s calculate expectations for some basic distributions:\n\nBernoulli(0.3): \\mathbb{E}(X) = 0 \\times 0.7 + 1 \\times 0.3 = 0.3\nFair six-sided die: \\mathbb{E}(X) = \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = \\frac{21}{6} = 3.5\nTwo coin flips (X = number of heads): \\mathbb{E}(X) = 0 \\times \\frac{1}{4} + 1 \\times \\frac{1}{2} + 2 \\times \\frac{1}{4} = 1\n\n\n\nIntuitiveMathematicalComputationalExpectation is the “center of mass” of a distribution. Imagine:\nPhysical analogy: If you made a histogram out of metal,\nthe expected value is where you’d place a fulcrum to balance it\nperfectly.\nLong-run average: If you repeat an experiment\nmillions of times and average the results, you’ll get very close to the\nexpectation. This isn’t just intuition—it’s a theorem (the Law of Large\nNumbers) we’ll prove in Chapter 3.\nFair price: In gambling, the expectation tells\nyou the fair price to pay for a game. If a lottery ticket has expected\nwinnings of €2, then €2 is the break-even price.\nThink of expectation as answering: “If I had to summarize this entire\ndistribution with a single number pointing at its center, what\nwould it be?” The expectation or mean is not the only number we\ncould use to represent the center of a distribution, but it is a very\ncommon choice suitable for most situations.The expectation is a linear functional on the space of random\nvariables. For a random variable \\(X\\)\nwith distribution function \\(F\\), the\ncorrect mathematical notation would be:\\[\\mathbb{E}(X) = \\int x \\, dF(x)\\]This notation correctly unifies the discrete and continuous\ncases:\nFor discrete \\(X\\): the integral\nbecomes a sum over the jump points of\n\\(F\\)\nFor continuous \\(X\\): we have\n\\(dF(x) = f_X(x)dx\\)\nThis notation is particularly useful when dealing with mixed\ndistributions or when stating results that apply to both discrete and\ncontinuous random variables without writing separate formulas. We won’t\nbe using this notation in the course, but you may find it in\nmathematical or statistical textbooks, including Wasserman (2013).Let’s demonstrate expectation through simulation, showing how sample\naverages converge to the true expectation. We’ll also show a case where\nexpectation doesn’t exist (see next section).\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import cauchy\n\n# Set up figure with subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n\n# 1. Convergence for Bernoulli(0.3)\nnp.random.seed(42)\np = 0.3\nn_flips = 40000\nflips = np.random.choice([0, 1], size=n_flips, p=[1-p, p])\nrunning_mean = np.cumsum(flips) / np.arange(1, n_flips + 1)\n\nax1.plot(running_mean, linewidth=2, alpha=0.8, color='blue')\nax1.axhline(y=p, color='red', linestyle='--', label=f'True E[X] = {p}', linewidth=2)\nax1.set_xlabel('Number of trials')\nax1.set_ylabel('Sample mean')\nax1.set_title('Sample Mean Converges to Expectation: Bernoulli(0.3)')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim(0.2, 0.4)\n\n# 2. Cauchy distribution - no expectation exists\nn_samples = 40000\ncauchy_samples = cauchy.rvs(size=n_samples, random_state=42)\ncauchy_running_mean = np.cumsum(cauchy_samples) / np.arange(1, n_samples + 1)\n\nax2.plot(cauchy_running_mean, linewidth=2, alpha=0.8, color='green')\nax2.axhline(y=0, color='black', linestyle=':', alpha=0.5)\nax2.set_xlabel('Number of samples')\nax2.set_ylabel('Sample mean')\nax2.set_title('Cauchy Distribution: Sample Mean Does Not Converge (No Expectation)')\nax2.grid(True, alpha=0.3)\nax2.set_ylim(-10, 10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Bernoulli: After {n_flips} flips, sample mean = {running_mean[-1]:.4f}\")\nprint(f\"Cauchy: After {n_samples} samples, sample mean = {cauchy_running_mean[-1]:.4f}\")\nprint(\"Notice how Cauchy's sample mean keeps eventually jumping around,\")\nprint(\"even when you think it's converging to zero!\")\n\n\n\n\nBernoulli: After 40000 flips, sample mean = 0.2969\nCauchy: After 40000 samples, sample mean = -2.9548\nNotice how Cauchy's sample mean keeps eventually jumping around,\neven when you think it's converging to zero!\n\n\n\n\n2.3.2 Existence of Expectation\nNot all random variables have well-defined expectations.\n\nThe expectation \\mathbb{E}(X) exists if and only if: \\mathbb{E}(|X|) = \\int |x| f_X(x) \\, dx &lt; \\infty\n\n\n\n\n\n\n\nThe Cauchy Distribution\n\n\n\nThe Cauchy distribution is a classic example of probability density with no expectation:\nf_X(x) = \\frac{1}{\\pi(1 + x^2)}\nTo check if expectation exists: \\int_{-\\infty}^{\\infty} |x| \\cdot \\frac{1}{\\pi(1 + x^2)} \\, dx = \\frac{2}{\\pi} \\int_0^{\\infty} \\frac{x}{1 + x^2} \\, dx = \\infty\nThe integral diverges! This means:\n\nSample averages don’t converge to any value\nThe Law of Large Numbers doesn’t apply\nExtreme observations are common due to heavy tails\n\n\n\n\n\n2.3.3 Expectation of Functions\nOften we need the expectation of a function of a random variable. The “Rule of the Lazy Statistician” saves us from finding the distribution of the transformed variable.\n\nLet Y = r(X). Then: \\mathbb{E}(Y) = \\mathbb{E}(r(X)) = \\int r(x) f_X(x) \\, dx\n\nThis result is incredibly useful—we can find \\mathbb{E}(Y) without determining f_Y(y)!\n\n\n\n\n\n\nExample: Breaking a Stick\n\n\n\nA stick of unit length is broken at a random point. What’s the expected length of the longer piece?\nLet X \\sim \\text{Uniform}(0,1) be the break point. The longer piece has length: Y = r(X) = \\max\\{X, 1-X\\}\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can identify that:\n\nIf X &lt; 1/2: longer piece = 1-X\nIf X \\geq 1/2: longer piece = X\n\nTherefore:\n\\mathbb{E}(Y) = \\int_0^{1/2} (1-x) \\cdot 1 \\, dx + \\int_{1/2}^1 x \\cdot 1 \\, dx = \\left[x - \\frac{x^2}{2}\\right]_0^{1/2} + \\left[\\frac{x^2}{2}\\right]_{1/2}^1 = \\left(\\frac{1}{2} - \\frac{1}{8}\\right) + \\left(\\frac{1}{2} - \\frac{1}{8}\\right) = \\frac{3}{4}\n\n\nShow code\n# Visualizing the breaking stick problem\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 1, 1000)\nlonger_piece = np.maximum(x, 1-x)\n\nplt.figure(figsize=(7, 4))\nplt.plot(x, longer_piece, 'b-', linewidth=3, label='Length of longer piece')\nplt.fill_between(x, 0, longer_piece, alpha=0.3, color='lightblue')\nplt.axhline(y=0.75, color='red', linestyle='--', linewidth=2, \n            label='E[longer piece] = 3/4')\nplt.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\nplt.xlabel('Break point (X)')\nplt.ylabel('Length of longer piece')\nplt.title('Breaking a Unit Stick: Expected Length of Longer Piece')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Exponential Prize Game\n\n\n\nA game show offers a prize based on rolling a die: if you roll X, you win 2^X euros. What’s your expected winnings?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the lazy statistician’s rule with X \\sim \\text{DiscreteUniform}(1,6): \\mathbb{E}(2^X) = \\sum_{x=1}^6 2^x \\cdot \\frac{1}{6} = \\frac{1}{6}(2^1 + 2^2 + \\cdots + 2^6)\nDirect calculation: = \\frac{1}{6}(2 + 4 + 8 + 16 + 32 + 64) = \\frac{126}{6} = 21\nSo you expect to win €21 on average.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSpecial case: Probability as expectation of indicator functions.\nIf A is an event and the indicator function is defined as: I_A(x) = \\begin{cases}\n1 & \\text{if } x \\in A \\\\\n0 & \\text{if } x \\notin A\n\\end{cases} then: \\mathbb{E}(I_A(X)) = \\mathbb{P}(X \\in A)\nThis shows that probability is just a special case of expectation!\nThis trick of using the indicator function with expectations is used commonly in probability, statistics and machine learning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#properties-of-expectation",
    "href": "chapters/02-expectation.html#properties-of-expectation",
    "title": "2  Expectation",
    "section": "2.4 Properties of Expectation",
    "text": "2.4 Properties of Expectation\n\n2.4.1 The Linearity Property\n\nIf X_1, \\ldots, X_n are random variables and a_1, \\ldots, a_n are constants, then: \\mathbb{E}\\left(\\sum_{i=1}^n a_i X_i\\right) = \\sum_{i=1}^n a_i \\mathbb{E}(X_i)\n\n\n\n\n\n\n\nPossibly The Most Important Result in This Course\n\n\n\nExpectation is LINEAR!\nThis property:\n\nWorks WITHOUT independence (unlike the product rule)\nSimplifies hard calculations\nIs the key to understanding sampling distributions\nWill be used in almost every proof and application\n\nIf you remember only one thing from this chapter, remember that expectation is linear. You’ll use it constantly throughout statistics and machine learning!\n\n\n\n\n2.4.2 Applications of Linearity\nThe power of linearity becomes clear when we use it to solve problems that would be difficult otherwise.\n\n\n\n\n\n\nExample: Binomial Mean via Indicator Decomposition\n\n\n\nLet X \\sim \\text{Binomial}(n, p). Finding \\mathbb{E}(X) directly requires evaluating: \\mathbb{E}(X) = \\sum_{x=0}^n x \\binom{n}{x} p^x (1-p)^{n-x}\nHave fun calculating this! But with linearity, it’s trivial.\nRemember that \\text{Binomial}(n, p) is the distribution of the sum of n \\text{Bernoulli}(p) random variables.\nThus, we can write X = \\sum_{i=1}^n X_i, where X_i are independent Bernoulli(p) indicators.\nWith this, we have: \\mathbb{E}(X) = \\mathbb{E}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\mathbb{E}(X_i) = \\sum_{i=1}^n p = np\nDone!\n\n\n\n\n\n\n\n\nExample: Linearity Works Even with Dependent Variables\n\n\n\nA common misconception is that linearity of expectation requires independence. It doesn’t! Let’s demonstrate this crucial fact by computing \\mathbb{E}[2X + 3Y] where X and Y are strongly correlated.\nWe’ll generate X and Y with correlation 0.8 (highly dependent!) and verify that linearity still holds:\n\n\nShow code\n# Demonstrating linearity even with dependence\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn_sims = 10000\n\n# Generate correlated X and Y\nmean = [2, 3]\ncov = [[1, 0.8], [0.8, 1]]  # Correlation = 0.8\nsamples = np.random.multivariate_normal(mean, cov, n_sims)\nX = samples[:, 0]\nY = samples[:, 1]\n\n# Compute E[2X + 3Y] empirically\nZ = 2*X + 3*Y\nempirical_mean = np.mean(Z)\n\n# Theoretical value using linearity\ntheoretical_mean = 2*mean[0] + 3*mean[1]\n\nplt.figure(figsize=(7, 4))\nplt.hist(Z, bins=50, density=True, alpha=0.7, color='green', edgecolor='black')\nplt.axvline(empirical_mean, color='blue', linestyle='-', linewidth=2, \n            label=f'Empirical: {empirical_mean:.3f}')\nplt.axvline(theoretical_mean, color='red', linestyle='--', linewidth=2, \n            label=f'Theoretical: {theoretical_mean:.3f}')\nplt.xlabel('2X + 3Y')\nplt.ylabel('Density')\nplt.title('Linearity of Expectation Works Even with Dependent Variables!')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"X and Y are dependent (correlation = 0.8)\")\nprint(f\"But E[2X + 3Y] = 2E[X] + 3E[Y] still holds!\")\nprint(f\"Theoretical: 2×{mean[0]} + 3×{mean[1]} = {theoretical_mean}\")\nprint(f\"Empirical: {empirical_mean:.3f}\")\n\n\n\n\n\n\n\n\n\nX and Y are dependent (correlation = 0.8)\nBut E[2X + 3Y] = 2E[X] + 3E[Y] still holds!\nTheoretical: 2×2 + 3×3 = 13\nEmpirical: 12.985\n\n\nKey takeaway: Despite the strong correlation between X and Y, the empirical mean of 2X + 3Y matches the theoretical value 2\\mathbb{E}[X] + 3\\mathbb{E}[Y] perfectly. This is why linearity of expectation is so powerful—it works unconditionally!\n\n\n\n\n\n\n\n\nAdditional Examples: More Applications of Linearity\n\n\n\n\n\nExpected Number of Fixed Points in Random Permutation:\nIn a random permutation of {1, 2, …, n}, what’s the expected number of elements that stay in their original position?\nLet X_i = 1 if element i stays in position i, and 0 otherwise. The total number of fixed points is X = \\sum_{i=1}^n X_i.\nFor any position i: \\mathbb{P}(X_i = 1) = \\frac{1}{n} (element i has probability 1/n of being in position i).\nTherefore: \\mathbb{E}(X_i) = \\frac{1}{n}\nBy linearity: \\mathbb{E}(X) = \\sum_{i=1}^n \\mathbb{E}(X_i) = \\sum_{i=1}^n \\frac{1}{n} = 1\nAmazing! No matter how large n is, we expect exactly 1 fixed point on average.\nFortune Doubling Game (from Wasserman Exercise 3.1):\nYou start with c dollars. On each play, you either double your money or halve it, each with probability 1/2. What’s your expected fortune after n plays?\nLet X_i be your fortune after i plays. Then: - X_0 = c - X_{i+1} = 2X_i with probability 1/2 - X_{i+1} = X_i/2 with probability 1/2\nUsing conditional expectation: \\mathbb{E}(X_{i+1} | X_i) = \\frac{1}{2}(2X_i) + \\frac{1}{2}\\left(\\frac{X_i}{2}\\right) = X_i + \\frac{X_i}{4} = X_i\nBy the law of iterated expectations: \\mathbb{E}(X_{i+1}) = \\mathbb{E}[\\mathbb{E}(X_{i+1} | X_i)] = \\mathbb{E}(X_i)\nTherefore, by induction: \\mathbb{E}(X_n) = \\mathbb{E}(X_0) = c\nYour expected fortune never changes! This is an example of a martingale—a fair game where the expected future value equals the current value.\n\n\n\n\n\n2.4.3 Independence and Products\nWhile expectation is linear for all random variables, products require independence.\n\nIf X_1, \\ldots, X_n are independent random variables, then: \\mathbb{E}\\left(\\prod_{i=1}^n X_i\\right) = \\prod_{i=1}^n \\mathbb{E}(X_i)\n\n\n\n\n\n\n\nWarning\n\n\n\nThis ONLY works with independent random variables! As a clear counterexample, \\mathbb{E}(X^2) \\neq (\\mathbb{E}(X))^2 in general, since X and X are clearly not independent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#variance-and-its-properties",
    "href": "chapters/02-expectation.html#variance-and-its-properties",
    "title": "2  Expectation",
    "section": "2.5 Variance and Its Properties",
    "text": "2.5 Variance and Its Properties\n\n2.5.1 Measuring Spread\nWhile expectation tells us the center of a distribution, variance measures how “spread out” it is.\n\nLet X be a random variable with mean \\mu. The variance of X – denoted by \\sigma^2, \\sigma_X^2, \\mathbb{V}(X), \\mathbb{V}X or \\text{Var}(X) – is defined as: \\sigma^2 = \\mathbb{V}(X) = \\mathbb{E}[(X - \\mu)^2] assuming this expectation exists. The standard deviation is\n\\mathrm{sd}(X) = \\sqrt{\\mathbb{V}(X)}\nand is also denoted by \\sigma and \\sigma_X.\n\nNotation: The lowercase Greek letter \\sigma (sigma) is almost universally used to denote the standard deviation (and more generally the “scale” of a distribution, related to its spread).\n\n\n\n\n\n\nWhy Both Variance and Standard Deviation?\n\n\n\nVariance (\\sigma^2) is in squared units—if X measures height in cm, then \\mathbb{V}(X) is in cm². This makes it hard to interpret directly.\nStandard deviation (\\sigma) is in the same units as X, making it more interpretable: “typical deviation from the mean.”\nSo why use variance at all? Variance works better for doing math because:\n\nIt has nicer properties (like additivity for independent variables, as we will see later)\nIt appears naturally in formulas and proofs\nIt’s easier to manipulate algebraically\n\nIn short: we do calculations with variance, then take the square root for interpretation.\n\n\nIntuitiveMathematicalComputationalThink of variance as measuring how wrong your guess will\ntypically be if you always guess the mean.Imagine predicting tomorrow’s temperature. If you live in Nice or\nLisbon (low variance), guessing the average temperature works well\nyear-round. If you live in Helsinki or Berlin (high variance), that same\nstrategy leads to large errors – you’ll be way off in both summer and\nwinter.Standard deviation puts this in interpretable\nunits:\nLow \\(\\sigma\\): Your guesses are\nusually close (precise manufacturing, stable processes)\nHigh \\(\\sigma\\): Your guesses are\noften far off (volatile stocks, unpredictable weather)\nThe famous 68-95-99.7 rule tells us that for\nbell-shaped data:\n68% of observations fall within\n1\\(\\sigma\\) of the mean\n95% fall within 2\\(\\sigma\\)\n\n99.7% fall within 3\\(\\sigma\\)\nThis is why “3-sigma events” are considered rare outliers in quality\ncontrol.(This rule is exactly true for normally-distributed\ndata.)Variance has an elegant mathematical interpretation as the\nexpected squared distance from the mean:\n\\[\\mathbb{V}(X) = \\mathbb{E}[(X - \\mu)^2]\\]This squared distance has deep connections:\nMinimization property: The mean\n\\(\\mu\\) minimizes\n\\(\\mathbb{E}[(X - c)^2]\\) over all\nconstants \\(c\\)\nPythagorean theorem: For independent\n\\(X, Y\\):\n\\[\\mathbb{V}(X + Y) = \\mathbb{V}(X) + \\mathbb{V}(Y)\\]\nJust like \\(|a + b|^2 = |a|^2 + |b|^2\\)\nfor perpendicular vectors!\nInformation theory: Variance of a Gaussian\ndetermines its entropy (uncertainty)\nThe quadratic nature (\\(a^2\\)\nscaling) reflects that variance measures squared deviations:\ndoubling the scale quadruples the variance.Let’s visualize how variance controls the spread of a distribution,\nusing exam scores as an example.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set up the plot\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Common mean for all distributions\nmean = 75\nx = np.linspace(40, 110, 1000)\n\n# Three different standard deviations\nsigmas = [5, 10, 20]\ncolors = ['#2E86AB', '#A23B72', '#F18F01']\nlabels = ['σ = 5 (Low variance)', 'σ = 10 (Medium variance)', 'σ = 20 (High variance)']\n\n# Plot each distribution\nfor sigma, color, label in zip(sigmas, colors, labels):\n    y = stats.norm.pdf(x, mean, sigma)\n    ax.plot(x, y, color=color, linewidth=2.5, label=label)\n    \n    # Shade ±1σ region\n    x_fill = x[(x &gt;= mean - sigma) & (x &lt;= mean + sigma)]\n    y_fill = stats.norm.pdf(x_fill, mean, sigma)\n    ax.fill_between(x_fill, y_fill, alpha=0.2, color=color)\n\n# Add vertical line at mean\nax.axvline(mean, color='black', linestyle='--', alpha=0.7, linewidth=1.5)\nax.text(mean + 1, 0.085, 'Mean = 75', ha='left', va='bottom')\n\n# Styling\nax.set_xlabel('Exam Score')\nax.set_ylabel('Probability Density')\nax.set_title('Same Mean, Different Variances: The Effect of Standard Deviation')\nax.legend(loc='upper left')\nax.set_xlim(40, 110)\nax.set_ylim(0, 0.09)\nax.grid(True, alpha=0.3)\n\n# Add annotations for interpretation\nax.annotate('68% of scores\\nwithin ±σ', xy=(mean + 5, 0.055), xytext=(mean + 12, 0.065),\n            arrowprops=dict(arrowstyle='-&gt;', color='#2E86AB', alpha=0.7),\n            fontsize=9, ha='center', color='#2E86AB')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Interpreting the visualization:\")\nprint(\"• Small σ (blue): Scores cluster tightly around 75. Most students perform similarly.\")\nprint(\"• Medium σ (pink): Moderate spread. Typical variation in a well-designed exam.\")\nprint(\"• Large σ (orange): Wide spread. Large differences in student performance.\")\nprint(f\"\\nFor any normal distribution, about 68% of values fall within ±1σ of the mean.\")\n\n\n\n\nInterpreting the visualization:\n• Small σ (blue): Scores cluster tightly around 75. Most students perform similarly.\n• Medium σ (pink): Moderate spread. Typical variation in a well-designed exam.\n• Large σ (orange): Wide spread. Large differences in student performance.\n\nFor any normal distribution, about 68% of values fall within ±1σ of the mean.\n\n\n\n\n2.5.2 Properties of Variance\nThe variance has a useful computational formula:\n\n\\mathbb{V}(X) = \\mathbb{E}(X^2) - (\\mathbb{E}(X))^2\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nStarting from the definition of variance: \\begin{align}\n\\mathbb{V}(X) &= \\mathbb{E}[(X - \\mu)^2] \\\\\n&= \\mathbb{E}[X^2 - 2X\\mu + \\mu^2] \\\\\n&= \\mathbb{E}(X^2) - 2\\mu\\mathbb{E}(X) + \\mu^2 \\\\\n&= \\mathbb{E}(X^2) - 2\\mu^2 + \\mu^2 \\\\\n&= \\mathbb{E}(X^2) - \\mu^2\n\\end{align} where we used linearity of expectation and the fact that \\mathbb{E}(X) = \\mu.\n\n\n\nThis formula simplifies many calculations and can be used to prove multiple properties of the variance.\n\nAssuming the variance is well defined, it satisfies:\n\n\\mathbb{V}(X) \\geq 0, with \\mathbb{V}(X) = 0 if and only if X is constant (a.s.)1\nFor constants a, b: \\mathbb{V}(aX + b) = a^2\\mathbb{V}(X)\nIf X and Y are independent: \\mathbb{V}(X + Y) = \\mathbb{V}(X) + \\mathbb{V}(Y)\nIf X and Y are independent: \\mathbb{V}(X - Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) (not minus!)\nIf X_1, \\ldots, X_n are independent, for constants a_1, \\ldots, a_n: \\mathbb{V}\\left(\\sum_{i=1}^n a_i X_i\\right) = \\sum_{i=1}^n a_i^2 \\mathbb{V}(X_i)\n\n\nProperty 4 often surprises students. You can find the proof below.\n\n\n\n\n\n\nProof of Property 4\n\n\n\n\n\nIf X and Y are independent with means \\mu_X, \\mu_Y: \\begin{align}\n\\mathbb{V}(X - Y) &= \\mathbb{E}[(X - Y - (\\mu_X - \\mu_Y))^2] \\\\\n&= \\mathbb{E}[((X - \\mu_X) - (Y - \\mu_Y))^2] \\\\\n&= \\mathbb{E}[(X - \\mu_X)^2 - 2(X - \\mu_X)(Y - \\mu_Y) + (Y - \\mu_Y)^2] \\\\\n&= \\mathbb{E}[(X - \\mu_X)^2] + \\mathbb{E}[(Y - \\mu_Y)^2] - 2\\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] \\\\\n&= \\mathbb{V}(X) + \\mathbb{V}(Y) - 2 \\cdot 0 \\\\\n&= \\mathbb{V}(X) + \\mathbb{V}(Y)\n\\end{align}\nThe key step uses independence: \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] = \\mathbb{E}[X - \\mu_X]\\mathbb{E}[Y - \\mu_Y] = 0 \\cdot 0 = 0.\nLet’s visualize how subtracting independent variables increases variance, while subtracting dependent variables can reduce it – to the point that substracting perfectly correlated variables completely eliminates any variance!\n\n\nShow code\n# Visualizing Var(X-Y) = Var(X) + Var(Y) for independent variables\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn = 10000\n\n# Independent case\nX_indep = np.random.normal(0, 1, n)  # Var = 1\nY_indep = np.random.normal(0, 1, n)  # Var = 1\ndiff_indep = X_indep - Y_indep      # Var should be 2\n\n# Perfectly correlated case (not independent)\nX_corr = np.random.normal(0, 1, n)\nY_corr = X_corr  # Perfect correlation\ndiff_corr = X_corr - Y_corr  # Should be 0\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n\n# Independent case\nax1.hist(diff_indep, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black')\nax1.set_xlabel('X - Y')\nax1.set_ylabel('Density')\nax1.set_title(f'Independent: Var(X-Y) = {np.var(diff_indep, ddof=1):.3f} ≈ 2')\nax1.set_xlim(-6, 6)\n\n# Correlated case\nax2.hist(diff_corr, bins=50, density=True, alpha=0.7, color='red', edgecolor='black')\nax2.set_xlabel('X - Y')\nax2.set_ylabel('Density')\nax2.set_title(f'Perfect Correlation: Var(X-Y) = {np.var(diff_corr, ddof=1):.3f} ≈ 0')\nax2.set_xlim(-6, 6)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"When X and Y are independent N(0,1):\")\nprint(f\"  Var(X) = {np.var(X_indep, ddof=1):.3f}\")\nprint(f\"  Var(Y) = {np.var(Y_indep, ddof=1):.3f}\")\nprint(f\"  Var(X-Y) = {np.var(diff_indep, ddof=1):.3f} ≈ Var(X) + Var(Y) = 2\")\nprint(\"\\nWhen Y = X (perfect dependence):\")\nprint(f\"  Var(X-Y) = Var(0) = 0\")\n\n\n\n\n\n\n\n\n\nWhen X and Y are independent N(0,1):\n  Var(X) = 1.007\n  Var(Y) = 1.002\n  Var(X-Y) = 2.026 ≈ Var(X) + Var(Y) = 2\n\nWhen Y = X (perfect dependence):\n  Var(X-Y) = Var(0) = 0\n\n\n\n\n\n\n\n\n\n\n\nExample: Variance of Binomial via Decomposition\n\n\n\nLet X \\sim \\text{Binomial}(n, p). We already know \\mathbb{E}(X) = np. What’s the variance?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWrite X = \\sum_{i=1}^n X_i where X_i \\sim \\text{Bernoulli}(p) independently.\nFor a single Bernoulli:\n\n\\mathbb{E}(X_i) = p\n\\mathbb{E}(X_i^2) = 0^2 \\cdot (1-p) + 1^2 \\cdot p = p\n\\mathbb{V}(X_i) = \\mathbb{E}(X_i^2) - (\\mathbb{E}(X_i))^2 = p - p^2 = p(1-p)\n\nSince the X_i are independent: \\mathbb{V}(X) = \\mathbb{V}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\mathbb{V}(X_i) = np(1-p)\nNote that variance is maximized when p = 1/2, which makes intuitive sense – there’s most uncertainty when success and failure are equally likely.\n\n\n\n\n\n\n\n\n\n\n\nMean and Variance of Common Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\\mathbb{E}[X]\n\\mathbb{V}(X)\nWhen to Use\n\n\n\n\nBernoulli(p)\np\np(1-p)\nSingle yes/no trial\n\n\nBinomial(n,p)\nnp\nnp(1-p)\nCount of successes\n\n\nPoisson(\\lambda)\n\\lambda\n\\lambda\nCount of rare events\n\n\nGeometric(p)\n1/p\n(1-p)/p^2\nTrials until success\n\n\nUniform(a,b)\n(a+b)/2\n(b-a)^2/12\nEqual likelihood\n\n\nNormal(\\mu,\\sigma^2)\n\\mu\n\\sigma^2\nSums of many effects\n\n\nExponential(\\beta)\n\\beta\n\\beta^2\nTime between events\n\n\nGamma(\\alpha,\\beta)\n\\alpha\\beta\n\\alpha\\beta^2\nSum of exponentials\n\n\nBeta(\\alpha,\\beta)\n\\alpha/(\\alpha+\\beta)\n\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\nProportions\n\n\nt_{\\nu}\n0 (if \\nu &gt; 1)\n\\nu/(\\nu-2) (if \\nu &gt; 2)\nHeavy-tailed data\n\n\n\\chi^2_p\np\n2p\nSum of squared normals",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#sample-mean-and-variance",
    "href": "chapters/02-expectation.html#sample-mean-and-variance",
    "title": "2  Expectation",
    "section": "2.6 Sample Mean and Variance",
    "text": "2.6 Sample Mean and Variance\nWhen we observe data, we compute sample statistics to estimate population parameters.\nRecall from our introduction: we have a sample X_1, \\ldots, X_n drawn from a population distribution F_X. The population has true parameters (like \\mu = \\mathbb{E}(X) and \\sigma^2 = \\mathbb{V}(X)) that we want to know, but we can only compute statistics from our finite sample. This gap between what we can calculate and what we want to know is fundamental to statistics.\n\nGiven random variables X_1, \\ldots, X_n:\nThe sample mean is: \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\nThe sample variance is: S_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2\n\nNote the n-1 in the denominator of the sample variance. This makes it an unbiased estimator of the population variance (see below).\n\nLet X_1, \\ldots, X_n be IID with \\mu = \\mathbb{E}(X_i) and \\sigma^2 = \\mathbb{V}(X_i). Then: \\mathbb{E}(\\bar{X}_n) = \\mu, \\quad \\mathbb{V}(\\bar{X}_n) = \\frac{\\sigma^2}{n}, \\quad \\mathbb{E}(S_n^2) = \\sigma^2\n\nThis theorem tells us:\n\nThe sample mean is unbiased (its expectation is equal to the population mean)\nIts variance decreases as n increases\nThe sample variance (with n-1) is unbiased\n\n\n\n\n\n\n\nWait, What Does “Unbiased” Mean?\n\n\n\n\n\nAn estimator or sample statistic is unbiased if its expected value equals the parameter it’s trying to estimate.\n\nUnbiased: \\mathbb{E}(\\text{estimator}) = \\text{true parameter}\nBiased: \\mathbb{E}(\\text{estimator}) \\neq \\text{true parameter}\n\nFor example:\n\nAs stated above, \\bar{X}_n is unbiased for \\mu because \\mathbb{E}(\\bar{X}_n) = \\mu\nS_n^2 (with n-1) is unbiased for \\sigma^2 because \\mathbb{E}(S_n^2) = \\sigma^2\nIf we used n instead of n-1 at the denominator, we’d get \\mathbb{E}(S_n^2) = \\frac{n-1}{n}\\sigma^2 &lt; \\sigma^2 (biased!)\n\nBeing unbiased means that on average across many sets of samples, our sample statistic would match the true value – though any individual estimate may be too high or too low. This also doesn’t tell us anything about the rate of convergence – how fast the estimator converges to the true value.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#covariance-and-correlation",
    "href": "chapters/02-expectation.html#covariance-and-correlation",
    "title": "2  Expectation",
    "section": "2.7 Covariance and Correlation",
    "text": "2.7 Covariance and Correlation\n\n2.7.1 Linear Relationships\nWhen we have two random variables, we often want to measure how they vary together and quantify the strength of their linear relation.\n\nLet X and Y be random variables with means \\mu_X and \\mu_Y and standard deviations \\sigma_X and \\sigma_Y. The covariance between X and Y is: \\mathrm{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)]\nThe correlation is: \\rho = \\rho_{X,Y} = \\rho(X, Y) = \\frac{\\mathrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\n\n\n2.7.2 Properties of Covariance and Correlation\n\nThe covariance can be rewritten as: \\mathrm{Cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\nThe correlation satisfies: -1 \\leq \\rho(X, Y) \\leq 1\n\nThe correlation is a sort of “normalized covariance”. By dividing the covariance by \\sigma_X and \\sigma_Y, we remove the magnitude (and units/scale) of the two random variables, and what remains is a pure number that measures of how much they change together on average, in a range from -1 to 1.\n\nCovariance and correlation further satisfy the following properties:\n\nIf Y = aX + b for constants a, b: \\rho(X, Y) = \\begin{cases}\n1 & \\text{if } a &gt; 0 \\\\\n-1 & \\text{if } a &lt; 0\n\\end{cases}\nIf X and Y are independent: \\mathrm{Cov}(X, Y) = \\rho = 0\nThe converse is NOT true in general!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon Misconception: Uncorrelated ≠ Independent!\nIndependence implies zero correlation, but zero correlation does NOT imply independence.\n\n\n\n\n\n\n\n\nExample: Uncorrelated but Dependent\n\n\n\n\n\nLet X \\sim \\text{Uniform}(-1, 1) and Y = X^2.\nThese two random variables are clearly dependent (knowing X determines Y exactly!), but:\n\\mathbb{E}(X) = 0 \\mathbb{E}(Y) = \\mathbb{E}(X^2) = \\int_{-1}^1 x^2 \\cdot \\frac{1}{2} \\, dx = \\frac{1}{3} \\mathbb{E}(XY) = \\mathbb{E}(X^3) = \\int_{-1}^1 x^3 \\cdot \\frac{1}{2} \\, dx = 0\nTherefore: \\mathrm{Cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) = 0 - 0 \\cdot \\frac{1}{3} = 0\nSo X and Y are uncorrelated despite being perfectly dependent!\nThe plot below shows X and Y. See also this article on Scientific American for more examples.\n\n\nShow code\n# Visualizing uncorrelated but dependent variables\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn = 2000\n\n# X ~ Uniform(-1, 1), Y = X²\nX = np.random.uniform(-1, 1, n)\nY = X**2\n\nplt.figure(figsize=(7, 4))\nplt.scatter(X, Y, alpha=0.5, s=10, color='blue')\nplt.xlabel('X')\nplt.ylabel('Y = X²')\nplt.title('Uncorrelated but Dependent: ρ(X,Y) = 0')\nplt.grid(True, alpha=0.3)\n\n# Add the parabola\nx_line = np.linspace(-1, 1, 100)\ny_line = x_line**2\nplt.plot(x_line, y_line, 'r-', linewidth=2, label='Y = X²')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(\"Y is completely determined by X, yet they are uncorrelated!\")\nprint(\"This is because the linear association is zero due to symmetry.\")\n\n\n\n\n\n\n\n\n\nY is completely determined by X, yet they are uncorrelated!\nThis is because the linear association is zero due to symmetry.\n\n\n\n\n\n\n\n2.7.3 Variance of Sums (General Case)\nWe saw in Section 2.5.2 that for independent variables,\n\\mathbb{V}\\left(\\sum_{i=1}^n a_i X_i\\right) = \\sum_{i=1}^n a_i^2 \\mathbb{V}(X_i) \\qquad \\text{(independent)}\nWe now generalize this result to any random variables, whether dependent or independent:\n\n\\mathbb{V}\\left(\\sum_{i=1}^n a_i X_i\\right) = \\sum_{i=1}^n a_i^2 \\mathbb{V}(X_i) + 2\\sum_{i=1}^n \\sum_{j=i+1}^n a_i a_j \\mathrm{Cov}(X_i, X_j)\n\nSpecial cases:\n\n\\mathbb{V}(X + Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) + 2\\mathrm{Cov}(X, Y)\n\\mathbb{V}(X - Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) - 2\\mathrm{Cov}(X, Y)\nWhen the variables are independent, check that this indeed reduces to the simpler formula for independent variables",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#expectation-with-matrices",
    "href": "chapters/02-expectation.html#expectation-with-matrices",
    "title": "2  Expectation",
    "section": "2.8 Expectation with Matrices",
    "text": "2.8 Expectation with Matrices\n\n2.8.1 Random Vectors\nIn multivariate settings – that is, involving multiple random variables –, we work with random vectors and their expectations.\nNotation: The mathematical convention is to work with column vectors. For example, we write\n\\mathbf{X} = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}\nrather than (X_1, X_2, X_3). The column vector can also be written as the transpose of a row vector: \\mathbf{X} = (X_1, X_2, X_3)^T.\n\nFor a random vector \\mathbf{X} = (X_1, \\ldots, X_k)^T:\nThe mean vector is: \\boldsymbol{\\mu} = \\mathbb{E}(\\mathbf{X}) = \\begin{pmatrix} \\mathbb{E}(X_1) \\\\ \\vdots \\\\ \\mathbb{E}(X_k) \\end{pmatrix}\nThe covariance matrix \\boldsymbol{\\Sigma} (also written \\mathbb{V}(\\mathbf{X})) is: \\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\mathbb{V}(X_1) & \\mathrm{Cov}(X_1, X_2) & \\cdots & \\mathrm{Cov}(X_1, X_k) \\\\\n\\mathrm{Cov}(X_2, X_1) & \\mathbb{V}(X_2) & \\cdots & \\mathrm{Cov}(X_2, X_k) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathrm{Cov}(X_k, X_1) & \\mathrm{Cov}(X_k, X_2) & \\cdots & \\mathbb{V}(X_k)\n\\end{bmatrix}\n\nThe inverse \\boldsymbol{\\Sigma}^{-1} is called the precision matrix.\nNotation: The uppercase Greek letter \\Sigma (sigma) is almost invariably used to denote a covariance matrix. Note that the lowercase \\sigma denotes the standard deviation.\n\n\n2.8.2 Covariance Matrix Properties\nThe covariance matrix can be written compactly as: \\boldsymbol{\\Sigma} = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]\nAn alternative formula for the covariance matrix is: \\boldsymbol{\\Sigma} = \\mathbb{E}(\\mathbf{X}\\mathbf{X}^T) - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T\n\n\n\n\n\n\nProof\n\n\n\n\n\nStarting from the definition: \\begin{align}\n\\boldsymbol{\\Sigma} &= \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T] \\\\\n&= \\mathbb{E}[\\mathbf{X}\\mathbf{X}^T - \\mathbf{X}\\boldsymbol{\\mu}^T - \\boldsymbol{\\mu}\\mathbf{X}^T + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T] \\\\\n&= \\mathbb{E}(\\mathbf{X}\\mathbf{X}^T) - \\mathbb{E}(\\mathbf{X})\\boldsymbol{\\mu}^T - \\boldsymbol{\\mu}\\mathbb{E}(\\mathbf{X}^T) + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T \\\\\n&= \\mathbb{E}(\\mathbf{X}\\mathbf{X}^T) - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T \\\\\n&= \\mathbb{E}(\\mathbf{X}\\mathbf{X}^T) - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T\n\\end{align} where we used the fact that \\mathbb{E}(\\mathbf{X}) = \\boldsymbol{\\mu} and the linearity of expectation.\n\n\n\nProperties:\n\nSymmetric: \\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T\nPositive semi-definite: \\mathbf{a}^T\\boldsymbol{\\Sigma}\\mathbf{a} \\geq 0 for all \\mathbf{a}\nDiagonal elements are variances (non-negative)\nOff-diagonal elements are covariances\n\n\n\n2.8.3 Linear Transformations\n\nIf \\mathbf{X} has mean \\boldsymbol{\\mu} and covariance \\boldsymbol{\\Sigma}, and \\mathbf{A} is a matrix: \\mathbb{E}(\\mathbf{A}\\mathbf{X}) = \\mathbf{A}\\boldsymbol{\\mu} \\mathbb{V}(\\mathbf{A}\\mathbf{X}) = \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T\n\n\n\n\n\n\nProof of the Variance Formula\n\n\n\n\n\nUsing the definition of variance for vectors and the fact that \\mathbb{E}(\\mathbf{A}\\mathbf{X}) = \\mathbf{A}\\boldsymbol{\\mu}: \\begin{align}\n\\mathbb{V}(\\mathbf{A}\\mathbf{X}) &= \\mathbb{E}[(\\mathbf{A}\\mathbf{X} - \\mathbf{A}\\boldsymbol{\\mu})(\\mathbf{A}\\mathbf{X} - \\mathbf{A}\\boldsymbol{\\mu})^T] \\\\\n&= \\mathbb{E}[\\mathbf{A}(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{A}(\\mathbf{X} - \\boldsymbol{\\mu}))^T] \\\\\n&= \\mathbb{E}[\\mathbf{A}(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T\\mathbf{A}^T] \\\\\n&= \\mathbf{A}\\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]\\mathbf{A}^T \\\\\n&= \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T\n\\end{align} where we used the fact that \\mathbf{A} is a constant matrix that can be taken outside the expectation.\n\n\n\nSimilarly, for a vector \\mathbf{a} (this is just a special case of the equations above – why?): \\mathbb{E}(\\mathbf{a}^T\\mathbf{X}) = \\mathbf{a}^T\\boldsymbol{\\mu} \\mathbb{V}(\\mathbf{a}^T\\mathbf{X}) = \\mathbf{a}^T\\boldsymbol{\\Sigma}\\mathbf{a}\n\n\n\n2.8.4 Interpreting the Covariance Matrix\nThe covariance matrix encodes the second-order structure of a random vector—that is, how the variables vary and co-vary together. To understand this structure, we can examine its spectral decomposition (eigendecomposition).\nIntuitiveMathematicalComputationalImagine your data as a cloud of points in space. This cloud rarely\nforms a perfect sphere—it’s usually stretched more in some directions\nthan others, like an ellipse or ellipsoid.The covariance matrix captures this shape:\nEigenvectors are the “natural axes” of your data\ncloud—the directions along which it stretches\nEigenvalues tell you how much the cloud stretches\nin each direction\nThe largest eigenvalue corresponds to the direction of greatest\nspread\nThis is like finding the best way to orient a box around your\ndata:\nThe box edges align with the eigenvectors\nThe box dimensions are proportional to the square roots of\neigenvalues\nPrincipal Component Analysis (PCA) uses this\ninsight: keep the directions with large spread (high variance), discard\nthose with little spread. This reduces dimensions while preserving most\nof the data’s structure.Since \\(\\boldsymbol{\\Sigma}\\) is\nsymmetric and positive semi-definite, recall from earlier linear algebra\nclasses that it has spectral decomposition:\n\\[\\boldsymbol{\\Sigma} = \\sum_{i=1}^k \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\]where:\n\\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_k \\geq 0\\)\nare the eigenvalues (which we can order from larger to\nsmaller)\n\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\)\nare the corresponding orthonormal eigenvectors\nThis decomposition reveals the geometric structure of the data:\nEigenvalues\n\\(\\lambda_i\\): represent the variance\nalong each principal axis\nEigenvectors\n\\(\\mathbf{v}_i\\): define the directions\nof these principal axes\nLargest eigenvalue/vector: indicates the direction\nof maximum variance in the data\nLet’s visualize how eigendecomposition reveals the structure of\ndata:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate correlated 2D data\nnp.random.seed(42)\nmean = [2, 3]\ncov = [[2.5, 1.5], \n       [1.5, 1.5]]\ndata = np.random.multivariate_normal(mean, cov, 300)\n\n# Compute eigendecomposition\neigenvalues, eigenvectors = np.linalg.eigh(cov)\n# Sort by eigenvalue (largest first)\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\n# Plot the data and principal axes\nplt.figure(figsize=(7, 6))\nplt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=30)\n\n# Plot eigenvectors from the mean\ncolors = ['red', 'blue']\nfor i in range(2):\n    # Scale eigenvector by sqrt(eigenvalue) for visualization\n    v = eigenvectors[:, i] * np.sqrt(eigenvalues[i]) * 1.96\n    plt.arrow(mean[0], mean[1], v[0], v[1], \n              head_width=0.1, head_length=0.1, \n              fc=colors[i], ec=colors[i], linewidth=2,\n              label=f'PC{i+1}: λ={eigenvalues[i]:.2f}')\n    \n    # Also draw the negative direction\n    plt.arrow(mean[0], mean[1], -v[0], -v[1], \n              head_width=0.1, head_length=0.1, \n              fc=colors[i], ec=colors[i], linewidth=2)\n\nplt.xlabel('X₁')\nplt.ylabel('X₂')\nplt.title('Principal Axes of the Covariance Matrix')\nplt.legend()\nplt.axis('equal')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Covariance matrix:\\n{np.array(cov)}\")\nprint(f\"\\nEigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors (as columns):\\n{eigenvectors}\")\nprint(f\"\\nThe first principal component explains {100*eigenvalues[0]/sum(eigenvalues):.1f}% of the variance\")\n\n\n\n\nCovariance matrix:\n[[2.5 1.5]\n [1.5 1.5]]\n\nEigenvalues: [3.58113883 0.41886117]\nEigenvectors (as columns):\n[[-0.81124219  0.58471028]\n [-0.58471028 -0.81124219]]\n\nThe first principal component explains 89.5% of the variance\n\nThe red arrow shows the first principal component (direction of\nmaximum variance), while the blue arrow shows the second. In the plot,\neach eigenvector \\(\\mathbf{v}_i\\) is\nrescaled by \\(1.96 \\sqrt{\\lambda_i}\\)\nwhich covers about \\(95 \\%\\) of the\nnormal distribution.In Principal Component Analysis (PCA), we might keep only the first\ncomponent to reduce from 2D to 1D while preserving most of the\nstructure.\n\n\n\n\n\n\nExample: Multinomial Covariance Structure\n\n\n\nThe multinomial distribution provides a concrete example of covariance matrix structure. If\n\\mathbf{X} = (X_1, \\ldots, X_k)^T \\sim \\text{Multinomial}(n, \\mathbf{p})\nwhere \\mathbf{p} = (p_1, \\ldots, p_k)^T with \\sum p_i = 1, then:\nMean vector: \\mathbb{E}(\\mathbf{X}) = n\\mathbf{p} = (np_1, \\ldots, np_k)^T\nCovariance matrix: \\boldsymbol{\\Sigma} = \\begin{pmatrix}\nnp_1(1-p_1) & -np_1p_2 & \\cdots & -np_1p_k \\\\\n-np_2p_1 & np_2(1-p_2) & \\cdots & -np_2p_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-np_kp_1 & -np_kp_2 & \\cdots & np_k(1-p_k)\n\\end{pmatrix}\nKey observations:\n\nDiagonal: \\mathbb{V}(X_i) = np_i(1-p_i) (same as binomial)\nOff-diagonal: \\mathrm{Cov}(X_i, X_j) = -np_ip_j for i \\neq j (always negative!)\nIntuition: If more outcomes fall in category i, fewer can fall in category j\n\nSpecial case: For a die roll with equal probabilities (p_i = 1/6 for all i):\n\n\\mathbb{V}(X_i) = n \\cdot \\frac{1}{6} \\cdot \\frac{5}{6} = \\frac{5n}{36}\n\\mathrm{Cov}(X_i, X_j) = -n \\cdot \\frac{1}{6} \\cdot \\frac{1}{6} = -\\frac{n}{36} for i \\neq j",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#conditional-expectation",
    "href": "chapters/02-expectation.html#conditional-expectation",
    "title": "2  Expectation",
    "section": "2.9 Conditional Expectation",
    "text": "2.9 Conditional Expectation\n\n2.9.1 Expectation Given Information\nConditional expectation captures how the mean changes when we have additional information. It is computed similarly to a regular expectation, just replacing the pdf (or PMF) with a conditional pdf (or PMF).\n\nThe conditional expectation of X given Y = y is: \\mathbb{E}(X | Y = y) = \\begin{cases}\n\\sum_x x \\mathbb{P}_{X|Y}(x|y) & \\text{discrete case} \\\\\n\\int x f_{X|Y}(x|y) \\, dx & \\text{continuous case}\n\\end{cases}\n\n\n\n\n\n\n\nWarning\n\n\n\nSubtle but Important:\n\n\\mathbb{E}(X) is a number\n\\mathbb{E}(X | Y = y) is a number (for fixed y)\n\\mathbb{E}(X | Y) is a random variable (because it’s a function of Y!)\n\n\n\n\n\n2.9.2 Properties of Conditional Expectation\n\n\\mathbb{E}[\\mathbb{E}(Y | X)] = \\mathbb{E}(Y)\nMore generally, for any function r(x, y): \\mathbb{E}[\\mathbb{E}(r(X, Y) | X)] = \\mathbb{E}(r(X, Y))\n\n\n\n\n\n\n\nProof of the Law of Iterated Expectations\n\n\n\n\n\nWe’ll prove the first equation. Using the definition of conditional expectation and the fact that the joint pdf can be written as f(x, y) = f_X(x) f_{Y|X}(y|x):\n\\begin{align}\n\\mathbb{E}[\\mathbb{E}(Y | X)] &= \\mathbb{E}\\left[\\int y f_{Y|X}(y|X) \\, dy\\right] \\\\\n&= \\int \\left[\\int y f_{Y|X}(y|x) \\, dy\\right] f_X(x) \\, dx \\\\\n&= \\int \\int y f_{Y|X}(y|x) f_X(x) \\, dy \\, dx \\\\\n&= \\int \\int y f(x, y) \\, dy \\, dx \\\\\n&= \\int y \\left[\\int f(x, y) \\, dx\\right] dy \\\\\n&= \\int y f_Y(y) \\, dy \\\\\n&= \\mathbb{E}(Y)\n\\end{align}\nThe key steps are:\n\n\\mathbb{E}(Y|X) is a function of X, so we take its expectation with respect to X\nWe can interchange the order of integration\nf_{Y|X}(y|x) f_X(x) = f(x,y) by the definition of conditional probability\nIntegrating the joint pdf over x gives the marginal pdf f_Y(y)\n\n\n\n\nThis powerful result lets us compute expectations by conditioning on useful information.\n\n\n\n\n\n\nExample: Breaking Stick Revisited\n\n\n\nImagine placing two random points on a unit stick. First, we place point X uniformly at random. Then, we place point Y uniformly at random between X and the end of the stick.\nFormally: Draw X \\sim \\text{Uniform}(0, 1). After observing X = x, draw Y | X = x \\sim \\text{Uniform}(x, 1).\nQuestion: What is the expected position of the second point Y?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe could find the marginal distribution of Y (which is complex), but it’s easier to use conditional expectation:\nFirst, find \\mathbb{E}(Y | X = x): \\mathbb{E}(Y | X = x) = \\frac{x + 1}{2} This makes sense: given X = x, point Y is uniform on (x, 1), so its expected position is the midpoint.\nSo \\mathbb{E}(Y | X) = \\frac{X + 1}{2} (a random variable).\nNow use iterated expectations: \\mathbb{E}(Y) = \\mathbb{E}[\\mathbb{E}(Y | X)] = \\mathbb{E}\\left[\\frac{X + 1}{2}\\right] = \\frac{\\mathbb{E}(X) + 1}{2} = \\frac{1/2 + 1}{2} = \\frac{3}{4}\nThe second point lands, on average, at position 3/4 along the stick.\n\n\n\n\n\n\n\n2.9.3 Conditional Variance\n\nThe conditional variance is: \\mathbb{V}(Y | X = x) = \\mathbb{E}[(Y - \\mathbb{E}(Y | X = x))^2 | X = x]\n\n\n\\mathbb{V}(Y) = \\mathbb{E}[\\mathbb{V}(Y | X)] + \\mathbb{V}[\\mathbb{E}(Y | X)]\n\nThis decomposition says: Total variance = Average within-group variance + Between-group variance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#more-about-the-normal-distribution",
    "href": "chapters/02-expectation.html#more-about-the-normal-distribution",
    "title": "2  Expectation",
    "section": "2.10 More About the Normal Distribution",
    "text": "2.10 More About the Normal Distribution\n\n2.10.1 Quick Recap\nRecall that if X \\sim \\mathcal{N}(\\mu, \\sigma^2), then:\n\nPDF: f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\nMean: \\mathbb{E}(X) = \\mu\nVariance: \\mathbb{V}(X) = \\sigma^2\n\nThe normal distribution plays a central role in statistics due to the Central Limit Theorem (Chapter 3) and its many convenient mathematical properties.\n\n\n2.10.2 Entropy of the Normal Distribution\nWe can use the expectation to compute the entropy of the normal distribution.\n\n\n\n\n\n\nExample: Normal Distribution Entropy\n\n\n\nThe differential entropy of a continuous random variable measures the average uncertainty in the distribution: H(X) = \\mathbb{E}[-\\ln f_X(X)] = -\\int f_X(x) \\ln f_X(x) \\, dx\nLet’s calculate this for X \\sim \\mathcal{N}(\\mu, \\sigma^2). First, find -\\ln f_X(x): -\\ln f_X(x) = \\ln(\\sqrt{2\\pi\\sigma^2}) + \\frac{(x-\\mu)^2}{2\\sigma^2} = \\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\frac{(x-\\mu)^2}{2\\sigma^2}\nNow compute the expectation: \\begin{align}\nH(X) &= \\mathbb{E}[-\\ln f_X(X)] \\\\\n&= \\mathbb{E}\\left[\\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\frac{(X-\\mu)^2}{2\\sigma^2}\\right] \\\\\n&= \\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\mathbb{E}[(X-\\mu)^2] \\\\\n&= \\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\cdot \\sigma^2 \\\\\n&= \\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\frac{1}{2} \\\\\n&= \\frac{1}{2}\\ln(2\\pi e\\sigma^2) \\\\\n&= \\ln(\\sqrt{2\\pi e\\sigma^2})\n\\end{align}\nKey insights:\n\nThe entropy increases with \\sigma (more spread = more uncertainty)\nAmong all distributions with fixed variance \\sigma^2, the normal has maximum entropy\n\n\n\n\n\n2.10.3 Multivariate Normal Properties\nThe d-dimensional multivariate normal is parametrized by mean vector \\boldsymbol{\\mu} \\in \\mathbb{R}^d and covariance matrix \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d} (symmetric and positive definite).\nFor \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}):\n\nIndependence and diagonal covariance: Components X_i and X_j are independent if and only if \\Sigma_{ij} = 0. Thus, the components are mutually independent if and only if \\boldsymbol{\\Sigma} is diagonal.\nStandard multivariate normal: If Z_1, \\ldots, Z_d \\sim \\mathcal{N}(0, 1) are independent, then \\mathbf{Z} = (Z_1, \\ldots, Z_d)^T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d), where \\mathbf{I}_d is the d \\times d identity matrix.\nMarginals are normal: Each X_i \\sim \\mathcal{N}(\\mu_i, \\Sigma_{ii})\nLinear combinations are normal: For any vector \\mathbf{a}, \\mathbf{a}^T\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{a}^T\\boldsymbol{\\mu}, \\mathbf{a}^T\\boldsymbol{\\Sigma}\\mathbf{a})\nConditionals are normal: Suppose we partition: \\mathbf{X} = \\begin{pmatrix} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\end{pmatrix}, \\quad\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\quad\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}\nThen the conditional distribution of \\mathbf{X}_2 given \\mathbf{X}_1 = \\mathbf{x}_1 is: \\mathbf{X}_2 | \\mathbf{X}_1 = \\mathbf{x}_1 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{2|1}, \\boldsymbol{\\Sigma}_{2|1}) where:\n\nConditional mean: \\boldsymbol{\\mu}_{2|1} = \\boldsymbol{\\mu}_2 + \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}_{11}^{-1}(\\mathbf{x}_1 - \\boldsymbol{\\mu}_1)\nConditional covariance: \\boldsymbol{\\Sigma}_{2|1} = \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}_{11}^{-1}\\boldsymbol{\\Sigma}_{12}\n\nNote that the conditional covariance doesn’t depend on the observed value \\mathbf{x}_1!\n\nThese properties are used in multiple algorithms and methods in statistics, including for example:\n\nGaussian processes\nKalman filtering\nLinear regression theory\nMultivariate statistical methods\n\n\n\n\n\n\n\nCholesky Decomposition\n\n\n\nEvery symmetric positive definite covariance matrix \\boldsymbol{\\Sigma} can be decomposed as: \\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^T where \\mathbf{L} is a lower-triangular matrix called the Cholesky decomposition (or Cholesky factor).\nThis decomposition is crucial for:\n\nSimulating multivariate normals: If \\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) and \\mathbf{X} = \\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z}, then \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\nTransforming to standard form: If \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), then \\mathbf{L}^{-1}(\\mathbf{X} - \\boldsymbol{\\mu}) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\nEfficient computation: Solving linear systems and computing determinants\n\n\n\n\n\n\n\n\n\nExample: Generating Multivariate Normal Random Vectors via Cholesky\n\n\n\nLet’s see how the Cholesky decomposition can be used to transform independent standard normals into correlated multivariate normals.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\n# Define mean and covariance\nmu = np.array([1, 2])\nSigma = np.array([[2, 1.2], \n                  [1.2, 1]])\n\n# Compute Cholesky decomposition\nL = np.linalg.cholesky(Sigma)\nprint(\"Covariance matrix Σ:\")\nprint(Sigma)\nprint(\"\\nCholesky factor L (lower triangular):\")\nprint(L)\nprint(\"\\nVerification: L @ L.T =\")\nprint(L @ L.T)\n\n# Generate samples step by step\nnp.random.seed(42)\nn_samples = 500\n\n# Step 1: Generate independent standard normals\nZ = np.random.standard_normal((n_samples, 2))  # N(0, I)\n\n# Step 2: Transform using Cholesky\nX = mu + Z @ L.T  # Transform to N(mu, Sigma)\n\n# Visualize the transformation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n\n# Plot independent standard normals\nax1.scatter(Z[:, 0], Z[:, 1], alpha=0.5, s=20, color='blue')\nax1.set_xlabel('Z₁')\nax1.set_ylabel('Z₂')\nax1.set_title('Independent N(0,1)')\nax1.set_xlim(-4, 4)\nax1.set_ylim(-4, 4)\nax1.grid(True, alpha=0.3)\nax1.axis('equal')\n\n# Plot transformed correlated normals\nax2.scatter(X[:, 0], X[:, 1], alpha=0.5, s=20, color='red')\n\n# Add confidence ellipse\neigenvalues, eigenvectors = np.linalg.eigh(Sigma)\nangle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\nellipse = Ellipse(mu, 2*np.sqrt(eigenvalues[1]), 2*np.sqrt(eigenvalues[0]),\n                  angle=angle, facecolor='none', edgecolor='red', linewidth=2)\nax2.add_patch(ellipse)\n\nax2.set_xlabel('X₁')\nax2.set_ylabel('X₂')\nax2.set_title('Correlated N(μ, Σ)')\nax2.grid(True, alpha=0.3)\nax2.axis('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nThe Cholesky decomposition transforms:\")\nprint(f\"• Independent standard normals Z ~ N(0, I)\")\nprint(f\"• Into correlated normals X = μ + LZ ~ N(μ, Σ)\")\nprint(f\"\\nSample covariance:\\n{np.cov(X.T)}\")\n\n\nCovariance matrix Σ:\n[[2.  1.2]\n [1.2 1. ]]\n\nCholesky factor L (lower triangular):\n[[1.41421356 0.        ]\n [0.84852814 0.52915026]]\n\nVerification: L @ L.T =\n[[2.  1.2]\n [1.2 1. ]]\n\nThe Cholesky decomposition transforms:\n• Independent standard normals Z ~ N(0, I)\n• Into correlated normals X = μ + LZ ~ N(μ, Σ)\n\nSample covariance:\n[[1.87031161 1.10325785]\n [1.10325785 0.92611656]]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#chapter-summary-and-connections",
    "href": "chapters/02-expectation.html#chapter-summary-and-connections",
    "title": "2  Expectation",
    "section": "2.11 Chapter Summary and Connections",
    "text": "2.11 Chapter Summary and Connections\n\n2.11.1 Key Concepts Review\nWe’ve explored the fundamental tools for summarizing and understanding random variables:\n\nExpectation as weighted average: The fundamental summary of a distribution\nLinearity—the master property: \\mathbb{E}(\\sum a_i X_i) = \\sum a_i \\mathbb{E}(X_i) always works!\nVariance measures spread: How far from the mean should we expect outcomes?\nCovariance measures linear relationships: Do variables move together?\nConditional expectation as best prediction: What’s our best guess given information?\nMatrix operations extend naturally: Same concepts work for random vectors\n\n\n\n2.11.2 Why These Concepts Matter\nFor Statistical Inference:\n\nSample means estimate population expectations\nVariance quantifies uncertainty in estimates\nCovariance reveals relationships between variables\nConditional expectation enables regression analysis\n\nFor Machine Learning:\n\nLoss functions are expectations over data distributions\nGradient descent minimizes expected loss\nFeature correlations affect model performance\nConditional expectations define optimal predictors\n\nFor Data Science Practice:\n\nSummary statistics (mean, variance) describe data concisely\nCorrelation analysis reveals variable relationships\nVariance decomposition explains data structure\nLinear algebra connects to dimensionality reduction (PCA)\n\n\n\n2.11.3 Common Pitfalls to Avoid\n\nAssuming \\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y) without independence\n\nThis only works when X and Y are independent!\n\nConfusing the \\mathbb{V}(X - Y) formula\n\nRemember: \\mathbb{V}(X - Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) when independent (plus, not minus!)\n\nTreating \\mathbb{E}(X|Y) as a number instead of a random variable\n\n\\mathbb{E}(X|Y = y) is a number, but \\mathbb{E}(X|Y) is a function of Y\n\nAssuming uncorrelated means independent\n\nZero correlation does NOT imply independence (remember X and X^2)\n\nForgetting existence conditions\n\nNot all distributions have finite expectation (Cauchy!)\n\n\n\n\n2.11.4 Chapter Connections\nThis chapter builds on Chapter 1’s probability foundations and provides essential tools for all statistical inference:\n\nFrom Chapter 1: We’ve formalized the expectation concept briefly introduced with random variables, showing how it connects to supervised learning through risk minimization and cross-entropy loss\nNext - Chapter 3 (Convergence & Inference): The sample mean and variance we studied will be shown to converge to population values (Law of Large Numbers) and have predictable distributions (Central Limit Theorem), justifying their use as estimators\nChapter 4 (Bootstrap): We’ll use the plug-in principle with empirical distributions to estimate variances and other functionals when theoretical calculations become intractable\nFuture Applications: Conditional expectation forms the foundation for regression (Chapter 5+), while variance decomposition and covariance matrices are central to multivariate methods throughout the course\n\n\n\n2.11.5 Self-Test Problems\nTry these problems to test your understanding:\n\nLinearity puzzle: In a class of 30 students, each has probability 1/365 of having a birthday today. What’s the expected number of birthdays today? (Ignore leap years) Hint: Define indicator variables X_i for each student. What is \\mathbb{E}(X_i)? Then use linearity.\nVariance with correlation: If \\mathbb{V}(X) = 4, \\mathbb{V}(Y) = 9, and \\rho(X,Y) = 0.5, find \\mathbb{V}(2X - Y).\nConditional expectation: Toss a fair coin. If heads, draw X \\sim \\mathcal{N}(0, 1). If tails, draw X \\sim \\mathcal{N}(2, 1). Find \\mathbb{E}(X) and \\mathbb{V}(X).\nMatrix expectation: If \\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_2) and \\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}, find the distribution of \\mathbf{AX}.\n\n\n\n2.11.6 Python and R Reference\nPythonRimport numpy as np\nfrom scipy import stats\n\n# Basic expectations\nx = np.array([1, 2, 3, 4, 5])\nprobabilities = np.array([0.1, 0.2, 0.3, 0.3, 0.1])\n\n# Discrete expectation\nmean = np.sum(x * probabilities)\nvariance = np.sum((x - mean)**2 * probabilities)\n\n# Sample statistics\ndata = np.random.normal(100, 15, 1000)\nsample_mean = np.mean(data)\nsample_var = np.var(data, ddof=1)  # ddof=1 for unbiased\nsample_std = np.std(data, ddof=1)\n\n# Covariance and correlation\nx, y = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 1000).T\ncovariance = np.cov(x, y)[0, 1]  # or np.cov(x, y, ddof=1)\ncorrelation = np.corrcoef(x, y)[0, 1]\n\n# Matrix operations\nmean_vector = np.array([1, 2])\ncov_matrix = np.array([[2, 0.5], [0.5, 1]])\nsamples = np.random.multivariate_normal(mean_vector, cov_matrix, 1000)\n\n# Linear transformation\nA = np.array([[1, 1], [1, -1]])\ntransformed_mean = A @ mean_vector\ntransformed_cov = A @ cov_matrix @ A.T# Basic expectations\nx &lt;- 1:5\nprobabilities &lt;- c(0.1, 0.2, 0.3, 0.3, 0.1)\n\n# Discrete expectation\nmean_val &lt;- sum(x * probabilities)\nvariance_val &lt;- sum((x - mean_val)^2 * probabilities)\n\n# Sample statistics\ndata &lt;- rnorm(1000, mean = 100, sd = 15)\nsample_mean &lt;- mean(data)\nsample_var &lt;- var(data)  # Automatically uses n-1\nsample_sd &lt;- sd(data)\n\n# Covariance and correlation\nlibrary(MASS)\nsamples &lt;- mvrnorm(1000, mu = c(0, 0), \n                   Sigma = matrix(c(1, 0.5, 0.5, 1), 2, 2))\ncovariance &lt;- cov(samples[,1], samples[,2])\ncorrelation &lt;- cor(samples[,1], samples[,2])\n\n# Matrix operations\nmean_vector &lt;- c(1, 2)\ncov_matrix &lt;- matrix(c(2, 0.5, 0.5, 1), 2, 2)\n\n# Linear transformation\nA &lt;- matrix(c(1, 1, 1, -1), 2, 2, byrow = TRUE)\ntransformed_mean &lt;- A %*% mean_vector\ntransformed_cov &lt;- A %*% cov_matrix %*% t(A)\n\n\n2.11.7 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\nThis table maps sections in these lecture notes to the corresponding sections in Wasserman (2013) (“All of Statistics” or AoS).\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding AoS Section(s)\n\n\n\n\nIntroduction and Motivation\nExpanded material from the slides, contextualizing expectation for machine learning.\n\n\nFoundations of Expectation\n\n\n\n↳ Definition and Basic Properties\nAoS §3.1\n\n\n↳ Existence of Expectation\nAoS §3.1\n\n\n↳ Expectation of Functions\nAoS §3.1 (Rule of the Lazy Statistician)\n\n\nProperties of Expectation\n\n\n\n↳ The Linearity Property\nAoS §3.2 (Theorem 3.11)\n\n\n↳ Independence and Products\nAoS §3.2 (Theorem 3.13)\n\n\nVariance and Its Properties\n\n\n\n↳ Measuring Spread\nAoS §3.3 (Definition 3.14)\n\n\n↳ Properties of Variance\nAoS §3.3 (Theorem 3.15)\n\n\nSample Mean and Variance\nAoS §3.3 (Definitions and Theorem 3.17)\n\n\nCovariance and Correlation\n\n\n\n↳ Linear Relationships\nAoS §3.3 (Definition 3.18)\n\n\n↳ Properties of Covariance and Correlation\nAoS §3.3 (Theorem 3.19)\n\n\n↳ Variance of Sums (General Case)\nAoS §3.3 (Theorem 3.20)\n\n\nExpectation with Matrices\n\n\n\n↳ Random Vectors & Covariance Matrix\nAoS §3.4, §14.1\n\n\n↳ Linear Transformations\nAoS §3.4 (Lemma 3.21)\n\n\n↳ Interpreting the Covariance Matrix\nNew material (connects to PCA).\n\n\n↳ Example: Multinomial Covariance\nAoS §3.4\n\n\nConditional Expectation\n\n\n\n↳ Expectation Given Information\nAoS §3.5 (Definition 3.22)\n\n\n↳ Properties (Iterated Expectations)\nAoS §3.5 (Theorem 3.24)\n\n\n↳ Conditional Variance (Total Variance)\nAoS §3.5 (Definition 3.26, Theorem 3.27)\n\n\nMore About the Normal Distribution\nNew material, applying expectation concepts. Some properties from AoS §2.10 are revisited.\n\n\nChapter Summary and Connections\nNew summary material.\n\n\n\n\n\n\n\n\n2.11.8 Further Reading\n\nStatistical perspective: Casella & Berger, “Statistical Inference”\nMachine learning view: Bishop, “Pattern Recognition and Machine Learning” Chapters 1 and 2\nMatrix cookbook: Petersen & Pedersen, “The Matrix Cookbook” (for multivariate formulas) – link\n\n\nNext time you compute a sample mean, remember: you’re estimating an expectation. When you minimize a loss function, you’re approximating an expected loss. The gap between what we can compute (sample statistics) and what we want to know (population parameters) drives all of statistical inference. Expectation is the bridge!\n\n\n\n\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/02-expectation.html#footnotes",
    "href": "chapters/02-expectation.html#footnotes",
    "title": "2  Expectation",
    "section": "",
    "text": "Almost surely (a.s.) means “with probability 1”. A random variable is almost surely constant if \\mathbb{P}(X = c) = 1 for some constant c. The “almost” acknowledges that technically there could be probability-0 events where X \\neq c, but these never occur in practice.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expectation</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html",
    "href": "chapters/03-convergence-inference.html",
    "title": "3  Convergence and The Basics of Inference",
    "section": "",
    "text": "3.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#learning-objectives",
    "href": "chapters/03-convergence-inference.html#learning-objectives",
    "title": "3  Convergence and The Basics of Inference",
    "section": "",
    "text": "Explain how probability inequalities provide bounds on uncertainty.\nDefine concepts of probabilistic convergence and apply the Law of Large Numbers and Central Limit Theorem.\nDefine the core vocabulary of statistical inference (models, parameters, estimators).\nEvaluate an estimator’s quality using its standard error, bias, and variance.\nExplain the bias-variance tradeoff in the context of Mean Squared Error (MSE).\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter covers probability inequalities, convergence concepts, and the foundations of statistical inference. The material is adapted from Chapters 4, 5, and 6 of Wasserman (2013), supplemented with additional examples and perspectives relevant to data science applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#introduction-and-motivation",
    "href": "chapters/03-convergence-inference.html#introduction-and-motivation",
    "title": "3  Convergence and The Basics of Inference",
    "section": "3.2 Introduction and Motivation",
    "text": "3.2 Introduction and Motivation\n\n3.2.1 Convergence Matters to Understand Machine Learning Algorithms\nDeep learning models are trained with stochastic optimization algorithms. These algorithms produce a sequence of parameter estimates  \\theta_1, \\theta_2, \\theta_3, \\ldots\nas they iterate through the data. But here’s the fundamental question: do these estimates eventually converge to a good solution, and how do we establish that?\nThe challenge is that these parameter estimates are random variables – they depend on random initialization, random mini-batch selection, and random data shuffling. We can’t use the simple definition of convergence where |x_n - x| &lt; \\epsilon for all large n, which you may remember from calculus. We need new mathematical tools.\nThis chapter develops the language of probabilistic convergence to understand and analyze such algorithms. We’ll then use these tools to build the foundation of statistical inference – the science of drawing conclusions about populations from samples.\nConsider a concrete example: training a neural network for image classification. At each iteration t:\n\nPick a random subset S of training images\nCompute the gradient g = \\sum_{x_i \\in S} \\nabla_\\theta L(\\theta_t; x_i) of the loss1\nCompute next estimate of the model parameters, \\theta_{t+1}, using g and the current parameters2\n\nThe randomness in batch selection makes each \\theta_t a random variable. As mentioned before, ideally we would want \\theta_1, \\theta_2, \\ldots to converge to a good solution. But what does it even mean to say the algorithm “converges”? This chapter provides the answer.\nConvergence isn’t just about optimization algorithms. It’s central to all of statistics:\n\nWhen we compute a sample mean with increasing amount of data, does it converge to the population mean?\nMore generally, when we estimate a model parameter, does our estimate improve with more data?\nWhen we approximate a distribution, does the approximation get better?\n\nThe remarkable answers to these questions – provided by the Law of Large Numbers and Central Limit Theorem – form the theoretical backbone of statistical inference and machine learning.\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nMarkov’s inequality\nMarkovin epäyhtälö\nBounds probability of large values\n\n\nChebyshev’s inequality\nTšebyšovin epäyhtälö\nUses variance to bound deviations\n\n\nConvergence in probability\nStokastinen suppeneminen\nRandom variable settling to a value\n\n\nConvergence in distribution\nJakaumasuppeneminen\nDistribution shape converging\n\n\nLaw of Large Numbers\nSuurten lukujen laki\nSample mean → population mean\n\n\nCentral Limit Theorem\nKeskeinen raja-arvolause\nSums become normally distributed\n\n\nStatistical model\nTilastollinen malli\nSet of possible distributions\n\n\nParametric model\nParametrinen malli\nFinite-dimensional parameter space\n\n\nNonparametric model\nEpäparametrinen malli\nInfinite-dimensional space\n\n\nNuisance parameter\nKiusaparametri\nParameter not of primary interest\n\n\nPoint estimation\nPiste-estimointi\nSingle best guess of parameter\n\n\nEstimator\nEstimaattori\nFunction of data estimating parameter\n\n\nBias\nHarha\nExpected error of estimator\n\n\nUnbiased\nHarhaton\nZero expected error\n\n\nConsistent\nTarkentuva\nConverges to true value\n\n\nStandard error\nKeskivirhe\nStandard deviation of estimator\n\n\nMean Squared Error (MSE)\nKeskimääräinen neliövirhe\nAverage squared error\n\n\nSampling distribution\nOtantajakauma\nDistribution of the estimator",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#inequalities-bounding-the-unknown",
    "href": "chapters/03-convergence-inference.html#inequalities-bounding-the-unknown",
    "title": "3  Convergence and The Basics of Inference",
    "section": "3.3 Inequalities: Bounding the Unknown",
    "text": "3.3 Inequalities: Bounding the Unknown\n\n3.3.1 Why We Need Inequalities\nIn probability and statistics, we often encounter quantities that are difficult or impossible to compute exactly. Inequalities provide bounds – upper or lower limits – that give us useful information even when exact calculations are intractable. They serve three critical purposes:\n\nBounding quantities: When we can’t compute a probability exactly, an upper bound tells us it’s “at most this large”\nProving theorems: The Law of Large Numbers and Central Limit Theorem rely on inequalities in their proofs\nPractical guarantees: In machine learning, we use inequalities to create bounds on critical quantities such as generalization error3\n\nThink of inequalities as providing universal statistical guarantees. They tell us that no matter how complicated the underlying distribution, certain bounds will always hold.\n\n\n3.3.2 Markov’s Inequality\n\nMarkov’s Inequality: For a non-negative random variable X with finite expectation: \\mathbb{P}(X \\geq t) \\leq \\frac{\\mathbb{E}(X)}{t} \\quad \\text{for all } t &gt; 0\n\nThis remarkably simple inequality says that – no matter what – the probability of a non-negative random variable exceeding a threshold t is bounded by its mean divided by t.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince X \\geq 0: \\begin{align}\n\\mathbb{E}(X) &= \\int_0^{\\infty} x f(x) \\, dx \\\\\n&= \\int_0^t x f(x) \\, dx + \\int_t^{\\infty} x f(x) \\, dx \\\\\n&\\geq \\int_t^{\\infty} x f(x) \\, dx \\\\\n&\\geq t \\int_t^{\\infty} f(x) \\, dx \\\\\n&= t \\mathbb{P}(X \\geq t)\n\\end{align}\nRearranging gives the result.\n\n\n\n\n\n\n\n\n\nExample: Exceeding a Multiple of the Mean\n\n\n\nLet X be a non-negative random variable with mean \\mathbb{E}(X) = \\mu. What can we say about the probability that X exceeds k times its mean, for some k &gt; 1?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing Markov’s inequality by setting t = k\\mu: \\mathbb{P}(X \\geq k\\mu) \\leq \\frac{\\mathbb{E}(X)}{k\\mu} = \\frac{\\mu}{k\\mu} = \\frac{1}{k}\nFor example, the probability of a non-negative random variable exceeding twice its mean is at most 1/2. The probability of it exceeding 10 times its mean is at most 1/10. This universal bound is surprisingly useful.\n\n\n\n\n\n\n\n\n\n\n\nExample: Exam Scores\n\n\n\nIf the average exam score is 50 points, what’s the maximum probability that a randomly selected student scored 90 or more?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing Markov’s inequality: \\mathbb{P}(X \\geq 90) \\leq \\frac{50}{90} = \\frac{5}{9} \\approx 0.556\nAt most 55.6% of students can score 90 or more. This bound requires only knowing the average – no other information about the distribution!\n\n\n\n\n\nIntuitiveMathematicalComputationalMarkov’s inequality captures a fundamental truth: averages\nconstrain extremes.Imagine a village where the average wealth is €50,000. What fraction\nof villagers could be millionaires? If everyone were a millionaire, the\naverage would be at least €1,000,000. Since the average is only €50,000,\nat most 5% can be millionaires:\n\\[\\text{Fraction of millionaires} \\leq \\frac{€50,000}{€1,000,000} = 0.05\\]This reasoning works for any non-negative quantity: test scores,\nwaiting times, file sizes, or loss values in machine learning. The\naverage puts a hard limit on how often extreme values can occur.Markov’s inequality is the foundation for many other inequalities.\nIts power lies in its generality—it applies to any non-negative random\nvariable with finite expectation.The inequality is tight (best possible) for certain distributions.\nConsider: \\[X = \\begin{cases}\n0 & \\text{with probability } 1 - \\frac{\\mu}{t} \\\\\nt & \\text{with probability } \\frac{\\mu}{t}\n\\end{cases}\\]Then \\(\\mathbb{E}(X) = \\mu\\) and\n\\(\\mathbb{P}(X \\geq t) = \\frac{\\mu}{t}\\),\nachieving equality in Markov’s inequality.Let’s visualize Markov’s inequality by comparing the true tail\nprobability with the bound for an exponential distribution.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set up the exponential distribution\nbeta = 2  # mean = 2\nx = np.linspace(0, 10, 1000)\npdf = stats.expon.pdf(x, scale=beta)\n\n# Compute true probabilities and Markov bounds\nt_values = np.linspace(0.5, 10, 100)\ntrue_probs = 1 - stats.expon.cdf(t_values, scale=beta)\nmarkov_bounds = np.minimum(beta / t_values, 1)  # E[X]/t, capped at 1\n\n# Create the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n\n# Left plot: PDF with shaded tail\nt_example = 5\nax1.plot(x, pdf, 'b-', linewidth=2, label='Exponential(2) PDF')\nax1.fill_between(x[x &gt;= t_example], pdf[x &gt;= t_example], alpha=0.3, color='red', \n                 label=f'P(X ≥ {t_example})')\nax1.axvline(beta, color='green', linestyle='--', linewidth=2, label=f'E[X] = {beta}')\nax1.set_xlabel('x')\nax1.set_ylabel('Probability density')\nax1.set_title('Exponential Distribution')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right plot: True probability vs Markov bound\nax2.plot(t_values, true_probs, 'b-', linewidth=2, label='True P(X ≥ t)')\nax2.plot(t_values, markov_bounds, 'r--', linewidth=2, label='Markov bound E[X]/t')\nax2.set_xlabel('t')\nax2.set_ylabel('Probability')\nax2.set_title('Markov\\'s Inequality for Exponential(2)')\nax2.legend()\nax2.grid(True, alpha=0.3)\nax2.set_ylim(0, 1.1)\n\nplt.tight_layout()\nplt.show()\n\n# Numerical comparison at specific points\nprint(\"Comparison of true probability vs Markov bound:\")\nfor t in [1, 2, 4, 8]:\n    true_p = 1 - stats.expon.cdf(t, scale=beta)\n    markov_p = beta / t\n    print(f\"t = {t}: True P(X ≥ {t}) = {true_p:.4f}, Markov bound = {markov_p:.4f}\")\n\n\n\n\nComparison of true probability vs Markov bound:\nt = 1: True P(X ≥ 1) = 0.6065, Markov bound = 2.0000\nt = 2: True P(X ≥ 2) = 0.3679, Markov bound = 1.0000\nt = 4: True P(X ≥ 4) = 0.1353, Markov bound = 0.5000\nt = 8: True P(X ≥ 8) = 0.0183, Markov bound = 0.2500\n\nNotice that the Markov bound is always valid but often loose. It\nbecomes tighter as \\(t\\) increases\nrelative to the mean.\n\n\n3.3.3 Chebyshev’s Inequality\nWhile Markov’s inequality uses only the mean, Chebyshev’s inequality leverages the variance to provide a tighter bound on deviations from the mean.\n\nChebyshev’s Inequality: Let X have mean \\mu and variance \\sigma^2. Then: \\mathbb{P}(|X - \\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2} \\quad \\text{for all } t &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nApply Markov’s inequality to the non-negative random variable (X - \\mu)^2: \\mathbb{P}(|X - \\mu| \\geq t) = \\mathbb{P}((X - \\mu)^2 \\geq t^2) \\leq \\frac{\\mathbb{E}[(X - \\mu)^2]}{t^2} = \\frac{\\sigma^2}{t^2}\n\n\n\nEquivalently, in terms of standard deviations: \\mathbb{P}(|X - \\mu| \\geq k\\sigma) \\leq \\frac{\\sigma^2}{k^2 \\sigma^2 } = \\frac{1}{k^2} \\quad \\text{for all } k &gt; 0\n\n\n\n\n\n\nExample: Universal Two-Sigma Rule\n\n\n\nFor any distribution (not just normal!), Chebyshev’s inequality tells us:\n \\mathbb{P}(|X - \\mu| &lt; k \\sigma) \\ge 1 - \\frac{1}{k^2}. \nThus, for k=2 and k=3 we find:\n\nAt least 75% of the data lies within 2 standard deviations of the mean: \\mathbb{P}(|X - \\mu| &lt; 2\\sigma) \\geq 1 - \\frac{1}{4} = 0.75\nAt least 89% lies within 3 standard deviations: \\mathbb{P}(|X - \\mu| &lt; 3\\sigma) \\geq 1 - \\frac{1}{9} \\approx 0.889\n\nCompare this to the normal distribution where about 95% lies within 2\\sigma and 99.7% within 3\\sigma. Chebyshev’s bounds are weaker but universal.\n\n\nWe show below the Chebyshev’s bound compared to the actual tail probabilities of a few famous distributions (normal, uniform and exponential).\n\n\nShow code\n# Visualizing Chebyshev's inequality\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set up comparison for different distributions\nk_values = np.linspace(0.5, 4, 100)\nchebyshev_bound = np.minimum(1 / k_values**2, 1)\n\n# Compute actual probabilities for different distributions\nnormal_probs = 2 * (1 - stats.norm.cdf(k_values))\nuniform_probs = np.maximum(0, 1 - k_values / np.sqrt(3))  # Uniform on [-sqrt(3), sqrt(3)]\nexp_probs = []\nfor k in k_values:\n    # For exponential with mean 1, mu=1, sigma=1\n    exp_probs.append(stats.expon.cdf(1 - k, scale=1) + (1 - stats.expon.cdf(1 + k, scale=1)))\n\nplt.figure(figsize=(7, 4))\nplt.plot(k_values, chebyshev_bound, 'k-', linewidth=3, label='Chebyshev bound')\nplt.plot(k_values, normal_probs, 'b--', linewidth=2, label='Normal')\nplt.plot(k_values, uniform_probs, 'g--', linewidth=2, label='Uniform')\nplt.plot(k_values, exp_probs, 'r--', linewidth=2, label='Exponential')\nplt.xlabel('Number of standard deviations (k)')\nplt.ylabel('P(|X - μ| ≥ kσ)')\nplt.title('Chebyshev\\'s Inequality vs Actual Tail Probabilities')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0.5, 4)\nplt.ylim(0, 1)\nplt.tight_layout()\nplt.show()\n\n# Print specific values\nprint(\"Probability of being more than k standard deviations from the mean:\")\nprint(\"k\\tChebyshev\\tNormal\\t\\tUniform\\t\\tExponential\")\nfor k in [1, 2, 3]:\n    cheby = 1/k**2\n    normal = 2 * (1 - stats.norm.cdf(k))\n    uniform = max(0, 1 - k/np.sqrt(3))\n    exp_val = stats.expon.cdf(1 - k, scale=1) + (1 - stats.expon.cdf(1 + k, scale=1))\n    print(f\"{k}\\t{cheby:.4f}\\t\\t{normal:.4f}\\t\\t{uniform:.4f}\\t\\t{exp_val:.4f}\")\n\n\n\n\n\n\n\n\n\nProbability of being more than k standard deviations from the mean:\nk   Chebyshev   Normal      Uniform     Exponential\n1   1.0000      0.3173      0.4226      0.1353\n2   0.2500      0.0455      0.0000      0.0498\n3   0.1111      0.0027      0.0000      0.0183\n\n\n\n\n\n\n\n\nAdvanced: Hoeffding’s Inequality\n\n\n\n\n\nWhile Chebyshev’s inequality is universal, it can be quite loose. For bounded random variables, Hoeffding’s inequality provides an exponentially decaying bound that’s much sharper.\n\nLet X_1, \\ldots, X_n be independent random variables with X_i \\in [a_i, b_i]. Let S_n = \\sum_{i=1}^n X_i. Then for any t &gt; 0: \\mathbb{P}(S_n - \\mathbb{E}[S_n] \\geq t) \\leq \\exp\\left(-\\frac{2t^2}{\\sum_{i=1}^n (b_i - a_i)^2}\\right)\nFor the special case of n independent Bernoulli(p) random variables: \\mathbb{P}\\left(|\\bar{X}_n - p| &gt; \\epsilon\\right) \\leq 2e^{-2n\\epsilon^2} where \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\nThe key insight is the exponential decay in n. This makes Hoeffding’s inequality the foundation for many machine learning generalization bounds.\n\n\n\n\n\n\nExample: Comparing Bounds\n\n\n\nConsider estimating a probability p from n = 100 Bernoulli trials. How likely is our estimate to be off by more than \\epsilon = 0.2?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nChebyshev’s bound: Since \\mathbb{V}(\\bar{X}_n) = p(1-p)/n \\leq 1/(4n): \\mathbb{P}(|\\bar{X}_n - p| &gt; 0.2) \\leq \\frac{1/(4 \\times 100)}{0.2^2} = \\frac{1/400}{0.04} = 0.0625\nHoeffding’s bound: \\mathbb{P}(|\\bar{X}_n - p| &gt; 0.2) \\leq 2e^{-2 \\times 100 \\times 0.2^2} = 2e^{-8} \\approx 0.00067\nHoeffding’s bound is nearly 100 times tighter! This exponential improvement is crucial for machine learning theory.\n\n\n\n\n\nThe proof of Hoeffding’s inequality uses moment generating functions and is beyond the scope of this course, but the intuition is that bounded random variables have light tails, allowing for much stronger concentration.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#convergence-of-random-variables",
    "href": "chapters/03-convergence-inference.html#convergence-of-random-variables",
    "title": "3  Convergence and The Basics of Inference",
    "section": "3.4 Convergence of Random Variables",
    "text": "3.4 Convergence of Random Variables\n\n3.4.1 The Need for Probabilistic Convergence\nIn calculus, we say a sequence x_n converges to x if for every \\epsilon &gt; 0, we have |x_n - x| &lt; \\epsilon for all sufficiently large n. But what about sequences of random variables?\nThere are multiple scenarios:\n\nConcentrating distribution: Let X_n \\sim \\mathcal{N}(0, 1/n). As n increases, the distribution concentrates more tightly around 0. Intuitively, X_n is “converging” to 0.\nTracking outcomes: The case above can be generalized where X_n does not converge to a constant (such as 0), but converges to the values taken by another random variable X.\n\nThe problem is that for any specific x, \\mathbb{P}(X_n = x) = 0 for all n: continuous random variables never exactly equal any specific value.\nThere is then a completely different kind of convergence.\n\nStable distribution: Let X_n \\sim \\mathcal{N}(0, 1) for all n. Each X_n has the same distribution, but they’re different random variables. Is there a broader sense in which this sequence “converges”?\n\nIn sum, we need new definitions that capture different notions of what it means for random variables to converge.\n\n\n3.4.2 Convergence in Probability\nWe consider the first two cases mentioned earlier: convergence of outcomes of a sequence of random variables to a constant or to the outcomes of another random variable, known as convergence in probability.\n\nA sequence of random variables X_n converges in probability to a random variable X, written X_n \\xrightarrow{P} X, if for every \\epsilon &gt; 0: \\mathbb{P}(|X_n - X| &gt; \\epsilon) \\to 0 \\text{ as } n \\to \\infty\nWhen X = c (a constant), we write X_n \\xrightarrow{P} c.\n\nThis definition captures the idea that X_n becomes increasingly likely to be close to X as n grows. The probability of X_n being “far” from X (more than \\epsilon away) vanishes. In other words, the sequence of outcomes of the random variable X_n “track” the outcomes of X with ever-increasing accuracy as n increases.\n\n\n\n\n\n\nExample: Convergence to Zero\n\n\n\nLet X_n \\sim \\mathcal{N}(0, 1/n). We’ll show that X_n \\xrightarrow{P} 0.\nFor any \\epsilon &gt; 0, using Chebyshev’s inequality: \\mathbb{P}(|X_n - 0| &gt; \\epsilon) = \\mathbb{P}(|X_n| &gt; \\epsilon) \\leq \\frac{\\mathbb{V}(X_n)}{\\epsilon^2} = \\frac{1/n}{\\epsilon^2} = \\frac{1}{n\\epsilon^2}\nSince \\frac{1}{n\\epsilon^2} \\to 0 as n \\to \\infty, we have X_n \\xrightarrow{P} 0.\n\n\n\n\n3.4.3 Convergence in Distribution\nWe now consider the other case, where it’s not the random variables to converge but their distribution.\n\nA sequence of random variables X_n converges in distribution to a random variable X, written X_n \\rightsquigarrow X, if: \\lim_{n \\to \\infty} F_n(t) = F(t) at all points t where F is continuous. Here F_n is the CDF of X_n and F is the CDF of X.\n\nThis captures the idea that the distribution (or “shape”) of X_n becomes increasingly similar to that of X. We’re not saying the random variables themselves are close – just their overall probability distributions.\nIf X is a point mass at c, we denote X_n \\rightsquigarrow c.\n\n\n\n\n\n\nWarning\n\n\n\nKey Distinction:\n\nConvergence in probability: The random variables themselves get close\nConvergence in distribution: Only the distributions get close\n\n\n\nLet’s visualize this with the X_n \\sim \\mathcal{N}(0, 1/n) example:\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set up the figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 4))\n\n# Left plot: PDFs converging to a spike at 0\nx = np.linspace(-3, 3, 1000)\nn_values = [1, 2, 5, 10, 50]\ncolors = plt.cm.Blues(np.linspace(0.3, 0.9, len(n_values)))\n\nfor n, color in zip(n_values, colors):\n    pdf = stats.norm.pdf(x, loc=0, scale=1/np.sqrt(n))\n    ax1.plot(x, pdf, linewidth=2, color=color, label=f'n = {n}')\n\nax1.axvline(0, color='red', linestyle='--', alpha=0.7)\nax1.set_xlabel('x')\nax1.set_ylabel('Probability density')\nax1.set_title('PDFs of N(0, 1/n)')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim(0, 2.5)\n\n# Right plot: CDFs converging to step function\nfor n, color in zip(n_values, colors):\n    cdf = stats.norm.cdf(x, loc=0, scale=1/np.sqrt(n))\n    ax2.plot(x, cdf, linewidth=2, color=color, label=f'n = {n}')\n\n# Plot limiting step function\nax2.plot(x[x &lt; 0], np.zeros(sum(x &lt; 0)), 'r--', linewidth=2, label='Limit')\nax2.plot(x[x &gt;= 0], np.ones(sum(x &gt;= 0)), 'r--', linewidth=2)\nax2.plot([0, 0], [0, 1], 'ro', markersize=8, fillstyle='none')\nax2.plot([0], [1], 'ro', markersize=8)\n\nax2.set_xlabel('x')\nax2.set_ylabel('Cumulative probability')\nax2.set_title('CDFs converging to step function')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs n increases:\n\nThe PDF becomes more concentrated at 0 (spike)\nThe CDF approaches a step function jumping from 0 to 1 at x=0\nThis is convergence in distribution to a point mass at 0\n\n\n\n3.4.4 Comparing Modes of Convergence\n\nRelationships Between Convergence Types\n\nIf X_n \\xrightarrow{P} X then X_n \\rightsquigarrow X (always)\nIf X is a point mass at c and X_n \\rightsquigarrow X, then X_n \\xrightarrow{P} c\n\n\nConvergence in probability implies convergence in distribution, but the converse holds only for constants.\nIntuitiveMathematicalComputationalConvergence in Probability: The Perfect Weather\nForecastLet \\(X\\) be the actual temperature\ntomorrow and \\(X_n\\) be its forecast\nfrom an ever-improving machine learning model where\n\\(n\\) is the model version, as we make\nit bigger and feed it more data.Convergence in probability\n(\\(X_n \\xrightarrow{P} X\\))\nmeans the forecast becomes more and more accurate as the model gets\nbetter and better. Eventually, the temperature prediction\n\\(X_n\\) gets so close to the actual\ntemperature \\(X\\) that the forecast\nerror, \\(|X_n - X|\\), becomes\nnegligible. The individual outcomes match.Convergence in Distribution: The Perfect Climate\nModelA climate model doesn’t predict a specific day’s temperature; it\ncaptures the statistical “character” of a season. Let\n\\(X\\) be the random variable for daily\ntemperature, and \\(X_n\\) be a model’s\nsimulation of a typical day.Convergence in distribution\n(\\(X_n \\rightsquigarrow X\\))\nmeans the model’s simulated statistics (e.g., its histogram of\ntemperatures) become identical to the real climate’s statistics. The\npatterns match, but the individual outcomes\ndon’t.The Takeaway:\nProbability implies Distribution: A perfect daily\nforecast naturally captures the climate’s long-term statistics.\nDistribution does NOT imply Probability: A perfect\nclimate model can’t predict the actual temperature on next Friday.\nWe can construct a counterexample showing that convergence in\ndistribution does NOT imply convergence in probability.Counterexample: Let\n\\(X \\sim \\mathcal{N}(0,1)\\) and define\n\\(X_n = -X\\) for all\n\\(n\\). Then:\nEach \\(X_n \\sim \\mathcal{N}(0,1)\\),\nso trivially\n\\(X_n \\rightsquigarrow X\\)\nBut \\(|X_n - X| = |2X|\\), so\n\\(\\mathbb{P}(|X_n - X| &gt; \\epsilon) = \\mathbb{P}(|X| &gt; \\epsilon/2) \\not\\to 0\\)\nTherefore \\(X_n\\) does NOT converge\nto \\(X\\) in probability!\nThe random variables have the same distribution but are perfectly\nanti-correlated.Let’s demonstrate both types of convergence and the counterexample\ncomputationally.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nfig, axes = plt.subplots(2, 2, figsize=(7, 8))\n\n# Case 1: X_n ~ N(0, 1/n) → 0 (both types of convergence)\nax = axes[0, 0]\nn_values = [1, 10, 100, 1000]\nn_samples = 5000\n\nfor i, n in enumerate(n_values):\n    samples = np.random.normal(0, 1/np.sqrt(n), n_samples)\n    ax.hist(samples, bins=50, alpha=0.6, density=True, \n            label=f'n={n}', range=(-3, 3))\n\nax.axvline(0, color='red', linestyle='--', linewidth=2)\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Case 1: N(0,1/n) → 0\\n(Converges in both senses)')\nax.legend()\nax.set_xlim(-3, 3)\n\n# Case 1 continued: Show |X_n - 0| for different epsilon\nax = axes[0, 1]\nepsilon = 0.5\nprob_far = []\nfor n in range(1, 101):\n    samples = np.random.normal(0, 1/np.sqrt(n), n_samples)\n    prob_far.append(np.mean(np.abs(samples) &gt; epsilon))\n\nax.plot(range(1, 101), prob_far, 'b-', linewidth=2)\nax.axhline(0, color='red', linestyle='--')\nax.set_xlabel('n')\nax.set_ylabel(f'P(|X_n| &gt; {epsilon})')\nax.set_title('Convergence in Probability to 0')\nax.grid(True, alpha=0.3)\n\n# Case 2: X_n = -X counterexample\nax = axes[1, 0]\nX = np.random.normal(0, 1, n_samples)\nX_n = -X  # X_n for all n\n\n# Plot distributions (they're identical!)\nax.hist(X, bins=50, alpha=0.6, density=True, label='X ~ N(0,1)')\nax.hist(X_n, bins=50, alpha=0.6, density=True, label='X_n = -X ~ N(0,1)')\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Case 2: Same distribution\\n(Converges in distribution)')\nax.legend()\n\n# But |X_n - X| = |2X| doesn't converge to 0\nax = axes[1, 1]\ndiff = np.abs(X_n - X)\nax.hist(diff, bins=50, alpha=0.8, density=True, color='red')\nax.set_xlabel('|X_n - X| = |2X|')\nax.set_ylabel('Density')\nax.set_title('But NOT convergence in probability!\\n|X_n - X| doesn\\'t concentrate at 0')\n\n# Add theoretical chi distribution (|2X| where X~N(0,1))\nx_theory = np.linspace(0, 8, 1000)\ny_theory = stats.chi.pdf(x_theory * 0.5, df=1) * 0.5  # Scale for |2X|\nax.plot(x_theory, y_theory, 'k--', linewidth=2, label='Theory')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\nSummary:\nCase 1:\n\\(X_n ~ \\mathcal{N}\\left(0, 1/n\\right)\\)\nconverges to 0 in BOTH senses\nCase 2: \\(X_n = -X\\) has same\ndistribution as X but does NOT converge in probability\nConvergence in distribution is weaker than convergence in\nprobability\n\n\n\n3.4.5 Properties and Transformations\nUnderstanding how convergence behaves under various operations is crucial for statistical theory. Here are the key properties:\n\nOperations Under Convergence in Probability\nIf X_n \\xrightarrow{P} X and Y_n \\xrightarrow{P} Y, then:\n\nX_n + Y_n \\xrightarrow{P} X + Y\nX_n Y_n \\xrightarrow{P} XY\nX_n / Y_n \\xrightarrow{P} X/Y (if \\mathbb{P}(Y = 0) = 0)\n\n\nThis shows that convergence in probability is well-beheaved under standard operations of sum, product, and division not-by-zero.\n\nSlutsky’s Theorem\nIf X_n \\rightsquigarrow X and Y_n \\xrightarrow{P} c (constant), then:\n\nX_n + Y_n \\rightsquigarrow X + c\nX_n Y_n \\rightsquigarrow cX\nX_n / Y_n \\rightsquigarrow X/c (if c \\neq 0)\n\n\nSlutsky’s theorem tells us that convergence in distribution behaves nicely when paired with random variables that converge to a constant (this is not true in general!).\n\nContinuous Mapping Theorem\nIf g is a continuous function:\n\nX_n \\xrightarrow{P} X \\implies g(X_n) \\xrightarrow{P} g(X)\nX_n \\rightsquigarrow X \\implies g(X_n) \\rightsquigarrow g(X)\n\n\nFinally, we see that continuous mappings behave nicely for both types of convergence.\n\n\n\n\n\n\nWarning\n\n\n\nImportant limitation: In general, if X_n \\rightsquigarrow X and Y_n \\rightsquigarrow Y, we cannot conclude that X_n + Y_n \\rightsquigarrow X + Y. Convergence in distribution does not preserve sums unless one component converges to a constant!\nCounterexample: Let X \\sim \\mathcal{N}(0,1) and define Y_n = -X for all n. Then Y_n \\sim \\mathcal{N}(0,1), so Y_n \\rightsquigarrow Y \\sim \\mathcal{N}(0,1). But X + Y_n = X - X = 0, which does not converge in distribution to X + Y \\sim \\mathcal{N}(0,2).\n\n\n\n\n\n\n\n\nKey takeaway\n\n\n\nThe rules for convergence are subtle. Generally speaking, convergence in probability behaves nicely under algebraic operations, but convergence in distribution requires more care. Always verify which type of convergence you have before applying these properties!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#the-two-fundamental-theorems-of-statistics",
    "href": "chapters/03-convergence-inference.html#the-two-fundamental-theorems-of-statistics",
    "title": "3  Convergence and The Basics of Inference",
    "section": "3.5 The Two Fundamental Theorems of Statistics",
    "text": "3.5 The Two Fundamental Theorems of Statistics\n\n3.5.1 The Law of Large Numbers (LLN)\nThe Law of Large Numbers formalizes one of our most basic intuitions about probability: averages stabilize as we collect more data. When we flip a fair coin many times, the proportion of heads approaches 1/2. When we measure heights of many people, the sample mean approaches the population mean. This isn’t just intuition – it’s a mathematical theorem.\n\nLet X_1, X_2, \\ldots, X_n be independent and identically distributed (IID) random variables with \\mathbb{E}(X_i) = \\mu and \\mathbb{V}(X_i) = \\sigma^2 &lt; \\infty. Define the sample mean: \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\nThen \\bar{X}_n \\xrightarrow{P} \\mu.\n\nInterpretation: The sample mean converges in probability to the population mean. As we collect more data, our estimate gets arbitrarily close to the true value with high probability.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe’ll use Chebyshev’s inequality. First, compute the mean and variance of \\bar{X}_n:\n\\mathbb{E}(\\bar{X}_n) = \\mathbb{E}\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}(X_i) = \\frac{1}{n} \\cdot n\\mu = \\mu\n\\mathbb{V}(\\bar{X}_n) = \\mathbb{V}\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\mathbb{V}(X_i) = \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}\n(We used independence for the variance calculation.)\nNow apply Chebyshev’s inequality: for any \\epsilon &gt; 0, \\mathbb{P}(|\\bar{X}_n - \\mu| &gt; \\epsilon) \\leq \\frac{\\mathbb{V}(\\bar{X}_n)}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\to 0 \\text{ as } n \\to \\infty\nTherefore \\bar{X}_n \\xrightarrow{P} \\mu.\n\n\n\nLet’s visualize the Law of Large Numbers in action by simulating repeated rolls of a standard six-sided die and computing the mean of all rolls until that point.\nWe show this in two plots: on a normal scale (top) and on a log-scale (bottom) for the number of rolls on the x axis. The bottom plot also zooms in on the y-axis around 3.5.\nNote how the sample mean starts with high variability but converges to the true mean (3.5) as the number of rolls increases.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Simulate die rolls\nn_max = 10000\ndie_rolls = np.random.randint(1, 7, n_max)\ncumulative_mean = np.cumsum(die_rolls) / np.arange(1, n_max + 1)\ntrue_mean = 3.5\n\n# Create the plot without shared x-axis\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 5))\n\n# Top plot: Full scale with linear x-axis\nax1.plot(cumulative_mean, linewidth=2, color='blue', alpha=0.8)\nax1.axhline(y=true_mean, color='red', linestyle='--', linewidth=2, \n            label=f'True mean = {true_mean}')\nax1.set_ylabel('Sample mean')\nax1.set_title('Law of Large Numbers: Sample Mean of Die Rolls')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim(1, 6)\n\n# Bottom plot: Zoomed in to show convergence with log scale\nx_values = np.arange(100, n_max)\nax2.plot(x_values, cumulative_mean[100:], linewidth=2, color='blue', alpha=0.8)\nax2.axhline(y=true_mean, color='red', linestyle='--', linewidth=2)\nax2.set_xlabel('Number of rolls')\nax2.set_ylabel('Sample mean')\nax2.set_xscale('log')\nax2.grid(True, alpha=0.3)\nax2.set_ylim(3.2, 3.8)\n\nplt.tight_layout()\nplt.show()\n\n# Show convergence at specific sample sizes\nfor n in [10, 100, 1000, 10000]:\n    print(f\"After {n:5d} rolls: sample mean = {cumulative_mean[n-1]:.4f}, \"\n          f\"error = {abs(cumulative_mean[n-1] - true_mean):.4f}\")\n\n\n\n\n\n\n\n\n\nAfter    10 rolls: sample mean = 3.8000, error = 0.3000\nAfter   100 rolls: sample mean = 3.6900, error = 0.1900\nAfter  1000 rolls: sample mean = 3.4570, error = 0.0430\nAfter 10000 rolls: sample mean = 3.4999, error = 0.0001\n\n\n\n\n\n\n\n\nWeak vs Strong Laws of Large Numbers\n\n\n\n\n\nThe theorem above is known as the “Weak” Law of Large Numbers because it guarantees convergence in probability. There exists a stronger version that guarantees almost sure convergence: \\mathbb{P}(\\bar{X}_n \\to \\mu) = 1. The “Strong” LLN says that with probability 1, the sample mean will eventually get arbitrarily close to \\mu and stay close, while the Weak LLN only guarantees that the probability of being far from \\mu goes to zero. The Weak LLN requires only finite variance, while the Strong LLN typically needs additional assumptions (like finite fourth moments) but delivers a more powerful conclusion. We present the Weak version as it has minimal assumptions and suffices for most statistical applications.\n\n\n\n\n\n3.5.2 The Central Limit Theorem (CLT)\nWhile the Law of Large Numbers tells us that sample means converge to the population mean, it doesn’t tell us about the distribution of the sample mean. The Central Limit Theorem fills this gap with a remarkable result: properly scaled sample means are approximately normal, regardless of the underlying distribution!\n\nLet X_1, X_2, \\ldots, X_n be IID random variables with \\mathbb{E}(X_i) = \\mu and \\mathbb{V}(X_i) = \\sigma^2 &lt; \\infty. Define: Z_n = \\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma}\nThen Z_n \\rightsquigarrow Z where Z \\sim \\mathcal{N}(0, 1).\n\nAlternative notations (all mean the same thing):\n\n\\bar{X}_n \\approx \\mathcal{N}(\\mu, \\sigma^2/n) for large n\n\\sqrt{n}(\\bar{X}_n - \\mu) \\rightsquigarrow \\mathcal{N}(0, \\sigma^2)\n(\\bar{X}_n - \\mu)/(\\sigma/\\sqrt{n}) \\rightsquigarrow \\mathcal{N}(0, 1)\n\n\n\n\n\n\n\nWarning\n\n\n\nCritical Point: The CLT is about the distribution of the sample mean, not the data itself! The original data doesn’t become normal—only the sampling distribution of \\bar{X}_n does.\n\n\n\nInteractive Demonstration of the CLT\nLet’s visualize how the CLT works for different distributions from continuous to discrete and skewed (asymmetrical).\nThe interactive visualization below allows you to see this convergence in action. You can change the underlying distribution and adjust the sample size n to see how the distribution of the standardized sample mean approaches a standard normal distribution (red curves).\n\n\nShow code\nimport {cltDemo} from \"../js/clt-demo.js\"\nd3 = require(\"d3@7\")\nInputs = require(\"https://cdn.jsdelivr.net/npm/@observablehq/inputs@0.10.6/dist/inputs.min.js\")\n\ndemo = cltDemo(d3);\n\nviewof distName = Inputs.select(\n  [\"Uniform\", \"Exponential\", \"Bernoulli\", \"Skewed Discrete\"], \n  {label: \"Population Distribution\"}\n)\nviewof sampleSize = Inputs.range(\n  [1, 100], \n  {step: 1, value: 1, label: \"Sample Size (n)\"}\n)\nviewof numSimulations = Inputs.range(\n  [100, 10000], \n  {step: 100, value: 10000, label: \"Number of Simulations\"}\n)\n\n{\n  const plot = demo.createVisualization({\n    distName: distName,\n    sampleSize: sampleSize,\n    numSimulations: numSimulations,\n  });\n  return plot; \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: CLT in Practice\n\n\n\nA factory produces bolts with mean length \\mu = 5 cm and standard deviation \\sigma = 0.1 cm. If we randomly sample 100 bolts, what’s the probability their average length exceeds 5.02 cm?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBy the CLT, \\bar{X}_{100} \\approx \\mathcal{N}(5, 0.1^2/100) = \\mathcal{N}(5, 0.0001).\nWe want: \\mathbb{P}(\\bar{X}_{100} &gt; 5.02) = \\mathbb{P}\\left(\\frac{\\bar{X}_{100} - 5}{0.01} &gt; \\frac{5.02 - 5}{0.01}\\right) = \\mathbb{P}(Z &gt; 2)\nwhere Z \\sim \\mathcal{N}(0,1). From standard normal tables: \\mathbb{P}(Z &gt; 2) \\approx 0.0228.\nSo there’s about a 2.3% chance the sample mean exceeds 5.02 cm.\n\n\n\n\n\n\nCLT with Unknown Variance: If we replace \\sigma with the sample standard deviation S_n = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2}\nthen we still have: \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{S_n} \\rightsquigarrow \\mathcal{N}(0, 1)\n\nThis version is crucial for practice since we rarely know the true variance!\n\n\n\n\n\n\nRejoinder: Understanding Algorithms\n\n\n\nRemember the sequence of random variables \\theta_1, \\theta_2, \\ldots from our stochastic optimization algorithm at the beginning of this chapter? We can now answer what kind of convergence we should expect:\nConvergence in probability: We want \\theta_n \\xrightarrow{P} \\theta^* where \\theta^* is the true optimal solution. This means the probability of \\theta_n being far from the optimum vanishes as iterations increase.\nThe tools we’ve covered – probability inequalities (to bound deviations), convergence concepts (to formalize what “converges” means), and limit theorems (to understand averaging behavior) – are the foundation for analyzing when and why algorithms like stochastic gradient descent converge to good solutions. Modern machine learning theory relies heavily on these concepts to provide theoretical guarantees about algorithm performance!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#the-language-of-statistical-inference",
    "href": "chapters/03-convergence-inference.html#the-language-of-statistical-inference",
    "title": "3  Convergence and The Basics of Inference",
    "section": "3.6 The Language of Statistical Inference",
    "text": "3.6 The Language of Statistical Inference\n\n3.6.1 From Probability to Inference\nWe’ve developed powerful tools: inequalities that bound uncertainty, convergence concepts that describe limiting behavior, and fundamental theorems that guarantee nice properties of averages. Now we flip the perspective.\nProbability: Given a known distribution, what can we say about the data we’ll observe?\nStatistical Inference: Given observed data, what can we infer about the unknown distribution that generated it? More formally: given a sample X_1, \\ldots, X_n \\sim F, how do we infer F?\nThis process – often called “learning” in computer science – is at the core of both classical statistics and modern machine learning. Sometimes we want to infer the entire distribution F, but often we focus on specific features like its mean, variance, or other parameters.\n\n\n\n\n\n\nExample: Modeling Uncertainty in Real Decisions\n\n\n\nAn online retailer tests a new ad campaign. Out of 1000 users who see the ad, 30 make a purchase (3% conversion rate). But this raises critical questions:\nImmediate questions:\n\nWhat can we say about the true conversion rate? Is it exactly 3%?\nHow likely is it that at least 25 out of the next 1000 users will convert?\n\nComparative questions:\n\nA competing ad had 290 conversions out of 10,000 users (2.9% rate). Which is better?\nHow confident can we be that the 3% ad truly outperforms the 2.9% ad – could the difference just be due to random chance?\n\nLong-term questions:\n\nWhat’s the probability that the long-run conversion rate exceeds 2.5%?\nHow many more users do we need to test to be 95% confident about the true rate?\n\nThese questions – about uncertainty, confidence, and decision-making with limited data – are at the heart of statistical inference.\n\n\n\n\n3.6.2 Statistical Models\n\nA statistical model \\mathfrak{F} is a set of probability distributions (or densities or regression functions).\n\nIn the context of inference, we use models to represent our assumptions about which distributions could have generated our observed data. The model defines the “universe of possibilities” we’re considering – we then use data to identify which specific distribution within \\mathfrak{F} is most plausible.\nModels come in two main flavors, parametric and nonparametric.\n\nParametric Models\n\nA parametric model is indexed by a finite number of parameters. We write it as: \\mathfrak{F} = \\{f(x; \\theta) : \\theta \\in \\Theta\\}\nwhere:\n\n\\theta (theta) is the parameter (possibly vector-valued)4\n\\Theta (capital theta) is the parameter space (the set of all possible parameter values)\nf(x; \\theta) is the density or distribution function indexed by \\theta\n\n\nTypically, the parameters \\theta are unknown quantities we want to estimate. If there are elements of the vector \\theta that we are not interested in, those are called nuisance parameters.\n\n\n\n\n\n\nExample: Feature Performance as a Parametric Model\n\n\n\n\n\nA product manager at a tech company launches a new “AI Recap” feature in their app. To determine if the feature is a success, they track the number of daily views over the first month. They hypothesize that the daily view count approximately follows a normal distribution.\nThe model for daily views is a parametric family \\mathfrak{F}: \\mathfrak{F} = \\{f(x; \\theta) : \\theta = (\\mu, \\sigma^2), \\mu \\in \\mathbb{R}, \\sigma^2 &gt; 0\\}\nwhere the density function is:f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\nThis is a 2-dimensional parametric model with:\n\nParameter vector: \\theta = (\\mu, \\sigma^2)\nParameter space: \\Theta = \\mathbb{R} \\times (0, \\infty)\n\nNuisance parameters in action: The company has set a target: the feature will be considered successful and receive further development only if it can reliably generate more than 100,000 views per day.\n\nThe parameter of interest is the average daily views, \\mu. The entire business decision hinges on testing the hypothesis that \\mu &gt; 100,000.\nThe nuisance parameter is the variance, \\sigma^2. The day-to-day fluctuation in views is critical for assessing the statistical certainty of our estimate for \\mu, but it’s not the primary metric for success. The product manager needs to account for this variability, but their core question is about the average performance, not the variability itself.\n\n\n\n\n\n\nNonparametric Models\nNonparametric Models cannot be parameterized by a finite number of parameters. These models make minimal assumptions about the distribution. For example: \\mathfrak{F}_{\\text{ALL}} = \\{\\text{all continuous CDFs}\\}\nor with some constraints: \\mathfrak{F} = \\{\\text{all distributions with finite variance}\\}\n\n\n\n\n\n\nHow can we work with “all distributions”?\n\n\n\n\n\nThis seems impossibly broad! In practice, we don’t explicitly enumerate all possible distributions. Instead, nonparametric methods directly use the data without assuming a specific functional form or parameter to be estimated. We will see multiple concrete example of nonparametric techniques in the next chapter. So, in theory the model space is infinite-dimensional, but in practice nonparametric estimation procedures are still concrete and computable.\n\n\n\n\n\n\n\n\n\nExample: Choosing a Model\n\n\n\nScenario 1: Heights of adult males in Finland.\n\nParametric choice: \\mathfrak{F} = \\{\\mathcal{N}(\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma &gt; 0\\}\nJustification: Heights are often approximately normal due to many small genetic and environmental factors (CLT in action!)\n\nScenario 2: Time between website visits.\n\nParametric choice: \\mathfrak{F} = \\{\\text{Exponential}(\\lambda) : \\lambda &gt; 0\\}\nJustification: Exponential models “memoryless” waiting times\n\nScenario 3: Unknown distribution shape.\n\nNonparametric choice: \\mathfrak{F} = \\{\\text{all distributions with finite variance}\\}\nJustification: Make minimal assumptions, let data speak\n\n\n\n\n\n\n3.6.3 Point Estimation\n\nPoint estimation is the task of providing a single “best guess” for an unknown quantity based on data.\n\nThis quantity can be a single parameter, a full vector of parameters, even a full CDF or PDF, or prediction for a future value of some random variable.\nA point estimate of \\theta is denoted by \\hat{\\theta}.\n\nGiven data X_1, \\ldots, X_n, a point estimator is a function: \\hat{\\theta}_n = g(X_1, \\ldots, X_n)\n\nThe “hat” notation \\hat{\\theta} can indicate both an estimator and the estimate.\n\n\n\n\n\n\nWarning\n\n\n\nCritical Distinction:\n\nParameter \\theta: Fixed, unknown number we want to learn\nEstimator \\hat{\\theta}_n: Random variable (before seeing data)\nEstimate \\hat{\\theta}_n: Specific number (after seeing data) – notation can be overlapping\n\nFor example, \\bar{X}_n is an estimator; \\bar{x}_n = 3.7 is an estimate.\n\n\nThe distribution of \\hat{\\theta}_n is called the sampling distribution. The standard deviation of this distribution is the standard error: \\text{se}(\\hat{\\theta}_n) = \\sqrt{\\mathbb{V}(\\hat{\\theta}_n)}\nWhen the standard error depends on unknown parameters, we use the estimated standard error \\widehat{\\text{se}}.\nA particularly common standard error in stastistics is the standard error of the mean (SEM).\n\n\n3.6.4 How to Evaluate Estimators\nHow do we judge if an estimator is “good”? Several criteria have emerged:\n\nBias: The systematic error of an estimator. \\text{bias}(\\hat{\\theta}_n) = \\mathbb{E}(\\hat{\\theta}_n) - \\theta\nAn estimator is unbiased if \\mathbb{E}(\\hat{\\theta}_n) = \\theta.\n\n\nConsistency: An estimator is consistent if it converges to the true value. \\hat{\\theta}_n \\xrightarrow{P} \\theta \\text{ as } n \\to \\infty\n\n\nMean Squared Error (MSE): The average squared distance from the truth. \\text{MSE}(\\hat{\\theta}_n) = \\mathbb{E}[(\\hat{\\theta}_n - \\theta)^2]\n\n\n\n\n\n\n\nExample: Evaluating Estimators for the Mean\n\n\n\nSuppose X_1, \\ldots, X_n \\sim \\mathcal{N}(\\theta, \\sigma^2) where \\theta is unknown. Consider three estimators:\n\nConstant estimator: \\hat{\\theta}_n^{(1)} = 3\n\nBias: \\mathbb{E}(3) - \\theta = 3 - \\theta (biased unless \\theta = 3)\nVariance: \\mathbb{V}(3) = 0\nConsistent: No, always equals 3\nMSE: (3 - \\theta)^2\n\nFirst observation: \\hat{\\theta}_n^{(2)} = X_1\n\nBias: \\mathbb{E}(X_1) - \\theta = 0 (unbiased!)\nVariance: \\mathbb{V}(X_1) = \\sigma^2\nConsistent: No, variance doesn’t shrink\nMSE: \\sigma^2\n\nSample mean: \\hat{\\theta}_n^{(3)} = \\bar{X}_n\n\nBias: \\mathbb{E}(\\bar{X}_n) - \\theta = 0 (unbiased!)\nVariance: \\mathbb{V}(\\bar{X}_n) = \\sigma^2/n\nConsistent: Yes! (by LLN)\nMSE: \\sigma^2/n \\to 0\n\n\nThe sample mean is unbiased AND consistent—it improves with more data!\n\n\nA classic example shows that unbiased isn’t everything:\nSample variance: Two common estimators for population variance \\sigma^2:\n\nUnbiased version: S^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X})^2\n\n\\mathbb{E}(S^2) = \\sigma^2 (unbiased by design)\n\nMaximum likelihood estimator: \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n(X_i - \\bar{X})^2\n\n\\mathbb{E}(\\hat{\\sigma}^2) = \\frac{n-1}{n}\\sigma^2 (biased!)\n\n\nWhich is better? It depends on the criterion!\n\n\n3.6.5 The Bias-Variance Tradeoff\n\nThe MSE decomposes as: \\text{MSE} = \\text{bias}^2(\\hat{\\theta}_n) + \\mathbb{V}(\\hat{\\theta}_n)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\bar{\\theta}_n = \\mathbb{E}(\\hat{\\theta}_n). Then: \\begin{align}\n\\mathbb{E}[(\\hat{\\theta}_n - \\theta)^2]\n&= \\mathbb{E}[(\\hat{\\theta}_n - \\bar{\\theta}_n + \\bar{\\theta}_n - \\theta)^2] \\\\\n&= \\mathbb{E}[(\\hat{\\theta}_n - \\bar{\\theta}_n)^2] + 2(\\bar{\\theta}_n - \\theta)\\mathbb{E}[\\hat{\\theta}_n - \\bar{\\theta}_n] + (\\bar{\\theta}_n - \\theta)^2 \\\\\n&= \\mathbb{E}[(\\hat{\\theta}_n - \\bar{\\theta}_n)^2] + 2(\\bar{\\theta}_n - \\theta) \\cdot 0 + (\\bar{\\theta}_n - \\theta)^2 \\\\\n&= \\mathbb{V}(\\hat{\\theta}_n) + \\text{bias}^2(\\hat{\\theta}_n)\n\\end{align}\nwhere we used that \\mathbb{E}[\\hat{\\theta}_n - \\bar{\\theta}_n] = \\mathbb{E}[\\hat{\\theta}_n] - \\bar{\\theta}_n = \\bar{\\theta}_n - \\bar{\\theta}_n = 0.\n\n\n\nThis decomposition reveals a fundamental tradeoff in statistics.\nIntuitiveMathematicalComputationalImagine you’re an archer trying to hit a target. Your performance\ndepends on two things:Bias: How far your average shot is from the\nbullseye. A biased archer consistently aims too high or too far\nleft.Variance: How spread out your shots are. A\nhigh-variance archer is inconsistent—sometimes dead on, sometimes way\noff.The best archer has low bias AND low variance. But here’s the key\ninsight: sometimes accepting a little bias can dramatically reduce\nvariance, improving overall accuracy!Think of it this way:\nA complex model (like memorizing training data) has low bias but\nhigh variance\nA simple model (like always predicting the average) has higher bias\nbut low variance\nThe sweet spot balances both\nThis tradeoff is why regularization works in machine learning – we\naccept a bit of bias to gain a lot in variance reduction.The bias-variance decomposition gives us a precise way to understand\nprediction error:\\[\\text{MSE}(\\hat{\\theta}_n) = \\text{bias}^2(\\hat{\\theta}_n) + \\mathbb{V}(\\hat{\\theta}_n)\\]This isn’t just algebra – it reveals the two fundamental sources of\nerror:\nSystematic error (bias): Being consistently\nwrong\nRandom error (variance): Being inconsistently\nwrong\nFor prediction problems where\n\\(\\hat{f}(x)\\) estimates\n\\(f(x)\\):\n\\[\\mathbb{E}[(\\hat{f}(x) - f(x))^2] = \\underbrace{(E[\\hat{f}(x)] - f(x))^2}_{\\text{bias}^2} + \\underbrace{\\mathbb{V}(\\hat{f}(x))}_{\\text{variance}}\\]The optimal predictor minimizes their sum. In machine learning:\nIncreasing model complexity typically decreases bias but increases\nvariance\nThe art is finding the right complexity for your data\nLet’s visualize the bias-variance tradeoff by comparing estimators\nfor population variance.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Simulation parameters\ntrue_variance = 1.0  # True σ² = 1\nn_values = np.arange(5, 101, 5)\nn_simulations = 10000\n\n# Storage for results\nbias_unbiased = []\nbias_mle = []\nvariance_unbiased = []\nvariance_mle = []\nmse_unbiased = []\nmse_mle = []\n\nfor n in n_values:\n    # Generate many samples and compute both estimators\n    unbiased_estimates = []\n    mle_estimates = []\n    \n    for _ in range(n_simulations):\n        # Generate sample from N(0, 1)\n        sample = np.random.normal(0, 1, n)\n        sample_mean = np.mean(sample)\n        \n        # Unbiased estimator (n-1 denominator)\n        s_squared = np.sum((sample - sample_mean)**2) / (n - 1)\n        unbiased_estimates.append(s_squared)\n        \n        # MLE (n denominator)\n        sigma_hat_squared = np.sum((sample - sample_mean)**2) / n\n        mle_estimates.append(sigma_hat_squared)\n    \n    # Calculate bias, variance, and MSE\n    unbiased_estimates = np.array(unbiased_estimates)\n    mle_estimates = np.array(mle_estimates)\n    \n    # Bias\n    bias_unbiased.append(np.mean(unbiased_estimates) - true_variance)\n    bias_mle.append(np.mean(mle_estimates) - true_variance)\n    \n    # Variance\n    variance_unbiased.append(np.var(unbiased_estimates))\n    variance_mle.append(np.var(mle_estimates))\n    \n    # MSE\n    mse_unbiased.append(np.mean((unbiased_estimates - true_variance)**2))\n    mse_mle.append(np.mean((mle_estimates - true_variance)**2))\n\n# Create plots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(7, 5))\n\n# Bias plot\nax1.plot(n_values, bias_unbiased, 'b-', linewidth=2, label='Unbiased (S²)')\nax1.plot(n_values, bias_mle, 'r--', linewidth=2, label='MLE (σ̂²)')\nax1.axhline(y=0, color='black', linestyle=':', alpha=0.5)\nax1.set_xlabel('Sample size (n)')\nax1.set_ylabel('Bias')\nax1.set_title('Bias')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Variance plot  \nax2.plot(n_values, variance_unbiased, 'b-', linewidth=2, label='Unbiased (S²)')\nax2.plot(n_values, variance_mle, 'r--', linewidth=2, label='MLE (σ̂²)')\nax2.set_xlabel('Sample size (n)')\nax2.set_ylabel('Variance')\nax2.set_title('Variance of Estimator')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# MSE plot\nax3.plot(n_values, mse_unbiased, 'b-', linewidth=2, label='Unbiased (S²)')\nax3.plot(n_values, mse_mle, 'r--', linewidth=2, label='MLE (σ̂²)')\nax3.set_xlabel('Sample size (n)')\nax3.set_ylabel('MSE')\nax3.set_title('Mean Squared Error')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.suptitle('Bias-Variance Tradeoff: Variance Estimators', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Print numerical comparison for n=10\nn_idx = 1  # n=10\nprint(f\"For n=10:\")\nprint(f\"Unbiased (S²): Bias = {bias_unbiased[n_idx]:.4f}, Variance = {variance_unbiased[n_idx]:.4f}, MSE = {mse_unbiased[n_idx]:.4f}\")\nprint(f\"MLE (σ̂²):      Bias = {bias_mle[n_idx]:.4f}, Variance = {variance_mle[n_idx]:.4f}, MSE = {mse_mle[n_idx]:.4f}\")\nprint(f\"\\nThe MLE has lower MSE despite being biased!\")\n\n\n\n\nFor n=10:\nUnbiased (S²): Bias = -0.0013, Variance = 0.2219, MSE = 0.2219\nMLE (σ̂²):      Bias = -0.1012, Variance = 0.1797, MSE = 0.1899\n\nThe MLE has lower MSE despite being biased!\n\nKey insights from the visualization:\nThe unbiased estimator \\(S^2\\) has\nzero bias (blue solid line at 0)\nThe MLE \\(\\hat{\\sigma}^2\\) has\nnegative bias that shrinks as \\(n\\)\ngrows\nThe MLE has lower variance than the unbiased estimator\nFor finite samples, the biased MLE has lower\nMSE!\nThis demonstrates a profound principle: the “best” estimator depends\non your criterion. Unbiasedness is nice, but minimizing MSE often\nmatters more in practice.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#chapter-summary-and-connections",
    "href": "chapters/03-convergence-inference.html#chapter-summary-and-connections",
    "title": "3  Convergence and The Basics of Inference",
    "section": "3.7 Chapter Summary and Connections",
    "text": "3.7 Chapter Summary and Connections\n\n3.7.1 Key Concepts Review\nWe’ve built a complete framework for understanding randomness and inference:\nInequalities bound the unknown:\n\nMarkov: \\mathbb{P}(X \\geq t) \\leq \\mathbb{E}(X)/t — averages constrain extremes\nChebyshev: \\mathbb{P}(|X - \\mu| \\geq k\\sigma) \\leq 1/k^2 — universal bounds using variance\n\nConvergence describes limiting behavior:\n\nIn probability: X_n \\xrightarrow{P} X — the random variable itself settles down\nIn distribution: X_n \\rightsquigarrow X — the shape of the distribution stabilizes\nKey relationship: Convergence in probability implies convergence in distribution\n\nFundamental theorems guarantee nice behavior:\n\nLaw of Large Numbers: \\bar{X}_n \\xrightarrow{P} \\mu — sample means converge to population means\nCentral Limit Theorem: \\sqrt{n}(\\bar{X}_n - \\mu)/\\sigma \\rightsquigarrow \\mathcal{N}(0,1) — sample means are approximately normal\n\nStatistical inference flips the perspective:\n\nModels: Parametric (finite-dimensional) vs nonparametric (infinite-dimensional)\nEstimators: Functions of data that guess parameters\nStandard error: Standard deviation of an estimator’s sampling distribution\nEvaluation criteria: Bias, variance, consistency, MSE\nBias-variance tradeoff: \\text{MSE} = \\text{bias}^2 + \\text{variance}\n\n\n\n3.7.2 Why These Concepts Matter\nFor Statistical Inference:\n\nLLN justifies using sample statistics to estimate population parameters\nCLT enables confidence intervals and hypothesis tests (to be seen in the next chapters)\nBias-variance tradeoff guides choice of estimators\nConsistency ensures our methods improve with more data\n\nFor Machine Learning:\n\nConvergence concepts analyze iterative algorithms such as stochastic optimization\nBias-variance tradeoff explains overfitting vs underfitting\nCLT justifies bootstrap and cross-validation, as we will see in the next chapters\n\nFor Data Science Practice:\n\nUnderstanding variability in estimates prevents overconfidence\nRecognizing when CLT applies (and when it doesn’t)\nChoosing between simple and complex models\nInterpreting A/B test results correctly\n\n\n\n3.7.3 Common Pitfalls to Avoid\n\nConfusing convergence types:\n\n“My algorithm converged” – in what sense?\nConvergence in distribution does not imply convergence in probability!\n\nMisapplying the CLT:\n\nCLT is about sample means, not individual observations\nNeed large enough n (depends on skewness)\nDoesn’t work without finite variance (Cauchy!)\n\nOvervaluing unbiasedness:\n\nUnbiased doesn’t mean good (e.g., using just X_1)\nBiased can be better in statistics (regularization, priors)\n\nIgnoring assumptions:\n\nIndependence matters for variance calculations\nFinite variance required for CLT\n\nMisinterpreting bounds:\n\nMarkov/Chebyshev give worst-case bounds\nOften very loose in practice\nTighter bounds exist for specific distributions\n\n\n\n\n3.7.4 Chapter Connections\nThis chapter connects fundamental probability theory to practical statistical inference:\n\nFrom Previous Chapters: We’ve applied Chapter 1’s probability framework and Chapter 2’s expectation/variance concepts to prove convergence theorems (LLN, CLT) that explain why sample statistics work as estimators\nNext - Chapter 4 (Bootstrap): While this chapter gave us theoretical tools for inference (CLT-based confidence intervals), the bootstrap will provide a computational approach that works even when theoretical distributions are intractable\nStatistical Modeling (Chapters 5+): The bias-variance tradeoff introduced here becomes central to model selection, while MSE serves as our primary tool for comparing estimators in regression and machine learning\nThroughout the Course: The convergence concepts (especially CLT) and inference framework established here underpin virtually every statistical method—from hypothesis testing to Bayesian inference\n\n\n\n3.7.5 Self-Test Problems\n\nApplying Chebyshev: A website’s daily visitors have mean 10,000 and standard deviation 2,000. Without assuming any distribution, what can you say about the probability of getting fewer than 4,000 or more than 16,000 visitors?\nCLT Application: A casino’s slot machine pays out €1 with probability 0.4 and €0 otherwise. If someone plays 400 times, approximate the probability their total winnings exceed €170.\nComparing Estimators: Given X_1, \\ldots, X_n \\sim \\text{Uniform}(0, \\theta), consider two estimators:\n\n\\hat{\\theta}_1 = 2\\bar{X}_n\n\\hat{\\theta}_2 = \\frac{n+1}{n} \\max\\{X_1, \\ldots, X_n\\}\n\nCalculate bias and variance for each. Which has lower MSE?\nConvergence Concepts: Let X_n have PMF: P(X_n = 0) = 1 - 1/n, \\quad P(X_n = n) = 1/n\n\nDoes X_n \\xrightarrow{P} 0?\nDoes X_n \\rightsquigarrow 0?\nDoes \\mathbb{E}(X_n) \\to 0?\n\n\n\n\n3.7.6 Python and R Reference\nPythonR#| eval: false\nimport numpy as np\nfrom scipy import stats\n\n# Probability inequalities\ndef markov_bound(mean, t):\n    \"\"\"Markov inequality bound P(X &gt;= t)\"\"\"\n    return min(mean / t, 1) if t &gt; 0 else 1\n\ndef chebyshev_bound(k):\n    \"\"\"Chebyshev bound P(|X - μ| &gt;= kσ)\"\"\"\n    return min(1 / k**2, 1) if k &gt; 0 else 1\n\n# Simulating convergence\ndef demonstrate_lln(dist, n_max=10000):\n    \"\"\"Show Law of Large Numbers\"\"\"\n    samples = dist.rvs(n_max)\n    cumulative_mean = np.cumsum(samples) / np.arange(1, n_max + 1)\n    return cumulative_mean\n\ndef demonstrate_clt(dist, n, n_simulations=10000):\n    \"\"\"Show Central Limit Theorem\"\"\"\n    sample_means = []\n    for _ in range(n_simulations):\n        sample = dist.rvs(n)\n        sample_means.append(np.mean(sample))\n    return np.array(sample_means)\n\n# Estimator evaluation\ndef evaluate_estimator(estimator_func, true_value, n, sample_func, n_simulations=10000):\n    \"\"\"Compute bias, variance, and MSE of an estimator via simulation.\"\"\"\n    estimates = []\n    for _ in range(n_simulations):\n        sample = sample_func(n)\n        estimate = estimator_func(sample)\n        estimates.append(estimate)\n    \n    estimates = np.array(estimates)\n    bias = np.mean(estimates) - true_value\n    variance = np.var(estimates)\n    mse = np.mean((estimates - true_value)**2)\n    \n    return {'bias': bias, 'variance': variance, 'mse': mse}\n\n# Common distributions for examples\nnormal_dist = stats.norm(loc=0, scale=1)\nexp_dist = stats.expon(scale=1)\nuniform_dist = stats.uniform(loc=0, scale=1)\n\n# Sample statistics\ndef sample_mean(x):\n    return np.mean(x)\n\ndef sample_variance_unbiased(x):\n    return np.var(x, ddof=1)  # n-1 denominator\n\ndef sample_variance_mle(x):\n    return np.var(x, ddof=0)  # n denominator\n\n# Example: Evaluate variance estimators for N(0,1) samples (true variance = 1)\ndef generate_normal_sample(n):\n    return np.random.normal(loc=0, scale=1, size=n)\n\nprint(\"--- Evaluating Variance Estimators (n=10) ---\")\nmle_results = evaluate_estimator(\n    estimator_func=sample_variance_mle,\n    true_value=1.0,\n    n=10,\n    sample_func=generate_normal_sample\n)\nprint(f\"MLE Estimator: {mle_results}\")\n\nunbiased_results = evaluate_estimator(\n    estimator_func=sample_variance_unbiased,\n    true_value=1.0,\n    n=10,\n    sample_func=generate_normal_sample\n)\nprint(f\"Unbiased Estimator: {unbiased_results}\")#| eval: false\n# Probability inequalities\nmarkov_bound &lt;- function(mean_val, t) {\n  if (t &lt;= 0) return(1)\n  min(mean_val / t, 1)\n}\n\nchebyshev_bound &lt;- function(k) {\n  if (k &lt;= 0) return(1)\n  min(1 / k^2, 1)\n}\n\n# Simulating convergence\ndemonstrate_lln &lt;- function(dist_func, n_max = 10000, ...) {\n  # dist_func should be a function like rnorm, rexp, etc.\n  samples &lt;- dist_func(n_max, ...)\n  cumulative_mean &lt;- cumsum(samples) / seq_len(n_max)\n  return(cumulative_mean)\n}\n\ndemonstrate_clt &lt;- function(dist_func, n, n_simulations = 10000, ...) {\n  sample_means &lt;- replicate(n_simulations, {\n    sample &lt;- dist_func(n, ...)\n    mean(sample)\n  })\n  return(sample_means)\n}\n\n# Estimator evaluation\nevaluate_estimator &lt;- function(estimator_func, true_value, n, \n                              sample_func, n_simulations = 10000) {\n  estimates &lt;- replicate(n_simulations, {\n    sample &lt;- sample_func(n)\n    estimator_func(sample)\n  })\n  \n  bias &lt;- mean(estimates) - true_value\n  variance &lt;- var(estimates)\n  mse &lt;- mean((estimates - true_value)^2)\n  \n  list(bias = bias, variance = variance, mse = mse)\n}\n\n# Example usage\n# LLN for exponential\nlln_exp &lt;- demonstrate_lln(rexp, n_max = 10000, rate = 1)\nplot(lln_exp, type = 'l', ylim = c(0.5, 1.5), main = \"LLN for Exponential(1)\")\nabline(h = 1, col = 'red', lty = 2) # True mean is 1\n\n# CLT for uniform\nclt_unif &lt;- demonstrate_clt(runif, n = 30, n_simulations = 10000)\nhist(clt_unif, breaks = 50, probability = TRUE, main = \"CLT for Uniform(0,1)\")\n# Variance of U(0,1) is 1/12. SD of sample mean = sqrt(Var(X)/n)\ncurve(dnorm(x, mean = 0.5, sd = sqrt((1/12)/30)), add = TRUE, col = 'red', lwd=2)\n\n# Compare variance estimators\nmle_var &lt;- function(x) { var(x) * (length(x) - 1) / length(x) }\n\ncompare_var_estimators &lt;- function(n) {\n  sample_func &lt;- function(k) rnorm(k, 0, 1) # True variance = 1\n  \n  unbiased_res &lt;- evaluate_estimator(\n    var,  # R's var() is the unbiased (n-1) version\n    true_value = 1, n = n, sample_func = sample_func\n  )\n  mle_res &lt;- evaluate_estimator(\n    mle_var,\n    true_value = 1, n = n, sample_func = sample_func\n  )\n  list(unbiased = unbiased_res, mle = mle_res)\n}\n\nprint(\"--- Comparing Variance Estimators (n=10) ---\")\nprint(compare_var_estimators(10))\n\n\n3.7.7 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\nThis table maps sections in these lecture notes to the corresponding sections in Wasserman (2013) (“All of Statistics” or AoS).\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding AoS Section(s)\n\n\n\n\nIntroduction and Motivation\nExpanded material from the slides, contextualizing convergence for machine learning.\n\n\nInequalities: Bounding the Unknown\n\n\n\n↳ Markov’s Inequality\nAoS §4.1 (Theorem 4.1)\n\n\n↳ Chebyshev’s Inequality\nAoS §4.1 (Theorem 4.2)\n\n\n↳ Hoeffding’s Inequality\nAoS §4.1 (Theorems 4.4 & 4.5)\n\n\nConvergence of Random Variables\n\n\n\n↳ The Need for Probabilistic Convergence\nAoS §5.1\n\n\n↳ Convergence in Probability\nAoS §5.2 (Definition 5.1)\n\n\n↳ Convergence in Distribution\nAoS §5.2 (Definition 5.1)\n\n\n↳ Comparing Modes of Convergence\nAoS §5.2 (Theorem 5.4)\n\n\n↳ Properties and Transformations\nAoS §5.2 (Theorem 5.5, including Slutsky’s Theorem)\n\n\nThe Two Fundamental Theorems of Statistics\n\n\n\n↳ The Law of Large Numbers (LLN)\nAoS §5.3 (The Weak Law of Large Numbers, Theorem 5.6)\n\n\n↳ The Central Limit Theorem (CLT)\nAoS §5.4 (Theorems 5.8 & 5.10)\n\n\nThe Language of Statistical Inference\n\n\n\n↳ From Probability to Inference\nAoS §6.1\n\n\n↳ Statistical Models\nAoS §6.2\n\n\n↳ Point Estimation\nAoS §6.3.1\n\n\n↳ How to Evaluate Estimators\nAoS §6.3.1 (covers bias, consistency, MSE)\n\n\n↳ The Bias-Variance Tradeoff\nAoS §6.3.1 (MSE decomposition, Theorem 6.9)\n\n\nChapter Summary and Connections\nNew summary material.\n\n\n\n\n\n\n\n\n3.7.8 Further Reading\n\nStatistical inference: Casella & Berger, “Statistical Inference”\nMachine learning perspective: Shalev-Shwartz & Ben-David, “Understanding Machine Learning: From Theory to Algorithms”\n\n\nRemember: Convergence and inference concepts are the bedrock of statistics. The Law of Large Numbers tells us why sampling works. The Central Limit Theorem tells us how to quantify uncertainty. The bias-variance tradeoff tells us how to choose good estimators. Master these ideas – they’re the key to everything that follows!\n\n\n\n\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/03-convergence-inference.html#footnotes",
    "href": "chapters/03-convergence-inference.html#footnotes",
    "title": "3  Convergence and The Basics of Inference",
    "section": "",
    "text": "Remember that \\nabla_\\theta f denotes the gradient of function f (\\theta; x) with respect to \\theta – its “vector derivative” with respect to \\theta in more than dimension.↩︎\nFor example, via a simple gradient descent step: \\theta_{t+1} = \\theta_t - \\alpha_t g where \\alpha_t &gt; 0 is the learning rate at step t.↩︎\nFor example, in the framework known as probably approximately correct (PAC) learning.↩︎\nThe symbol \\theta is almost universally reserved to represent generic “parameters” of a model in statistics and machine learning.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Convergence and The Basics of Inference</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html",
    "href": "chapters/04-nonparametric-bootstrap.html",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "",
    "text": "4.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#learning-objectives",
    "href": "chapters/04-nonparametric-bootstrap.html#learning-objectives",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "",
    "text": "Explain the definition and frequentist interpretation of a confidence interval\nApply the plug-in principle with the Empirical Distribution Function (EDF) to estimate statistical functionals\nUse the bootstrap to simulate the sampling distribution of a statistic and estimate its standard error\nConstruct and compare the three main types of bootstrap confidence intervals: Normal, percentile, and pivotal\nIdentify the assumptions and limitations of the bootstrap, recognizing common situations where it can fail\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter covers nonparametric estimation methods and the bootstrap. The material is adapted from Chapters 7 and 8 of Wasserman (2013), with reference to Chapter 6 for confidence intervals. Additional examples and computational perspectives have been added for data science applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#introduction-and-motivation",
    "href": "chapters/04-nonparametric-bootstrap.html#introduction-and-motivation",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.2 Introduction and Motivation",
    "text": "4.2 Introduction and Motivation\n\n4.2.1 Beyond Point Estimates: Quantifying Uncertainty\nImagine you’re a healthcare administrator planning for hospital capacity during an epidemic. Your data scientists provide a point estimate: “We expect 500 patients next week.” But is this estimate reliable enough to base critical decisions on? What if the true number could reasonably be anywhere from 300 to 700? A single point estimate, without quantifying its uncertainty, is often useless for decision-making. You need a plausible range – this is called a confidence interval.\nIn Chapter 3, we learned how to create point estimates – single “best guesses” for unknown quantities. We saw that the sample mean \\bar{X}_n estimates the population mean \\mu, and we even derived its standard error. But what about more complex statistics? How do we find the standard error of the median? The correlation coefficient? The 90th percentile?\nTraditional statistical theory provides formulas for simple cases, but quickly becomes intractable for complex statistics. This chapter introduces two powerful ideas that revolutionized modern statistics:\n\nThe Plug-In Principle: A simple, intuitive method for estimating almost any quantity by “plugging in” the empirical distribution for the true distribution.\nThe Bootstrap: A computational method for estimating the standard error and confidence interval of virtually any statistic, even when no formula exists.\n\nThese methods exemplify the computational approach to statistics that has become dominant in the era of cheap computing power. Instead of deriving complex mathematical formulas, we let the computer simulate what would happen if we could repeat our experiment many times.\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nConfidence interval\nLuottamusväli\nRange that contains parameter with specified probability\n\n\nCoverage\nPeitto\nProbability that interval contains true parameter\n\n\nEmpirical distribution function\nEmpiirinen kertymäfunktio\nData-based estimate of CDF\n\n\nStatistical functional\nTilastollinen funktionaali\nFunction of the distribution\n\n\nPlug-in estimator\nPistoke-estimaattori\nEstimate using empirical distribution\n\n\nBootstrap\nUusio-otanta\nResampling method for uncertainty\n\n\nResampling\nUudelleenotanta\nDrawing samples from data\n\n\nBootstrap sample\nBootstrap-otos\nSample drawn with replacement\n\n\nPercentile interval\nProsenttipiste-luottamusväli\nCI using bootstrap quantiles\n\n\nPivotal interval\nPivotaalinen luottamusväli\nCI using pivot quantity\n\n\nStandard error\nKeskivirhe\nStandard deviation of estimator\n\n\nMonte Carlo error\nMonte Carlo -virhe\nError from finite simulations",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#confidence-sets-the-foundation",
    "href": "chapters/04-nonparametric-bootstrap.html#confidence-sets-the-foundation",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.3 Confidence Sets: The Foundation",
    "text": "4.3 Confidence Sets: The Foundation\nBefore diving into nonparametric estimation and the bootstrap, we need to establish the formal framework for confidence intervals, a staple of classical statistics.\n\nA 1-\\alpha confidence interval for a parameter \\theta is an interval C_n = (a, b) where a = a(X_1, \\ldots, X_n) and b = b(X_1, \\ldots, X_n) are functions of the data such that \\mathbb{P}_{\\theta}(\\theta \\in C_n) \\geq 1 - \\alpha, \\quad \\text{for all } \\theta \\in \\Theta.\nIn other words, C_n encloses \\theta with probability 1-\\alpha. This probability is called the coverage of the interval.\n\nA common choice is \\alpha = 0.05 which yields 95\\% confidence intervals.\n\n4.3.1 Interpretation of Confidence Sets\nConfidence intervals are not probability statements about \\theta. The parameter \\theta is fixed; it’s the interval C_n that varies with different samples.\nThe correct interpretation is: if you repeatedly collect data and construct confidence intervals using the same procedure, (1-\\alpha) \\times 100\\% of those intervals will contain the true parameter. This is the frequentist guarantee.\nBayesian credible intervals provide a different type of statement – they give a probability that \\theta lies in the interval given your observed data. However, Bayesian intervals are not guaranteed to achieve frequentist coverage (containing the true parameter (1-\\alpha) proportion of the time across repeated sampling).\n\n\n\n\n\n\nCritical Point: What is Random?\n\n\n\nRemember that \\theta is fixed and C_n is random. The parameter doesn’t change; the interval does. Each time we collect new data, we get a different interval. The guarantee is that (1-\\alpha) \\times 100\\% of these intervals will contain the true parameter.\n\n\n\n\n\n\n\n\nAdvanced: When Coverage Fails\n\n\n\n\n\nThe frequentist guarantee is an average over all possible datasets. For any specific confidence interval procedure, there may exist parameter values where the actual coverage is less than the nominal (1-\\alpha) level.\nAdditionally, for some “unlucky” datasets (the \\alpha proportion), the computed interval may be far from the true parameter. This is an inherent limitation of the frequentist approach – it provides no guarantees for individual intervals, only for the long-run performance of the procedure.\nThis suggests different approaches may be preferred in different contexts:\n\nFrequentist methods: Well-suited for repeated analyses where long-run guarantees matter\nBayesian methods: May be preferred when prior information is available and you need probabilistic statements about parameters given your specific data\n\nBoth approaches are valuable tools for quantifying uncertainty.\n\n\n\n\n\n4.3.2 Normal-Based Confidence Intervals\nWhen an estimator \\hat{\\theta}_n is approximately normally distributed, we can form confidence intervals using the normal approximation. This is often the case for large samples due to the Central Limit Theorem.\nSuppose \\hat{\\theta}_n \\approx \\mathcal{N}(\\theta, \\widehat{\\text{se}}^2), where \\widehat{\\text{se}} is the (estimated) standard error of the estimator. Then we can standardize to get: \\frac{\\hat{\\theta}_n - \\theta}{\\widehat{\\text{se}}} \\approx \\mathcal{N}(0, 1)\nLet z_{\\alpha/2} = \\Phi^{-1}(1 - \\alpha/2) be the upper \\alpha/2 quantile of the standard normal distribution, where \\Phi is the standard normal CDF. This means:\n\n\\mathbb{P}(Z &gt; z_{\\alpha/2}) = \\alpha/2 for Z \\sim \\mathcal{N}(0, 1)\n\\mathbb{P}(-z_{\\alpha/2} &lt; Z &lt; z_{\\alpha/2}) = 1 - \\alpha\n\nTherefore: \\mathbb{P}\\left(-z_{\\alpha/2} &lt; \\frac{\\hat{\\theta}_n - \\theta}{\\widehat{\\text{se}}} &lt; z_{\\alpha/2}\\right) \\approx 1 - \\alpha\nRearranging to isolate \\theta: \\mathbb{P}\\left(\\hat{\\theta}_n - z_{\\alpha/2} \\widehat{\\text{se}} &lt; \\theta &lt; \\hat{\\theta}_n + z_{\\alpha/2} \\widehat{\\text{se}}\\right) \\approx 1 - \\alpha\nThis gives us the approximate (1-\\alpha) confidence interval: C_n = \\left(\\hat{\\theta}_n - z_{\\alpha/2} \\widehat{\\text{se}}, \\hat{\\theta}_n + z_{\\alpha/2} \\widehat{\\text{se}}\\right)\nFor the common case of 95% confidence intervals (\\alpha = 0.05):\n\nz_{0.025} = \\Phi^{-1}(0.975) \\approx 1.96 \\approx 2\nThis leads to the familiar rule of thumb: \\hat{\\theta}_n \\pm 2 \\widehat{\\text{se}}\n\n\n\n\n\n\n\nExample: Confidence Interval for the Mean\n\n\n\nFor the sample mean \\bar{X}_n with known population variance \\sigma^2:\n\nEstimator: \\hat{\\theta}_n = \\bar{X}_n\nStandard error: \\text{se} = \\sigma/\\sqrt{n}\n95% CI: \\bar{X}_n \\pm 1.96 \\cdot \\sigma/\\sqrt{n}\n\nIf \\sigma is unknown, we substitute the sample standard deviation s to get:\n\n95% CI: \\bar{X}_n \\pm 1.96 \\cdot s/\\sqrt{n}\n\nFor small samples, we would use the t-distribution instead of the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#the-plug-in-principle-a-general-method-for-estimation",
    "href": "chapters/04-nonparametric-bootstrap.html#the-plug-in-principle-a-general-method-for-estimation",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.4 The Plug-In Principle: A General Method for Estimation",
    "text": "4.4 The Plug-In Principle: A General Method for Estimation\nThis lecture focuses on nonparametric estimation, and the plug-in principle is one key instrument of nonparametric statistics.\n\n4.4.1 Estimating the Entire Distribution: The EDF\nThe plug-in principle is a simple idea that provides a unified framework for nonparametric estimation that works for virtually any statistical problem.\nThe Core Idea: When we need to estimate some property of an unknown distribution F, we simply:\n\nEstimate the distribution itself using our data\nCalculate the property using our estimated distribution\n\nThe most fundamental nonparametric estimate is of the CDF itself. Let X_1, \\ldots, X_n \\sim F be an i.i.d. sample where F is a distribution function on the real line. Since we don’t know the true CDF F, we estimate it with the Empirical Distribution Function (EDF).\n\nThe Empirical Distribution Function (EDF) \\hat{F}_n is the CDF that puts probability mass 1/n on each data point. Formally: \\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n I(X_i \\leq x) where I(X_i \\leq x) = \\begin{cases}\n1 & \\text{if } X_i \\leq x \\\\\n0 & \\text{if } X_i &gt; x\n\\end{cases}\n\nIntuitively, \\hat{F}_n(x) is simply the proportion of data points less than or equal to x. It’s the most natural estimate of F(x) = \\mathbb{P}(X \\leq x).\nProperties of the EDF:\nFor any fixed point x:\n\nUnbiased: \\mathbb{E}(\\hat{F}_n(x)) = F(x)\nVariance: \\mathbb{V}(\\hat{F}_n(x)) = \\frac{F(x)(1-F(x))}{n}\nMSE: \\text{MSE}(\\hat{F}_n(x)) = \\mathbb{V}(\\hat{F}_n(x)) = \\frac{F(x)(1-F(x))}{n} \\to 0 as n \\to \\infty\nConsistent: \\hat{F}_n(x) \\xrightarrow{P} F(x) as n \\to \\infty\n\nBut the EDF is even better than these pointwise properties suggest:\n\nThe EDF converges to the true CDF uniformly: \\sup_x |\\hat{F}_n(x) - F(x)| \\xrightarrow{P} 0\n\nThis is a remarkably strong guarantee – the EDF approximates the true CDF well everywhere, not just at individual points.\nLet’s visualize the EDF with an example dataset:\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Simulate nerve data (waiting times between pulses)\n# Using exponential distribution as a model for waiting times\nnp.random.seed(42)\nn = 100\nnerve_data = np.random.exponential(scale=0.5, size=n)\n\n# Create the EDF plot\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# Sort data for plotting\nsorted_data = np.sort(nerve_data)\n\n# Plot the EDF as a step function\nax.step(sorted_data, np.arange(1, n+1)/n, where='post', \n        linewidth=2, color='blue', label='Empirical CDF')\n\n# Add rug plot to show data points\nax.plot(sorted_data, np.zeros_like(sorted_data), '|', \n        color='black', markersize=10, alpha=0.5)\n\n# Add true CDF for comparison\nx_range = np.linspace(0, max(sorted_data)*1.1, 1000)\ntrue_cdf = 1 - np.exp(-x_range/0.5)  # Exponential CDF\nax.plot(x_range, true_cdf, 'r--', linewidth=2, alpha=0.7, label='True CDF')\n\nax.set_xlabel('Waiting time (seconds)')\nax.set_ylabel('Cumulative probability')\nax.set_title('Empirical Distribution Function for Nerve Pulse Data (Simulated)')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(0, max(sorted_data)*1.05)\nax.set_ylim(0, 1.05)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe step function shows \\hat{F}_n, jumping by 1/n at each data point. The vertical lines at the bottom show the actual data points. Notice how the EDF closely follows the true CDF (shown for comparison).\n\n\n\n\n\n\nAdvanced: Confidence Bands for the CDF\n\n\n\n\n\nWe can construct confidence bands for the entire CDF using the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality:\n\\mathbb{P}\\left(\\sup_x |F(x) - \\hat{F}_n(x)| &gt; \\epsilon\\right) \\leq 2e^{-2n\\epsilon^2}\nThis leads to a 1-\\alpha confidence band: L(x) = \\max\\{\\hat{F}_n(x) - \\epsilon_n, 0\\} U(x) = \\min\\{\\hat{F}_n(x) + \\epsilon_n, 1\\} where \\epsilon_n = \\sqrt{\\frac{1}{2n}\\log\\left(\\frac{2}{\\alpha}\\right)}.\n\n\nShow code\n# Add confidence bands to previous plot\nalpha = 0.05\nepsilon_n = np.sqrt(np.log(2/alpha) / (2*n))\n\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# Plot EDF\nax.step(sorted_data, np.arange(1, n+1)/n, where='post', \n        linewidth=2, color='blue', label='Empirical CDF')\n\n# Add confidence bands\ny_values = np.arange(1, n+1)/n\nlower_band = np.maximum(y_values - epsilon_n, 0)\nupper_band = np.minimum(y_values + epsilon_n, 1)\n\nax.fill_between(sorted_data, lower_band, upper_band, \n                step='post', alpha=0.3, color='gray', \n                label=f'{int((1-alpha)*100)}% Confidence band')\n\n# Add rug plot\nax.plot(sorted_data, np.zeros_like(sorted_data), '|', \n        color='black', markersize=10, alpha=0.5)\n\nax.set_xlabel('Waiting time (seconds)')\nax.set_ylabel('Cumulative probability')\nax.set_title('EDF with Confidence Band')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(0, max(sorted_data)*1.05)\nax.set_ylim(0, 1.05)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Width of confidence band: ±{epsilon_n:.3f}\")\nprint(f\"This means we're 95% confident the true CDF lies within this gray region\")\n\n\n\n\n\n\n\n\n\nWidth of confidence band: ±0.136\nThis means we're 95% confident the true CDF lies within this gray region\n\n\n\n\n\n\n\n4.4.2 Estimating Functionals: The Plug-In Estimator\nNow that we can estimate the entire distribution, we can estimate any property of it. A statistical functional is any function of the CDF F or the PDF f.\nExamples include:\n\nThe mean: \\mu = \\int x f(x) d x\nThe variance: \\sigma^2 = \\int (x-\\mu)^2 f(x) d x\n\nThe median: m = F^{-1}(1/2)\n\n\nThe plug-in estimator of \\theta = T(F) is defined by: \\hat{\\theta}_n = T(\\hat{F}_n)\n\nIn other words, just plug in the empirical CDF \\hat{F}_n for the unknown CDF F.\nThis principle is remarkably general. To estimate any property of the population, we calculate that same property on our empirical distribution. Let’s see how this works for various functionals:\nThe Mean:\nLet \\mu = T(F) = \\int x f(x) dx.\nThe plug-in estimator is: \\hat{\\mu}_n = \\sum x \\hat{f}_n(x) = \\frac{1}{n}\\sum_{i=1}^n X_i = \\bar{X}_n\nThe plug-in estimator for the mean is just the sample mean!\nThe Variance:\nLet \\sigma^2 = T(F) = \\mathbb{V}(X) = \\int x^2 f(x) d x - \\left(\\int x f(x) d x\\right)^2.\nThe plug-in estimator is: \\begin{align*}\n\\hat{\\sigma}^2_n &= \\sum_x x^2 \\hat{f}_n(x) - \\left(\\sum_x x \\hat{f}_n(x)\\right)^2 \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n X_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)^2 \\\\\n& = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2\n\\end{align*}\n\n\n\n\n\n\nAlternative: The Sample Variance\n\n\n\nAnother common estimator of \\sigma^2 is the sample variance with the n-1 correction: S_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2\nThis estimator is unbiased (i.e., \\mathbb{E}[S_n^2] = \\sigma^2) and is almost identical to the plug-in estimator in practice. The factor \\frac{n}{n-1} makes little difference for moderate to large samples. Most statistical software uses the n-1 version by default.\n\n\nThe Median:\nThe median is the value that splits the distribution in half. For the theoretical distribution: m = T(F) = F^{-1}(0.5)\nThe plug-in estimator is simply the sample median - the middle value when we sort our data. For an odd number of observations, it’s the middle value. For an even number, it’s the average of the two middle values.\n\n\n\n\n\n\nAdvanced: Other Statistical Functionals\n\n\n\n\n\nSkewness (measures asymmetry): T(F) = \\frac{\\mathbb{E}[(X-\\mu)^3]}{\\sigma^3} \\implies \\hat{\\kappa}_n = \\frac{\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^3}{(\\hat{\\sigma}^2_n)^{3/2}}\nCorrelation for bivariate data (X,Y): T(F) = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\implies \\hat{\\rho}_n = \\frac{\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)(Y_i-\\bar{Y}_n)}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2}\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\bar{Y}_n)^2}}\nQuantiles in general: T(F) = F^{-1}(p) \\implies \\hat{q}_p = \\hat{F}_n^{-1}(p)\nSince \\hat{F}_n is a step function, we define: \\hat{F}_n^{-1}(p) = \\inf\\{x : \\hat{F}_n(x) \\geq p\\}\nThis gives us the sample quantile at level p.\n\n\n\nConfidence Intervals for Plug-in Estimators:\nWhen the plug-in estimator \\hat{\\theta}_n = T(\\hat{F}_n) is approximately normally distributed, we can form confidence intervals using: \\hat{\\theta}_n \\pm z_{\\alpha/2} \\widehat{\\text{se}}\nas we saw earlier in the Normal-Based Confidence Intervals section.\nThe challenge is finding \\widehat{\\text{se}}. For the mean, we know from theory that \\text{se}(\\bar{X}_n) = \\sigma/\\sqrt{n}, which we can estimate by plugging in \\hat{\\sigma} to get: \\bar{X}_n \\pm z_{\\alpha/2} \\frac{\\hat{\\sigma}}{\\sqrt{n}}\nBut what about the median? The 90th percentile? The interquartile range? For most functionals, there is no simple formula for the standard error.\nThis is where the bootstrap comes to the rescue, as we see next.\n\n\n\n\n\n\nRecap: Nonparametric Estimation\n\n\n\nLet X_1, \\ldots, X_n \\sim F be an i.i.d. sample where F is a distribution function on the real line.\n\nThe empirical distribution function \\hat{F}_n is the CDF that puts mass 1/n at each data point X_i\nA statistical functional T(F) is any function of F (e.g., mean, variance, median)\nThe plug-in estimator of \\theta = T(F) is \\hat{\\theta}_n = T(\\hat{F}_n)\n\nWe’ve seen how to create point estimates for any functional. The challenge is quantifying their uncertainty when no formula exists for the standard error.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#the-bootstrap-simulating-uncertainty",
    "href": "chapters/04-nonparametric-bootstrap.html#the-bootstrap-simulating-uncertainty",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.5 The Bootstrap: Simulating Uncertainty",
    "text": "4.5 The Bootstrap: Simulating Uncertainty\n\n4.5.1 The Core Idea\nThe bootstrap, invented by Bradley Efron in 1979, is one of the most important statistical innovations of the 20th century. It provides a general, computational method for assessing the uncertainty of virtually any statistic or functional of the data.\nLet T_n = g(X_1, \\ldots, X_n) be our statistic of interest (e.g., the median, the variance, a given quantile). What’s its variability?\nThe key insight is simple: We can learn about the variability of our statistic by seeing how it varies across different samples. Since we only have one sample, we create new samples by resampling from our data.\nHere’s the crucial diagram that illustrates the bootstrap principle:\nReal World:      F   ==&gt;   X₁,...,Xₙ   ==&gt;   Tₙ = g(X)\n                 ↑                            ↑\n                 Unknown                      What we want to understand\n\nBootstrap World: F̂ₙ  ==&gt;   X₁*,...,Xₙ*  ==&gt;  Tₙ* = g(X*)\n                 ↑                            ↑\n                 Known!                       What we can simulate\nIn the real world:\n\nNature draws from the unknown distribution F\nWe observe one sample X_1, \\ldots, X_n\nWe calculate our statistic T_n = g(X_1, \\ldots, X_n)\nWe want to know the sampling distribution of T_n\n\nIn the bootstrap world:\n\nWe draw from the known empirical distribution \\hat{F}_n (see below)\nWe get a bootstrap sample X_1^*, \\ldots, X_n^*\nWe calculate the bootstrap statistic T_n^* = g(X_1^*, \\ldots, X_n^*)\nWe can repeat the process multiple times to simulate the sampling distribution of T_n^*\n\nThe Bootstrap Principle: The distribution of T_n^* around T_n approximates the distribution of T_n around T(F).\nHow do we draw from \\hat{F}_n? Remember that \\hat{F}_n puts mass 1/n at each observed data point. Therefore:\n\n\n\n\n\n\nWarning\n\n\n\nKey Point: Drawing from \\hat{F}_n means sampling with replacement from the original data. Each bootstrap sample contains n observations, some repeated, some omitted.\n\n\n\n\n4.5.2 Bootstrap Variance and Standard Error Estimation\nLet’s make this concrete with an algorithm:\nBootstrap Algorithm for Standard Error Estimation:\n\nFor b = 1, \\ldots, B:\n\nDraw a bootstrap sample X_1^*, \\ldots, X_n^* by sampling with replacement from \\{X_1, \\ldots, X_n\\}\nCompute the statistic for this bootstrap sample: T_{n,b}^* = g(X_1^*, \\ldots, X_n^*)\n\nThe bootstrap estimate of the standard error is: \\widehat{\\text{se}}_{\\text{boot}} = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B \\left(T_{n,b}^* - \\bar{T}_n^*\\right)^2} where \\bar{T}_n^* = \\frac{1}{B}\\sum_{b=1}^B T_{n,b}^*\n\nLet’s implement this for a concrete example – estimating the standard error of the median:\nPythonR\nimport numpy as np\n\ndef bootstrap_se(data, statistic, B=1000):\n    \"\"\"\n    Compute bootstrap standard error of a statistic.\n    \n    Parameters:\n    -----------\n    data : array-like\n        Original sample\n    statistic : function\n        Function that computes the statistic of interest\n    B : int\n        Number of bootstrap replications\n    \n    Returns:\n    --------\n    se : float\n        Bootstrap standard error\n    boot_samples : array\n        Bootstrap replications of the statistic\n    \"\"\"\n    n = len(data)\n    boot_samples = np.zeros(B)\n    \n    for b in range(B):\n        # Draw bootstrap sample\n        x_star = np.random.choice(data, size=n, replace=True)\n        # Compute statistic\n        boot_samples[b] = statistic(x_star)\n    \n    # Compute standard error\n    se = np.std(boot_samples, ddof=1)\n    \n    return se, boot_samples\n\n# Example: Standard error of the median\nnp.random.seed(42)\ndata = np.random.exponential(scale=2, size=50)\n\n# Point estimate\nmedian_est = np.median(data)\n\n# Bootstrap standard error\nse_boot, boot_medians = bootstrap_se(data, np.median, B=2000)\n\nprint(f\"Sample median: {median_est:.3f}\")\nprint(f\"Bootstrap SE: {se_boot:.3f}\")\nprint(f\"Approximate 95% CI: ({median_est - 2*se_boot:.3f}, {median_est + 2*se_boot:.3f})\")\n\nSample median: 1.146\nBootstrap SE: 0.276\nApproximate 95% CI: (0.594, 1.697)\n\nbootstrap_se &lt;- function(data, statistic, B = 1000) {\n  #' Compute bootstrap standard error of a statistic\n  #' \n  #' @param data Original sample vector\n  #' @param statistic Function that computes the statistic of interest\n  #' @param B Number of bootstrap replications\n  #' @return List with standard error and bootstrap samples\n  \n  n &lt;- length(data)\n  boot_samples &lt;- numeric(B)\n  \n  for (b in 1:B) {\n    # Draw bootstrap sample\n    x_star &lt;- sample(data, size = n, replace = TRUE)\n    # Compute statistic\n    boot_samples[b] &lt;- statistic(x_star)\n  }\n  \n  # Compute standard error\n  se &lt;- sd(boot_samples)\n  \n  return(list(se = se, boot_samples = boot_samples))\n}\n\n# Example: Standard error of the median\nset.seed(42)\ndata &lt;- rexp(50, rate = 1/2)\n\n# Point estimate\nmedian_est &lt;- median(data)\n\n# Bootstrap standard error\nresult &lt;- bootstrap_se(data, median, B = 2000)\nse_boot &lt;- result$se\nboot_medians &lt;- result$boot_samples\n\ncat(sprintf(\"Sample median: %.3f\\n\", median_est))\ncat(sprintf(\"Bootstrap SE: %.3f\\n\", se_boot))\ncat(sprintf(\"Approximate 95%% CI: (%.3f, %.3f)\\n\", \n            median_est - 2*se_boot, median_est + 2*se_boot))\nLet’s visualize the bootstrap distribution:\n\n\nShow code\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n\n# Left panel: Original data\nax1.hist(data, bins=20, density=True, alpha=0.7, color='blue', edgecolor='black')\nax1.axvline(median_est, color='red', linestyle='--', linewidth=2, label=f'Median = {median_est:.3f}')\nax1.set_xlabel('Value')\nax1.set_ylabel('Density')\nax1.set_title('Original Data')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right panel: Bootstrap distribution\nax2.hist(boot_medians, bins=30, density=True, alpha=0.7, color='green', edgecolor='black')\nax2.axvline(median_est, color='red', linestyle='--', linewidth=2)\nax2.axvline(median_est - 2*se_boot, color='orange', linestyle=':', linewidth=2)\nax2.axvline(median_est + 2*se_boot, color='orange', linestyle=':', linewidth=2, \n            label='95% CI')\nax2.set_xlabel('Bootstrap median')\nax2.set_ylabel('Density')\nax2.set_title('Bootstrap Distribution')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe left panel shows our original data. The right panel shows the distribution of the median across 2000 bootstrap samples. This distribution tells us about the uncertainty in our median estimate.\n\n\n\n\n\n\nTwo Sources of Error\n\n\n\nThe bootstrap involves two approximations:\n\nStatistical Approximation: \\mathbb{V}_F(T_n) \\approx \\mathbb{V}_{\\hat{F}_n}(T_n)\n\nThis depends on how well \\hat{F}_n approximates F\nWe can’t control this – it depends on our sample size n\n\nMonte Carlo Approximation: \\mathbb{V}_{\\hat{F}_n}(T_n) \\approx v_{\\text{boot}}\n\nThis depends on the number of bootstrap samples B\nWe can make this arbitrarily small by increasing B\nTypically B \\geq 1000 is sufficient\n\n\nIn formulas: \\mathbb{V}_F(T_n) \\underbrace{\\approx}_{\\text{not so small}} \\mathbb{V}_{\\hat{F}_n}(T_n) \\underbrace{\\approx}_{\\text{small}} v_{\\text{boot}}\nRemember: by increasing the bootstrap samples B we can reduce the Monte Carlo error (due to simulation), but we cannot improve the statistical approximation error without obtaining more real data.\n\n\n\n\n\n\n\n\nExample: Monte Carlo Error Decreases With More Bootstrap Samples\n\n\n\n\n\nLet’s verify that the Monte Carlo error decreases with B:\n\n\nShow code\n# Show convergence of bootstrap SE as B increases\nB_values = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\nse_estimates = []\n\nfor B in B_values:\n    se, _ = bootstrap_se(data, np.median, B=B)\n    se_estimates.append(se)\n\nplt.figure(figsize=(7, 4))\nplt.plot(B_values, se_estimates, 'bo-', linewidth=2, markersize=8)\nplt.axhline(se_estimates[-1], color='red', linestyle='--', \n            label=f'Converged value ≈ {se_estimates[-1]:.3f}')\nplt.xlabel('Number of bootstrap samples (B)')\nplt.ylabel('Bootstrap SE estimate')\nplt.title('Monte Carlo Error Decreases with B')\nplt.xscale('log')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe fluctuations for small B are due to Monte Carlo variability. As B increases, the bootstrap standard error estimate stabilizes. Still, this is only one component of the error – to improve further we need additional real data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#bootstrap-confidence-intervals",
    "href": "chapters/04-nonparametric-bootstrap.html#bootstrap-confidence-intervals",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.6 Bootstrap Confidence Intervals",
    "text": "4.6 Bootstrap Confidence Intervals\n\n4.6.1 Three Common Methods\nNow that we can estimate the sampling distribution of any statistic via the bootstrap, we can construct confidence intervals. But how exactly should we use the bootstrap distribution to form an interval?\nThere are three main approaches, each with different strengths and weaknesses. We’ll illustrate all three using a real example: estimating the correlation between a country’s economic prosperity and the health of its population.\n\n\n\n\n\n\nExample: European Health and Wealth Data\n\n\n\nThis example explores the correlation between a country’s wealth (GDP per capita) and the average lifespan of its citizens (life expectancy at birth). The data is for a subset of EU countries, with GDP per capita for 2025 and life expectancy from 2023.\n\n\nShow code\n# European Health and Wealth Data\n# GDP per capita (2025 forecast), Life Expectancy at birth (2023) for a subset of EU countries.\ncountries = [\n    'Germany', 'France', 'Italy', 'Spain', 'Netherlands', 'Poland', \n    'Belgium', 'Sweden', 'Austria', 'Ireland', 'Denmark', 'Finland', \n    'Portugal', 'Greece', 'Czech Republic'\n]\n\ngdp_per_capita = np.array([\n    55911, 46792, 41091, 36192, 70480, 26805, 57772, 58100, 58192, \n    108919, 74969, 54163, 30002, 25756, 33039\n])\n\nlife_expectancy = np.array([\n    81.38, 83.33, 83.72, 83.67, 82.16, 78.63, 82.11, 83.26, 81.96, \n    82.41, 81.93, 81.91, 82.36, 81.86, 79.83\n])\n\n# Combine into a single dataset for resampling\ndata_combined = np.column_stack((gdp_per_capita, life_expectancy))\nn = len(life_expectancy)\n\n# Define correlation function\ndef correlation(data):\n    x, y = data[:, 0], data[:, 1]\n    return np.corrcoef(x, y)[0, 1]\n\n# Original correlation\nrho_hat = correlation(data_combined)\nprint(f\"Sample correlation: {rho_hat:.3f}\")\n\n\nSample correlation: 0.238\n\n\n\n\nNow let’s generate bootstrap samples:\n\n\nShow code\n# Bootstrap the correlation\nB = 5000\nboot_correlations = np.zeros(B)\n\nnp.random.seed(42)\nfor b in range(B):\n    # Resample pairs (important to maintain pairing!)\n    indices = np.random.choice(n, size=n, replace=True)\n    boot_sample = data_combined[indices]\n    boot_correlations[b] = correlation(boot_sample)\n\n# Bootstrap standard error\nse_boot = np.std(boot_correlations, ddof=1)\nprint(f\"Bootstrap SE: {se_boot:.3f}\")\n\n\nBootstrap SE: 0.241\n\n\nLet’s visualize the bootstrap distribution:\n\n\nShow code\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n\n# Scatter plot of original data\nax1.scatter(gdp_per_capita, life_expectancy, alpha=0.6, s=50)\nax1.set_xlabel('GDP per Capita (US$)')\nax1.set_ylabel('Life Expectancy (Years)')\nax1.set_title(f'European Countries (ρ = {rho_hat:.3f})')\nax1.grid(True, alpha=0.3)\n\n# Bootstrap distribution\nax2.hist(boot_correlations, bins=40, density=True, alpha=0.7, \n         color='green', edgecolor='black')\nax2.axvline(rho_hat, color='red', linestyle='--', linewidth=2, \n            label=f'Original ρ = {rho_hat:.3f}')\nax2.set_xlabel('Bootstrap Correlation')\nax2.set_ylabel('Density')\nax2.set_title('Bootstrap Distribution')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice that the bootstrap distribution is somewhat skewed. This skewness will affect our confidence intervals.\n\n\n4.6.2 Comparing Bootstrap Confidence Intervals\nLet \\hat{T}_n = T_n(\\hat{F}_n) be the statistic evaluated on the empirical distribution, and T_n^* the bootstrap statistic.\nMethod 1: Normal IntervalMethod 2: Percentile IntervalMethod 3: Pivotal IntervalFormula:\n\\(\\hat{T}_n \\pm z_{\\alpha/2} \\widehat{\\text{se}}_{\\text{boot}}\\)This is the simplest method – we just use the bootstrap standard\nerror in the usual normal-based formula.\n# Normal interval\nalpha = 0.05\nz_alpha = stats.norm.ppf(1 - alpha/2)\nnormal_lower = rho_hat - z_alpha * se_boot\nnormal_upper = rho_hat + z_alpha * se_boot\n\nprint(f\"95% Normal interval: ({normal_lower:.3f}, {normal_upper:.3f})\")\n\n95% Normal interval: (-0.235, 0.711)\n\nPros:\nSimple and intuitive\nOnly requires the standard error, not the full distribution\nFamiliar to those who know basic statistics\nCons:\nAssumes the sampling distribution is approximately normal\nCan give nonsensical intervals (e.g., correlation &gt; 1)\nIgnores skewness in the bootstrap distribution\nLet’s check if our interval makes sense:\nif normal_upper &gt; 1:\n    print(f\"Warning: Upper limit {normal_upper:.3f} exceeds 1!\")\n    print(\"This is impossible for a correlation!\")\nFormula:\n\\(\\left(T^*_{n, \\alpha/2}, T^*_{n, 1-\\alpha/2}\\right)\\)where \\(T^*_{n,\\beta}\\) denotes the\n\\(\\beta\\) sample quantile of the\nbootstrapped statistic values\n\\(T^*_n\\).The percentile interval method uses the\n\\(\\alpha/2\\) and\n\\(1-\\alpha/2\\) quantiles of the\nbootstrap distribution directly.\n# Percentile interval\npercentile_lower = np.percentile(boot_correlations, 100 * alpha/2)\npercentile_upper = np.percentile(boot_correlations, 100 * (1 - alpha/2))\n\nprint(f\"95% Percentile interval: ({percentile_lower:.3f}, {percentile_upper:.3f})\")\n\n95% Percentile interval: (-0.389, 0.596)\n\nPros:\nDoesn’t assume normality – adapts to the actual shape\nAlways respects parameter bounds (correlation stays in [-1, 1])\nSimple to understand: “middle 95% of bootstrap values”\nWorks well when the bootstrap distribution is approximately\nunbiased\nCons:\nCan have poor coverage when there’s bias\nNot transformation-invariant\nMay not be accurate for small samples\nFormula:\n\\(\\left(2\\hat{T}_n - T^*_{n, 1-\\alpha/2}, 2\\hat{T}_n - T^*_{n, \\alpha/2}\\right)\\)This method assumes that the distribution of\n\\(T_n - \\theta\\) is approximately the\nsame as the distribution of\n\\(T_n^* - T_n\\), where\n\\(\\theta\\) is the true parameter.\n# Pivotal interval  \npivotal_lower = 2 * rho_hat - np.percentile(boot_correlations, 100 * (1 - alpha/2))\npivotal_upper = 2 * rho_hat - np.percentile(boot_correlations, 100 * alpha/2)\n\nprint(f\"95% Pivotal interval: ({pivotal_lower:.3f}, {pivotal_upper:.3f})\")\n\n95% Pivotal interval: (-0.120, 0.865)\n\nPros:\nOften more accurate than the other two methods\nCorrects for bias in the estimator\nTransformation-respecting (invariant under monotone\ntransformations)\nCons:\nLess intuitive – the formula seems backwards at first\nCan occasionally give values outside parameter bounds\nRequires symmetric error distribution for best performance\n\nLet’s compare all three intervals visually:\n\n\nShow code\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# Plot bootstrap distribution\nhistogram_data = ax.hist(boot_correlations, bins=40, density=True, alpha=0.5, \n                        color='gray', edgecolor='black', label='Bootstrap distribution')\n\n# Get the maximum height for positioning intervals\nmax_density = max(histogram_data[0])\n\n# Add vertical lines for original estimate\nax.axvline(rho_hat, color='red', linestyle='-', linewidth=3, label='Original estimate')\n\n# Add confidence intervals overlaid on the histogram\nmethods = ['Normal', 'Percentile', 'Pivotal']\nintervals = [(normal_lower, normal_upper), \n             (percentile_lower, percentile_upper),\n             (pivotal_lower, pivotal_upper)]\ncolors = ['blue', 'green', 'orange']\n\n# Position intervals at different heights on the histogram\ny_positions = [max_density * 0.2, max_density * 0.15, max_density * 0.1]\n\nfor method, (lower, upper), color, y_pos in zip(methods, intervals, colors, y_positions):\n    # Draw the interval line\n    ax.plot([lower, upper], [y_pos, y_pos], color=color, linewidth=4, \n            marker='|', markersize=10, label=f'{method}')\n    # Add vertical lines at interval endpoints\n    ax.vlines([lower, upper], 0, y_pos, color=color, linestyle=':', alpha=0.5)\n\nax.set_xlabel('Correlation')\nax.set_ylabel('Density')\nax.set_title('Bootstrap Distribution with Confidence Intervals')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim(0, max_density * 1.1)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s summarize the confidence intervals in a table for easy comparison:\n\n\n| Method     | Lower Bound | Upper Bound | Width |\n|------------|-------------|-------------|-------|\n| Normal     |   -0.235    |    0.711    | 0.946 |\n| Percentile |   -0.389    |    0.596    | 0.986 |\n| Pivotal    |   -0.120    |    0.865    | 0.986 |\n\n\nIn this example:\n\nThe Normal interval is symmetric around the point estimate, ignoring the skewness\nThe Percentile interval reflects the skewness of the bootstrap distribution\n\nThe Pivotal interval adjusts for bias and is slightly shifted from the percentile interval\n\n\n\n\n\n\n\nAdvanced: Justification for the Pivotal Interval\n\n\n\n\n\nThe pivotal method seems counterintuitive at first. Why do we subtract the upper quantile from 2\\hat{T}_n to get the lower bound?\nThe key insight is that the pivotal interval assumes the error \\hat{T}_n - \\theta (where \\theta = T(F) is the true value) behaves similarly to the bootstrap error T_n^* - \\hat{T}_n.\nQuick derivation: Start with the bootstrap distribution: \\mathbb{P}( T^*_{n,\\alpha/2} \\leq T_n^* \\leq T^*_{n,1-\\alpha/2} ) = 1-\\alpha\nSubtract \\hat{T}_n throughout: \\mathbb{P}( T^*_{n,\\alpha/2} - \\hat{T}_n \\leq T_n^* - \\hat{T}_n \\leq T^*_{n,1-\\alpha/2} - \\hat{T}_n ) = 1-\\alpha\nThe key assumption: The bootstrap error T_n^* - \\hat{T}_n has approximately the same distribution as the real error \\hat{T}_n - \\theta. Therefore: \\mathbb{P}( T^*_{n,\\alpha/2} - \\hat{T}_n \\leq \\hat{T}_n - \\theta \\leq T^*_{n,1-\\alpha/2} - \\hat{T}_n ) \\approx 1-\\alpha\nRearranging to isolate \\theta: \\mathbb{P}( 2\\hat{T}_n - T^*_{n,1-\\alpha/2} \\leq \\theta \\leq 2\\hat{T}_n - T^*_{n,\\alpha/2} ) \\approx 1-\\alpha\nThis gives us the pivotal interval: (2\\hat{T}_n - T^*_{n,1-\\alpha/2}, 2\\hat{T}_n - T^*_{n,\\alpha/2}).\nIntuition: If bootstrap values tend to be above our estimate, then our estimate is probably below the truth by a similar amount. The “2×estimate minus bootstrap quantile” formula automatically corrects for this bias.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#bootstrap-application-higher-moments",
    "href": "chapters/04-nonparametric-bootstrap.html#bootstrap-application-higher-moments",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.7 Bootstrap Application: Higher Moments",
    "text": "4.7 Bootstrap Application: Higher Moments\nThe following example demonstrates both the power and limitations of the bootstrap.\n\n\n\n\n\n\nExample: Bootstrap for Higher Moments\n\n\n\nConsider a sample of n = 20 observations from a standard normal distribution \\mathcal{N}(0, 1). We’ll use the bootstrap to estimate confidence intervals for two related statistics:\n\nT^{(1)}(F) = \\mathbb{E}[X^4] - the fourth raw moment\nT^{(2)}(F) = \\mathbb{E}[(X - \\mu)^4] - the fourth central moment\n\nFor the standard normal, both have the same true value: T^{(1)}(F) = T^{(2)}(F) = 3.\nThis example is valuable because:\n\nWe can compute the true sampling distribution via simulation\nIt shows how bootstrap performs for non-standard statistics\nIt reveals differences between seemingly similar estimators\n\n\n\nLet’s implement this comparison:\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set up the experiment\nnp.random.seed(42)\nn = 20\ntrue_value = 3  # True 4th moment for N(0,1)\n\n# Generate one sample\nsample = np.random.normal(0, 1, n)\n\n# Compute point estimates\nT1_hat = np.mean(sample**4)  # Raw 4th moment\nT2_hat = np.mean((sample - np.mean(sample))**4)  # Central 4th moment\n\nprint(f\"True value: {true_value}\")\nprint(f\"T¹ (raw 4th moment): {T1_hat:.3f}\")\nprint(f\"T² (central 4th moment): {T2_hat:.3f}\")\n\n# Bootstrap distributions\nB = 2000\nboot_T1 = np.zeros(B)\nboot_T2 = np.zeros(B)\n\nfor b in range(B):\n    boot_sample = np.random.choice(sample, size=n, replace=True)\n    boot_T1[b] = np.mean(boot_sample**4)\n    boot_T2[b] = np.mean((boot_sample - np.mean(boot_sample))**4)\n\n# True sampling distributions (via simulation)\nn_true = 10000\ntrue_T1 = np.zeros(n_true)\ntrue_T2 = np.zeros(n_true)\n\nfor i in range(n_true):\n    true_sample = np.random.normal(0, 1, n)\n    true_T1[i] = np.mean(true_sample**4)\n    true_T2[i] = np.mean((true_sample - np.mean(true_sample))**4)\n\n# Determine common x-axis range to show the dramatic difference\nx_min = min(np.min(boot_T1), np.min(boot_T2), np.min(true_T1), np.min(true_T2))\nx_max = max(np.max(boot_T1), np.max(boot_T2), np.max(true_T1), np.max(true_T2))\n\n# Create figure with 4 subplots\nfig, axes = plt.subplots(2, 2, figsize=(8, 10))\n\n# T1: Raw 4th moment\n# Bootstrap distribution\nax1 = axes[0, 0]\nax1.hist(boot_T1, bins=40, density=True, alpha=0.7, color='blue', \n         edgecolor='black', label='Bootstrap')\nax1.axvline(T1_hat, color='red', linestyle='--', linewidth=2, \n            label=f'Estimate = {T1_hat:.2f}')\nax1.axvline(true_value, color='black', linestyle=':', linewidth=2, \n            label=f'True = {true_value}')\nax1.set_title('T¹: Bootstrap Distribution')\nax1.set_xlabel('Value')\nax1.set_ylabel('Density')\nax1.set_xlim(x_min, x_max)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# True sampling distribution\nax2 = axes[0, 1]\nax2.hist(true_T1, bins=40, density=True, alpha=0.7, color='green', \n         edgecolor='black', label='True sampling dist')\nax2.axvline(np.mean(true_T1), color='red', linestyle='--', linewidth=2, \n            label=f'Mean = {np.mean(true_T1):.2f}')\nax2.axvline(true_value, color='black', linestyle=':', linewidth=2, \n            label=f'True = {true_value}')\nax2.set_title('T¹: True Sampling Distribution')\nax2.set_xlabel('Value')\nax2.set_ylabel('Density')\nax2.set_xlim(x_min, x_max)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# T2: Central 4th moment\n# Bootstrap distribution\nax3 = axes[1, 0]\nax3.hist(boot_T2, bins=40, density=True, alpha=0.7, color='blue', \n         edgecolor='black', label='Bootstrap')\nax3.axvline(T2_hat, color='red', linestyle='--', linewidth=2, \n            label=f'Estimate = {T2_hat:.2f}')\nax3.axvline(true_value, color='black', linestyle=':', linewidth=2, \n            label=f'True = {true_value}')\nax3.set_title('T²: Bootstrap Distribution')\nax3.set_xlabel('Value')\nax3.set_ylabel('Density')\nax3.set_xlim(x_min, x_max)\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# True sampling distribution\nax4 = axes[1, 1]\nax4.hist(true_T2, bins=40, density=True, alpha=0.7, color='green', \n         edgecolor='black', label='True sampling dist')\nax4.axvline(np.mean(true_T2), color='red', linestyle='--', linewidth=2, \n            label=f'Mean = {np.mean(true_T2):.2f}')\nax4.axvline(true_value, color='black', linestyle=':', linewidth=2, \n            label=f'True = {true_value}')\nax4.set_title('T²: True Sampling Distribution')\nax4.set_xlabel('Value')\nax4.set_ylabel('Density')\nax4.set_xlim(x_min, x_max)\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.suptitle('Bootstrap vs True Sampling Distributions', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\nTrue value: 3\nT¹ (raw 4th moment): 2.025\nT² (central 4th moment): 1.882\n\n\n\n\n\n\n\n\n\nNow let’s compare the confidence intervals:\n\n\nConfidence Intervals for T¹ (Raw 4th Moment):\n  True 95% CI:        (0.58, 8.84)\n  Bootstrap Normal:   (0.42, 3.63)\n  Bootstrap Percent:  (0.64, 3.79)\n  Bootstrap Pivotal:  (0.26, 3.41)\n\nConfidence Intervals for T² (Central 4th Moment):\n  True 95% CI:        (0.50, 7.97)\n  Bootstrap Normal:   (0.41, 3.36)\n  Bootstrap Percent:  (0.48, 3.36)\n  Bootstrap Pivotal:  (0.40, 3.29)\n\n\n\n\n\n\n\n\nWhat Went Wrong?\n\n\n\nThe bootstrap catastrophically fails here – all methods estimate upper confidence bounds of ~3.4-3.8, while the true bounds are ~8-9 (more than double!).\nWhy? Fourth moments have heavy-tailed sampling distributions. With only n=20 observations, we likely missed the rare extreme values that drive the true variability. The bootstrap can only resample what it sees, so it fundamentally cannot capture the full range of uncertainty.\nThis isn’t just a small sample problem – it’s a fundamental limitation when statistics depend heavily on extreme values. Let’s explore when else the bootstrap fails…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#when-the-bootstrap-fails",
    "href": "chapters/04-nonparametric-bootstrap.html#when-the-bootstrap-fails",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.8 When The Bootstrap Fails",
    "text": "4.8 When The Bootstrap Fails\nThe bootstrap is remarkably general, but it’s not foolproof. It relies on the sample being a good representation of the population. When this assumption breaks down, so does the bootstrap.\nThe bootstrap may fail or perform poorly when:\n\nSample size is too small (typically n &lt; 20-30)\nEstimating extreme order statistics (min, max, extreme quantiles)\nHeavy-tailed distributions without finite moments\nNon-smooth statistics (e.g., number of modes)\nDependent data (unless using specialized methods like block bootstrap)\n\n\n\n\n\n\n\nExample: Estimators at the Boundary\n\n\n\nThe bootstrap performs poorly for statistics at the edge of the parameter space. The classic example is estimating the maximum of a uniform distribution:\n\n\nShow code\n# Uniform distribution example\nnp.random.seed(42)\ntrue_max = 10\nn = 50\nuniform_sample = np.random.uniform(0, true_max, n)\nsample_max = np.max(uniform_sample)\n\n# True sampling distribution of the maximum\n# For Uniform(0, θ), the maximum has CDF F(x) = (x/θ)^n\nx_theory = np.linspace(sample_max * 0.8, true_max, 1000)\npdf_theory = n * (x_theory/true_max)**(n-1) / true_max\n\n# Bootstrap distribution\nB = 2000\nboot_max_uniform = np.zeros(B)\nfor b in range(B):\n    boot_sample = np.random.choice(uniform_sample, size=n, replace=True)\n    boot_max_uniform[b] = np.max(boot_sample)\n\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# Bootstrap histogram\nax.hist(boot_max_uniform, bins=40, density=True, alpha=0.7, \n        color='blue', edgecolor='black', label='Bootstrap distribution')\n\n# True distribution\nax.plot(x_theory, pdf_theory, 'r-', linewidth=2, label='True distribution')\n\n# Key values\nax.axvline(sample_max, color='green', linestyle='--', linewidth=2, \n           label=f'Sample max = {sample_max:.2f}')\nax.axvline(true_max, color='black', linestyle=':', linewidth=2, \n           label=f'True θ = {true_max}')\n\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Bootstrap Fails for Uniform Maximum')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(8, 10.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhy it fails:\n\nThe true maximum (\\theta = 10) is always ≥ the sample maximum\nThe true sampling distribution has support on [\\text{sample max}, \\theta]\nBut bootstrap samples can never exceed the original sample maximum!\nThe bootstrap distribution is entirely to the left of where it should be\n\nNote: Similar biases arise when estimating extreme statistics (e.g., very small or very large quantiles) of any distribution.\n\n\nDespite these limitations, the bootstrap remains one of the most useful tools in modern statistics. Just remember: it’s a powerful method, not a magical one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/04-nonparametric-bootstrap.html#chapter-summary-and-connections",
    "href": "chapters/04-nonparametric-bootstrap.html#chapter-summary-and-connections",
    "title": "4  Nonparametric Estimation and The Bootstrap",
    "section": "4.9 Chapter Summary and Connections",
    "text": "4.9 Chapter Summary and Connections\n\n4.9.1 Key Concepts Review\nWe’ve covered two fundamental ideas that revolutionized statistical practice:\nThe Plug-In Principle:\n\nEstimate the distribution with the empirical distribution function (EDF)\nEstimate any functional T(F) by computing T(\\hat{F}_n)\nSimple, intuitive, and widely applicable\nGives us point estimates for any statistic\n\nThe Bootstrap:\n\nAssess uncertainty by resampling from the data\nCreate the “bootstrap world” that mimics the real world\nEstimate standard errors and confidence intervals for any statistic\nThree types of confidence intervals: Normal, Percentile, Pivotal\n\n\n\n4.9.2 Why These Concepts Matter\nFor Statistical Practice:\n\nNo need to derive complex formulas for standard errors\nWorks for statistics where theory is intractable (median, correlation, etc.)\nProvides a unified approach to uncertainty quantification\nDemocratizes statistics – complex inference becomes accessible\n\nFor Data Science:\n\nComputational approach aligns with modern computing power\nEasy to implement and parallelize\nWorks with complex models and machine learning algorithms\nProvides uncertainty estimates crucial for decision-making\n\nFor Understanding:\n\nMakes abstract concepts concrete through simulation\nReveals the sampling distribution visually\nHelps build intuition about statistical variability\nConnects theoretical statistics to computational practice\n\n\n\n4.9.3 Common Pitfalls to Avoid\n\nForgetting to sample with replacement\n\nBootstrap samples must be the same size as original\nSampling without replacement gives wrong answers\n\nUsing too few bootstrap samples\n\nUse at least B = 1,000 for standard errors\nUse B = 10,000 or more for confidence intervals\nMonte Carlo error decreases with \\sqrt{B}\nIn practice, use as many as you can given your available compute\n\nMisinterpreting confidence intervals\n\nThey quantify uncertainty in the estimate\nThey are not probability statements about parameters\nDifferent methods can give different intervals\n\nApplying bootstrap blindly\n\nCheck if your statistic is smooth\nBe cautious with very small samples\nWatch out for boundary cases\n\nIgnoring the assumptions\n\nBootstrap assumes the sample represents the population\nIt can’t fix biased sampling or systematic errors\nIt’s not magic – just clever resampling\n\n\n\n\n4.9.4 Chapter Connections\nThe bootstrap builds on our theoretical foundations and provides a computational path forward:\n\nFrom Previous Chapters: We’ve applied the plug-in principle to the empirical distribution (Chapter 1’s probability concepts), used Chapter 2’s variance formulas for bootstrap standard errors, and provided an alternative to Chapter 3’s CLT-based confidence intervals when theoretical distributions are intractable\nNext - Hypothesis Testing (Chapter 5): Bootstrap will create null distributions for complex test statistics, complemented by permutation tests as another resampling approach\nParametric Methods (Chapters 6-7): Compare bootstrap to theoretical approaches, use it to validate assumptions, and construct confidence intervals for maximum likelihood estimates when standard theory is difficult\nMachine Learning Applications: Bootstrap underpins ensemble methods (bagging), provides uncertainty quantification for predictions, and helps with model selection—making it essential for modern data science\n\n\n\n4.9.5 Rejoinder: Coming Full Circle\nRecall our opening example: the healthcare administrator planning hospital capacity during an epidemic. We began by asking how to quantify the uncertainty in our estimates, not just provide point predictions.\nThrough this chapter, we’ve answered that question with two powerful tools:\n\nThe plug-in principle gave us a way to estimate any property of a distribution\nThe bootstrap gave us a way to quantify the uncertainty of those estimates\n\nThese tools enable data-driven decision making by providing not just estimates, but confidence intervals that capture the range of plausible values. Whether you’re estimating hospital capacity, financial risk, or any other critical quantity, you now have the tools to say not just “we expect 500 patients” but “we’re 95% confident it will be between 300 and 700 patients.”\nThis transformation from point estimates to uncertainty quantification is what makes statistics invaluable for real-world decision making.\n\n\n4.9.6 Practical Advice\n\nStart simple: Use percentile intervals as your default\nVisualize: Always plot the bootstrap distribution\nCompare methods: Try different CI methods to check robustness\nThink about assumptions: Is your sample representative?\nUse modern tools: Most software has built-in bootstrap functions\n\nThe bootstrap exemplifies the shift from mathematical to computational statistics. Master it, and you’ll have a powerful tool for almost any statistical problem you encounter.\n\n\n4.9.7 Quick Self-Check\nBefore moving on, test your understanding with these questions:\n\nWhat is bootstrap sampling used for?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nEstimating the sampling distribution of a statistic\nComputing standard errors and confidence intervals\n\nAssessing uncertainty when no formula exists\n\nThe bootstrap is particularly valuable when theoretical formulas are intractable or don’t exist, such as for the median, correlation coefficient, or complex machine learning predictions.\n\n\n\n\nWhat approximations does the method make?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nStatistical approximation: Assumes \\hat{F}_n approximates F well\n\nThis depends on sample size n and cannot be improved without more data\n\nMonte Carlo approximation: Finite B approximates infinite resampling\n\nThis can be made arbitrarily small by increasing B (typically B \\geq 1,000)\n\n\nRemember: \\mathbb{V}_F(T_n) \\underbrace{\\approx}_{\\text{statistical error}} \\mathbb{V}_{\\hat{F}_n}(T_n) \\underbrace{\\approx}_{\\text{Monte Carlo error}} v_{\\text{boot}}\n\n\n\n\nWhat are its limitations?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nSmall samples (typically n &lt; 20-30): The empirical distribution poorly represents the population\nExtreme order statistics: Cannot extrapolate beyond observed data range (e.g., max of uniform distribution)\nHeavy-tailed distributions: May miss rare extreme values that drive variability (as we saw with 4th moments)\nNon-smooth statistics: Discontinuous functions like the number of modes\nDependent data: Requires specialized methods like block bootstrap for time series\n\nThe key limitation: bootstrap cannot see what’s not in your sample!\n\n\n\n\nHow can bootstrap be used to construct confidence intervals?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThree main methods, each with different strengths:\n\nNormal interval: \\hat{T}_n \\pm z_{\\alpha/2} \\cdot \\widehat{\\text{se}}_{\\text{boot}}\n\nSimplest, but assumes normality\nCan give impossible values (e.g., correlation &gt; 1)\n\nPercentile interval: (T^*_{n,\\alpha/2}, T^*_{n,1-\\alpha/2})\n\nUses bootstrap quantiles directly\nRespects parameter bounds, good default choice\n\nPivotal interval: (2\\hat{T}_n - T^*_{n,1-\\alpha/2}, 2\\hat{T}_n - T^*_{n,\\alpha/2})\n\nCorrects for bias, often most accurate\nCan occasionally exceed parameter bounds\n\n\nChoose based on your specific problem and always visualize the bootstrap distribution!\n\n\n\n\n\n4.9.8 Self-Test Problems\n\nImplementing Bootstrap: Write a function to bootstrap the trimmed mean (removing top and bottom 10% before averaging). Compare its standard error to the regular mean for normal and heavy-tailed data.\nCorrelation CI: Using the European Health and Wealth data from the chapter, compute bootstrap confidence intervals for \\rho^2 (squared correlation). How do the three methods compare? What happens to the intervals when you transform from \\rho to \\rho^2?\nRatio Statistics: Given paired data (X_i, Y_i), bootstrap the ratio \\bar{Y}/\\bar{X}. Why might this be challenging? Compare the three CI methods.\nBootstrap Failure - Range Statistic: Generate n=30 observations from a standard normal distribution and bootstrap the range (max - min). Compare the bootstrap distribution to the true sampling distribution (via simulation). Why does the bootstrap underestimate the variability? How does this relate to our discussion of extreme order statistics?\n\n\n\n4.9.9 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\nThis table maps sections in these lecture notes to the corresponding sections in Wasserman (2013) (“All of Statistics” or AoS).\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding AoS Section(s)\n\n\n\n\nIntroduction and Motivation\nExpanded material from the slides, providing context for nonparametric estimation and the bootstrap.\n\n\nConfidence Sets: The Foundation\n\n\n\n↳ Definition and Interpretation of Confidence Intervals\nAoS §6.3.2\n\n\n↳ Normal-Based Confidence Intervals\nAoS §6.3.2 (Theorem 6.16)\n\n\nThe Plug-In Principle: A General Method for Estimation\n\n\n\n↳ The Empirical Distribution Function (EDF)\nAoS §7.1 (Definition 7.1)\n\n\n↳ Properties of the EDF (Glivenko-Cantelli)\nAoS §7.1 (Theorems 7.3, 7.4)\n\n\n↳ Confidence Bands for the CDF (DKW Inequality)\nAoS §7.1 (Theorem 7.5)\n\n\n↳ The Plug-In Estimator for Statistical Functionals\nAoS §7.2 (Definition 7.7)\n\n\n↳ Plug-in Examples (Mean, Variance, etc.)\nAoS §7.2 (Examples 7.10, 7.11, etc.)\n\n\nThe Bootstrap: Simulating Uncertainty\n\n\n\n↳ The Core Idea and Bootstrap World\nAoS §8 (Introduction), §8.2\n\n\n↳ Bootstrap Variance and Standard Error Estimation\nAoS §8.2\n\n\nBootstrap Confidence Intervals\n\n\n\n↳ Three Common Methods (Normal, Percentile, Pivotal)\nAoS §8.3\n\n\n↳ Comparing Bootstrap CIs (Health and Wealth Example)\nNew example, applies concepts from AoS §8.3.\n\n\nBootstrap Application: Higher Moments\nNew example from the slides.\n\n\nWhen The Bootstrap Fails\nNew material, summarizing common failure modes. Examples inspired by AoS exercises (e.g., §8.6 Q7 for Uniform max).\n\n\nChapter Summary and Connections\nNew summary material.\n\n\n\n\n\n\n\n\n4.9.10 Python and R Reference\nPythonR\n# Essential bootstrap code template\nimport numpy as np\nfrom scipy import stats\n\ndef bootstrap(data, statistic, B=1000, alpha=0.05):\n    \"\"\"\n    Generic bootstrap function for any statistic.\n    \n    Returns:\n    - point_estimate: Original statistic value\n    - se: Bootstrap standard error  \n    - ci_normal: Normal-based CI\n    - ci_percentile: Percentile CI\n    - ci_pivotal: Pivotal CI\n    - boot_dist: Bootstrap distribution\n    \"\"\"\n    n = len(data)\n    boot_samples = np.zeros(B)\n    \n    # Original estimate\n    point_estimate = statistic(data)\n    \n    # Bootstrap\n    for b in range(B):\n        boot_data = np.random.choice(data, size=n, replace=True)\n        boot_samples[b] = statistic(boot_data)\n    \n    # Standard error\n    se = np.std(boot_samples, ddof=1)\n    \n    # Confidence intervals\n    z = stats.norm.ppf(1 - alpha/2)\n    ci_normal = (point_estimate - z*se, point_estimate + z*se)\n    \n    ci_percentile = (np.percentile(boot_samples, 100*alpha/2),\n                     np.percentile(boot_samples, 100*(1-alpha/2)))\n    \n    ci_pivotal = (2*point_estimate - np.percentile(boot_samples, 100*(1-alpha/2)),\n                  2*point_estimate - np.percentile(boot_samples, 100*alpha/2))\n    \n    return {\n        'estimate': point_estimate,\n        'se': se,\n        'ci_normal': ci_normal,\n        'ci_percentile': ci_percentile,\n        'ci_pivotal': ci_pivotal,\n        'boot_dist': boot_samples\n    }\n\n# Example usage\ndata = np.random.exponential(2, 50)\nresults = bootstrap(data, np.median, B=2000)\nprint(f\"Median: {results['estimate']:.3f} (SE: {results['se']:.3f})\")\nprint(f\"95% Percentile CI: {results['ci_percentile']}\")\n# Essential bootstrap code template\nbootstrap &lt;- function(data, statistic, B = 1000, alpha = 0.05) {\n  n &lt;- length(data)\n  boot_samples &lt;- numeric(B)\n  \n  # Original estimate\n  point_estimate &lt;- statistic(data)\n  \n  # Bootstrap\n  for (b in 1:B) {\n    boot_data &lt;- sample(data, size = n, replace = TRUE)\n    boot_samples[b] &lt;- statistic(boot_data)\n  }\n  \n  # Standard error\n  se &lt;- sd(boot_samples)\n  \n  # Confidence intervals\n  z &lt;- qnorm(1 - alpha/2)\n  ci_normal &lt;- c(point_estimate - z*se, point_estimate + z*se)\n  \n  ci_percentile &lt;- quantile(boot_samples, c(alpha/2, 1-alpha/2))\n  \n  ci_pivotal &lt;- c(2*point_estimate - quantile(boot_samples, 1-alpha/2),\n                  2*point_estimate - quantile(boot_samples, alpha/2))\n  \n  list(\n    estimate = point_estimate,\n    se = se,\n    ci_normal = ci_normal,\n    ci_percentile = ci_percentile,\n    ci_pivotal = ci_pivotal,\n    boot_dist = boot_samples\n  )\n}\n\n# Example usage\nset.seed(42)\ndata &lt;- rexp(50, rate = 1/2)\nresults &lt;- bootstrap(data, median, B = 2000)\ncat(sprintf(\"Median: %.3f (SE: %.3f)\\n\", results$estimate, results$se))\ncat(sprintf(\"95%% Percentile CI: (%.3f, %.3f)\\n\", \n            results$ci_percentile[1], results$ci_percentile[2]))\n\nRemember: The bootstrap transforms the abstract problem of understanding sampling distributions into the concrete task of resampling from your data. It’s statistics made tangible through computation. When in doubt, bootstrap it!\n\n\n\n\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Estimation and The Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html",
    "href": "chapters/05-parametric-inference-I.html",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "",
    "text": "5.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#learning-objectives",
    "href": "chapters/05-parametric-inference-I.html#learning-objectives",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "",
    "text": "Define a parametric model and explain its role in statistical inference, distinguishing between parameters of interest and nuisance parameters.\nDerive estimators using the Method of Moments (MoM) by equating theoretical and sample moments.\nExplain the principle of Maximum Likelihood Estimation (MLE) and formulate the likelihood and log-likelihood functions for a given model.\nFind the Maximum Likelihood Estimator (MLE) analytically in simple cases by maximizing the (log-)likelihood.\nExplain when and why numerical optimization is necessary for MLE, and apply standard optimization libraries to find estimators computationally.\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter covers parametric models and two fundamental approaches to finding estimators: the Method of Moments and Maximum Likelihood Estimation. The material is adapted from Chapter 9 of Wasserman (2013) and supplemented with computational examples and optimization techniques relevant to modern data science applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#introduction-machine-learning-as-statistical-estimation",
    "href": "chapters/05-parametric-inference-I.html#introduction-machine-learning-as-statistical-estimation",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "5.2 Introduction: Machine Learning As Statistical Estimation",
    "text": "5.2 Introduction: Machine Learning As Statistical Estimation\nWhen fitting a machine learning model f(x, \\theta) for regression with inputs x_i and targets y_i, i = 1, \\dots, n, a common approach is to minimize the mean squared error (MSE):  \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\left( f(x_i, \\theta) - y_i \\right)^2. \nThe terms in this expression are similar to the log-probability of a normal distribution:  \\log \\mathcal{N}(y_i;\\; f(x_i, \\theta), \\sigma^2) = -\\frac{1}{2}\\log(2\\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\left( f(x_i, \\theta) - y_i \\right)^2. \nThis similarity is not a coincidence. When \\sigma is constant, maximizing the log-likelihood means maximizing -\\frac{1}{2\\sigma^2} \\sum_i (f(x_i, \\theta) - y_i)^2 plus a constant – which is equivalent to minimizing the MSE.\n\n\n\n\n\n\nThe ML-Statistics Connection\n\n\n\nWhen you minimize MSE in machine learning, you’re performing maximum likelihood estimation under a Gaussian noise assumption! This connection reveals a fundamental truth: many machine learning algorithms are secretly (or openly) solving statistical estimation problems.\n\n\nThis chapter introduces the foundational principles of parametric inference – the engine that powers both classical statistics and modern machine learning. We’ll learn two primary methods for finding estimators for model parameters: the Method of Moments and the celebrated Maximum Likelihood Estimation.\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nParametric model\nParametrinen malli\nModels with finite parameters\n\n\nParameter of interest\nKiinnostuksen parametri\nThe quantity we want to estimate\n\n\nNuisance parameter\nKiusaparametri\nParameters not of primary interest\n\n\nMethod of Moments (MoM)\nMomenttimenetelmä\nEstimation by matching moments\n\n\nMoment\nMomentti\nExpected value of powers\n\n\nSample moment\nOtosmomentti\nEmpirical average of powers\n\n\nMaximum Likelihood Estimation (MLE)\nSuurimman uskottavuuden menetelmä\nMost common estimation method\n\n\nLikelihood function\nUskottavuusfunktio\nJoint density as function of parameters\n\n\nLog-likelihood function\nLog-uskottavuusfunktio\nLogarithm of likelihood\n\n\nMaximum Likelihood Estimator\nSU-estimaattori\nParameter maximizing likelihood\n\n\nNumerical optimization\nNumeerinen optimointi\nComputational methods for finding optima\n\n\nGradient\nGradientti\nVector of partial derivatives\n\n\nGradient descent\nGradienttimenetelmä\nIterative optimization algorithm",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#parametric-models",
    "href": "chapters/05-parametric-inference-I.html#parametric-models",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "5.3 Parametric Models",
    "text": "5.3 Parametric Models\nWe introduced parametric models in Chapter 3 when discussing statistical inference frameworks. Recall that in the world of statistical inference, we often make assumptions about the structure of our data-generating process. A parametric model is one such assumption – it postulates that our data comes from a distribution that can be fully characterized by a finite number of parameters.\nNow that we’re diving into estimation methods, let’s revisit this concept with a focus on how we actually find these parameters.\n\nA parametric model is a set of distributions \\mathfrak{F} = \\{ f(x; \\theta) : \\theta \\in \\Theta \\} where:\n\n\\theta = (\\theta_1, \\ldots, \\theta_k) is the parameter (possibly vector-valued)\n\\Theta \\subseteq \\mathbb{R}^k is the parameter space (the set of all possible parameter values)\nf(x; \\theta) is the density or distribution function indexed by \\theta\n\n\nThe key insight: We assume the data-generating process belongs to a specific family of distributions, and our job is just to find the right parameter \\theta within that family.\nIn other words, the problem of inference reduces to estimating the parameter(s) \\theta.\n\n\n\n\n\n\nAll Models Are Wrong, But Some Are Useful\n\n\n\nParametric models are widely used although the underlying models are usually not perfect. As the statistician George Box famously said – in what is possibly the most repeated quote in statistics – “All models are wrong, but some are useful.”\nWhen the model is good enough, parametric models can be very useful because they offer a simple representation for potentially complex phenomena. The art of statistical modeling is finding a model that is wrong in acceptable ways while still capturing the essential features of your data.\n\n\nExamples of Parametric Models:\n\nSimple distributions:\n\nBernoulli(p): One parameter determining success probability\nPoisson(\\lambda): One parameter determining both mean and variance\nNormal(\\mu, \\sigma^2): Two parameters for location and scale\n\nRegression models:\n\nLinear regression: Y = \\beta_0 + \\beta_1 X + \\epsilon where \\epsilon \\sim N(0, \\sigma^2)\nLogistic regression: P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\nFinite mixture models:\n\nMixture of Gaussians: f(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x; \\mu_k, \\sigma_k^2)\nParameters include mixing weights \\pi_k and component parameters (\\mu_k, \\sigma_k)\n\nMachine Learning models:\n\nDeep Neural Networks: Often millions of parameters (weights and biases)\nDespite their complexity, these are still parametric models!\n\n\n\n\n\n\n\n\nRemember: Parameters of Interest vs. Nuisance Parameters\n\n\n\nWe introduced this distinction in Chapter 3, which it’s crucial for estimation:\n\nParameter of interest: The specific quantity T(\\theta) we want to estimate\nNuisance parameter: Other parameters we must estimate but don’t care about directly\n\n\n\n\n\n\n\n\n\nExample: Mean Lifetime Estimation\n\n\n\nEquipment lifetimes often follow a Gamma distribution. If X_1, \\ldots, X_n \\sim \\text{Gamma}(\\alpha, \\beta), then:\nf(x; \\alpha, \\beta) = \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{\\alpha-1} e^{-x/\\beta}, \\quad x &gt; 0\nIf we want to estimate the mean lifetime:\n\nParameter of interest: T(\\alpha, \\beta) = \\mathbb{E}(X) = \\alpha\\beta\nNuisance parameters: The individual shape (\\alpha) and scale (\\beta) parameters\n\nNote that we must estimate both parameters, but only their product matters for our question.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#the-method-of-moments-mom",
    "href": "chapters/05-parametric-inference-I.html#the-method-of-moments-mom",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "5.4 The Method of Moments (MoM)",
    "text": "5.4 The Method of Moments (MoM)\nThe Method of Moments is a simple estimation technique that does not yield optimal estimators, but provides easy-to-compute values that can serve as good starting points for more sophisticated methods.\n\n5.4.1 The Principle: Matching Moments\nThe Method of Moments is based on a straightforward idea: if our model is correct, then theoretical properties of the distribution (moments) should match their empirical counterparts in the data. It’s like saying, “If this really is the right distribution, then the average I calculate from my model should match the average I see in my data.”\n\nFor a model with parameter \\theta = (\\theta_1, \\ldots, \\theta_k):\n\nThe j^{\\text{th}} theoretical moment is: \\alpha_j(\\theta) = \\mathbb{E}_{\\theta}(X^j)\nThe j^{\\text{th}} sample moment is: \\hat{\\alpha}_j = \\frac{1}{n} \\sum_{i=1}^n X_i^j\n\n\n\nThe Method of Moments estimator \\hat{\\theta}_n is the value of \\theta such that: \\begin{align}\n\\alpha_1(\\hat{\\theta}_n) &= \\hat{\\alpha}_1 \\\\\n\\alpha_2(\\hat{\\theta}_n) &= \\hat{\\alpha}_2 \\\\\n&\\vdots \\\\\n\\alpha_k(\\hat{\\theta}_n) &= \\hat{\\alpha}_k\n\\end{align}\n\nThis gives us a system of k equations with k unknowns – exactly what we need to solve for k parameters!\n\n\n5.4.2 MoM in Action: Examples\n\n\n\n\n\n\nExample: Bernoulli Distribution\n\n\n\nFor X_1, \\ldots, X_n \\sim \\text{Bernoulli}(p), we have one parameter to estimate.\n\nTheoretical first moment: \\alpha_1(p) = \\mathbb{E}_p(X) = p\nSample first moment: \\hat{\\alpha}_1 = \\frac{1}{n}\\sum_{i=1}^n X_i = \\bar{X}_n\n\nEquating them: p = \\bar{X}_n\nTherefore, the MoM estimator is \\hat{p}_{\\text{MoM}} = \\bar{X}_n – simply the proportion of successes!\n\n\n\n\n\n\n\n\nExample: Normal Distribution\n\n\n\nFor X_1, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2), we have two parameters, so we need two equations.\nFirst moment equation:\n\n\\alpha_1(\\theta) = \\mathbb{E}(X) = \\mu\n\\hat{\\alpha}_1 = \\bar{X}_n\nSetting equal: \\mu = \\bar{X}_n\n\nSecond moment equation:\n\n\\alpha_2(\\theta) = \\mathbb{E}(X^2) = \\mathbb{V}(X) + (\\mathbb{E}(X))^2 = \\sigma^2 + \\mu^2\n\\hat{\\alpha}_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2\nSetting equal: \\sigma^2 + \\mu^2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2\n\nSolving this system:\n\n\\hat{\\mu}_{\\text{MoM}} = \\bar{X}_n\n\\hat{\\sigma}^2_{\\text{MoM}} = \\frac{1}{n}\\sum_{i=1}^n X_i^2 - \\bar{X}_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2\n\n\n\n\n\n\n\n\n\nExample: Gamma Distribution\n\n\n\n\n\nFor X_1, \\ldots, X_n \\sim \\text{Gamma}(\\alpha, \\beta), let’s derive the MoM estimators.\nThe Gamma distribution has:\n\nFirst moment: \\mathbb{E}(X) = \\alpha\\beta\nSecond moment: \\mathbb{E}(X^2) = \\mathbb{V}(X) + (\\mathbb{E}(X))^2 = \\alpha\\beta^2 + (\\alpha\\beta)^2\n\nSetting up the moment equations: \\alpha\\beta = \\bar{X}_n \\alpha\\beta^2 + (\\alpha\\beta)^2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2\nFrom the first equation: \\alpha = \\bar{X}_n / \\beta\nSubstituting into the second equation: \\frac{\\bar{X}_n}{\\beta} \\cdot \\beta^2 + \\bar{X}_n^2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2\nSimplifying: \\bar{X}_n \\beta + \\bar{X}_n^2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2\nSolving for \\beta: \\beta = \\frac{\\frac{1}{n}\\sum_{i=1}^n X_i^2 - \\bar{X}_n^2}{\\bar{X}_n} = \\frac{\\text{sample variance}}{\\bar{X}_n}\nTherefore, the MoM estimators are:\n\n\\hat{\\beta}_{\\text{MoM}} = \\frac{\\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{\\bar{X}_n}\n\\hat{\\alpha}_{\\text{MoM}} = \\frac{\\bar{X}_n}{\\hat{\\beta}_{\\text{MoM}}} = \\frac{\\bar{X}_n^2}{\\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Generate data from a Gamma distribution\nnp.random.seed(42)\ntrue_alpha, true_beta = 3.0, 2.0  # True parameters\nn = 100\ndata = stats.gamma.rvs(a=true_alpha, scale=true_beta, size=n)\n\n# Method of Moments for Gamma distribution\n# For Gamma(α, β): E[X] = αβ, E[X²] = α(α+1)β²\nsample_mean = np.mean(data)\nsample_second_moment = np.mean(data**2)\n\n# Solve the system of equations\n# mean = α * β\n# second_moment = α * β² * (α + 1) = α * β² + α² * β²\n# This gives us: β = (second_moment - mean²) / mean\nmom_beta = (sample_second_moment - sample_mean**2) / sample_mean\nmom_alpha = sample_mean / mom_beta\n\nprint(f\"True parameters: α = {true_alpha}, β = {true_beta}\")\nprint(f\"MoM estimates:   α = {mom_alpha:.3f}, β = {mom_beta:.3f}\")\n\n# Visualize the fit\nx = np.linspace(0, 20, 200)\nplt.figure(figsize=(7, 4))\nplt.hist(data, bins=30, density=True, alpha=0.7, label='Data')\nplt.plot(x, stats.gamma.pdf(x, a=true_alpha, scale=true_beta), \n         'g-', linewidth=2, label='True distribution')\nplt.plot(x, stats.gamma.pdf(x, a=mom_alpha, scale=mom_beta), \n         'r--', linewidth=2, label='MoM fit')\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('Method of Moments Estimation for Gamma Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\nTrue parameters: α = 3.0, β = 2.0\nMoM estimates:   α = 3.869, β = 1.511\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 Properties of Method of Moments Estimator\nUnder regular conditions, Method of Moments estimators have some desirable properties:\n\nLet \\hat{\\theta}_n denote the method of moments estimator. Under appropriate conditions on the model, the following statements hold:\n\nExistence: The estimate \\hat{\\theta}_n exists with probability tending to 1.\nConsistency: The estimate is consistent: \\hat{\\theta}_n \\xrightarrow{P} \\theta\nAsymptotic Normality: The estimate is asymptotically Normal: \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\rightsquigarrow N(0, \\Sigma) where \\Sigma = g \\mathbb{E}_{\\theta}(Y Y^T) g^T, with Y = (X, X^2, \\ldots, X^k)^T and g = (g_1, \\ldots, g_k) where g_j = \\frac{\\partial \\alpha_j^{-1}(\\theta)}{\\partial \\theta}.\n\n\nThe last result can yield confidence intervals, but bootstrap is usually easier.\n\n\n\n\n\n\nHow Good are MoM Estimators?\n\n\n\nStrengths:\n\nSimple to compute – just solve algebraic equations!\nGuaranteed existence and consistency under mild conditions\nAsymptotically normal, enabling confidence intervals\n\nWeaknesses:\n\nNot efficient: Other estimators (like MLE) typically have smaller variance\nMay give impossible values: Can produce estimates outside the parameter space\nArbitrary: Why use moments? Why not other features of the distribution?\n\nPrimary Use Case: MoM estimators are excellent starting values for more sophisticated methods like maximum likelihood estimation, which often require numerical optimization.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#maximum-likelihood-estimation-mle",
    "href": "chapters/05-parametric-inference-I.html#maximum-likelihood-estimation-mle",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "5.5 Maximum Likelihood Estimation (MLE)",
    "text": "5.5 Maximum Likelihood Estimation (MLE)\n\n5.5.1 The Principle: What Parameter Makes My Data Most Probable?\nMaximum Likelihood Estimation is arguably the most important estimation method in statistics. While the Method of Moments asks “what parameters make the theoretical moments match the empirical ones?”, MLE asks a more direct question: “what parameter values make my observed data most plausible?”\nThe elegance of MLE is that it reverses our usual probability thinking:\n\nProbability: Given parameters, what data would we expect to see?\nLikelihood: Given data, which parameters make this data most probable?\n\n\n\n5.5.2 The Likelihood Function\nThe mathematical object at the core of MLE is the likelihood function.\n\nLet X_1, \\ldots, X_n be IID with PDF f(x; \\theta).\nThe likelihood function is: \\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i; \\theta)\n\nFor both mathematical and numerical convenience, in practice we often work with the log-likelihood function: \\ell_n(\\theta) = \\log \\mathcal{L}_n(\\theta) = \\sum_{i=1}^n \\log f(X_i; \\theta)\n\nThe Maximum Likelihood Estimator (MLE) is: \\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta \\in \\Theta} \\mathcal{L}_n(\\theta) = \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta)\n\nNote that maximizing the likelihood is the same as maximizing the log-likelihood, since the logarithm is a strictly increasing (monotonic) function.\nIntuitiveMathematicalComputationalLikelihood is a “what if” game. Imagine you have a coin and you flip\nit 10 times, getting 7 heads. You ask yourself:“What if the probability of heads were 0.5? How likely would 7 heads\nin 10 flips be?” “What if the probability were 0.6? Would that make my\ndata more or less likely?” “What if it were 0.7? 0.8?”For each possible value of the parameter (here, the probability of\nheads), you calculate how probable your exact observed data would be.\nThe parameter value that maximizes this probability is your maximum\nlikelihood estimate.The likelihood function is this “what if” calculation formalized –\nit’s the probability (or probability density) of your observed data,\ntreated as a function of the unknown parameters.Why do we often prefer working with the log-likelihood?\\[\\ell_n(\\theta) = \\log \\mathcal{L}_n(\\theta) = \\sum_{i=1}^n \\log f(x_i; \\theta)\\]\nDifferentiation: For exponential\nfamily distributions (normal, exponential, gamma, etc.), log\nderivatives eliminate exponentials. For example, for normal:\n\\(\\frac{d}{d\\mu} e^{-(x-\\mu)^2/2}\\)\nbecomes just \\((x-\\mu)\\) after taking\nlogs\nNumerical stability: Products of small\nprobabilities underflow; sums of logs don’t\nAdditive structure: Log-likelihood is a sum over\nobservations, making derivatives and optimization more\ntractable\nLet’s visualize the likelihood function for a simple Bernoulli\nexample:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate coin flips: n=20, observed 12 successes\nn = 20\nsuccesses = 12\n\n# Define the log-likelihood function\np_values = np.linspace(0.01, 0.99, 200)\nlog_likelihood = successes * np.log(p_values) + (n - successes) * np.log(1 - p_values)\n\n# Find the MLE (exact analytical solution)\np_mle = successes / n  # Exact MLE for Bernoulli\n\n# Create the plot\nplt.figure(figsize=(7, 4))\nplt.plot(p_values, log_likelihood, 'b-', linewidth=2)\nplt.axvline(p_mle, color='red', linestyle='--', linewidth=2, label=f'MLE: p̂ = {p_mle}')\nplt.xlabel('p')\nplt.ylabel('Log-likelihood ℓₙ(p)')\nplt.title(f'Log-likelihood for Bernoulli with n={n}, S={successes}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Maximum likelihood estimate: p̂ = {successes/n}\")\nprint(f\"This makes intuitive sense: it's simply the observed proportion of successes!\")\n\n\n\n\nMaximum likelihood estimate: p̂ = 0.6\nThis makes intuitive sense: it's simply the observed proportion of successes!\n\nNotice how the log-likelihood is maximized exactly at the observed\nproportion of successes. This is no coincidence – the MLE often has an\nintuitive interpretation.\n\n\n\n\n\n\nImportant: Likelihood function is NOT a probability distribution!\n\n\n\nThe likelihood \\mathcal{L}_n(\\theta) is NOT a probability distribution over \\theta. In general: \\int_\\Theta \\mathcal{L}_n(\\theta) d\\theta \\neq 1\nWhy? The same mathematical expression f(x; \\theta) plays two different roles:\n\nAs a PDF: Fix \\theta, vary x → \\int f(x; \\theta) dx = 1 (proper probability distribution)\nAs a likelihood: Fix x (observed data), vary \\theta → \\int \\mathcal{L}_n(\\theta) d\\theta is usually not 1\n\nExample: Observe one coin flip with result X = 1 (heads) from Bernoulli(p):\n\nThe likelihood is \\mathcal{L}(p) = p for p \\in [0,1]\n\\int_0^1 p \\, dp = \\frac{1}{2} \\neq 1\n\nThe likelihood tells us relative plausibility of parameter values, not their probabilities. This is why we need Bayesian methods if we want actual probability distributions over parameters!\n\n\n\n\n5.5.3 Finding the MLE Analytically\nFor simple models, we can find the MLE by taking derivatives and setting them to zero. The general recipe:\n\nWrite down the likelihood \\mathcal{L}_n(\\theta)\nTake the logarithm to get \\ell_n(\\theta) (this simplifies products to sums)\nTake the derivative with respect to \\theta\nSet equal to zero and solve\nVerify it’s a maximum (not a minimum or saddle point)\n\n\n\n\n\n\n\nSimplifying MLE Calculations\n\n\n\nWhen finding the MLE, multiplying the likelihood (or log-likelihood) by a positive constant or adding a constant doesn’t change where the maximum occurs. This means we can often ignore:\n\nNormalization constants that don’t depend on \\theta\nTerms like \\frac{1}{(2\\pi)^{n/2}} in the normal distribution\nFactorials in discrete distributions\n\nThis greatly simplifies calculations – focus only on terms involving \\theta!\n\n\nLet’s work through some examples:\n\n\n\n\n\n\nExample: Bernoulli Distribution\n\n\n\nFor X_1, \\ldots, X_n \\sim \\text{Bernoulli}(p):\nStep 1: The likelihood is \\mathcal{L}_n(p) = \\prod_{i=1}^n p^{X_i}(1-p)^{1-X_i} = p^S(1-p)^{n-S} where S = \\sum_{i=1}^n X_i is the total number of successes.\nStep 2: The log-likelihood is \\ell_n(p) = S \\log p + (n-S) \\log(1-p)\nStep 3: Taking the derivative: \\frac{d\\ell_n}{dp} = \\frac{S}{p} - \\frac{n-S}{1-p}\nStep 4: Setting to zero and solving: \\frac{S}{p} = \\frac{n-S}{1-p} \\implies S(1-p) = (n-S)p \\implies S = np\nTherefore, \\hat{p}_{\\text{MLE}} = S/n = \\bar{X}_n.\nNote: This is the same as the Method of Moments estimator!\n\n\n\n\n\n\n\n\nExample: Normal Distribution\n\n\n\nFor X_1, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2):\nThe log-likelihood (ignoring constants) is: \\ell_n(\\mu, \\sigma) = -n \\log \\sigma - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i - \\mu)^2\nTaking partial derivatives and setting to zero:\n\\frac{\\partial \\ell_n}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (X_i - \\mu) = 0\n\\frac{\\partial \\ell_n}{\\partial \\sigma} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^n (X_i - \\mu)^2 = 0\nSolving these equations:\n\n\\hat{\\mu}_{\\text{MLE}} = \\bar{X}_n\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2\n\nAgain, these match the Method of Moments estimators!\n\n\n\n\n\n\n\n\nExample: A Harder Case - Uniform(0, θ)\n\n\n\nNot all MLEs can be found by differentiation! Consider X_1, \\ldots, X_n \\sim \\text{Uniform}(0, \\theta).\nThe PDF is: f(x; \\theta) = \\begin{cases}\n1/\\theta & \\text{if } 0 \\leq x \\leq \\theta \\\\\n0 & \\text{otherwise}\n\\end{cases}\nThe likelihood is: \\mathcal{L}_n(\\theta) = \\begin{cases}\n(1/\\theta)^n & \\text{if } \\theta \\geq \\max\\{X_1, \\ldots, X_n\\} \\\\\n0 & \\text{if } \\theta &lt; \\max\\{X_1, \\ldots, X_n\\}\n\\end{cases}\nThis function:\n\nIs 0 when \\theta is less than the largest observation\nDecreases as (1/\\theta)^n for larger \\theta\n\nTherefore, the likelihood is maximized at the boundary: \\hat{\\theta}_{\\text{MLE}} = X_{(n)} = \\max\\{X_1, \\ldots, X_n\\}.\nThis example shows that not all optimization problems are solved by calculus – sometimes we need to think more carefully about the function’s behavior!\nLet’s visualize this unusual likelihood function:\n\n\nShow code\n# Uniform(0, θ) MLE visualization\nnp.random.seed(42)\nn = 10\ntrue_theta = 2.0\ndata = np.random.uniform(0, true_theta, n)\nx_max = np.max(data)\n\n# Create theta values\ntheta_values = np.linspace(0.1, 3.0, 300)\nlikelihood = np.zeros_like(theta_values)\n\n# Calculate likelihood (proportional to)\nfor i, theta in enumerate(theta_values):\n    if theta &gt;= x_max:\n        likelihood[i] = (1/theta)**n\n    else:\n        likelihood[i] = 0\n\nplt.figure(figsize=(7, 4))\nplt.subplot(1, 2, 1)\n# Plot data points\nplt.scatter(data, np.zeros_like(data), color='red', s=50, zorder=5, label='Data')\nplt.axvline(x_max, color='green', linestyle='--', linewidth=2, label=f'max(X) = {x_max:.3f}')\nplt.xlim(-0.5, 3.0)\nplt.ylim(-0.1, 0.1)\nplt.xlabel('x')\nplt.title('Observed Data')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(theta_values, likelihood, 'b-', linewidth=2)\nplt.axvline(x_max, color='green', linestyle='--', linewidth=2, label=f'MLE = {x_max:.3f}')\nplt.xlabel('θ')\nplt.ylabel('Likelihood (proportional to)')\nplt.title('Likelihood Function')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#mle-via-numerical-optimization",
    "href": "chapters/05-parametric-inference-I.html#mle-via-numerical-optimization",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "5.6 MLE Via Numerical Optimization",
    "text": "5.6 MLE Via Numerical Optimization\nFinding the maximum of the log-likelihood by taking a derivative and setting it to zero is clean and satisfying, but it only works for the simplest models. For most real-world problems, the log-likelihood function is a complex, high-dimensional surface, and we cannot find the peak analytically.\nInstead, we must turn to numerical optimization. The core idea of most optimization algorithms is simple: we start with an initial guess for the parameters, \\theta_0, and then iteratively take steps “uphill” on the likelihood surface until we can no longer find a higher point.\nThis section explores the concepts and tools behind this fundamental process.1\n\n\n\n\n\n\nThe Need for Numerical Methods\n\n\n\nFor many important models, we cannot solve for the MLE analytically:\n\nLogistic regression: No closed-form solution for the regression coefficients\nMixture models: Complex likelihood with multiple local maxima\n\nMost complex ML models: Neural networks, random forests, etc.\n\nWe must use iterative numerical optimization algorithms to find the maximum of the likelihood function (or minimum of the negative log-likelihood).\n\n\n\n5.6.1 The Optimization Setup for MLE\nFinding the MLE requires solving an optimization problem to find the parameters \\theta that maximize the likelihood function \\mathcal{L}_n(\\theta) or equivalently the log-likelihood function \\ell_n(\\theta).\nSince optimization methods in software libraries are conventionally written to perform minimization, our practical goal becomes:\n \\hat{\\theta}_{\\text{MLE}} = \\arg\\min_{\\theta} \\left[-\\ell_n(\\theta)\\right] \nThis is a crucial point: we minimize the negative log-likelihood. Forgetting this minus sign is one of the most common programming errors in statistical computing!\nThroughout this section, we’ll follow the standard convention and present algorithms for minimization. We’ll use examples from Python’s scipy.optimize, but other languages and libraries (R’s optim, Julia’s Optim.jl, etc.) offer similar algorithms with comparable interfaces.\nWhen in optimization we say that a problem is D-dimensional, we refer to the number of variables being optimized over – here, the number of elements of \\theta.\n\n\n5.6.2 Numerical Optimization in 1D\nTo understand how numerical optimization works, it’s helpful to start with the simplest case: finding the minimum of a function with a single parameter, f(\\theta).\nLet’s take a simple example function to minimize:\n f(\\theta) = \\frac{\\theta^2}{10} + \\frac{1}{\\theta}; \\quad \\theta &gt; 0 \nwhose global minimum is at \\theta^\\star \\approx 1.71.\n\n\n\n\n\n\n\n\n\nHow can a computer find the minimum of this function? There are two main families of approaches.\n\n1D Optimization without Derivatives\nThe simplest algorithms work by bracketing the minimum. If you can find three points (a, c, b) such that a &lt; c &lt; b and f(c) &lt; f(a) and f(c) &lt; f(b), you know a minimum lies somewhere in the interval (a, b).\n\nGolden-section search: A simple but robust method that is analogous to binary search for finding a root. It progressively narrows the bracket until the desired precision is reached.\nBrent’s method: A more sophisticated and generally preferred method. It combines the guaranteed (but slow) progress of golden-section search with the potentially faster convergence of fitting a parabola to the three points and jumping to the minimum of the parabola.\n\nThese methods are useful when the function’s derivative is unavailable or unreliable.\n\n\n1D Optimization with Derivatives: Newton’s Method\nIf we can compute derivatives, we can often find the minimum much faster. The most famous derivative-based method is Newton’s method (or Newton-Raphson).\nThe idea is to iteratively approximate the function with a quadratic and jump to the minimum of that quadratic. A second-order Taylor expansion of f(\\theta) around a point \\theta_t is:  f(\\theta) \\approx f(\\theta_t) + f'(\\theta_t) (\\theta - \\theta_t) + \\frac{1}{2} f''(\\theta_t) (\\theta - \\theta_t)^2. \nTo find the minimum of this quadratic approximation, we take its derivative with respect to \\theta and set it to zero:  \\frac{d}{d\\theta} (\\text{approx}) = f'(\\theta_t) + f''(\\theta_t)(\\theta - \\theta_t) = 0 \nSolving for \\theta gives us the next point in our iteration, \\theta_{t+1}:  \\theta_{t+1} = \\theta_t - \\frac{f'(\\theta_t)}{f''(\\theta_t)}. \nThis simple update rule is the core of Newton’s method. When close to a well-behaved minimum, it converges very rapidly (quadratically). However, it can be unstable if the function is not “nice” or if the starting point is far from the minimum.\n\n\n\n\n\n\nExample: 1D Optimization in scipy\n\n\n\nLet’s find the minimum of our example function using scipy.optimize. We’ll use Brent’s method, which only requires the function itself and a bracket.\n\n\nShow code\nimport numpy as np\nimport scipy.optimize\n\ndef f(theta):\n  return theta**2/10 + 1/theta\n\n# Find the minimum using Brent's method\n# We need to provide a bracket (a, b) or (a, c, b)\n# where the minimum is expected to lie.\n# Let's use (0.1, 10.0)\nresult = scipy.optimize.brent(f, brack=(0.1, 10.0))\n\nprint(f\"The minimum found by Brent's method is at θ = {result:.6f}\")\nprint(f\"The value of the function at the minimum is f(θ) = {f(result):.6f}\")\n\n# Analytical minimum is at (5)^(1/3)\nanalytical_min = 5**(1/3)\nprint(f\"The analytical minimum is at θ = {analytical_min:.6f}\")\n\n\nThe minimum found by Brent's method is at θ = -0.000000\nThe value of the function at the minimum is f(θ) = -456934496164.231018\nThe analytical minimum is at θ = 1.709976\n\n\nThe output shows that the numerical method finds the correct minimum with high precision.\n\n\n\n\nMoving to Multiple Dimensions\nThe concepts from 1D optimization extend to the multi-dimensional case, but with added complexity.\nDerivative-free methods:\nGeneralizing derivative-free methods to multiple dimensions, i.e., optimizing f(\\theta_1, \\ldots, \\theta_n) with respect to \\theta_1, \\ldots, \\theta_n is hard! As the dimensionality increases, the number of possible directions that need to be searched for the optimum grows very rapidly.\n\nClassical approaches: Very old algorithms like direct search methods are mostly inefficient and struggle as dimensionality grows. Searching in many directions at once becomes prohibitively expensive.\nModern approaches: Recent methods can be more effective in specific scenarios:\n\nBayesian optimization: Works well in low dimensions (typically &lt; 20) and can be very sample-efficient when function evaluations are expensive\nCMA-ES (Covariance Matrix Adaptation Evolution Strategy): If you can afford many function evaluations and don’t have gradients, this method can work in surprisingly high dimensions – up to tens of thousands of parameters\n\nThese methods are mainly used when gradients are unavailable or unreliable (e.g., noisy simulations, black-box models)\n\nDerivative-based methods:\n\nThe standard choice: When gradients are available and the function is relatively inexpensive to evaluate, gradient-based methods dominate\nScalability: These methods can handle problems with millions or even billions of parameters (think deep neural networks)\nKey transformation: The first derivative (slope) becomes the gradient (\\nabla f), and the second derivative (curvature) becomes the Hessian matrix (H)\nWhy they work: Gradient information provides a principled direction for improvement at each step, making the search much more efficient than blind exploration\n\nYou can find visualizations of several optimization algorithms at work here.\n\n\n\n5.6.3 The Gradient: The Key to Multidimensional Optimization\nThe main tool for optimization of multidimensional functions\nf: \\mathbb{R}^n \\to \\mathbb{R}\nis the gradient:\n\\nabla f(\\theta) = \\left(\\frac{\\partial f}{\\partial \\theta_1}, \\ldots, \\frac{\\partial f}{\\partial \\theta_n}\\right)^T\nHere \\frac{\\partial f}{\\partial \\theta_i} is the partial derivative of f with respect to \\theta_i. It can be evaluated by computing the derivative w.r.t. \\theta_i while keeping the other variables constant.\nGeometric interpretation: The gradient points in the direction where the function values grow fastest. Its magnitude measures the rate of change in that direction.\n\n\n\n\n\n\nIntuition: The Gradient as a Compass\n\n\n\nThink of finding the MLE as hiking blindfolded on a hilly terrain (the negative log-likelihood surface). At each point, the gradient tells you which direction is steepest uphill. Since we want to minimize, we go in the opposite direction – downhill.\n\n\nLet’s visualize this concept:\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a 2D function with interesting topology (negative log-likelihood surface)\ndef f(x, y):\n    \"\"\"Example negative log-likelihood with two local minima\"\"\"\n    return -2 * np.exp(-(x-1)**2 - y**2) + 3 * np.exp(-(x+1)**2 - y**2) + 0.1*(x**2 + y**2)\n\n# Create grid\nx = np.linspace(-3, 3, 50)\ny = np.linspace(-2, 2, 40)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\n# Create figure with two subplots\nfig = plt.figure(figsize=(7, 10))\n\n# First subplot: 3D surface\nax1 = fig.add_subplot(2, 1, 1, projection='3d')\nsurf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\nax1.contour(X, Y, Z, levels=15, cmap='viridis', offset=Z.min(), alpha=0.4)\n\n# Mark the global minimum\nax1.scatter([1], [0], [f(1, 0)], color='black', s=100, marker='*', label='Global minimum')\n\nax1.set_xlabel('θ₁')\nax1.set_ylabel('θ₂')\nax1.set_zlabel('Negative log-likelihood')\nax1.set_title('3D Optimization Landscape')\n# Adjust viewing angle to swap visual perspective\nax1.view_init(elev=30, azim=-45)\n\n# Second subplot: 2D contour with gradient field\nax2 = fig.add_subplot(2, 1, 2)\n\n# Compute gradient (numerically)\ndx = 0.01\ndy = 0.01\ngrad_x = (f(X + dx, Y) - f(X - dx, Y)) / (2 * dx)\ngrad_y = (f(X, Y + dy) - f(X, Y - dy)) / (2 * dy)\n\n# Normalize gradient vectors for better visualization\ngrad_norm = np.sqrt(grad_x**2 + grad_y**2)\ngrad_x_norm = -grad_x / (grad_norm + 1e-10)  # Negative for descent\ngrad_y_norm = -grad_y / (grad_norm + 1e-10)\n\n# Contour plot\ncontour = ax2.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.6)\nax2.clabel(contour, inline=True, fontsize=8)\n\n# Gradient vectors\nskip = 3  # Show every 3rd arrow for clarity\nax2.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n           grad_x_norm[::skip, ::skip], grad_y_norm[::skip, ::skip], \n           scale=20, alpha=0.7, width=0.003, color='red')\n\n# Mark global minimum\nax2.plot([1], [0], 'k*', markersize=10, label='Global minimum')\n\nax2.set_xlabel('θ₁')\nax2.set_ylabel('θ₂')\nax2.set_title('Gradient Field on Negative Log-Likelihood Surface')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese visualizations show:\n\nTop (3D): The optimization landscape as a surface, showing the “hills and valleys” that optimization algorithms must navigate\nBottom (2D): The same landscape from above with:\n\nContour lines showing level sets of the negative log-likelihood\nRed arrows pointing in the direction of steepest descent (negative gradient)\nBlack star marking the global minimum at θ₁ = 1, θ₂ = 0\n\nNote how the gradient vectors always point toward lower values, guiding optimization algorithms downhill\n\n\n\n5.6.4 Evaluating the Gradient\nThe first practical challenge in using gradients for optimization is how to evaluate the gradient. There are three main approaches:\n\nAnalytically: Derive by hand and implement – fast but error-prone\nFinite differences: Approximate numerically by evaluating nearby points – easy but expensive2 and less precise\nAutomatic differentiation (autodiff): Best of both worlds – exact, fast and automatic\n\nModern frameworks like PyTorch and JAX make automatic differentiation the preferred choice. However, some older languages such as R still lack full autodiff support. Gradient computation usually uses backward-mode autodiff, which is suitable for computing derivatives of real-valued functions f: \\mathbb{R}^n \\to \\mathbb{R}.\n\n\n\n\n\n\nExample: Gradients in Python with JAX\n\n\n\n\n\nHere’s how to compute gradients automatically using JAX:\nimport jax.numpy as jnp\nimport jax\n\ndef f(theta):\n    return theta**2/10 + 1/theta\n\ng = jax.grad(f)\ng(1.5)  # Evaluating at a scalar value\n\n# For multiple variables:\ndef f2(theta):\n    return (jnp.exp(-(theta[0]-1)**2 - theta[1]**2)\n            - jnp.exp(-(theta[0]+1)**2 - theta[1]**2))\n\ng2 = jax.grad(f2)\ng2(jnp.array([-3.0, 0.5]))\n\n\n\n\n\n5.6.5 Gradient-Based Optimization Methods\nOnce we have gradients, we can use various algorithms to find the optimum.\nThe simplest gradient-based algorithm is gradient descent (also called steepest descent), which iteratively takes steps in the direction of the negative gradient:\n\\theta_{t+1} = \\theta_t - \\eta \\nabla f(\\theta_t)\nwhere \\eta &gt; 0 (eta) is the learning rate, which defines the step length.\nGradient descent is inefficient, often zigzagging toward the optimum, but it’s the foundation for understanding more sophisticated methods.\n\n\n\n\n\n\nAdvanced Gradient-Based Methods\n\n\n\n\n\nBeyond basic gradient descent, there are many sophisticated optimization algorithms:\nClassical improvements:\n\nConjugate gradient methods: Choose search directions that are conjugate with respect to the Hessian matrix. For quadratic functions, this guarantees finding the exact minimum in at most n steps (where n is the number of dimensions). Unlike gradient descent which can take tiny steps in nearly orthogonal directions, conjugate gradient methods take larger, more efficient steps.\nQuasi-Newton methods: Approximate the Hessian matrix without computing second derivatives directly.\n\nBFGS (Broyden-Fletcher-Goldfarb-Shanno): Builds up an approximation to the inverse Hessian using gradient information from previous steps\nL-BFGS (Limited-memory BFGS): Stores only a few vectors instead of the full Hessian approximation, making it practical for high-dimensional problems\n\n\nModern methods:\n\nMomentum methods (heavy ball method): Add a fraction of the previous step to the current gradient step: \\theta_{t+1} = \\theta_t - \\eta \\nabla f(\\theta_t) + \\beta(\\theta_t - \\theta_{t-1}) where \\beta \\in [0, 1) is the momentum coefficient. This helps the optimizer “roll through” small local variations and reduces zigzagging in narrow valleys.\nAccelerated gradient methods: Achieve provably faster convergence rates. Nesterov’s accelerated gradient has convergence rate O(1/t^2) compared to O(1/t) for standard gradient descent on convex functions.\n\nThese methods are especially important in deep learning where simple gradient descent would be too slow to navigate the complex, high-dimensional loss landscapes of neural networks.\n\n\n\n\n\n5.6.6 Stochastic Gradient Methods\nMany machine learning problems, especially in deep learning, involve optimizing functions of the form:\n\\min_\\theta f(X, \\theta) = \\sum_{i=1}^n f(x_i, \\theta)\nWhen n is large and \\theta is high-dimensional, evaluating the full gradient becomes computationally prohibitive.\nStochastic gradient descent (SGD) approximates the gradient using a random subset of data:\n\\theta_{n+1} = \\theta_n - \\eta_n \\nabla \\sum_{i \\in S_n} f(x_i, \\theta_n)\nwhere S_n \\subset \\{1, \\ldots, n\\} is a randomly selected mini-batch.\nConvergence: SGD converges for well-behaved functions when the learning rate sequence satisfies: \\sum_{i=1}^\\infty \\eta_i = \\infty, \\quad \\sum_{i=1}^\\infty \\eta_i^2 &lt; \\infty\nNote that constant learning rates don’t guarantee convergence to the exact optimum!\n\n\n\n\n\n\nPopular SGD Variants\n\n\n\nThe huge popularity of SGD has spawned many improved variants:\n\nAdam (Adaptive Moment Estimation) (Kingma and Ba 2015): Combines momentum with adaptive learning rates for each parameter\nAdaGrad (Adaptive Gradient): Adapts learning rate based on historical gradients - parameters with frequent updates get smaller learning rates\nRMSprop (Root Mean Square Propagation): Uses a running average of recent gradients to normalize the learning rate\n\n\n\n\n\n5.6.7 Which Optimizer Should I Use?\nFor smaller datasets (&lt; 10K observations):\n\nL-BFGS is usually the best first choice\nFast convergence, reliable for smooth problems\nStandard choice in traditional statistical software\n\nFor large datasets or deep learning:\n\nSGD and variants (especially Adam) are often the only practical choice\nRequire careful tuning of learning rates\nCan handle millions and even billions of parameters\n\nFor black-box problems (no gradients available):\n\nCMA-ES: Principled “population-based” method that can handle up to thousands of parameters (requiring many evaluations)\nBayesian Optimization: Efficient for expensive functions with D &lt; 20 parameters\nBADS (Bayesian Adaptive Direct Search): Combines Bayesian optimization with mesh adaptive direct search, good for noisy and mildly expensive functions with D &lt; 20\n\n\n\n\n\n\n\nAdvanced Topic: Constrained Optimization\n\n\n\n\n\nOften parameters have natural constraints:\n\nStandard deviations must be positive: \\sigma &gt; 0\nProbabilities must be in [0,1]: 0 \\leq p \\leq 1\nCorrelation matrices must be positive definite\n\nHow do we enforce these? There are typically three different approaches:\n\nReparameterization: Optimize \\phi = \\log \\sigma and then use \\sigma = e^\\phi to ensure positivity3\nConstrained optimization: Use algorithms like L-BFGS-B that allows you to specify parameter bounds\nBarrier methods: Add penalty terms that blow up at boundaries (can be unstable – not recommended)\n\n\n\n\n\n\n\n\n\n\nExample: 2D MLE with Numerical Optimization\n\n\n\n\n\nNow let’s apply the multidimensional optimization concepts to a real statistical problem. We’ll estimate both parameters of a Gamma distribution, which provides an excellent illustration of:\n\nTrue 2D optimization: Unlike our 1D examples, we need to simultaneously find the shape parameter \\alpha and scale parameter \\beta.\nConstrained optimization in practice: Both parameters must be positive, so we’ll use L-BFGS-B with bounds.\nThe importance of starting values: We’ll launch optimization from multiple starting points to see if they converge to the same solution. We choose the first starting point using the Method of Moments, while the others arbitrarily (could have been random).\nVisualizing the likelihood surface: We’ll create both 3D and contour plots to understand the optimization landscape.\n\nThe Gamma distribution is particularly interesting because its two parameters interact in the likelihood function, creating a curved valley in the likelihood surface rather than a simple bowl.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats, optimize\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Generate data from Gamma distribution\nnp.random.seed(42)\ntrue_alpha = 3.0  # shape parameter\ntrue_beta = 2.0   # scale parameter\nn = 100\ndata = stats.gamma.rvs(a=true_alpha, scale=true_beta, size=n)\n\n# Define negative log-likelihood for Gamma(α, β)\ndef neg_log_likelihood(params, data):\n    \"\"\"Negative log-likelihood for Gamma distribution\"\"\"\n    alpha, beta = params\n    if alpha &lt;= 0 or beta &lt;= 0:\n        return np.inf  # Return infinity for invalid parameters\n    return -np.sum(stats.gamma.logpdf(data, a=alpha, scale=beta))\n\n# Method of Moments starting values\nsample_mean = np.mean(data)\nsample_var = np.var(data)\nmom_beta = sample_var / sample_mean\nmom_alpha = sample_mean / mom_beta\n\n# Try optimization from different starting points\nstarting_points = [\n    [mom_alpha, mom_beta],  # MoM estimate\n    [1.0, 1.0],             # Generic start\n    [5.0, 5.0],             # Far from truth\n]\n\n# Store optimization paths\npaths = []\nresults = []\n\nfor i, start in enumerate(starting_points):\n    # Create a list to store the path for this optimization\n    path = []\n    \n    # Callback function to record each iteration\n    def callback(xk):\n        path.append(xk.copy())\n    \n    result = optimize.minimize(\n        fun=neg_log_likelihood,\n        x0=start,\n        args=(data,),\n        method='L-BFGS-B',\n        bounds=[(0.01, None), (0.01, None)],  # Enforce positivity\n        callback=callback\n    )\n    \n    # Add starting point to path\n    full_path = [np.array(start)] + path\n    paths.append(np.array(full_path))\n    results.append(result)\n    \n    print(f\"Start {i+1}: {start} → MLE: [{result.x[0]:.3f}, {result.x[1]:.3f}], \"\n          f\"NLL: {result.fun:.3f}, Iterations: {len(full_path)-1}\")\n\n# Best result\nbest_result = min(results, key=lambda r: r.fun)\nmle_alpha, mle_beta = best_result.x\n\n# Visualize the likelihood surface\nalpha_range = np.linspace(0.5, 6, 50)\nbeta_range = np.linspace(0.5, 5.2, 50)\nAlpha, Beta = np.meshgrid(alpha_range, beta_range)\nNLL = np.zeros_like(Alpha)\n\nfor i in range(Alpha.shape[0]):\n    for j in range(Alpha.shape[1]):\n        NLL[i, j] = neg_log_likelihood([Alpha[i, j], Beta[i, j]], data)\n\nfig = plt.figure(figsize=(7, 10))\n\n# 3D surface plot\nax1 = fig.add_subplot(2, 1, 1, projection='3d')\nsurf = ax1.plot_surface(Alpha, Beta, NLL, cmap='viridis', alpha=0.8, edgecolor='none')\nax1.contour(Alpha, Beta, NLL, levels=20, cmap='viridis', offset=NLL.min(), alpha=0.4)\n\n# Mark the different solutions\nax1.scatter([true_alpha], [true_beta], [neg_log_likelihood([true_alpha, true_beta], data)], \n           color='black', s=100, marker='*', label='True parameters')\nax1.scatter([mle_alpha], [mle_beta], [best_result.fun], \n           color='red', s=100, marker='o', label='MLE')\n\nax1.set_xlabel('α (shape)')\nax1.set_ylabel('β (scale)')\nax1.set_zlabel('Negative log-likelihood')\nax1.set_title('Likelihood Surface for Gamma Distribution')\nax1.view_init(elev=25, azim=45)\n\n# 2D contour plot with optimization paths\nax2 = fig.add_subplot(2, 1, 2)\ncontour = ax2.contour(Alpha, Beta, NLL, levels=30, cmap='viridis', alpha=0.6)\nax2.clabel(contour, inline=True, fontsize=8)\n\n# Plot optimization paths\ncolors = ['blue', 'green', 'orange']\nfor i, (path, color) in enumerate(zip(paths, colors)):\n    # Plot the full optimization path\n    ax2.plot(path[:, 0], path[:, 1], '-', color=color, linewidth=2, alpha=0.7)\n    # Mark start point\n    ax2.plot(path[0, 0], path[0, 1], 'o', color=color, markersize=10, \n             label=f'Start {i+1}')\n    # Mark intermediate points\n    ax2.plot(path[1:-1, 0], path[1:-1, 1], '.', color=color, markersize=6)\n    # Mark end point\n    ax2.plot(path[-1, 0], path[-1, 1], 's', color=color, markersize=10)\n\nax2.plot(true_alpha, true_beta, 'k*', markersize=12, label='True')\nax2.plot(mle_alpha, mle_beta, 'ro', markersize=10, label='MLE')\n\nax2.set_xlabel('α (shape)')\nax2.set_ylabel('β (scale)')\nax2.set_title('L-BFGS Optimization Traces from Different Starting Points')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTrue parameters: α = {true_alpha}, β = {true_beta}\")\nprint(f\"MLE estimates:   α = {mle_alpha:.3f}, β = {mle_beta:.3f}\")\nprint(f\"MoM estimates:   α = {mom_alpha:.3f}, β = {mom_beta:.3f}\")\n\n\nStart 1: [3.868907522031454, 1.510860197180884] → MLE: [4.021, 1.454], NLL: 240.066, Iterations: 6\nStart 2: [1.0, 1.0] → MLE: [4.021, 1.454], NLL: 240.066, Iterations: 14\nStart 3: [5.0, 5.0] → MLE: [4.021, 1.454], NLL: 240.066, Iterations: 13\n\nTrue parameters: α = 3.0, β = 2.0\nMLE estimates:   α = 4.021, β = 1.454\nMoM estimates:   α = 3.869, β = 1.511\n\n\n\n\n\n\n\n\n\nNotice how all three starting points converged to the same MLE, demonstrating that this likelihood surface is well-behaved with a single global optimum. The actual L-BFGS traces (with intermediate points marked as dots) reveal interesting optimization behavior:\n\nBlue path (MoM start): Converges in very few iterations since it starts close to the optimum\nGreen path (generic start): Takes a curved path following the likelihood valley\nOrange path (far start): Makes larger initial steps, then follows the contours more carefully as it approaches the optimum\n\nThe 3D plot reveals the characteristic curved valley of the Gamma likelihood, explaining why the optimization paths curve rather than taking straight lines to the optimum.\n\n\n\n\n\n5.6.8 FAQ: Common Issues in Numerical Optimization\n\n\n\n\n\n\nFrequently Asked Questions\n\n\n\nQ: I used a minimizer, but I’m supposed to be doing maximum likelihood. What gives?\nA: A classic source of confusion! Maximizing a function f(x) is equivalent to minimizing its negative -f(x). All standard optimization libraries are built as minimizers. Therefore, in practice, we always find the MLE by minimizing the negative log-likelihood function.\nQ: I tried different starting points and the optimizer gave different answers. Is it broken?\nA: No, this is expected behavior! The algorithms we use are local optimizers. They find the nearest valley (local minimum), but they have no guarantee of finding the deepest valley (global minimum). If your likelihood surface has multiple local optima, the result will depend on your starting point. This is why using a good initial value (like the Method of Moments estimate!) is so important.\nQ: How do I know if the algorithm has actually converged to the right answer?\nA: In general, you don’t know with 100% certainty. Optimizers use heuristics, like stopping when the change in the parameter values or the likelihood is very small. Good practices include:\n\nAlways try different starting points!\nCheck the optimizer’s status messages\nVerify that the gradient is near zero at the solution\nCompare with simpler methods (like MoM) as a sanity check\n\n\n\nLet’s demonstrate the importance of starting values:\n\n\nShow code\n# Example: Multiple local optima in a mixture model\n# We'll create a bimodal likelihood surface\n\ndef bimodal_nll(theta):\n    \"\"\"A negative log-likelihood with two local minima\"\"\"\n    # Artificial example with two valleys\n    return -np.log(0.6 * np.exp(-2*(theta-1)**2) + \n                   0.4 * np.exp(-3*(theta-4)**2) + 0.01)\n\n# Try optimization from different starting points\ntheta_range = np.linspace(-2, 7, 200)\nnll_surface = [bimodal_nll(t) for t in theta_range]\n\nstarting_points = [-1, 2.5, 5]\ncolors = ['red', 'green', 'blue']\nresults = []\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_range, nll_surface, 'k-', linewidth=2, label='Objective function')\n\nfor start, color in zip(starting_points, colors):\n    result = optimize.minimize(bimodal_nll, x0=[start], method='L-BFGS-B')\n    results.append(result.x[0])\n    plt.scatter([start], [bimodal_nll(start)], color=color, s=100, \n                marker='o', label=f'Start: {start:.1f}')\n    plt.scatter([result.x[0]], [result.fun], color=color, s=100, \n                marker='*', label=f'End: {result.x[0]:.2f}')\n    \n    # Draw arrow from start to end\n    plt.annotate('', xy=(result.x[0], result.fun), \n                 xytext=(start, bimodal_nll(start)),\n                 arrowprops=dict(arrowstyle='-&gt;', color=color, lw=2))\n\nplt.xlabel('θ')\nplt.ylabel('Negative log-likelihood')\nplt.title('Local Optimization: Different Starting Points → Different Solutions')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Starting points and their corresponding local optima:\")\nfor start, end in zip(starting_points, results):\n    print(f\"  Start: {start:4.1f} → End: {end:.3f}\")\n\n\n\n\n\n\n\n\n\nStarting points and their corresponding local optima:\n  Start: -1.0 → End: 4.000\n  Start:  2.5 → End: 1.000\n  Start:  5.0 → End: 4.000",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#chapter-summary-and-connections",
    "href": "chapters/05-parametric-inference-I.html#chapter-summary-and-connections",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "5.7 Chapter Summary and Connections",
    "text": "5.7 Chapter Summary and Connections\n\n5.7.1 Key Concepts Review\nWe’ve explored two fundamental approaches to finding estimators in parametric models:\nParametric Models:\n\nAssume data comes from a specific family of distributions \\mathfrak{F} = \\{f(x; \\theta): \\theta \\in \\Theta\\}\nOur job reduces to estimating the finite-dimensional parameter \\theta\nOften have parameters of interest and nuisance parameters\n\nMethod of Moments (MoM):\n\nMatch theoretical moments \\mathbb{E}(X^j) with sample moments \\frac{1}{n}\\sum X_i^j\nSimple to compute – just solve algebraic equations\nConsistent and asymptotically normal\nNot efficient, but excellent starting values for other methods\n\nMaximum Likelihood Estimation (MLE):\n\nFind parameters that make observed data most probable\nLikelihood: \\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i; \\theta)\nOften requires numerical optimization\nThe gold standard of parametric estimation\n\nNumerical Optimization:\n\nMost MLEs require iterative algorithms\nGradient-based methods dominate (L-BFGS for small data, SGD/Adam for large)\nAutomatic differentiation (JAX) makes implementation easier\nLocal optima are a real concern – always try multiple starting points!\n\n\n\n5.7.2 The Big Picture\nThis chapter revealed a fundamental connection: much of modern machine learning is secretly Maximum Likelihood Estimation:\n\nLinear regression: MLE with normal errors\nLogistic regression: MLE for Bernoulli responses\n\nNeural networks: MLE with complex parametric models\nDeep learning: MLE with stochastic optimization\n\nThe principles we’ve learned – likelihood, optimization, gradients – are the foundation of both classical statistics and modern ML.\n\n\n5.7.3 Common Pitfalls to Avoid\n\nConfusing likelihood with probability: The likelihood is NOT a probability distribution over parameters\nForgetting the negative sign: Optimizers minimize, so use negative log-likelihood\nAssuming analytical solutions exist: Most real problems require numerical methods\nTrusting a single optimization run: Did I mention that you should always try multiple starting points?\nIgnoring convergence warnings: Check optimizer status and diagnostics\n\n\n\n5.7.4 Chapter Connections\n\nPrevious: Chapter 3 gave us convergence concepts and the framework for evaluating estimators. Now we know how to find estimators.\nNext: Chapter 6 will explore the properties of these estimators in detail – bias, variance, efficiency – and prove that MLEs have optimal properties.\nBootstrap (Chapter 4): Provides a computational alternative to analytical standard errors for our estimators\nLater chapters: These estimation principles extend to more complex models (regression, time series, etc.)\n\n\n\n5.7.5 Self-Test Problems\n\nMethod of Moments: For X_1, \\ldots, X_n \\sim \\text{Uniform}(a, b), find the Method of Moments estimators for a and b.\nHint: Use \\mathbb{E}(X) = \\frac{a+b}{2} and \\mathbb{V}(X) = \\frac{(b-a)^2}{12}.\nMaximum Likelihood: For X_1, \\ldots, X_n \\sim \\text{Poisson}(\\lambda), find the Maximum Likelihood Estimator for \\lambda.\nHint: The Poisson PMF is f(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}.\nNumerical Optimization: The log-likelihood for logistic regression with one covariate is: \\ell(y, x; \\beta_0, \\beta_1) = \\sum_{i=1}^n \\left[y_i(\\beta_0 + \\beta_1 x_i) - \\log(1 + e^{\\beta_0 + \\beta_1 x_i})\\right]\nExplain why you cannot find the MLE for (\\beta_0, \\beta_1) analytically and must use numerical optimization.\nComparing Estimators: For X_1, \\ldots, X_n \\sim \\text{Exponential}(\\lambda):\n\nFind the MoM estimator for \\lambda\nFind the MLE for \\lambda\nAre they the same? Why or why not?\n\n\n\n\n5.7.6 Python and R Reference\nPythonR#| eval: false\nimport numpy as np\nfrom scipy import stats, optimize\nimport jax\nimport jax.numpy as jnp\n\n# Method of Moments estimation\ndef method_of_moments_normal(data):\n    \"\"\"MoM for Normal distribution\"\"\"\n    mu_hat = np.mean(data)\n    sigma2_hat = np.mean((data - mu_hat)**2)\n    return mu_hat, np.sqrt(sigma2_hat)\n\n# Maximum Likelihood with scipy\ndef mle_poisson(data):\n    \"\"\"MLE for Poisson distribution\"\"\"\n    # For Poisson, MLE is just the sample mean\n    return np.mean(data)\n\n# General MLE with numerical optimization\ndef neg_log_likelihood(params, data, dist_logpdf):\n    \"\"\"Generic negative log-likelihood using log-pdf for numerical stability\"\"\"\n    return -np.sum(dist_logpdf(data, *params))\n\n# Using scipy.optimize\ndata = stats.gamma.rvs(a=3, scale=2, size=100)\nresult = optimize.minimize(\n    fun=lambda params: neg_log_likelihood(params, data, \n                                         lambda x, a, b: stats.gamma.logpdf(x, a, scale=b)),\n    x0=[1.0, 1.0],  # Initial guess\n    method='L-BFGS-B',\n    bounds=[(0.01, None), (0.01, None)]  # Parameters must be positive\n)\n\n# Using JAX for automatic differentiation\ndef neg_log_lik_jax(params, data):\n    alpha, beta = params\n    # Use JAX's scipy.stats\n    return -jnp.sum(jax.scipy.stats.gamma.logpdf(data, alpha, scale=beta))\n\n# Get gradient automatically\ngrad_nll = jax.grad(neg_log_lik_jax)\n\n# Optimize with gradient\nresult_jax = optimize.minimize(\n    fun=lambda p: float(neg_log_lik_jax(jnp.array(p), data)),\n    x0=jnp.array([1.0, 1.0]),\n    method='L-BFGS-B',\n    jac=lambda p: np.array(grad_nll(jnp.array(p), data)),\n    bounds=[(0.01, None), (0.01, None)]\n)#| eval: false\n# Method of Moments estimation\nmethod_of_moments_normal &lt;- function(data) {\n  mu_hat &lt;- mean(data)\n  sigma2_hat &lt;- mean((data - mu_hat)^2)\n  c(mu = mu_hat, sigma = sqrt(sigma2_hat))\n}\n\n# Maximum Likelihood with built-in functions\nlibrary(MASS)  # For fitdistr\ndata &lt;- rgamma(100, shape = 3, scale = 2)\n# Note: fitdistr estimates shape and rate (where rate = 1/scale)\nmle_fit &lt;- fitdistr(data, \"gamma\")\n\n# Manual MLE with optim\nneg_log_likelihood &lt;- function(params, data) {\n  alpha &lt;- params[1]\n  beta &lt;- params[2]\n  if (alpha &lt;= 0 || beta &lt;= 0) return(Inf)\n  -sum(dgamma(data, shape = alpha, scale = beta, log = TRUE))\n}\n\n# Optimize\nresult &lt;- optim(\n  par = c(1, 1),  # Initial values\n  fn = neg_log_likelihood,\n  data = data,\n  method = \"L-BFGS-B\",\n  lower = c(0.01, 0.01)  # Lower bounds\n)\n\n# Using gradient information (numerical approximation)\nlibrary(numDeriv)\nresult_with_grad &lt;- optim(\n  par = c(1, 1),\n  fn = neg_log_likelihood,\n  gr = function(p, data) grad(neg_log_likelihood, p, data = data),\n  data = data,\n  method = \"L-BFGS-B\",\n  lower = c(0.01, 0.01)\n)\n\n# For more complex models, use specialized packages\nlibrary(bbmle)  # For mle2 function\nmle2_fit &lt;- mle2(\n  minuslogl = function(shape, scale) {\n    -sum(dgamma(data, shape = shape, scale = scale, log = TRUE))\n  },\n  start = list(shape = 1, scale = 1),\n  method = \"L-BFGS-B\",\n  lower = c(shape = 0.01, scale = 0.01)\n)\n\n\n5.7.7 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\nThis table maps sections in these lecture notes to the corresponding sections in Wasserman (2013) (“All of Statistics” or AoS).\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding AoS Section(s)\n\n\n\n\nIntroduction: Machine Learning As Statistical Estimation\nExpanded motivation from the slides and general context from AoS §9 introduction.\n\n\nParametric Models\nAoS §9 (Introduction), AoS §9.1 (Parameter of Interest). The Gamma distribution example is from AoS Example 9.2.\n\n\nThe Method of Moments (MoM)\n\n\n\n↳ The Principle: Matching Moments\nAoS §9.2 (Definition 9.3).\n\n\n↳ MoM Examples (Bernoulli, Normal, Gamma)\nAoS §9.2 (Examples 9.4, 9.5). The Gamma example is from AoS §9.14 (Exercise 1).\n\n\n↳ Properties of MoM Estimator\nAoS §9.2 (Theorem 9.6).\n\n\nMaximum Likelihood Estimation (MLE)\n\n\n\n↳ The Principle and Likelihood Function\nAoS §9.3 (Definitions 9.7, 9.8).\n\n\n↳ Finding the MLE Analytically\n\n\n\n↳↳ General approach & examples (Bernoulli, Normal)\nAoS §9.3 (Remark 9.9, Examples 9.10, 9.11).\n\n\n↳↳ Harder case: Uniform(0, θ)\nAoS §9.3 (Example 9.12).\n\n\nMLE Via Numerical Optimization\nExpanded material from the slides and AoS §9.13.4 (Appendix), with a modern ML focus.\n\n\n↳ The Need for Numerical Methods & Setup\nGeneral optimization concepts, related to AoS §9.13.4.\n\n\n↳ Gradient-Based Optimization\nConcepts related to Newton-Raphson from AoS §9.13.4, but expanded with modern methods (SGD, Adam, etc.).\n\n\n↳ Example: 2D MLE for Gamma distribution\nPractical application. The Gamma MoM is from AoS §9.14 (Exercise 1), serving as a starting point.\n\n\nChapter Summary and Connections\nNew summary material.\n\n\n↳ Self-Test Problems\nProblems inspired by AoS §9.14 (Exercises 2a, 5).\n\n\n\n\n\n\n\n\n5.7.8 Further Materials\n\nBayesian Optimization: An interactive exploration on Distill (Agnihotri & Batra; 2020)\nClassic reference: Casella & Berger, “Statistical Inference”, Chapter 7\nModern perspective: Efron & Hastie, “Computer Age Statistical Inference”, Chapter 4\n\n\nRemember: Parametric inference is about making assumptions (choosing a model) and then finding the best parameters within that model. The Method of Moments is simple but not optimal. Maximum Likelihood is the gold standard but often requires numerical optimization. Master these concepts – they’re the foundation of statistical modeling and machine learning!\n\n\n\n\n\n\nKingma, Diederik P, and Jimmy Ba. 2015. “Adam: A Method for Stochastic Optimization.” In International Conference on Learning Representations (ICLR).\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/05-parametric-inference-I.html#footnotes",
    "href": "chapters/05-parametric-inference-I.html#footnotes",
    "title": "5  Parametric Inference I: Finding Estimators",
    "section": "",
    "text": "In the next chapter, we’ll explore a special optimization algorithm for MLE called the EM (Expectation-Maximization) algorithm, which is particularly useful for models with latent variables or missing data.↩︎\nGenerally, evaluating a gradient in D dimensions with finite differences will need at least D + 1 evaluations.↩︎\nFor numerical stability, it has become common to use the softplus instead of \\exp to ensure positivity.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parametric Inference I: Finding Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html",
    "href": "chapters/06-parametric-inference-II.html",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "",
    "text": "6.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#learning-objectives",
    "href": "chapters/06-parametric-inference-II.html#learning-objectives",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "",
    "text": "Explain the key properties of the MLE (consistency, equivariance, asymptotic normality, and efficiency) and their practical implications.\nDefine Fisher Information and use it to quantify the precision of parameter estimates.\nConstruct and interpret confidence intervals for parameters using the MLE and its standard error.\nApply the Delta Method to find confidence intervals for transformed parameters.\nImplement the EM algorithm for finding MLEs in models with latent variables.\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter explores the theoretical properties of Maximum Likelihood Estimators and provides practical tools for statistical inference. The material is adapted from Chapter 9 of Wasserman (2013), with additional computational examples and modern perspectives on optimization for latent variable models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#introduction-how-good-are-our-estimators",
    "href": "chapters/06-parametric-inference-II.html#introduction-how-good-are-our-estimators",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.2 Introduction: How Good Are Our Estimators?",
    "text": "6.2 Introduction: How Good Are Our Estimators?\nIn Chapter 5, we learned how to find estimators for parametric models using the Method of Moments (MoM) and Maximum Likelihood Estimation (MLE). We saw that finding the MLE often requires numerical optimization, and we explored practical algorithms for this task.\nBut finding an estimator is only half the story. The next natural question is: how good are these estimators?\nThis chapter addresses this fundamental question by exploring the properties of estimators, with a special focus on the MLE. We’ll discover why the MLE is considered the “gold standard” of parametric estimation – it turns out to have a remarkable collection of desirable properties that make it optimal in many senses.\n\n\n\n\n\n\nKey Questions We’ll Answer\n\n\n\n\nDoes our estimator converge to the true value as we get more data? (Consistency)\nHow much uncertainty is associated with our estimate? Can we quantify it? (Confidence Intervals)\nIs our estimator the best possible one, or could we do better? (Efficiency)\nHow do these properties extend to complex models with multiple parameters or latent variables?\n\n\n\nUnderstanding these properties is crucial for several reasons:\n\nPractical inference: Knowing that \\hat{\\theta}_n = 2.3 is not very useful without understanding the uncertainty. Is the true value likely between 2.2 and 2.4, or between 1 and 4?\nMethod selection: When multiple estimation methods exist, these properties help us choose the best one.\nFoundation for advanced methods: The concepts in this chapter – especially Fisher Information and the EM algorithm – are fundamental to modern statistical methods and machine learning.\nConnection to optimization: We’ll see that the same mathematical quantities that determine statistical uncertainty also appear in optimization algorithms.\n\nLet’s begin with a concrete example to bridge our previous work on finding MLEs with the new focus on analyzing their properties.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#warm-up-a-complete-mle-example-with-numerical-optimization",
    "href": "chapters/06-parametric-inference-II.html#warm-up-a-complete-mle-example-with-numerical-optimization",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.3 Warm-up: A Complete MLE Example with Numerical Optimization",
    "text": "6.3 Warm-up: A Complete MLE Example with Numerical Optimization\nBefore diving into the theoretical properties, let’s work through a complete example that bridges Chapter 5 (finding MLEs) and Chapter 6 (analyzing them). We’ll find the MLE for a Beta distribution, which requires numerical optimization.\nThe likelihood for the Beta distribution with parameters \\alpha, \\beta &gt; 0 is: f(x; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\nThe log-likelihood for n observations is: \\begin{aligned}\n\\ell_n(\\alpha, \\beta) = \\sum_{i=1}^n \\Big[&\\log \\Gamma(\\alpha + \\beta) - \\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n&+ (\\alpha - 1) \\log(X_i) + (\\beta - 1) \\log(1-X_i)\\Big]\n\\end{aligned}\nWe will implement it below as the negative log-likelihood (beta_nll).\n\nimport numpy as np\nimport scipy.optimize\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy.special as jsps\n\n# Generate data from a Beta distribution\nnp.random.seed(43)\nx = np.random.beta(1.5, 0.5, size=100)\n\n# Define the negative log-likelihood for Beta(α, β)\ndef beta_nll(theta, x):\n    \"\"\"Negative log-likelihood for Beta distribution using JAX\"\"\"\n    alpha, beta = theta\n    # Use log-gamma function for numerical stability\n    return -jnp.sum(\n        jsps.gammaln(alpha + beta) \n        - jsps.gammaln(alpha) \n        - jsps.gammaln(beta)\n        + (alpha - 1) * jnp.log(x)\n        + (beta - 1) * jnp.log(1 - x)\n    )\n\n# Get the gradient function automatically\nbeta_grad = jax.grad(beta_nll)\n\n# We specify bounds: α, β &gt; 0 \n# - Bounds are *inclusive* so we set a small positive number as lower bound\n# - None here denotes infinity\nbounds = [(0.0001, None), (0.0001, None)]\n\n# Optimize using L-BFGS-B with bounds to ensure positivity\nresult = scipy.optimize.minimize(\n    beta_nll, \n    x0=jnp.array([1.0, 1.0]),  # Initial guess\n    args=(x,), # Tuple of additional arguments to pass to jac (x is data)\n    jac=beta_grad,\n    method='L-BFGS-B',\n    bounds=bounds\n)\n\nprint(f\"Raw optimization result:\\n\")\nprint(result)\n\nprint(f\"\\nExtracted results:\\n\")\nprint(f\"  MLE: α̂ = {result.x[0]:.3f}, β̂ = {result.x[1]:.3f}\")\nprint(f\"  Negative log-likelihood: {result.fun:.3f}\")\nprint(f\"  Converged: {result.success}\")\nprint(f\"  Iterations: {result.nit}\")\n\nRaw optimization result:\n\n  message: CONVERGENCE: RELATIVE REDUCTION OF F &lt;= FACTR*EPSMCH\n  success: True\n   status: 0\n      fun: -55.24228286743164\n        x: [ 1.501e+00  4.985e-01]\n      nit: 10\n      jac: [-1.030e-04 -9.155e-05]\n     nfev: 41\n     njev: 41\n hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;\n\nExtracted results:\n\n  MLE: α̂ = 1.501, β̂ = 0.499\n  Negative log-likelihood: -55.242\n  Converged: True\n  Iterations: 10\n\n\nThis example showcases several important points:\n\nAutomatic differentiation: We use JAX to automatically compute gradients, avoiding error-prone manual derivations\nConstrained optimization: The L-BFGS-B method handles the constraint that both parameters must be positive\nNumerical stability: We work with log-gamma functions rather than raw factorials\n\n\n\n\n\n\n\nThe MLE as a Starting Point\n\n\n\nNow that we have \\hat{\\alpha} and \\hat{\\beta}, we can ask the crucial questions:\n\nHow accurate are these estimates?\nWhat’s their sampling distribution?\nAre they optimal?\n\nThese are exactly the questions this chapter will answer!\n\n\n\n\n\n\n\n\nFinnish-English Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here are the key terms we’ll use in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nEquivariant\nEkvivariantti\nProperty of MLE under reparameterization\n\n\nEfficient\nTehokas\nOptimal variance property\n\n\nScore function\nRinnefunktio\nGradient of log-likelihood\n\n\nFisher information\nFisherin informaatio\nVariance of score function\n\n\nDelta method\nDelta-menetelmä\nMethod for transformed parameters\n\n\nSufficient statistic\nTyhjentävä tunnusluku\nContains all information about parameter\n\n\nLatent variable\nPiilomuuttuja\nUnobserved variable in model\n\n\nEM Algorithm\nEM-algoritmi\nExpectation-Maximization algorithm\n\n\nAsymptotic normality\nAsymptoottinen normaalisuus\nLarge-sample distribution property\n\n\nConsistency\nKonsistenssi/Tarkentuva\nConvergence to true value",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#core-properties-of-the-maximum-likelihood-estimator",
    "href": "chapters/06-parametric-inference-II.html#core-properties-of-the-maximum-likelihood-estimator",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.4 Core Properties of the Maximum Likelihood Estimator",
    "text": "6.4 Core Properties of the Maximum Likelihood Estimator\nBefore exploring its properties, let’s recall the definition of the maximum likelihood estimator (MLE) from Chapter 5:1\n\nLet X_1, \\ldots, X_n be i.i.d. with PDF (or PMF) f(x; \\theta).\nThe likelihood function is: \\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i; \\theta)\nThe log-likelihood function is: \\ell_n(\\theta) = \\log \\mathcal{L}_n(\\theta) = \\sum_{i=1}^n \\log f(X_i; \\theta)\nThe maximum likelihood estimator (MLE), denoted by \\hat{\\theta}_n, is the value of \\theta that maximizes \\mathcal{L}_n(\\theta) (or equivalently, \\ell_n(\\theta)).\n\n\n6.4.1 Overview\nIn Chapter 5, we saw that the MLE is found by maximizing the likelihood function – a principle that seems intuitively reasonable. But intuition alone doesn’t make for good statistics. We need to understand why the MLE works so well.\nIt turns out that the MLE has several remarkable properties that make it the “gold standard” of parametric estimation:\n\n\n\n\n\n\n\n\nProperty\nMathematical Statement\nIntuitive Meaning\n\n\n\n\nConsistency\n\\hat{\\theta}_n \\xrightarrow{P} \\theta_*\nThe MLE converges to the true parameter value as n \\to \\infty\n\n\nEquivariance\nIf \\hat{\\theta}_n is the MLE of \\theta, then g(\\hat{\\theta}_n) is the MLE of g(\\theta)\nThe MLE behaves sensibly under parameter transformations\n\n\nAsymptotic Normality\n\\frac{(\\hat{\\theta}_n - \\theta_*)}{\\hat{\\text{se}}} \\rightsquigarrow \\mathcal{N}(0, 1)\nThe MLE has an approximately normal distribution for large samples\n\n\nAsymptotic Efficiency\n\\text{Var}(\\hat{\\theta}_n) achieves the Cramér-Rao lower bound\nThe MLE has the smallest possible variance among consistent estimators\n\n\nApproximate Bayes\n\\hat{\\theta}_n \\approx \\arg\\max_\\theta \\pi(\\theta \\mid X^n) with flat prior\nThe MLE approximates the Bayesian posterior mode\n\n\n\nLet’s explore each of these properties in detail.\n\n\n6.4.2 Consistency: Getting It Right Eventually\nThe most fundamental property we could ask of any estimator is that it gets closer to the truth as we collect more data. This is the property of consistency.\n\nAn estimator \\hat{\\theta}_n is consistent for \\theta_* if: \\hat{\\theta}_n \\xrightarrow{P} \\theta_* That is, for any \\epsilon &gt; 0: \\lim_{n \\to \\infty} P(|\\hat{\\theta}_n - \\theta_*| &gt; \\epsilon) = 0\n\nIn words: as we collect more data, the probability that our estimate is far from the truth goes to zero. The MLE has this property under mild conditions.\n\n\n\n\n\n\nThe Theory Behind Consistency\n\n\n\n\n\nThe consistency of the MLE is deeply connected to the Kullback-Leibler (KL) divergence. For two densities f and g, the KL divergence is:\nD(f, g) = \\int f(x) \\log \\frac{f(x)}{g(x)} dx\nKey properties:\n\nD(f, g) \\geq 0 always\nD(f, g) = 0 if and only if f = g (almost everywhere)\n\nThe crucial insight is that maximizing the log-likelihood is equivalent to minimizing the KL divergence between the true distribution and our model. As n \\to \\infty, the empirical distribution converges to the true distribution, so the MLE converges to the parameter that makes the model distribution equal to the true distribution.\nIdentifiability: For this to work, the model must be identifiable: different parameter values must correspond to different distributions. That is, \\theta \\neq \\psi implies D(f(\\cdot; \\theta), f(\\cdot; \\psi)) &gt; 0.\n\n\n\n\n\n\n\n\n\nExample: Consistency in Action\n\n\n\nLet’s visualize how the MLE becomes more accurate with increasing sample size n. We’ll use the Poisson distribution where the MLE for the rate parameter \\lambda is simply the sample mean:2\n\n\nShow code\nimport matplotlib.pyplot as plt\n\n# True parameter\ntrue_lambda = 3.0\n\n# Sample sizes to consider\nsample_sizes = [10, 50, 100, 500, 1000, 5000]\nnum_simulations = 100\n\n# Store results\nmle_estimates = {n: [] for n in sample_sizes}\n\n# Run simulations\nnp.random.seed(42)\nfor n in sample_sizes:\n    for _ in range(num_simulations):\n        # Generate Poisson data\n        data = np.random.poisson(true_lambda, size=n)\n        # MLE for Poisson is just the sample mean\n        mle = np.mean(data)\n        mle_estimates[n].append(mle)\n\n# Create plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5))\n\n# Left plot: Box plots showing distribution at each sample size\npositions = range(len(sample_sizes))\nax1.boxplot([mle_estimates[n] for n in sample_sizes], \n            positions=positions,\n            labels=[str(n) for n in sample_sizes])\nax1.axhline(y=true_lambda, color='red', linestyle='--', label=f'True λ = {true_lambda}')\nax1.set_xlabel('Sample size n')\nax1.set_ylabel('MLE estimate')\nax1.set_title('Distribution of MLE')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right plot: Standard deviation vs sample size\nstd_devs = [np.std(mle_estimates[n]) for n in sample_sizes]\nax2.loglog(sample_sizes, std_devs, 'bo-', label='Empirical SD')\n# Theoretical standard deviation: sqrt(λ/n)\ntheoretical_sd = [np.sqrt(true_lambda/n) for n in sample_sizes]\nax2.loglog(sample_sizes, theoretical_sd, 'r--', label='Theoretical SD')\nax2.set_xlabel('Sample size n')\nax2.set_ylabel('Standard deviation of MLE')\nax2.set_title('Convergence Rate')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plots demonstrate consistency: as n increases, the MLE concentrates more tightly around the true value, with standard deviation decreasing at rate 1/\\sqrt{n}.\n\n\n\n\n6.4.3 Equivariance: Reparameterization Invariance\nA subtle but important property of the MLE is equivariance (or “functional invariance”). This means that if we reparameterize our model, the MLE transforms in the natural way.\n\nIf \\hat{\\theta}_n is the MLE of \\theta, then for any function g: \\widehat{g(\\theta)} = g(\\hat{\\theta}_n) That is, the MLE of \\tau = g(\\theta) is \\hat{\\tau}_n = g(\\hat{\\theta}_n).\n\nProof: Let \\tau = g(\\theta) where g has inverse h, so \\theta = h(\\tau). For any \\tau:\n\\mathcal{L}_n(\\tau) = \\prod_{i=1}^n f(X_i; h(\\tau)) = \\prod_{i=1}^n f(X_i; \\theta) = \\mathcal{L}_n(\\theta)\nwhere \\theta = h(\\tau). Therefore:\n\\mathcal{L}_n(\\tau) = \\mathcal{L}_n(\\theta) \\leq \\mathcal{L}_n(\\hat{\\theta}_n) = \\mathcal{L}_n(\\hat{\\tau}_n)\nSince this holds for any \\tau, we have \\hat{\\tau}_n = g(\\hat{\\theta}_n). □\n\n\n\n\n\n\nExample: Equivariance in Practice\n\n\n\nConsider X_1, \\ldots, X_n \\sim \\mathcal{N}(\\theta, 1) where we’re interested in both:\n\nThe mean \\theta\nThe parameter \\tau = e^\\theta (perhaps \\theta is log-income and \\tau is income)\n\nThe MLE for \\theta is \\hat{\\theta}_n = \\bar{X}_n. By equivariance: \\hat{\\tau}_n = e^{\\hat{\\theta}_n} = e^{\\bar{X}_n}\nNo need to rederive from scratch! This is particularly convenient when dealing with complex transformations.\n\n\nWhy Equivariance Matters:\n\nConvenience: We can work in whatever parameterization is most natural for finding the MLE, then transform to the parameterization of interest.\nConsistency across parameterizations: Different researchers might parameterize the same model differently (e.g., variance vs. precision in a normal distribution). Equivariance ensures they’ll get equivalent results.\nNot universal: This property is special to MLEs! Other estimators, like the Bayesian posterior mode (also known as MAP or maximum a posteriori estimate), generally lack this property. For instance, if \\theta has a uniform prior, \\tau = \\theta^2 does not have a uniform prior, leading to different posterior modes.\n\n\n\n\n\n\n\nA Common Misconception\n\n\n\nEquivariance does NOT mean that: \\mathbb{E}[g(\\hat{\\theta}_n)] = g(\\mathbb{E}[\\hat{\\theta}_n])\nIn general, g(\\hat{\\theta}_n) is a biased estimator of g(\\theta) even if \\hat{\\theta}_n is unbiased for \\theta (unless g is linear). Equivariance is about what parameter value maximizes the likelihood, not about expected values.\n\n\n\n\n6.4.4 Asymptotic Normality & Optimality\nThe consistency and equivariance properties are nice, but they don’t tell us about the distribution of the MLE. How much uncertainty is there in our estimate? How efficient is it compared to other estimators?\nThe remarkable answer is that the MLE is approximately normally distributed with the smallest possible variance. We’ll explore these twin properties – asymptotic normality and efficiency – in detail in the next section, as they require us to first understand a fundamental concept: the Fisher Information.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#fisher-information-and-confidence-intervals",
    "href": "chapters/06-parametric-inference-II.html#fisher-information-and-confidence-intervals",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.5 Fisher Information and Confidence Intervals",
    "text": "6.5 Fisher Information and Confidence Intervals\n\n6.5.1 Fisher Information: Quantifying What Data Can Tell Us\nTo understand the precision of the MLE, we need to introduce one of the most important concepts in statistical theory: the Fisher Information. Named after statistician and polymath R.A. Fisher, this quantity measures how much “information” about a parameter is contained in the data.\nThe Fisher Information is formally defined through the score function and its variance.\n\nThe score function is the gradient of the log-likelihood: s(X; \\theta) = \\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta}\nThe Fisher Information is: I_n(\\theta) = \\mathbb{V}_\\theta\\left(\\sum_{i=1}^n s(X_i; \\theta)\\right) = \\sum_{i=1}^n \\mathbb{V}_\\theta(s(X_i; \\theta))\n\nFor a single observation (n=1), we often write I(\\theta) = I_1(\\theta).\nThe computation of the Fisher information can be simplified using the following results:\n\nFor an IID sample of size n:\n\nI_n(\\theta) = n \\cdot I(\\theta) (information accumulates linearly)\n\\mathbb{E}_\\theta[s(X; \\theta)] = 0 (expected score is zero at the true parameter)\nI(\\theta) = -\\mathbb{E}_\\theta\\left[\\frac{\\partial^2 \\log f(X; \\theta)}{\\partial \\theta^2}\\right] (expected negative curvature)\n\n\nThe last property shows that Fisher Information is literally the expected curvature of the log-likelihood – confirming our intuition about “sharpness”!\n\n\n\n\n\n\nProof of Properties of Fisher Information and Score\n\n\n\n\n\nProperty 1: Linear accumulation for IID samples\nFor IID samples X_1, \\ldots, X_n: I_n(\\theta) = \\mathbb{V}_\\theta\\left(\\sum_{i=1}^n s(X_i; \\theta)\\right)\nSince the X_i are independent and \\mathbb{V}(\\sum Y_i) = \\sum \\mathbb{V}(Y_i) for independent random variables: I_n(\\theta) = \\sum_{i=1}^n \\mathbb{V}_\\theta(s(X_i; \\theta)) = n \\cdot \\mathbb{V}_\\theta(s(X; \\theta)) = n \\cdot I(\\theta)\nProperty 2: Expected score is zero\nUnder regularity conditions that allow interchange of derivative and integral: \\mathbb{E}_\\theta[s(X; \\theta)] = \\mathbb{E}_\\theta\\left[\\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta}\\right] = \\int \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} f(x; \\theta) dx\nUsing \\frac{\\partial \\log f}{\\partial \\theta} = \\frac{1}{f} \\frac{\\partial f}{\\partial \\theta}: = \\int \\frac{\\partial f(x; \\theta)}{\\partial \\theta} dx = \\frac{\\partial}{\\partial \\theta} \\int f(x; \\theta) dx = \\frac{\\partial}{\\partial \\theta} (1) = 0\nProperty 3: Alternative formula using second derivative\nWe start with Property 2: \\mathbb{E}_\\theta[s(X; \\theta)] = 0, which we can write explicitly as: \\int s(x; \\theta) f(x; \\theta) dx = \\int \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} f(x; \\theta) dx = 0\nSince this holds for all \\theta, we can differentiate both sides with respect to \\theta: \\frac{\\partial}{\\partial \\theta} \\int s(x; \\theta) f(x; \\theta) dx = 0\nUnder regularity conditions (allowing interchange of derivative and integral): \\int \\frac{\\partial}{\\partial \\theta}[s(x; \\theta) f(x; \\theta)] dx = 0\nUsing the product rule: \\int \\left[\\frac{\\partial s(x; \\theta)}{\\partial \\theta} f(x; \\theta) + s(x; \\theta) \\frac{\\partial f(x; \\theta)}{\\partial \\theta}\\right] dx = 0\nNow, note that \\frac{\\partial f(x; \\theta)}{\\partial \\theta} = f(x; \\theta) \\cdot \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} = f(x; \\theta) \\cdot s(x; \\theta)\nSubstituting: \\int \\left[\\frac{\\partial s(x; \\theta)}{\\partial \\theta} f(x; \\theta) + s(x; \\theta)^2 f(x; \\theta)\\right] dx = 0\nThis can be rewritten as: \\mathbb{E}_\\theta\\left[\\frac{\\partial s(X; \\theta)}{\\partial \\theta}\\right] + \\mathbb{E}_\\theta[s(X; \\theta)^2] = 0\nSince s(X; \\theta) = \\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta}, we have:\n\n\\frac{\\partial s(X; \\theta)}{\\partial \\theta} = \\frac{\\partial^2 \\log f(X; \\theta)}{\\partial \\theta^2}\n\\mathbb{E}_\\theta[s(X; \\theta)^2] = \\mathbb{V}_\\theta[s(X; \\theta)] = I(\\theta) (since \\mathbb{E}_\\theta[s(X; \\theta)] = 0)\n\nTherefore: \\mathbb{E}_\\theta\\left[\\frac{\\partial^2 \\log f(X; \\theta)}{\\partial \\theta^2}\\right] + I(\\theta) = 0\nWhich gives us: I(\\theta) = -\\mathbb{E}_\\theta\\left[\\frac{\\partial^2 \\log f(X; \\theta)}{\\partial \\theta^2}\\right]\n\n\n\nIntuitiveMathematicalComputationalImagine you’re trying to estimate a parameter by maximizing the\nlog-likelihood function. Picture this function as a hill that you’re\nclimbing to find the peak (the MLE).Now think about two scenarios:\nSharp, pointy peak: The log-likelihood drops off\nsteeply as you move away from the maximum. Even a small change in the\nparameter makes the data much less likely. This means the data is very\n“informative” – it strongly prefers one specific parameter\nvalue.\nFlat, broad peak: The log-likelihood changes\nslowly near the maximum. Many different parameter values give similar\nlikelihoods. The data isn’t very informative about the exact parameter\nvalue.\nThe Fisher Information measures the “sharpness” or\ncurvature of the log-likelihood at its peak:\nSharp peak → High Fisher Information → Low variance\nfor \\(\\hat{\\theta}\\) → Narrow\nconfidence interval\nFlat peak → Low Fisher Information → High variance\nfor \\(\\hat{\\theta}\\) → Wide confidence\ninterval\nThis is the key insight: the same curvature that makes optimization\neasy (sharp peak = clear maximum) also makes estimation precise (sharp\npeak = low uncertainty)!The meaning of\n\\(\\mathbb{E}_\\theta[s(X; \\theta)] = 0\\)\nis subtle and often misunderstood. It does NOT mean:\nThe derivative is zero (that’s what happens at the MLE for a\nspecific dataset)\nThe log-likelihood is maximized at the true parameter\nWhat it DOES mean:\nWhen data is generated from\n\\(f(x; \\theta_*)\\), the score evaluated\nat \\(\\theta_*\\) (true parameter)\naverages to zero across all possible datasets\nIf you’re at the true parameter value and observe a random data\npoint, it’s equally likely to suggest increasing or decreasing\n\\(\\theta\\)\nThis is why the true parameter is “stable” – random samples don’t\nsystematically pull the estimate away from the truth\nThink of it as a balance point: at the true parameter, the data\nprovides no systematic evidence to move in either direction, even though\nany individual sample might suggest moving up or down.Let’s calculate and visualize the Fisher Information for a concrete\nexample. We’ll examine the Bernoulli\ndistribution, where the Fisher Information has a particularly\ninteresting form, which we will derive later:\\[I(p) = 1/(p(1-p))\\]This shows how the precision of estimating a probability depends on\nthe true probability value:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Example: Fisher Information for Bernoulli(p)\n# For Bernoulli: I(p) = 1/(p(1-p))\n\np_values = np.linspace(0.01, 0.99, 200)\nfisher_info = 1 / (p_values * (1 - p_values))\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 8))\n\n# Top plot: Fisher Information\nax1.plot(p_values, fisher_info, 'b-', linewidth=2)\nax1.set_xlabel('p')\nax1.set_ylabel('I(p)')\nax1.set_title('Fisher Information for Bernoulli(p)')\nax1.grid(True, alpha=0.3)\nax1.set_ylim(0, 50)\n\n# Middle plot: Standard error (1/sqrt(nI(p))) for different n\nn_values = [10, 50, 100, 500]\ncolors = ['red', 'orange', 'green', 'blue']\nfor n, color in zip(n_values, colors):\n    se = 1 / np.sqrt(n * fisher_info)\n    ax2.plot(p_values, se, color=color, linewidth=2, label=f'n = {n}')\nax2.set_xlabel('p')\nax2.set_ylabel('Standard Error')\nax2.set_title('Standard Error of MLE')\nax2.legend()\nax2.grid(True, alpha=0.3)\nax2.set_ylim(0, 0.2)\n\n# Bottom plot: Log-likelihood curves for different p values\nn_obs = 20\nsuccesses = 12\ntheta_range = np.linspace(0.01, 0.99, 200)\n\n# Show log-likelihood for different true p values\nfor p_true, color in [(0.5, 'red'), (0.6, 'green'), (0.8, 'blue')]:\n    log_lik = successes * np.log(theta_range) + (n_obs - successes) * np.log(1 - theta_range)\n    ax3.plot(theta_range, log_lik, color=color, linewidth=2, \n             label=f'Data from p = {p_true}')\n    \n    # Mark the MLE\n    mle = successes / n_obs\n    mle_ll = successes * np.log(mle) + (n_obs - successes) * np.log(1 - mle)\n    ax3.plot(mle, mle_ll, 'ko', markersize=8)\n\nax3.set_xlabel('θ')\nax3.set_ylabel('Log-likelihood')\nax3.set_title(f'Log-likelihood curves (n={n_obs}, observed {successes} successes)')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\nKey insights from the plots:\nFisher Information is lowest at\n\\(p=0.5\\) (hardest to estimate a fair\ncoin)\nFisher Information\n\\(\\rightarrow \\infty\\) as\n\\(p \\rightarrow 0\\) or\n\\(p \\rightarrow 1\\) (extreme\nprobabilities are ‘easier’ to pin down)\nStandard error decreases with both\n\\(n\\) and\n\\(I(p)\\)\nThe curvature of the log-likelihood reflects the Fisher\nInformation\n\n\n\n6.5.2 Asymptotic Normality of the MLE\nNow we can state the key theorem that connects Fisher Information to the distribution of the MLE:\n\nLet \\text{se} = \\sqrt{\\mathbb{V}(\\hat{\\theta}_n)}. Under appropriate regularity conditions we have the following:\n\n\\text{se} \\approx \\sqrt{1 / I_n(\\theta)} and \\frac{(\\hat{\\theta}_n - \\theta)}{\\text{se}} \\rightsquigarrow \\mathcal{N}(0,1)\nLet \\widehat{\\text{se}} = \\sqrt{1 / I_n(\\hat{\\theta}_n)}. Then \\frac{(\\hat{\\theta}_n - \\theta)}{\\widehat{\\text{se}}} \\rightsquigarrow \\mathcal{N}(0,1)\n\n\nAccording to this theorem, the distribution of the MLE is approximately \\mathcal{N}(\\theta, \\widehat{\\text{se}}^2). This is one of the most important results in statistics: it tells us not just that the MLE converges to the true value (consistency), but also gives us its approximate distribution for finite samples.\nThe key insight is that the same Fisher Information that measures how “sharp” the likelihood is also determines the precision of our estimate. Sharp likelihood → High Fisher Information → Small standard error → Precise estimate.\n\n\n6.5.3 Constructing Confidence Intervals for the MLE\nWith the asymptotic normality result in hand, we can now construct confidence intervals for our parameter estimates.\n\nThe interval C_n = \\left( \\hat{\\theta}_n - z_{\\alpha/2}\\widehat{\\text{se}}, \\hat{\\theta}_n + z_{\\alpha/2}\\widehat{\\text{se}} \\right) is an asymptotically valid (1-\\alpha) confidence interval for \\theta.\n\nFor a 95% confidence interval, z_{0.025} \\approx 1.96 \\approx 2, giving the simple rule:\n\\text{95\\% CI} \\approx \\hat{\\theta}_n \\pm 2 \\cdot \\widehat{\\text{se}}\nThis type of confidence interval is used as the basis for statements about expected error in opinion polls and many other applications.\n\n\n\n\n\n\nExample: Confidence Interval for a Proportion\n\n\n\nLet’s work through the complete derivation for the Bernoulli case, which gives us confidence intervals for proportions – one of the most common applications in practice.\nFor X_1, \\ldots, X_n \\sim \\text{Bernoulli}(p), we have f(x; p) = p^x (1-p)^{1-x} for x \\in \\{0, 1\\}.\nStep 1: Find the MLE The likelihood for n observations is: \\mathcal{L}_n(p) = \\prod_{i=1}^n p^{X_i} (1-p)^{1-X_i} = p^{S} (1-p)^{n-S} where S = \\sum_{i=1}^n X_i is the total number of successes.\nTaking the log: \\ell_n(p) = S \\log p + (n-S) \\log(1-p)\nSetting \\frac{d\\ell_n}{dp} = \\frac{S}{p} - \\frac{n-S}{1-p} = 0 gives \\hat{p}_n = \\frac{S}{n} = \\bar{X}_n\nStep 2: Compute the score function For a single observation: \\log f(X; p) = X \\log p + (1-X) \\log(1-p)\nThe score function is: s(X; p) = \\frac{\\partial \\log f(X; p)}{\\partial p} = \\frac{X}{p} - \\frac{1-X}{1-p}\nLet’s verify \\mathbb{E}_p[s(X; p)] = 0: \\mathbb{E}_p[s(X; p)] = \\mathbb{E}_p\\left[\\frac{X}{p} - \\frac{1-X}{1-p}\\right] = \\frac{p}{p} - \\frac{1-p}{1-p} = 1 - 1 = 0 \\checkmark\nStep 3: Compute the second derivative \\frac{\\partial^2 \\log f(X; p)}{\\partial p^2} = \\frac{\\partial}{\\partial p}\\left[\\frac{X}{p} - \\frac{1-X}{1-p}\\right] = -\\frac{X}{p^2} - \\frac{1-X}{(1-p)^2}\nStep 4: Find the Fisher Information Using the formula I(p) = -\\mathbb{E}_p\\left[\\frac{\\partial^2 \\log f(X; p)}{\\partial p^2}\\right]:\nI(p) = -\\mathbb{E}_p\\left[-\\frac{X}{p^2} - \\frac{1-X}{(1-p)^2}\\right] = \\mathbb{E}_p\\left[\\frac{X}{p^2}\\right] + \\mathbb{E}_p\\left[\\frac{1-X}{(1-p)^2}\\right]\nSince \\mathbb{E}_p[X] = p and \\mathbb{E}_p[1-X] = 1-p: I(p) = \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1-p+p}{p(1-p)} = \\frac{1}{p(1-p)}\nStep 5: Derive the standard error For n observations, I_n(p) = n \\cdot I(p) = \\frac{n}{p(1-p)}\nThe standard error of \\hat{p}_n is: \\widehat{\\text{se}} = \\frac{1}{\\sqrt{I_n(\\hat{p}_n)}} = \\frac{1}{\\sqrt{\\frac{n}{\\hat{p}_n(1-\\hat{p}_n)}}} = \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\nStep 6: Construct the confidence interval From the asymptotic normality theorem: \\frac{\\hat{p}_n - p}{\\widehat{\\text{se}}} \\rightsquigarrow \\mathcal{N}(0,1)\nFor a (1-\\alpha) confidence interval: \\mathbb{P}\\left(-z_{\\alpha/2} \\leq \\frac{\\hat{p}_n - p}{\\widehat{\\text{se}}} \\leq z_{\\alpha/2}\\right) \\approx 1-\\alpha\nRearranging: \\mathbb{P}\\left(\\hat{p}_n - z_{\\alpha/2} \\cdot \\widehat{\\text{se}} \\leq p \\leq \\hat{p}_n + z_{\\alpha/2} \\cdot \\widehat{\\text{se}}\\right) \\approx 1-\\alpha\nTherefore, the (1-\\alpha) confidence interval is: \\hat{p}_n \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\n\n\n\n\n\n\n\n\nExample: Political Opinion Polling\n\n\n\nLet’s apply the confidence interval for a proportion from the previous worked example to understand how political polls work and what their “margin of error” really means.\n\n\nShow code\n# Political polling example\n# Suppose we poll n=1000 randomly selected likely voters about a referendum\n\nn = 1000  # Sample size\nsuccesses = 520  # Number who support the referendum\np_hat = successes / n  # Sample proportion = 0.52 (52%)\n\n# Standard error using the formula we derived\nse = np.sqrt(p_hat * (1 - p_hat) / n)\n\n# 95% confidence interval (using z_{0.025} = 1.96)\nz_critical = 1.96\nci_lower = p_hat - z_critical * se\nci_upper = p_hat + z_critical * se\n\nprint(\"POLL RESULTS:\")\nprint(f\"Sample size: {n} voters\")\nprint(f\"Support for referendum: {p_hat*100:.1f}% ({successes} out of {n})\")\nprint(f\"Standard error: {se*100:.2f} percentage points\")\nprint(f\"95% confidence interval: ({ci_lower*100:.1f}%, {ci_upper*100:.1f}%)\")\nprint(f\"Margin of error: ±{z_critical*se*100:.1f} percentage points\")\n\n# Interpretation\nprint(\"\\nINTERPRETATION:\")\nif ci_lower &gt; 0.5:\n    print(\"The referendum has statistically significant majority support.\")\nelif ci_upper &lt; 0.5:\n    print(\"The referendum has statistically significant minority support.\")\nelse:\n    print(\"The poll cannot determine if there's majority support (CI includes 50%).\")\n\n# Compare different scenarios to understand margin of error\nprint(\"\\n\" + \"=\"*60)\nprint(\"HOW MARGIN OF ERROR VARIES:\")\nprint(\"=\"*60)\n\nscenarios = [\n    (0.50, 500, \"50% support, n=500\"),\n    (0.50, 1000, \"50% support, n=1000\"), \n    (0.50, 2000, \"50% support, n=2000\"),\n    (0.20, 1000, \"20% support, n=1000\"),\n    (0.10, 1000, \"10% support, n=1000\"),\n    (0.01, 1000, \"1% support, n=1000\")\n]\n\nprint(f\"{'Scenario':&lt;25} {'Margin of Error':&gt;20} {'95% CI':&gt;20}\")\nprint(\"-\" * 65)\nfor p, n, desc in scenarios:\n    margin = 1.96 * np.sqrt(p * (1 - p) / n) * 100\n    ci_low = (p - 1.96 * np.sqrt(p * (1 - p) / n)) * 100\n    ci_high = (p + 1.96 * np.sqrt(p * (1 - p) / n)) * 100\n    print(f\"{desc:&lt;25} ±{margin:&gt;6.1f} percentage pts  ({ci_low:&gt;5.1f}%, {ci_high:&gt;5.1f}%)\")\n\n# Key assumptions and limitations\nprint(\"\\nKEY ASSUMPTIONS:\")\nprint(\"1. Random sampling from the population of interest\")\nprint(\"2. Respondents answer truthfully\")\nprint(\"3. The sample size is large enough for normal approximation (np ≥ 10 and n(1-p) ≥ 10)\")\nprint(\"4. No systematic bias in who responds to the poll\")\n\n\nPOLL RESULTS:\nSample size: 1000 voters\nSupport for referendum: 52.0% (520 out of 1000)\nStandard error: 1.58 percentage points\n95% confidence interval: (48.9%, 55.1%)\nMargin of error: ±3.1 percentage points\n\nINTERPRETATION:\nThe poll cannot determine if there's majority support (CI includes 50%).\n\n============================================================\nHOW MARGIN OF ERROR VARIES:\n============================================================\nScenario                       Margin of Error               95% CI\n-----------------------------------------------------------------\n50% support, n=500        ±   4.4 percentage pts  ( 45.6%,  54.4%)\n50% support, n=1000       ±   3.1 percentage pts  ( 46.9%,  53.1%)\n50% support, n=2000       ±   2.2 percentage pts  ( 47.8%,  52.2%)\n20% support, n=1000       ±   2.5 percentage pts  ( 17.5%,  22.5%)\n10% support, n=1000       ±   1.9 percentage pts  (  8.1%,  11.9%)\n1% support, n=1000        ±   0.6 percentage pts  (  0.4%,   1.6%)\n\nKEY ASSUMPTIONS:\n1. Random sampling from the population of interest\n2. Respondents answer truthfully\n3. The sample size is large enough for normal approximation (np ≥ 10 and n(1-p) ≥ 10)\n4. No systematic bias in who responds to the poll\n\n\nImportant insights about polling:\n\nThe margin of error is largest when p = 0.5: This is why close races are hardest to call. When support is near 50%, we have maximum uncertainty about which side has the majority.\nSample size matters, but with diminishing returns: To halve the margin of error, you need to quadruple the sample size (since margin ∝ 1/√n).\nExtreme proportions have smaller margins: If only 1% support something, we can estimate that quite precisely even with modest sample sizes.\n“Statistical ties”: When the confidence interval includes 50%, we cannot conclude which side has majority support. This is often called a “statistical tie” in media reports.\nThe stated margin usually assumes 95% confidence: When polls report “±3 percentage points,” they typically mean the 95% confidence interval extends 3 points in each direction.\n\n\n\n\n\n\n\n\n\nAdvanced: Limitations of the Wald Interval\n\n\n\n\n\nThe confidence interval we derived above, known as the Wald interval, has poor coverage when p is near 0 or 1, or when n is small. In practice, consider using:\n\nAgresti-Coull interval: Add 2 successes and 2 failures before computing: \\tilde{p} = (S+2)/(n+4).\nWilson score interval: More complex but widely recommended as the default choice.\n\nThe Wald interval remains important for understanding the theory, but these alternatives perform better in practice.\n\n\n\n\n\n\n\n\n\nAdvanced: The Cramér-Rao Lower Bound\n\n\n\n\n\nWe mentioned that the MLE achieves the smallest possible variance. This is formalized by the Cramér-Rao Lower Bound:\nFor any unbiased estimator \\tilde{\\theta}: \\mathbb{V}(\\tilde{\\theta}) \\geq \\frac{1}{I_n(\\theta)}\nSince the MLE asymptotically achieves variance 1/I_n(\\theta), it is asymptotically efficient – no consistent estimator can do better!\nThis is why we call the MLE “optimal”: it extracts all possible information from the data about the parameter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#additional-topics",
    "href": "chapters/06-parametric-inference-II.html#additional-topics",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.6 Additional Topics",
    "text": "6.6 Additional Topics\n\n6.6.1 The Delta Method: Confidence Intervals for Transformations\nOften we’re not interested in the parameter \\theta itself, but in some transformation \\tau = g(\\theta). For example:\n\nIf \\theta is log-odds, we might want odds \\tau = e^\\theta\nIf \\theta is standard deviation, we might want variance \\tau = \\theta^2\nIf \\theta is a rate parameter, we might want mean lifetime \\tau = 1/\\theta\n\nDue to the equivariance property seen earlier, if we know the MLE \\hat{\\theta}, we know also that the MLE of \\tau is \\hat{\\tau} = g(\\hat{\\theta}). However, what about the confidence intervals of \\hat{\\tau}?\nThe Delta Method provides a way to find confidence intervals for transformed parameters.\n\nIf \\tau = g(\\theta) where g is differentiable and g'(\\theta) \\neq 0, then: \\widehat{\\text{se}}(\\hat{\\tau}_n) \\approx |g'(\\hat{\\theta}_n)| \\cdot \\widehat{\\text{se}}(\\hat{\\theta}_n)\nTherefore: \\frac{(\\hat{\\tau}_n - \\tau)}{\\widehat{\\text{se}}(\\hat{\\tau}_n)} \\rightsquigarrow \\mathcal{N}(0,1)\n\n\n\n\n\n\n\nIntuition Behind the Delta Method\n\n\n\nIf we zoom in enough, any smooth function looks linear. The Delta Method says that when we transform our estimate through a function g, the standard error gets multiplied by |g'(\\hat{\\theta}_n)| – the absolute slope of the function at our estimate.\nThis approximation becomes exact as n \\to \\infty because the variability of \\hat{\\theta}_n shrinks, effectively “zooming in” on the region where g is effectively linear.\n\n\n\n\n\n\n\n\nExample: Full Delta Method Workflow\n\n\n\nLet’s work through estimating \\psi = \\log \\sigma for a Normal distribution with known mean. This example demonstrates every step of the Delta Method process.\nSetup: X_1, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2) with \\mu known.\nStep 1: Find MLE for \\sigma\nThe log-likelihood is: \\ell(\\sigma) = -n \\log \\sigma - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i - \\mu)^2\nTaking the derivative and setting to zero: \\frac{d\\ell}{d\\sigma} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^n (X_i - \\mu)^2 = 0\nSolving: \\hat{\\sigma}_n = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2}\nStep 2: Find Fisher Information for \\sigma\nThe log density for a single observation is: \\log f(X; \\sigma) = -\\log \\sigma - \\frac{(X-\\mu)^2}{2\\sigma^2}\nFirst derivative: \\frac{\\partial \\log f(X; \\sigma)}{\\partial \\sigma} = -\\frac{1}{\\sigma} + \\frac{(X-\\mu)^2}{\\sigma^3}\nSecond derivative: \\frac{\\partial^2 \\log f(X; \\sigma)}{\\partial \\sigma^2} = \\frac{1}{\\sigma^2} - \\frac{3(X-\\mu)^2}{\\sigma^4}\nFisher Information: I(\\sigma) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\log f(X; \\sigma)}{\\partial \\sigma^2}\\right] = -\\frac{1}{\\sigma^2} + \\frac{3\\sigma^2}{\\sigma^4} = \\frac{2}{\\sigma^2}\nStep 3: Standard Error for \\hat{\\sigma} \\widehat{\\text{se}}(\\hat{\\sigma}_n) = \\frac{1}{\\sqrt{nI(\\hat{\\sigma}_n)}} = \\frac{\\hat{\\sigma}_n}{\\sqrt{2n}}\nStep 4: Apply the Delta Method\nFor \\psi = g(\\sigma) = \\log \\sigma, we have g'(\\sigma) = 1/\\sigma. Therefore:\n\\widehat{\\text{se}}(\\hat{\\psi}_n) = |g'(\\hat{\\sigma}_n)| \\cdot \\widehat{\\text{se}}(\\hat{\\sigma}_n) = \\frac{1}{\\hat{\\sigma}_n} \\cdot \\frac{\\hat{\\sigma}_n}{\\sqrt{2n}} = \\frac{1}{\\sqrt{2n}}\nStep 5: Confidence Interval\nA 95% confidence interval for \\psi = \\log \\sigma is: \\hat{\\psi}_n \\pm \\frac{2}{\\sqrt{2n}} = \\log \\hat{\\sigma}_n \\pm \\frac{2}{\\sqrt{2n}}\nNotice something remarkable: the standard error for \\log \\sigma doesn’t depend on \\sigma itself! This is one reason why log-transformations are often used for scale parameters.\nVerification Via Simulation\nLet’s verify the Delta Method through simulation. We’ll generate many samples, compute both \\hat{\\sigma} and \\log \\hat{\\sigma} for each, and check if their empirical standard errors match the theoretical predictions:\n\n\nShow code\n# Demonstrate the Delta Method with simulations\nnp.random.seed(42)\n\n# True parameters\nmu_true = 5.0\nsigma_true = 2.0\nn = 100\n\n# Generate data and compute MLEs\nn_sims = 1000\nsigma_mles = []\nlog_sigma_mles = []\n\nfor _ in range(n_sims):\n    data = np.random.normal(mu_true, sigma_true, n)\n    sigma_hat = np.sqrt(np.mean((data - mu_true)**2))\n    sigma_mles.append(sigma_hat)\n    log_sigma_mles.append(np.log(sigma_hat))\n\nsigma_mles = np.array(sigma_mles)\nlog_sigma_mles = np.array(log_sigma_mles)\n\n# Theoretical values\ntheoretical_se_sigma = sigma_true / np.sqrt(2*n)\ntheoretical_se_log_sigma = 1 / np.sqrt(2*n)\n\n# Create plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5))\n\n# Left: Distribution of sigma MLE\nax1.hist(sigma_mles, bins=30, density=True, alpha=0.7, color='blue')\nx = np.linspace(sigma_mles.min(), sigma_mles.max(), 100)\nax1.plot(x, stats.norm.pdf(x, sigma_true, theoretical_se_sigma), \n         'r-', linewidth=2, label='Theoretical')\nax1.axvline(sigma_true, color='black', linestyle='--', label='True value')\nax1.set_xlabel('$\\hat{\\sigma}$')\nax1.set_ylabel('Density')\nax1.set_title('Distribution of $\\hat{\\sigma}$')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right: Distribution of log(sigma) MLE\nax2.hist(log_sigma_mles, bins=30, density=True, alpha=0.7, color='green')\nx = np.linspace(log_sigma_mles.min(), log_sigma_mles.max(), 100)\nax2.plot(x, stats.norm.pdf(x, np.log(sigma_true), theoretical_se_log_sigma), \n         'r-', linewidth=2, label='Theoretical')\nax2.axvline(np.log(sigma_true), color='black', linestyle='--', label='True value')\nax2.set_xlabel('$\\hat{\\psi} = \\log \\hat{\\sigma}$')\nax2.set_ylabel('Density')\nax2.set_title('Distribution of $\\log \\hat{\\sigma}$')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Standard Error Comparison:\")\nprint(f\"Empirical SE($\\hat{{\\sigma}}$): {np.std(sigma_mles):.4f}\")\nprint(f\"Theoretical SE($\\hat{{\\sigma}}$): {theoretical_se_sigma:.4f}\")\nprint(f\"Empirical SE($\\log \\hat{{\\sigma}}$): {np.std(log_sigma_mles):.4f}\")\nprint(f\"Theoretical SE($\\log \\hat{{\\sigma}}$): {theoretical_se_log_sigma:.4f}\")\nprint(f\"\\nSE ratio (empirical): {np.std(log_sigma_mles)/np.std(sigma_mles):.4f}\")\nprint(f\"SE ratio (Delta method): {1/sigma_true:.4f}\")\n\n\n\n\n\n\n\n\n\nStandard Error Comparison:\nEmpirical SE($\\hat{\\sigma}$): 0.1403\nTheoretical SE($\\hat{\\sigma}$): 0.1414\nEmpirical SE($\\log \\hat{\\sigma}$): 0.0705\nTheoretical SE($\\log \\hat{\\sigma}$): 0.0707\n\nSE ratio (empirical): 0.5027\nSE ratio (Delta method): 0.5000\n\n\nKey takeaways: The simulation confirms that the Delta Method works perfectly! The empirical standard error ratio between \\log \\hat{\\sigma} and \\hat{\\sigma} matches exactly what the Delta Method predicts: 1/\\sigma. Both distributions are well-approximated by normal distributions, validating the asymptotic normality of MLEs.\n\n\n\n\n6.6.2 Multiparameter Models\nReal-world problems often involve multiple parameters. The theory we saw in this chapter extends naturally, with matrices replacing scalars.\nFor \\theta = (\\theta_1, \\ldots, \\theta_k), let \\hat{\\theta} = (\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_k) be the MLE. The key concepts become:\n\nLet \\ell_n(\\theta) = \\sum_{i=1}^n \\log f(X_i; \\theta) and define: H_{jk} = \\frac{\\partial^2 \\ell_n}{\\partial \\theta_j \\partial \\theta_k}\nThe Fisher Information Matrix is: I_n(\\theta) = -\n\\begin{pmatrix}\n  \\mathbb{E}_\\theta(H_{11}) & \\mathbb{E}_\\theta(H_{12}) & \\cdots & \\mathbb{E}_\\theta(H_{1k}) \\\\\n  \\mathbb{E}_\\theta(H_{21}) & \\mathbb{E}_\\theta(H_{22}) & \\cdots & \\mathbb{E}_\\theta(H_{2k}) \\\\\n    \\vdots           &    \\vdots           & \\ddots & \\vdots \\\\\n  \\mathbb{E}_\\theta(H_{k1}) & \\mathbb{E}_\\theta(H_{k2}) & \\cdots & \\mathbb{E}_\\theta(H_{kk})\n\\end{pmatrix}\n\n\nUnder regularity conditions:\n\nThe MLE is approximately multivariate normal: (\\hat{\\theta} - \\theta) \\approx \\mathcal{N}(0, J_n) where J_n = I_n^{-1}(\\theta) is the inverse Fisher Information matrix.\nFor parameter \\theta_j, the standard error is: \\widehat{\\text{se}}_j = \\sqrt{J_n(j,j)} (the square root of the j-th diagonal element of J_n)\nThe covariance between \\hat{\\theta}_j and \\hat{\\theta}_k is J_n(j,k).\n\n\n\n\n\n\n\n\nExample: Multiparameter Normal Distribution\n\n\n\nFor X_1, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2) with both parameters unknown:\nThe Fisher Information matrix is: I_n(\\mu, \\sigma) = n \\begin{pmatrix}\n\\frac{1}{\\sigma^2} & 0 \\\\\n0 & \\frac{2}{\\sigma^2}\n\\end{pmatrix}\nTherefore: J_n = \\frac{1}{n} \\begin{pmatrix}\n\\sigma^2 & 0 \\\\\n0 & \\frac{\\sigma^2}{2}\n\\end{pmatrix}\nThis tells us:\n\n\\widehat{\\text{se}}(\\hat{\\mu}) = \\sigma/\\sqrt{n} (familiar formula!)\n\\widehat{\\text{se}}(\\hat{\\sigma}) = \\sigma/\\sqrt{2n}\n\\text{Cov}(\\hat{\\mu}, \\hat{\\sigma}) = 0 (they’re independent!)\n\nThe zero off-diagonal terms mean that uncertainty about \\mu doesn’t affect our estimate of \\sigma, and vice versa. This orthogonality is special to the normal distribution.\n\n\n\n\n\n\n\n\nAdvanced: Multiparameter Delta Method\n\n\n\n\n\nFor a function \\tau = g(\\theta_1, \\ldots, \\theta_k), the Delta Method generalizes to:\n\\widehat{\\text{se}}(\\hat{\\tau}) = \\sqrt{(\\widehat{\\nabla} g)^T \\widehat{J}_n (\\widehat{\\nabla} g)}\nwhere \\widehat{\\nabla} g is the gradient of g evaluated at \\hat{\\theta}.\nExample: For the Normal distribution, if we’re interested in the coefficient of variation \\tau = \\sigma/\\mu:\n\\nabla g = \\begin{pmatrix} -\\sigma/\\mu^2 \\\\ 1/\\mu \\end{pmatrix}\nThe standard error involves both parameter uncertainties and their covariance (though the covariance is zero for the normal case).\n\n\n\n\n\n6.6.3 Sufficient Statistics\nA statistic is a function T(X^n) of the data. A sufficient statistic is a statistic that contains all the information in the data about the parameter.\n\nA statistic T(X^n) is sufficient for parameter \\theta if the conditional distribution of the data given T doesn’t depend on \\theta.\n\n\nLet X_1, \\ldots, X_n \\sim \\text{Bernoulli}(p). Then \\mathcal{L}(p) = p^S(1-p)^{n-S} where S = \\sum_{i=1}^n X_i.\nThe likelihood depends on the data only through S, so S is sufficient. This means that once we know the total number of successes, the individual outcomes provide no additional information about p.\n\nA sufficient statistic is minimal if it provides the most compressed summary possible while still retaining all information about the parameter. More precisely: any other sufficient statistic can be “reduced” to the minimal one, but not vice versa.\n\n\n\n\n\n\nConnection Between MLE and Sufficiency\n\n\n\nWhen a non-trivial sufficient statistic exists, the MLE depends on the data only through that statistic. Examples:\n\nBernoulli: MLE \\hat{p} = S/n depends only on sufficient statistic S = \\sum X_i\nNormal: MLE (\\hat{\\mu}, \\hat{\\sigma}^2) depends only on sufficient statistics (\\bar{X}, S^2)\nUniform(0,\\theta): MLE is exactly the sufficient statistic \\max\\{X_i\\}\n\nThis provides theoretical justification for data reduction: when sufficient statistics exist, we can compress our data without losing any information about the parameter. However, not all models have nice sufficient statistics beyond the trivial one (the entire dataset).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#connection-to-machine-learning-cross-entropy-as-mle",
    "href": "chapters/06-parametric-inference-II.html#connection-to-machine-learning-cross-entropy-as-mle",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.7 Connection to Machine Learning: Cross-Entropy as MLE",
    "text": "6.7 Connection to Machine Learning: Cross-Entropy as MLE\nOne of the most profound connections between statistics and modern machine learning is that many “loss functions” used in ML are actually negative log-likelihoods in disguise. This isn’t a coincidence – it reflects the deep statistical foundations of machine learning. Let’s consider the case of the cross-entropy loss used in classification tasks.\nConsider a classification problem where we predict probabilities for K classes using a machine learning model f(X; \\theta) parameterized by \\theta.\nThe cross-entropy loss is:\nH(p, q) = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^K p(Y_i = j) \\log q_\\theta(Y_i = j)\nwhere:\n\np(Y_i = j) is the observed distribution (1 if observation i is class j, 0 otherwise)\nq_\\theta(Y_i = j) = f(X_i; \\theta)_j is the predicted probability from our model with parameters \\theta\n\nLet’s show this is exactly the negative log-likelihood for a categorical distribution.\nThe likelihood for categorical data is: \\mathcal{L}_n(\\theta) = \\prod_{i=1}^n \\text{Categorical}(Y_i; f(X_i; \\theta))\nThe categorical probability for observation i is: \\text{Categorical}(Y_i; f(X_i; \\theta)) = \\prod_{j=1}^K f(X_i; \\theta)_j^{p(Y_i = j)}\nTaking the log: \\ell_n(\\theta) = \\sum_{i=1}^n \\sum_{j=1}^K p(Y_i = j) \\log f(X_i; \\theta)_j\nThis is exactly -n \\cdot H(p, q). Thus: Minimizing cross-entropy = Maximizing likelihood!\n\n\n\n\n\n\nOther Common ML Loss Functions as MLEs\n\n\n\nThis pattern appears throughout machine learning:\n\n\n\n\n\n\n\n\nML Loss Function\nStatistical Model\nMLE Interpretation\n\n\n\n\nMean Squared Error (MSE)\nY \\sim \\mathcal{N}(\\mu(x), \\sigma^2)\nGaussian likelihood\n\n\nMean Absolute Error (MAE)\nY \\sim \\text{Laplace}(\\mu(x), b)\nLaplace likelihood\n\n\nHuber Loss\nHybrid of L_1 and L_2\nRobust to outliers\n\n\n\n\n\nUnderstanding that common ML losses are negative log-likelihoods provides:\n\nPrincipled loss design: When facing a new problem, you can derive an appropriate loss function by specifying a probabilistic model for your data.\nHistorical context: It explains why these particular loss functions became standard – they weren’t arbitrary choices but emerged from statistical principles.\nIntuition: Knowing the probabilistic interpretation helps understand when each loss is appropriate (e.g., MAE for heavy-tailed errors, MSE for Gaussian noise).\n\n\n\n\n\n\n\nExample: Deriving a Custom Loss\n\n\n\nSuppose you believe errors in your regression problem approximately follow a Student-t distribution (heavy tails for robustness). The negative log-likelihood gives you the loss function:\n\\text{Loss}(y, \\hat{y}) = \\frac{\\nu + 1}{2} \\log\\left(1 + \\frac{(y - \\hat{y})^2}{\\nu s^2}\\right)\nwhere \\nu is degrees of freedom and s is scale. This naturally down-weights outliers!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#mle-for-latent-variable-models-the-em-algorithm",
    "href": "chapters/06-parametric-inference-II.html#mle-for-latent-variable-models-the-em-algorithm",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.8 MLE for Latent Variable Models: The EM Algorithm",
    "text": "6.8 MLE for Latent Variable Models: The EM Algorithm\n\n6.8.1 The Challenge of Latent Variables\nSo far, we’ve assumed that maximizing the likelihood is straightforward – we take derivatives, set them to zero, and solve (possibly numerically). But what if the likelihood itself is intractable?\nThis often happens when our model involves latent (unobserved) variables – hidden quantities that would make the problem easy if only we could observe them.\nCommon examples with latent variables:\n\nMixture models: Which component generated each observation?\nHidden Markov models: What’s the hidden state sequence?\nFactor analysis: What are the values of the latent factors?\nMissing data: What would the missing values have been?\n\nThe canonical example is the Gaussian Mixture Model (GMM):3\nf(y; \\theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(y; \\mu_k, \\sigma_k^2)\nwhere \\theta = (\\pi_1, \\ldots, \\pi_K, \\mu_1, \\ldots, \\mu_K, \\sigma_1, \\ldots, \\sigma_K).\nThe likelihood involves a sum inside the logarithm: \\ell_n(\\theta) = \\sum_{i=1}^n \\log\\left[\\sum_{k=1}^K \\pi_k \\mathcal{N}(y_i; \\mu_k, \\sigma_k^2)\\right]\nThis is a nightmare to differentiate! The derivatives involve ratios of sums – messy and hard to work with.\nThe key insight: If we knew which component Z_i \\in \\{1, \\ldots, K\\} generated each observation Y_i, the problem would become trivial – we’d just fit separate Gaussians to each group.\n\n\n\n\n\n\nExample: Two-Component Gaussian Mixture (K=2)\n\n\n\n\n\nFor the special case of K=2 (two components), the mixture model simplifies to:\nf(y; \\theta) = \\pi \\mathcal{N}(y; \\mu_1, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(y; \\mu_2, \\sigma_2^2)\nwhere:\n\n\\pi \\in [0,1] is the mixing weight (probability of component 1)\n(1-\\pi) is the probability of component 2\n\\mu_1, \\mu_2 are the means of the two Gaussian components\n\\sigma_1^2, \\sigma_2^2 are the variances of the two components\nThe parameter vector is \\theta = (\\pi, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\n\nInterpretation: Each observation Y_i is generated by:\n\nFirst, flip a biased coin with probability \\pi of heads\nIf heads: draw Y_i \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\nIf tails: draw Y_i \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\n\nThe latent variables Z_1, \\ldots, Z_n \\in \\{1, 2\\} indicate which component generated each observation. If we knew these Z_i values, estimation would be trivial – we’d simply:\n\nEstimate \\pi as the proportion of observations from component 1\nEstimate \\mu_1, \\sigma_1 using only observations where Z_i = 1\nEstimate \\mu_2, \\sigma_2 using only observations where Z_i = 2\n\nBut since the Z_i are unobserved, we need a method to estimate both the component assignments and the parameters.\n\n\n\n\n\n6.8.2 The Expectation-Maximization (EM) Algorithm\nThe EM algorithm is a clever iterative procedure that alternates between:\n\nGuessing the values of the latent variables (E-step)\nUpdating parameters as if those guesses were correct (M-step)\n\nIntuitiveMathematicalComputationalThink of the EM algorithm as a “chicken and egg” problem solver for\nmixture models.The dilemma:\nIf we knew which cluster each point belonged to, we could easily\nestimate the cluster parameters (just compute means and variances for\neach group)\nIf we knew the cluster parameters, we could easily assign points to\nclusters (pick the most likely cluster for each point)\nThe EM solution: Start with a guess and\nalternate!\nE-step (Expectation): Given current cluster\nparameters, compute “soft assignments” – the probability that each point\nbelongs to each cluster. These are called “responsibilities.”\n\nPoint near cluster 1 center: Maybe 90% probability for cluster 1,\n10% for cluster 2\nPoint between clusters: Maybe 50-50 split\n\nM-step (Maximization): Update cluster parameters\nusing weighted calculations, where weights are the responsibilities.\n\nNew mean for cluster 1: Weighted average of all points, using their\ncluster 1 responsibilities as weights\nPoints with high responsibility contribute more to the cluster’s\nparameters\n\nIterate: Keep alternating until convergence.\nIt’s like a dance where clusters and assignments gradually find their\nproper configuration, each step making the other more accurate.The EM algorithm maximizes the expected complete-data log-likelihood.\nLet:\n\\(Y\\) = observed data\n\\(Z\\) = latent/missing data\n\\((Y, Z)\\) = complete data\nThe algorithm iterates between:E-step: Compute the expected complete-data\nlog-likelihood:\n\\[Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{Z|Y,\\theta^{(t)}}[\\log P(Y, Z | \\theta)]\\]This expectation is over the distribution of\n\\(Z\\) given the observed data\n\\(Y\\) and current parameters\n\\(\\theta^{(t)}\\).M-step: Maximize with respect to\n\\(\\theta\\):\n\\[\\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta | \\theta^{(t)})\\]Key property: The likelihood is guaranteed to\nincrease (or stay the same) at each iteration:\n\\[\\mathcal{L}(\\theta^{(t+1)}) \\geq \\mathcal{L}(\\theta^{(t)})\\]This monotonic improvement ensures convergence to a local\nmaximum.Let’s implement and visualize the EM algorithm for a 2D Gaussian\nmixture:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nfrom scipy.stats import multivariate_normal\n\n# Generate synthetic data from a mixture of 3 Gaussians\nnp.random.seed(42)\nn_samples = 300\nn_components = 3\n\n# True parameters\ntrue_means = np.array([[-2, -2], [0, 2], [3, -1]])\ntrue_covs = [np.array([[0.5, 0.2], [0.2, 0.5]]),\n             np.array([[0.8, -0.3], [-0.3, 0.8]]),\n             np.array([[0.6, 0], [0, 0.3]])]\ntrue_weights = np.array([0.3, 0.5, 0.2])\n\n# Generate data\ndata = []\ntrue_labels = []\nfor i in range(n_samples):\n    # Choose component\n    k = np.random.choice(n_components, p=true_weights)\n    true_labels.append(k)\n    # Generate point from that component\n    data.append(np.random.multivariate_normal(true_means[k], true_covs[k]))\ndata = np.array(data)\ntrue_labels = np.array(true_labels)\n\n# Initialize EM with random parameters\nmeans = np.random.randn(n_components, 2) * 2\ncovs = [np.eye(2) for _ in range(n_components)]\nweights = np.ones(n_components) / n_components\n\n# Function to plot current state\ndef plot_em_state(data, means, covs, weights, responsibilities, iteration, log_likelihood_history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5))\n    \n    # Left plot: data colored by responsibilities\n    colors = responsibilities @ np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    ax1.scatter(data[:, 0], data[:, 1], c=colors, alpha=0.6, s=30)\n    \n    # Plot component centers and ellipses\n    for k in range(n_components):\n        ax1.plot(means[k, 0], means[k, 1], 'k*', markersize=15)\n        \n        # Draw ellipse representing covariance\n        eigenvalues, eigenvectors = np.linalg.eigh(covs[k])\n        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n        width, height = 2 * np.sqrt(eigenvalues)\n        ellipse = Ellipse(means[k], width, height, angle=angle,\n                         facecolor='none', edgecolor='black', linewidth=2)\n        ax1.add_patch(ellipse)\n    \n    ax1.set_title(f'Iteration {iteration}: Soft Assignments')\n    ax1.set_xlim(-5, 5)\n    ax1.set_ylim(-5, 5)\n    ax1.grid(True, alpha=0.3)\n    \n    # Right plot: log-likelihood history\n    ax2.plot(range(len(log_likelihood_history)), log_likelihood_history, 'b-', linewidth=2)\n    ax2.plot(iteration, log_likelihood_history[iteration], 'ro', markersize=8)  # Mark current iteration\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Log-likelihood')\n    ax2.set_title('Convergence')\n    ax2.set_xlim(-0.5, 9.5)  # Always show full range\n    ax2.set_xticks(range(10))  # Show only integer ticks 0-9\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    return fig\n\n# EM iterations\nn_iterations = 10\nlog_likelihood_history = []\n\nfor iteration in range(n_iterations):\n    # E-step: compute responsibilities\n    responsibilities = np.zeros((n_samples, n_components))\n    for k in range(n_components):\n        responsibilities[:, k] = weights[k] * multivariate_normal.pdf(\n            data, means[k], covs[k])\n    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n    \n    # Compute log-likelihood\n    log_likelihood = np.sum(np.log(np.sum([\n        weights[k] * multivariate_normal.pdf(data, means[k], covs[k])\n        for k in range(n_components)], axis=0)))\n    log_likelihood_history.append(log_likelihood)\n    \n    # Plot current state (only show a few iterations)\n    if iteration in [0, 1, 3, 9]:\n        fig = plot_em_state(data, means, covs, weights, responsibilities, iteration, log_likelihood_history)\n        plt.show()\n    \n    # M-step: update parameters\n    for k in range(n_components):\n        resp_k = responsibilities[:, k]\n        # Update weight\n        weights[k] = resp_k.sum() / n_samples\n        # Update mean\n        means[k] = (resp_k[:, np.newaxis] * data).sum(axis=0) / resp_k.sum()\n        # Update covariance\n        diff = data - means[k]\n        covs[k] = (resp_k[:, np.newaxis, np.newaxis] * \n                   diff[:, :, np.newaxis] @ diff[:, np.newaxis, :]).sum(axis=0) / resp_k.sum()\n\nprint(\"Final parameters:\")\nprint(f\"Weights: {weights}\")\nprint(f\"Means:\\n{means}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal parameters:\nWeights: [0.45669616 0.211968   0.33133584]\nMeans:\n[[ 0.19903644  1.95778334]\n [ 3.02378239 -0.94719336]\n [-2.06041818 -1.92642337]]\n\nThe visualizations show:\nLeft plots: Data points colored by their soft\nassignments (RGB = probabilities for 3 clusters)\nBlack stars: Cluster centers\nEllipses: Covariance structure of each\ncomponent\nRight plot: Log-likelihood increasing\nmonotonically\nNotice how the algorithm gradually discovers the true cluster\nstructure!\n\n\n6.8.3 Properties and Practical Considerations\nStrengths of EM:\n\nGuaranteed improvement: The likelihood never decreases\nNo step size tuning: Unlike gradient descent, no learning rate to choose\nNaturally handles constraints: Probabilities automatically sum to 1\nInterpretable intermediate results: Soft assignments have meaning\n\nLimitations and solutions:\n\nLocal optima: EM only finds a local maximum\n\nSolution: Run from multiple random initializations\nSmart initialization: Use k-means++ or similar\n\nSlow convergence: Can take many iterations near the optimum\n\nSolution: Switch to Newton’s method near convergence\nEarly stopping: Monitor log-likelihood changes\n\nChoosing number of components: How many clusters?\n\nSolution: Use information criteria (AIC, BIC)\nCross-validation: Evaluate on held-out data\n\n\n\n\n\n\n\n\nThe Initialization Trap\n\n\n\nThe EM algorithm is extremely sensitive to initialization. Poor starting values can lead to:\n\nConvergence to inferior local optima\nOne component “eating” all the data\nEmpty components (singularities)\n\nAlways run EM multiple times with different initializations and choose the result with the highest likelihood!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#chapter-summary-and-connections",
    "href": "chapters/06-parametric-inference-II.html#chapter-summary-and-connections",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "6.9 Chapter Summary and Connections",
    "text": "6.9 Chapter Summary and Connections\n\n6.9.1 Key Concepts Review\nWe’ve explored the remarkable properties that make the Maximum Likelihood Estimator the “gold standard” of parametric estimation:\nCore Properties of the MLE:\n\nConsistency: \\hat{\\theta}_n \\xrightarrow{P} \\theta_* – the MLE converges to the truth as n \\to \\infty.\nEquivariance: \\widehat{g(\\theta)} = g(\\hat{\\theta}_n) – reparameterization doesn’t affect the MLE.\nAsymptotic Normality: \\hat{\\theta}_n \\approx \\mathcal{N}(\\theta, 1/I_n(\\theta)) – enabling confidence intervals.\nAsymptotic Efficiency: Achieves the Cramér-Rao lower bound – optimal variance.\n\nFisher Information:\n\nMeasures the “information” about a parameter contained in data.\nEquals the expected curvature of the log-likelihood: I(\\theta) = -\\mathbb{E}[\\frac{\\partial^2 \\log f(X;\\theta)}{\\partial \\theta^2}].\nDetermines the precision of the MLE: \\text{Var}(\\hat{\\theta}_n) \\approx 1/(nI(\\theta)).\nSharp likelihood peak → High information → Precise estimates.\n\nDeep Connections:\n\nCross-entropy loss in ML = Negative log-likelihood.\nMany ML algorithms are secretly performing MLE.\n\nPractical Tools:\n\nConfidence Intervals: \\hat{\\theta}_n \\pm z_{\\alpha/2} \\cdot \\widehat{\\text{se}} where \\widehat{\\text{se}} = 1/\\sqrt{I_n(\\hat{\\theta}_n)}.\nDelta Method: For transformed parameters \\tau = g(\\theta): \\widehat{\\text{se}}(\\hat{\\tau}) \\approx |g'(\\hat{\\theta})| \\cdot \\widehat{\\text{se}}(\\hat{\\theta}).\nEM Algorithm: Iterative method for MLEs with latent variables – alternates between E-step (soft assignments) and M-step (parameter updates).\n\n\n\n6.9.2 The Big Picture\nWe have moved from simply finding estimators (Chapter 5) to evaluating their quality. This chapter revealed why the MLE is so widely used:\n\nIt works: Consistency ensures we get the right answer with enough data.\nIt’s optimal: No other estimator has smaller asymptotic variance.\nIt’s practical: To a degree, we can quantify uncertainty through Fisher Information.4\nIt’s flexible: Equivariance means we can work in convenient parameterizations.\nIt extends: The EM algorithm handles complex models with latent structure.\n\n\n\n6.9.3 Common Pitfalls to Avoid\n\nMisinterpreting Confidence Intervals: A 95% CI does NOT mean:\n\n“95% probability the parameter is in this interval” (that’s Bayesian!).\n“95% of the data falls in this interval” (that’s a prediction interval).\nCorrect: “95% of such intervals constructed from repeated samples would contain the true parameter”.\n\nAssuming Asymptotic Results for Small Samples:\n\nAsymptotic normality may not hold for small n.\nFisher Information formulas are approximate for finite samples.\nBootstrap or exact methods may be preferable when n &lt; 30.5\n\nGetting Stuck in Local Optima with EM:\n\nEM only guarantees local, not global, maximum.\nAlways run from multiple starting points.\nMonitor log-likelihood to ensure proper convergence.\n\nForgetting About Model Assumptions:\n\nMLE theory assumes the model is correctly specified (i.e., the true distribution belongs to our model family).\nWe briefly mentioned that parameters must be identifiable (Section on Consistency) – different parameter values must give different distributions.\nThe asymptotic results (normality, efficiency) require “regularity conditions” that we haven’t detailed but include smoothness of the likelihood.\n\nOver-interpreting Asymptotic Results:\n\nAll our results (asymptotic normality, efficiency) are for large samples.\nWith small samples, the MLE might not be normally distributed and confidence intervals may be inaccurate.\nThe MLE’s optimality is theoretical – in practice, we might prefer more robust methods when data contains outliers or model assumptions are questionable.\n\n\n\n\n6.9.4 Chapter Connections\nBuilding on Previous Chapters:\n\nChapter 3 (Statistical Inference): Provided the framework for evaluating estimators. Now we have seen that MLEs have optimal properties within this framework.\nChapter 4 (Bootstrap): Offers a computational alternative to the analytical standard errors we derived here. When Fisher Information is hard to compute, bootstrap it!\nChapter 5 (Finding Estimators): Showed how to find MLEs. This chapter justifies why we bother – they have provably good properties.\n\nLooking Ahead:\n\nBayesian Inference: While MLE obtains point estimates and confidence intervals, Bayesian methods provide full probability distributions over parameters. The MLE appears as the mode of the posterior with uniform priors.\nRegression Models: The theory in this chapter extends directly – least squares is MLE for normal errors, logistic regression is MLE for binary outcomes.\nModel Selection: Information criteria (AIC, BIC) build on the likelihood framework we’ve developed.\nRobust Statistics: When MLE assumptions fail, we need methods that sacrifice some efficiency for robustness.\n\nThe properties we’ve studied – consistency, asymptotic normality, efficiency – will reappear throughout statistics. Whether you’re fitting neural networks or analyzing clinical trials, you’re likely using some form of MLE, and the theory in this chapter explains why it works.\n\n\n6.9.5 Self-Test Problems\n\nFisher Information: For X_1, \\ldots, X_n \\sim \\text{Exponential}(\\lambda) with PDF f(x; \\lambda) = \\lambda e^{-\\lambda x} for x &gt; 0:\n\nFind the Fisher Information I(\\lambda) for a single observation.\nWhat is the approximate standard error of the MLE \\hat{\\lambda}_n?\n\nConfidence Intervals: You flip a coin 100 times and observe 58 heads. Use Fisher Information to construct an approximate 95% confidence interval for the probability of heads.\nDelta Method: If \\hat{p} = 0.4 with standard error 0.05, find the standard error of \\hat{\\tau} = \\log(p/(1-p)) using the Delta Method.\nEM Intuition: In a two-component Gaussian mixture, the E-step computes “responsibilities” for each data point. What do these represent? Why can’t we just assign each point to its most likely cluster?\nMLE Properties: True or False (with brief explanation):\n\nIf \\hat{\\theta}_n is the MLE of \\theta, then \\hat{\\theta}_n^2 is the MLE of \\theta^2.\nThe MLE is always unbiased.\nFisher Information measures how “peaked” the likelihood function is.\n\n\n\n\n6.9.6 Python and R Reference\nPythonR#| eval: false\nimport numpy as np\nimport scipy.stats as stats\nimport scipy.optimize as optimize\nfrom scipy.stats import multivariate_normal\nimport statsmodels.api as sm\nimport sympy as sp\nfrom sklearn.mixture import GaussianMixture\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy.stats as jstats\n\n# Fisher Information calculation (symbolic)\ndef fisher_info_symbolic():\n    \"\"\"Example: Fisher Information for Exponential distribution\"\"\"\n    # Define symbols\n    x, lam = sp.symbols('x lambda', positive=True)\n    \n    # Define log pdf\n    log_pdf = sp.log(lam) - lam * x\n    \n    # Score function (first derivative)\n    score = sp.diff(log_pdf, lam)\n    \n    # Second derivative\n    second_deriv = sp.diff(score, lam)\n    \n    # Fisher Information (negative expectation)\n    # For exponential: E[X] = 1/lambda\n    fisher_info = -second_deriv.subs(x, 1/lam)\n    return sp.simplify(fisher_info)\n\n# Fisher Information using JAX (automatic differentiation)\ndef fisher_info_jax(log_pdf, theta, n_samples=10000):\n    \"\"\"Estimate Fisher Information using automatic differentiation\"\"\"\n    # Get the Hessian (second derivatives)\n    hessian = jax.hessian(log_pdf)\n    \n    # Generate samples and compute expectation\n    key = jax.random.PRNGKey(42)\n    # This is problem-specific - would need appropriate sampler\n    # Example continues with general structure\n    \n    return -jnp.mean(hessian(theta))\n\n# Confidence intervals with Fisher Information\ndef mle_confidence_interval(mle, fisher_info_n, alpha=0.05):\n    \"\"\"Construct CI using Fisher Information\"\"\"\n    se = 1 / np.sqrt(fisher_info_n)\n    z_crit = stats.norm.ppf(1 - alpha/2)\n    ci_lower = mle - z_crit * se\n    ci_upper = mle + z_crit * se\n    return ci_lower, ci_upper\n\n# Delta Method implementation\ndef delta_method_se(mle, se_mle, g_derivative):\n    \"\"\"\n    Apply Delta Method to find SE of transformed parameter\n    \n    Parameters:\n    - mle: MLE of original parameter\n    - se_mle: Standard error of MLE\n    - g_derivative: Derivative of transformation g evaluated at MLE\n    \"\"\"\n    return np.abs(g_derivative) * se_mle\n\n# Example: Poisson rate to mean waiting time\nlam_hat = 3.0  # MLE for rate\nse_lam = 0.3   # Standard error\n# For tau = 1/lambda, g'(lambda) = -1/lambda^2\ng_prime = -1 / lam_hat**2\nse_tau = delta_method_se(lam_hat, se_lam, g_prime)\n\n# EM Algorithm using sklearn\ndef fit_gaussian_mixture(data, n_components=2, n_init=10):\n    \"\"\"Fit Gaussian Mixture Model using EM\"\"\"\n    gmm = GaussianMixture(\n        n_components=n_components,\n        n_init=n_init,  # Number of initializations\n        init_params='k-means++',  # Smart initialization\n        max_iter=100,\n        tol=1e-3,\n        verbose=0\n    )\n    gmm.fit(data)\n    \n    return {\n        'means': gmm.means_,\n        'covariances': gmm.covariances_,\n        'weights': gmm.weights_,\n        'log_likelihood': gmm.score(data) * len(data),\n        'converged': gmm.converged_\n    }\n\n# Advanced models with confidence intervals\ndef fit_model_with_cis(model, data):\n    \"\"\"Example using statsmodels for automatic CIs\"\"\"\n    # Example: Poisson regression\n    results = model.fit()\n    \n    # Extract MLEs and standard errors\n    params = results.params\n    se = results.bse\n    conf_int = results.conf_int(alpha=0.05)\n    \n    # Fisher Information matrix\n    fisher_info = results.cov_params()\n    \n    return {\n        'mle': params,\n        'se': se,\n        'confidence_intervals': conf_int,\n        'fisher_info_inverse': fisher_info\n    }\n\n# Manual EM implementation for mixture of normals\nclass GaussianMixtureEM:\n    def __init__(self, n_components=2):\n        self.n_components = n_components\n        \n    def e_step(self, X, means, covs, weights):\n        \"\"\"Compute responsibilities (soft assignments)\"\"\"\n        n_samples = X.shape[0]\n        resp = np.zeros((n_samples, self.n_components))\n        \n        for k in range(self.n_components):\n            resp[:, k] = weights[k] * multivariate_normal.pdf(\n                X, means[k], covs[k]\n            )\n        \n        # Normalize\n        resp /= resp.sum(axis=1, keepdims=True)\n        return resp\n    \n    def m_step(self, X, resp):\n        \"\"\"Update parameters given responsibilities\"\"\"\n        n_samples = X.shape[0]\n        \n        # Update weights\n        weights = resp.sum(axis=0) / n_samples\n        \n        # Update means\n        means = []\n        covs = []\n        for k in range(self.n_components):\n            resp_k = resp[:, k]\n            mean_k = (resp_k[:, np.newaxis] * X).sum(axis=0) / resp_k.sum()\n            means.append(mean_k)\n            \n            # Update covariances\n            diff = X - mean_k\n            cov_k = (resp_k[:, np.newaxis, np.newaxis] * \n                    diff[:, :, np.newaxis] @ diff[:, np.newaxis, :]).sum(axis=0)\n            cov_k /= resp_k.sum()\n            covs.append(cov_k)\n            \n        return np.array(means), covs, weights#| eval: false\nlibrary(MASS)      # For fitdistr\nlibrary(stats4)    # For mle function\nlibrary(numDeriv)  # For numerical derivatives\nlibrary(mclust)    # For Gaussian mixture models\nlibrary(msm)       # For deltamethod function\n\n# Fisher Information calculation\nfisher_info_exponential &lt;- function(lambda) {\n  # For Exponential(lambda): I(lambda) = 1/lambda^2\n  return(1 / lambda^2)\n}\n\n# Fisher Information via numerical methods\nfisher_info_numerical &lt;- function(loglik_fn, theta, ...) {\n  # Compute negative expected Hessian\n  hess &lt;- hessian(loglik_fn, theta, ...)\n  return(-hess)\n}\n\n# MLE with standard errors using fitdistr\nfit_with_se &lt;- function(data, distribution) {\n  fit &lt;- fitdistr(data, distribution)\n  \n  # Extract estimates and standard errors\n  mle &lt;- fit$estimate\n  se &lt;- fit$sd\n  \n  # Construct confidence intervals\n  ci_lower &lt;- mle - 1.96 * se\n  ci_upper &lt;- mle + 1.96 * se\n  \n  list(\n    mle = mle,\n    se = se,\n    ci = cbind(lower = ci_lower, upper = ci_upper),\n    loglik = fit$loglik\n  )\n}\n\n# Delta Method implementation\napply_delta_method &lt;- function(mle, var_mle, g_expr, param_name) {\n  # Using msm::deltamethod\n  # g_expr: expression for g(theta) as a string\n  # param_name: name of parameter in expression\n  \n  se_transformed &lt;- deltamethod(\n    as.formula(paste(\"~\", g_expr)), \n    mle, \n    var_mle\n  )\n  \n  return(se_transformed)\n}\n\n# Example: Delta method for Poisson\nlambda_hat &lt;- 3.0\nse_lambda &lt;- 0.3\nvar_lambda &lt;- se_lambda^2\n\n# For tau = 1/lambda\nse_tau &lt;- deltamethod(~ 1/x1, lambda_hat, var_lambda)\n\n# Manual Delta Method\ndelta_method_manual &lt;- function(mle, se_mle, g_deriv) {\n  # g_deriv: derivative of g evaluated at MLE\n  se_transformed &lt;- abs(g_deriv) * se_mle\n  return(se_transformed)\n}\n\n# EM Algorithm using mclust\nfit_gaussian_mixture &lt;- function(data, G = 2) {\n  # Fit Gaussian mixture model\n  fit &lt;- Mclust(data, G = G)\n  \n  list(\n    means = fit$parameters$mean,\n    covariances = fit$parameters$variance$sigma,\n    weights = fit$parameters$pro,\n    log_likelihood = fit$loglik,\n    classification = fit$classification,\n    uncertainty = fit$uncertainty\n  )\n}\n\n# Manual implementation of MLE with custom likelihood\nmle_custom &lt;- function(data, start, nll_function) {\n  # Negative log-likelihood function\n  nll &lt;- function(params) {\n    nll_function(params, data)\n  }\n  \n  # Optimize\n  fit &lt;- optim(\n    par = start,\n    fn = nll,\n    method = \"L-BFGS-B\",\n    hessian = TRUE  # Get Hessian for Fisher Information\n  )\n  \n  # Fisher Information is the Hessian at MLE\n  fisher_info &lt;- fit$hessian\n  \n  # Standard errors from inverse Fisher Information\n  se &lt;- sqrt(diag(solve(fisher_info)))\n  \n  list(\n    mle = fit$par,\n    se = se,\n    fisher_info = fisher_info,\n    convergence = fit$convergence,\n    log_likelihood = -fit$value\n  )\n}\n\n# Example: Beta distribution MLE\nbeta_nll &lt;- function(params, data) {\n  alpha &lt;- params[1]\n  beta &lt;- params[2]\n  if(alpha &lt;= 0 || beta &lt;= 0) return(Inf)\n  -sum(dbeta(data, alpha, beta, log = TRUE))\n}\n\n# Fit Beta distribution\n# data &lt;- rbeta(100, 1.5, 0.5)\n# fit &lt;- mle_custom(data, c(1, 1), beta_nll)\n\n# Computing profile likelihood confidence intervals\nprofile_ci &lt;- function(data, param_index, mle_full, nll_function, level = 0.95) {\n  # Critical value for likelihood ratio test\n  crit &lt;- qchisq(level, df = 1) / 2\n  \n  # Profile negative log-likelihood\n  profile_nll &lt;- function(param_value, other_params) {\n    params &lt;- mle_full\n    params[param_index] &lt;- param_value\n    # Optimize over other parameters...\n    # (simplified here)\n    nll_function(params, data)\n  }\n  \n  # Find confidence interval boundaries\n  # (simplified - would need root finding in practice)\n}\n\n# EM Algorithm manual implementation\nem_gaussian_mixture &lt;- function(X, n_components = 2, max_iter = 100, tol = 1e-4) {\n  n &lt;- nrow(X)\n  d &lt;- ncol(X)\n  \n  # Initialize parameters\n  km &lt;- kmeans(X, n_components)\n  means &lt;- km$centers\n  weights &lt;- table(km$cluster) / n\n  covs &lt;- list()\n  for(k in 1:n_components) {\n    covs[[k]] &lt;- cov(X[km$cluster == k, ])\n  }\n  \n  log_lik_old &lt;- -Inf\n  \n  for(iter in 1:max_iter) {\n    # E-step: compute responsibilities\n    resp &lt;- matrix(0, n, n_components)\n    for(k in 1:n_components) {\n      resp[, k] &lt;- weights[k] * dmvnorm(X, means[k, ], covs[[k]])\n    }\n    resp &lt;- resp / rowSums(resp)\n    \n    # Log-likelihood\n    log_lik &lt;- sum(log(rowSums(resp)))\n    if(abs(log_lik - log_lik_old) &lt; tol) break\n    log_lik_old &lt;- log_lik\n    \n    # M-step: update parameters\n    for(k in 1:n_components) {\n      nk &lt;- sum(resp[, k])\n      weights[k] &lt;- nk / n\n      means[k, ] &lt;- colSums(resp[, k] * X) / nk\n      \n      X_centered &lt;- sweep(X, 2, means[k, ])\n      covs[[k]] &lt;- t(X_centered) %*% (resp[, k] * X_centered) / nk\n    }\n  }\n  \n  list(\n    means = means,\n    covariances = covs,\n    weights = weights,\n    responsibilities = resp,\n    log_likelihood = log_lik,\n    iterations = iter\n  )\n}\n\n\n6.9.7 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\nThis table maps sections in these lecture notes to the corresponding sections in Wasserman (2013) (“All of Statistics” or AoS).\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding AoS Section(s)\n\n\n\n\nIntroduction: How Good Are Our Estimators?\nFrom slides and general context from AoS §9.4 introduction on properties of estimators.\n\n\nWarm-up: Beta Distribution MLE\nExample from slides; similar numerical optimization examples in AoS §9.13.4.\n\n\nCore Properties of the MLE\n\n\n\n↳ Overview\nAoS §9.4 (list of properties).\n\n\n↳ Consistency\nAoS §9.5 (Theorem 9.13 and related discussion).\n\n\n↳ Equivariance\nAoS §9.6 (Theorem and Example).\n\n\n↳ Asymptotic Normality & Optimality\nAoS §9.7 introduction, §9.8.\n\n\nFisher Information and Confidence Intervals\n\n\n\n↳ Fisher Information\nAoS §9.7 (Definitions and Theorem 9.17).\n\n\n↳ Constructing Confidence Intervals\nAoS §9.7 (Theorem 9.19 and Examples).\n\n\n↳ Cramér-Rao Lower Bound\nMentioned in AoS §9.8.\n\n\nAdditional Topics\n\n\n\n↳ The Delta Method\nAoS §9.9 (Theorem 9.24 and Examples).\n\n\n↳ Multiparameter Models\nAoS §9.10 (Fisher Information Matrix and related theorems).\n\n\n↳ Sufficient Statistics\nAoS §9.13.2 (brief treatment from slides).\n\n\nConnection to Machine Learning: Cross-Entropy as MLE\nExpanded from slides.\n\n\nMLE for Latent Variable Models: The EM Algorithm\nAoS §9.13.4 (Appendix on computing MLEs).\n\n\nSelf-Test Problems\nInspired by AoS §9.14 exercises.\n\n\n\n\n\n\n\n\n6.9.8 Further Materials\n\nConnection to machine learning: Murphy (2022), “Probabilistic Machine Learning: An Introduction”, Chapter 4\n\n\nRemember: The MLE isn’t just a computational procedure – it’s a principled approach to estimation with deep theoretical foundations. The properties we’ve studied explain why maximum likelihood appears everywhere from simple coin flips to complex neural networks. These concepts represent the statistical foundations of modern machine learning!\n\n\n\n\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/06-parametric-inference-II.html#footnotes",
    "href": "chapters/06-parametric-inference-II.html#footnotes",
    "title": "6  Parametric Inference II: Properties of Estimators",
    "section": "",
    "text": "Note that the term MLE is used interchangeably to denote maximum-likelihood estimation (the technique), maximum-likelihood estimator (the rule to compute the estimate) and maximum-likelihood estimate (the result).↩︎\nIn this example, we don’t need numerical optimization since we can compute the MLE analytically. We could still do numerical optimization and we would get the same results – it just would be slower and not very smart.↩︎\nAlso called Mixture of Gaussians (MoG).↩︎\nAs we will see in future lectures, a Bayesian approach is often more suitable for proper uncertainty quantitication.↩︎\nThis is just a rule of thumb.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parametric Inference II: Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html",
    "href": "chapters/07-hypothesis-testing.html",
    "title": "7  Hypothesis Testing and p-values",
    "section": "",
    "text": "7.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#learning-objectives",
    "href": "chapters/07-hypothesis-testing.html#learning-objectives",
    "title": "7  Hypothesis Testing and p-values",
    "section": "",
    "text": "Explain the core framework of null-hypothesis significance testing (NHST), including the roles of null/alternative hypotheses, Type I/II errors, and statistical power.\nDefine the p-value and correctly interpret its meaning, recognizing its limitations and common misinterpretations.\nApply and interpret common hypothesis tests, such as the Wald test and permutation test, and understand the connection between tests and confidence intervals.\nExplain the multiple testing problem and apply standard correction methods like the Bonferroni and Benjamini-Hochberg procedures.\nCritically evaluate the use of NHST and p-values in data analysis and scientific research.\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter covers null-hypothesis significance testing (NHST) and p-values, fundamental concepts in statistical inference. The material is adapted from Chapter 10 of Wasserman (2013), supplemented with computational examples and critical perspectives on the use of NHST in modern data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#introduction-is-an-observed-effect-real-or-just-random-chance",
    "href": "chapters/07-hypothesis-testing.html#introduction-is-an-observed-effect-real-or-just-random-chance",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.2 Introduction: Is an Observed Effect Real or Just Random Chance?",
    "text": "7.2 Introduction: Is an Observed Effect Real or Just Random Chance?\nA drug company is running a clinical trial of a new drug. Patients are randomly assigned to either a treatment group that receives the new drug (100 subjects), or a control group that receives a placebo (other 100 subjects). After a suitable period of observation, the results are tallied:\n\n\n\n\nBetter\nNot Better\n\n\n\n\nTreated\n50\n50\n\n\nControl\n40\n60\n\n\n\nLooking at these numbers, we see that 50% of the treated patients improved, compared to only 40% of the control patients. But is this 10 percentage point difference large enough to represent a real effect of the drug, or could it simply be due to random chance in how we happened to assign individuals to groups?\nSimilarly, an online retailer might run an A/B test comparing two different user interfaces. Users are randomly shown either the old system or a new design, and we track their purchase behavior:\n\n\n\n\nPurchase\nNo Purchase\n\n\n\n\nNew System\n850\n150\n\n\nOld System\n800\n200\n\n\n\nThe new system appears to have a higher conversion rate (85% vs 80%), but again we face the fundamental question: is this difference statistically meaningful, or just random variation?\nThese questions lie at the heart of Null-Hypothesis Significance Testing (NHST), one of the most widely used – and most debated1 – frameworks in statistics. The core idea is to start by assuming there is no effect (the “null hypothesis”) and then ask how surprising our observed data would be under that assumption. If the data would be very surprising under the null, we have evidence against it.\nThis chapter builds the NHST framework from the ground up, introducing key concepts like p-values, statistical power, and the critical issue of multiple testing. We’ll see how these tools help us distinguish real effects from random noise, while also understanding their limitations and why they’re often misused in practice.\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nNull Hypothesis\nNollahypoteesi\nThe default assumption of no effect\n\n\nAlternative Hypothesis\nVastahypoteesi, vaihtoehtoinen hypoteesi\nWhat we hope to find evidence for\n\n\nSimple Hypothesis\nYksinkertainen hypoteesi, pistehypoteesi\nHypothesis that completely specifies the distribution\n\n\nComposite Hypothesis\nYhdistetty hypoteesi\nHypothesis that specifies a range of values\n\n\nTwo-sided Test\nKaksisuuntainen testi\nTest detecting differences in either direction\n\n\nOne-sided Test\nYksitahoinen testi, yksisuuntainen testi\nTest detecting differences in one direction\n\n\nRejection Region\nHylkäysalue\nSet of outcomes leading to rejection\n\n\nTest Statistic\nTestisuure\nSummary of evidence against null\n\n\nCritical Value\nKriittinen arvo\nThreshold for rejection\n\n\nType I Error\nHylkäysvirhe\nFalse positive rejection\n\n\nType II Error\nHyväksymisvirhe\nFalse negative (failure to detect)\n\n\nPower\nVoima\nProbability of detecting true effect\n\n\nPower function\nVoimafunktio\nPower as function of parameter\n\n\nSize of a test\nTestin koko\nMaximum Type I error rate\n\n\nStatistically significant\nTilastollisesti merkitsevä\nResult unlikely under null\n\n\nWald Test\nWaldin testi\nTest based on asymptotic normality\n\n\nPaired test\nParittainen testi\nTest for dependent samples\n\n\nPermutation test\nPermutaatiotesti, satunnaistamistesti\nNon-parametric test\n\n\nLikelihood ratio statistic\nUskottavuusosamääräsuure\nRatio of likelihoods\n\n\nMultiple testing\nMonitestaus\nRunning many tests simultaneously\n\n\nFalse discovery rate (FDR)\nVäärien löydösten osuus\nExpected proportion of false positives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#the-framework-of-hypothesis-testing",
    "href": "chapters/07-hypothesis-testing.html#the-framework-of-hypothesis-testing",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.3 The Framework of Hypothesis Testing",
    "text": "7.3 The Framework of Hypothesis Testing\n\n7.3.1 Null and Alternative Hypotheses\nWhen we observe a difference between two groups or a pattern in data, we need a systematic way to determine whether this observation represents a genuine phenomenon or could simply be due to chance. Hypothesis testing provides this framework by setting up two competing explanations and evaluating the evidence against one of them.\n\nNull Hypothesis (H_0): A statement of “no effect” or “no difference.” It’s the default assumption we seek to find evidence against.\nAlternative Hypothesis (H_1): The statement we hope to find evidence for, typically representing the presence of an effect or difference.\n\nFor example, in our drug trial:\n\nH_0: The drug has the same efficacy as the placebo (no effect)\nH_1: The drug’s efficacy differs from the placebo\n\nIn the parametric framework we studied in Chapters 5-6, we can often formalize this by partitioning the parameter space \\Theta into two disjoint sets \\Theta_0 and \\Theta_1, and testing:\nH_0: \\theta \\in \\Theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\in \\Theta_1\nThe nature of the hypotheses determines the type of test:\n\nSimple hypothesis: A hypothesis that completely specifies the distribution, such as \\theta = \\theta_0.\nComposite hypothesis: A hypothesis that specifies a range of values, such as \\theta &gt; \\theta_0 or \\theta &lt; \\theta_0.\nTwo-sided test: Tests H_0: \\theta = \\theta_0 versus H_1: \\theta \\neq \\theta_0 (detects differences in either direction).\nOne-sided test: Tests H_0: \\theta \\leq \\theta_0 versus H_1: \\theta &gt; \\theta_0 (or the reverse), detecting differences in a specific direction.\n\nMost scientific applications use two-sided tests, as we’re typically interested in detecting any difference from the null, not just differences in a predetermined direction.\n\n\n7.3.2 The Machinery of a Test\nOnce we’ve specified our hypotheses, we need a systematic procedure for deciding between them based on the observed data. This involves defining what outcomes would lead us to reject the null hypothesis.\n\nLet X be a random variable with range \\mathcal{X}.\nRejection Region (R \\subset \\mathcal{X}): The subset of outcomes for which we will reject H_0. If X \\in R, we reject the null hypothesis; otherwise, we retain it.2\nTest Statistic (T(X^n)): A function of the data that summarizes the evidence against H_0. Common examples include differences in sample means, ratios of variances, or correlation coefficients.\nCritical Value (c): A threshold used to define the rejection region, often in terms of a test statistic, such as\n R = \\{x: T(x) &gt; c\\}  \\quad \\text{ or } \\quad  R = \\{x: |T(x)| &gt; c\\} \n\n\n\n\n\n\n\nExample: Testing Equality of Means\n\n\n\nSuppose we have samples X_1, \\ldots, X_n \\sim F_X and Y_1, \\ldots, Y_m \\sim F_Y and want to test:\nH_0: \\mathbb{E}(X) = \\mathbb{E}(Y) \\quad \\text{versus} \\quad H_1: \\mathbb{E}(X) \\neq \\mathbb{E}(Y)\nThe test statistic might be (a scaled version of) the difference in sample means: T = \\bar{X}_n - \\bar{Y}_m\nIf |T| is large, we have evidence against H_0. The rejection region would be R = \\{(x_1,\\ldots,x_n,y_1,\\ldots,y_m): |T| &gt; c\\} for some critical value c chosen to control the error rates, as explained in the next section.\n\n\n\n\n7.3.3 Two Ways to Be Wrong: Type I and Type II Errors\nWhen we make a decision based on data, we can make two types of errors. Understanding these errors is crucial for properly designing and interpreting hypothesis tests.\n\nType I Error: Rejecting H_0 when H_0 is true (false positive). The probability of Type I error is denoted \\alpha.\nType II Error: Failing to reject H_0 when H_1 is true (false negative). The probability of Type II error is denoted \\beta.\n\nThe possible outcomes of a hypothesis test can be summarized as:\n\n\n\n\n\n\n\n\n\nH_0 True\nH_0 False\n\n\n\n\nH_0 Retained\n✓ Correct (True Negative)\n✗ Type II Error (False Negative)\n\n\nH_0 Rejected\n✗ Type I Error (False Positive)\n✓ Correct (True Positive)\n\n\n\nKey quantities for characterizing test performance:\n\nPower Function: For a test with rejection region R, the power function is: \\beta(\\theta) = \\mathbb{P}_\\theta(X \\in R)\nSize of a Test: The maximum probability of Type I error: \\alpha = \\sup_{\\theta \\in \\Theta_0} \\beta(\\theta)\nLevel of a Test: A test has level \\alpha if its size is at most \\alpha.\nPower of a Test: The probability of correctly rejecting H_0 when it’s false: \\text{Power} = 1 - \\beta = \\mathbb{P}_\\theta(\\text{Reject } H_0 \\mid \\theta \\in \\Theta_1)\n\nThere’s an inherent trade-off between Type I and Type II errors: making it harder to commit a Type I error (lowering \\alpha) makes it easier to commit a Type II error (increasing \\beta), thus reducing power. Standard practice is to fix the Type I error rate at a conventional level (typically \\alpha = 0.05) and design the study to have adequate power (typically 80% or higher).\nThe relationship between these errors explains why we use asymmetric language: we “reject” or “fail to reject” H_0, never “accept” it. Failing to reject doesn’t prove H_0 is true – we might simply lack power to detect the effect!\nIntuitiveMathematicalComputationalThink of hypothesis testing like a criminal trial. The null\nhypothesis is “the defendant is innocent” – our default assumption until\nproven otherwise. We only reject this assumption (convict) if the\nevidence against innocence is very strong.Just as a trial can go wrong in two ways, so can a hypothesis\ntest:\nType I Error: Convicting an innocent person (false\npositive)\nType II Error: Acquitting a guilty person (false\nnegative)\nA common illustration uses the Voight-Kampff\ntest, which detects replicants (human-looking androids):\n\nType I and Type II Errors illustrated\nwith replicant detection. Credits: gpt-4o.\nWith \\(H_0\\): “The subject is\nhuman”:\nType I Error: Test says REPLICANT but\nsubject is actually human (wrongly flagged as android).\nType II Error: Test says HUMAN but\nsubject is actually an android (missed detection).\nLet’s derive the power function for a concrete example. Consider\ntesting the mean of a normal distribution:Setup: We have\n\\(X_1, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nwith known variance \\(\\sigma^2\\),\ntesting:\n\\[H_0: \\mu = 0 \\quad \\text{versus} \\quad H_1: \\mu \\neq 0\\]Test statistic: The sample mean\n\\(\\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)\\).\nUnder \\(H_0\\) where\n\\(\\mu = 0\\):\n\\[Z = \\frac{\\sqrt{n}\\bar{X}}{\\sigma} \\sim \\mathcal{N}(0, 1)\\]Decision rule: For a level\n\\(\\alpha\\) test, we reject\n\\(H_0\\) when\n\\(|Z| &gt; z_{\\alpha/2}\\), where\n\\(z_{\\alpha/2}\\) is the\n\\((1-\\alpha/2)\\) quantile of the\nstandard normal distribution.Power function: For any true value\n\\(\\mu\\), the power is:\n\\[\\beta(\\mu) = \\mathbb{P}_\\mu(\\text{Reject } H_0) = \\mathbb{P}_\\mu(|Z| &gt; z_{\\alpha/2})\\]Under the true \\(\\mu\\), the test\nstatistic follows\n\\(Z \\sim \\mathcal{N}(\\sqrt{n}\\mu/\\sigma, 1)\\).\nTherefore:\n\\[\\beta(\\mu) = \\mathbb{P}\\left(\\left|\\mathcal{N}\\left(\\frac{\\sqrt{n}\\mu}{\\sigma}, 1\\right)\\right| &gt; z_{\\alpha/2}\\right)\\]This probability depends on three key factors:\nEffect size:\n\\(\\delta = \\mu/\\sigma\\) (standardized\ndistance from null)\n\nSample size: Larger\n\\(n\\) → higher power\nSignificance level: Larger\n\\(\\alpha\\) → higher power (but more\nType I errors)\nLet’s visualize this power function for\n\\(H_0: \\mu = 0\\),\n\\(\\sigma = 1\\),\n\\(n = 25\\),\n\\(\\alpha = 0.05\\):\n\n\n\nThe mirrored S-shaped curve demonstrates that power is minimized at\nthe boundary of \\(H_0\\) (here,\n\\(\\mu = 0\\)) and increases\nmonotonically as the true parameter moves away from the null value,\nmaking detecting the difference easier.Let’s simulate hypothesis testing to see Type I and Type II errors in\naction. We’ll test \\(H_0: \\mu = 0\\)\nusing one-sample t-tests on data from normal distributions. By\ngenerating many datasets under different true means, we can observe the\nempirical error rates:\nimport numpy as np\nfrom scipy import stats\n\n# Simulate multiple hypothesis tests\nnp.random.seed(42)\nn_tests = 10000\nn_samples = 30\nalpha = 0.05\n\n# Scenario 1: H0 is true (μ = 0)\n# We should reject H0 about 5% of the time (Type I error rate)\ntype_i_errors = 0\nfor _ in range(n_tests):\n    sample = np.random.normal(0, 1, n_samples)  # H0 true: μ = 0\n    t_stat, p_value = stats.ttest_1samp(sample, 0)\n    if p_value &lt; alpha:\n        type_i_errors += 1\n\nprint(f\"Type I Error Rate (H₀ true, μ=0):\")\nprint(f\"  Theoretical: {alpha:.3f}\")\nprint(f\"  Observed:    {type_i_errors/n_tests:.3f}\")\n\n# Scenario 2: H0 is false (μ = 0.5)\n# Power = probability of correctly rejecting H0\ntrue_mu = 0.5\nrejections = 0\nfor _ in range(n_tests):\n    sample = np.random.normal(true_mu, 1, n_samples)  # H0 false: μ = 0.5\n    t_stat, p_value = stats.ttest_1samp(sample, 0)\n    if p_value &lt; alpha:\n        rejections += 1\n\npower = rejections / n_tests\ntype_ii_error_rate = 1 - power\n\nprint(f\"\\nWhen H₀ is false (true μ={true_mu}):\")\nprint(f\"  Power (correct rejection):     {power:.3f}\")\nprint(f\"  Type II Error Rate (miss):     {type_ii_error_rate:.3f}\")\n\nType I Error Rate (H₀ true, μ=0):\n  Theoretical: 0.050\n  Observed:    0.056\n\nWhen H₀ is false (true μ=0.5):\n  Power (correct rejection):     0.750\n  Type II Error Rate (miss):     0.250\n\nFactors affecting power:\nEffect size: Larger true differences from\n\\(H_0\\) → higher power\nSample size: More data → higher power\nVariability: Lower variance → higher power\nSignificance level: Higher\n\\(\\alpha\\) → higher power (but more\nType I errors)\n\n\n\n7.3.4 The Wald Test\nThe Wald test is one of the most widely used hypothesis tests in statistics. It leverages the asymptotic normality of estimators that we studied in Chapters 5-6, providing a general framework for testing hypotheses about parameters.\nThe key insight: If our estimator \\hat{\\theta} is approximately normal (which many estimators are for large samples), we can use this normality to construct a test statistic with a known distribution under the null hypothesis.\n\nThe Wald Test: For testing H_0: \\theta = \\theta_0 versus H_1: \\theta \\neq \\theta_0:\nAssume that \\hat{\\theta} is asymptotically normal: \\frac{(\\hat{\\theta} - \\theta_0)}{\\widehat{\\text{se}}} \\rightsquigarrow \\mathcal{N}(0,1)\nThe size \\alpha Wald test rejects H_0 when |W| &gt; z_{\\alpha/2}, where: W = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\text{se}}}\n\nKey Properties of the Wald Test:\n\nCorrect error rate: For large samples, the test has (approximately) the desired Type I error rate \\alpha\nPower increases with:\n\nLarger effect size (bigger difference from \\theta_0)\nLarger sample size (more data)\nSmaller variance (less noise)\n\nDuality with confidence intervals: The Wald test rejects H_0: \\theta = \\theta_0 if and only if \\theta_0 is outside the (1-\\alpha) confidence interval\n\n\n\n\n\n\n\nMathematical Details\n\n\n\n\n\n\n\nAsymptotic size: As n \\to \\infty, \\mathbb{P}_{\\theta_0}(|W| &gt; z_{\\alpha/2}) \\to \\alpha\nPower function: When the true parameter is \\theta_* \\neq \\theta_0, the power is: \\text{Power}(\\theta_*) = 1 - \\Phi\\left(\\frac{\\theta_0 - \\theta_*}{\\widehat{\\text{se}}} + z_{\\alpha/2}\\right) + \\Phi\\left(\\frac{\\theta_0 - \\theta_*}{\\widehat{\\text{se}}} - z_{\\alpha/2}\\right) where \\Phi is the standard normal CDF.\nConfidence interval duality: Reject H_0 if and only if \\theta_0 \\notin [\\hat{\\theta} - z_{\\alpha/2} \\cdot \\widehat{\\text{se}}, \\hat{\\theta} + z_{\\alpha/2} \\cdot \\widehat{\\text{se}}]\n\n\n\n\n\nThis last property provides a powerful duality: every confidence interval can be inverted to give a hypothesis test, and vice versa. This is why confidence intervals are often more informative than p-values alone – they show all parameter values that wouldn’t be rejected.\n\n\n\n\n\n\nExample: Comparing Two Means\n\n\n\nThe Wald test extends naturally to comparing means from two populations. For independent samples X_1, \\ldots, X_m and Y_1, \\ldots, Y_n with means \\mu_1 and \\mu_2:\nTesting H_0: \\mu_1 = \\mu_2 (or \\delta = \\mu_1 - \\mu_2 = 0):\n\nEstimator: \\hat{\\delta} = \\bar{X} - \\bar{Y}\nStandard error: \\widehat{\\text{se}} = \\sqrt{s_1^2/m + s_2^2/n}\nTest statistic: W = \\hat{\\delta} / \\widehat{\\text{se}}\n\nThis is the basis for the famous two-sample t-test (which uses a t-distribution for small samples instead of the normal approximation).\n\n\n\n\n7.3.5 Statistical and Scientific Significance\nWhen H_0 is rejected, the result is called statistically significant. This does not mean that the result has any practical or scientific relevance! The effect found could be very tiny and negligible for all practical purpose.\n\n\n\n\n\n\nStatistical vs. Scientific Significance\n\n\n\nStatistical significance ≠ Scientific significance\nLet’s assume \\alpha = 0.05 and \\theta_0 = 0. In this case, we reject the null if the 95 % confidence interval (CI) excludes 0. A result can be:\n\nStatistically significant but scientifically trivial: The CI excludes 0 but is very close to it (where “very close” depends on the domain).\nStatistically non-significant but scientifically important: The CI includes 0 but also includes large, meaningful effects.\n\nAlways report confidence intervals alongside p-values (described in the next section) to show both statistical and practical significance!\n\n\nLet’s visualize this distinction:\n\n\nShow code\nimport matplotlib.pyplot as plt\n\n# Two example confidence intervals\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 4))\n\n# Example 1: Statistically significant but small effect\ntheta0 = 0\nci1_lower, ci1_upper = 0.001, 0.003\nci1_center = (ci1_lower + ci1_upper) / 2\n\nax1.axvline(theta0, color='red', linestyle='--', linewidth=2, label='H₀: θ = 0')\nax1.barh(0, ci1_upper - ci1_lower, left=ci1_lower, height=0.3, \n         color='blue', alpha=0.5, label='95% CI')\nax1.plot(ci1_center, 0, 'ko', markersize=8, label='Estimate')\nax1.set_xlim(-0.01, 0.01)\nax1.set_ylim(-0.5, 0.5)\nax1.set_xlabel('Parameter value')\nax1.set_title('Statistically Significant\\nbut Tiny Effect')\nax1.legend(loc='upper right')\nax1.set_yticks([])\nax1.grid(True, alpha=0.3)\n\n# Example 2: Not significant but large potential effect\nci2_lower, ci2_upper = -0.5, 2.5\nci2_center = (ci2_lower + ci2_upper) / 2\n\nax2.axvline(theta0, color='red', linestyle='--', linewidth=2, label='H₀: θ = 0')\nax2.barh(0, ci2_upper - ci2_lower, left=ci2_lower, height=0.3, \n         color='blue', alpha=0.5, label='95% CI')\nax2.plot(ci2_center, 0, 'ko', markersize=8, label='Estimate')\nax2.set_xlim(-1, 3)\nax2.set_ylim(-0.5, 0.5)\nax2.set_xlabel('Parameter value')\nax2.set_title('Not Statistically Significant\\nbut Large Potential Effect')\nax2.legend(loc='upper right')\nax2.set_yticks([])\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization demonstrates the CI-test duality: using significance level \\alpha = 0.05, we construct a 95% CI (since 1 - \\alpha = 0.95). We reject H_0: \\theta = 0 if and only if 0 lies outside this interval.\nLeft panel: The 95% CI excludes 0, so we reject H_0 at the 5% level. But the effect is tiny (0.001 to 0.003) – statistically significant but practically negligible.\nRight panel: The 95% CI includes 0, so we fail to reject H_0. However, the interval also includes large values (up to 2.5) – we simply lack precision to determine if there’s an effect.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#the-p-value-a-continuous-measure-of-evidence",
    "href": "chapters/07-hypothesis-testing.html#the-p-value-a-continuous-measure-of-evidence",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.4 The p-value: A Continuous Measure of Evidence",
    "text": "7.4 The p-value: A Continuous Measure of Evidence\n\n7.4.1 Understanding the p-value\nSimply reporting “reject H_0” or “retain H_0” isn’t very informative – the result depends entirely on the chosen significance level \\alpha. What if we could provide a continuous measure of the strength of evidence against the null hypothesis? This is exactly what the p-value provides.\n\nThe p-value is the probability, calculated under H_0, of observing a test statistic value at least as extreme as the one actually observed:\n\\text{p-value} = \\mathbb{P}_{H_0}(T \\text{ as extreme or more extreme than } T_\\text{obs})\nEquivalently, it’s the smallest significance level \\alpha at which we would reject H_0: \\text{p-value} = \\inf\\{\\alpha : T(X^n) \\in R_\\alpha\\}\nwhere R_\\alpha is the rejection region for a size \\alpha test.\n\n\n\n\n\n\n\nWhy are these two definitions equivalent?\n\n\n\n\n\nThe rejection region R_\\alpha contains the most extreme \\alpha proportion of possible test statistics. If our observed statistic is “more extreme than 2.1% of possible values” (p = 0.021), then:\n\nWe’d reject for any \\alpha \\geq 0.021 (our observation is extreme enough)\nWe’d fail to reject for any \\alpha &lt; 0.021 (our observation isn’t extreme enough)\n\nThus 0.021 is exactly the boundary – the smallest \\alpha for which we’d reject\n\nThe p-value acts as a threshold: it tells us both how extreme our observation is AND the critical significance level where our decision switches.\n\n\n\nKey properties of p-values:\n\nRange: p-values lie between 0 and 1\nInterpretation: Small p-value = strong evidence against H_0\nDecision rule: Reject H_0 if p-value &lt; \\alpha\nUnder H_0: The p-value follows a \\text{Uniform}(0,1) distribution\n\n\n\n\n\n\n\nExample: p-value for the Wald Test\n\n\n\nFor the Wald test we studied earlier, if w = (\\hat{\\theta} - \\theta_0)/\\widehat{\\text{se}} is the observed Wald statistic, then: \\text{p-value} = \\mathbb{P}(|Z| &gt; |w|) = 2\\Phi(-|w|) where Z \\sim \\mathcal{N}(0,1) and \\Phi is the standard normal CDF.\nThis formula works because under H_0, the Wald statistic W \\sim \\mathcal{N}(0,1) asymptotically, so we calculate the probability of seeing a value as extreme as our observed w from a standard normal distribution.\n\n\nIntuitiveMathematicalComputationalThe p-value is often called a “surprise index.” It answers the\nquestion: “If there were truly no effect (if\n\\(H_0\\) were true), how likely would we\nbe to see a result at least as extreme as the one we actually\nobserved?”Think of it this way: Imagine you suspect a coin is biased. You flip\nit 10 times and get 9 heads. The p-value asks: “If this were actually a\nfair coin, what’s the probability of getting 9 or more heads in 10\nflips?” If that probability is very small, you have strong evidence the\ncoin isn’t fair.A small p-value means our data would be very surprising under the\nnull hypothesis, providing evidence against it. A large p-value means\nour data is consistent with the null hypothesis (though this doesn’t\nprove the null is true! Maybe we simply didn’t collect enough data).Let \\(T(X^n)\\) be the test statistic\nand \\(t_\\text{obs} = T(x^n)\\) be its\nobserved value from the data. The p-value formalizes the “surprise\nindex” under the null hypothesis\n\\(H_0\\).1. Simple Null Hypothesis\n(\\(H_0: \\theta = \\theta_0\\))For a simple null, the p-value is the probability of observing a test\nstatistic at least as extreme as\n\\(t_\\text{obs}\\). The definition of\n“extreme” depends on the alternative hypothesis\n\\(H_1\\):\nRight-tailed test\n(\\(H_1: \\theta &gt; \\theta_0\\)): Extreme\nmeans large values of \\(T\\).\n\\[ \\text{p-value} = \\mathbb{P}_{\\theta_0}(T(X^n) \\geq t_\\text{obs}) \\]\nLeft-tailed test\n(\\(H_1: \\theta &lt; \\theta_0\\)): Extreme\nmeans small values of \\(T\\).\n\\[ \\text{p-value} = \\mathbb{P}_{\\theta_0}(T(X^n) \\leq t_\\text{obs}) \\]\nTwo-tailed test\n(\\(H_1: \\theta \\neq \\theta_0\\)):\nExtreme means large values of \\(|T|\\)\n(or similar symmetric measure).\n\\[ \\text{p-value} = \\mathbb{P}_{\\theta_0}(|T(X^n)| \\geq |t_\\text{obs}|) \\]\n2. Composite Null Hypothesis\n(\\(H_0: \\theta \\in \\Theta_0\\))When the null hypothesis is composite (e.g.,\n\\(H_0: \\mu \\le 0\\)), there isn’t a\nsingle distribution under \\(H_0\\). We\nneed to find the probability of an extreme result under the “worst-case”\nscenario within \\(\\Theta_0\\) – the one\nthat makes our data look least surprising. This is achieved by taking\nthe supremum (least upper bound) of the probability over all possible\nparameter values in \\(\\Theta_0\\).For a right-tailed test, the formula is:\n\\[ \\text{p-value} = \\sup_{\\theta \\in \\Theta_0} \\mathbb{P}_\\theta(T(X^n) \\geq t_\\text{obs}) \\]This ensures that if we reject when p-value\n\\(&lt; \\alpha\\), the Type I error rate is\ncontrolled and does not exceed\n\\(\\alpha\\) for any\n\\(\\theta \\in \\Theta_0\\). For many\nstandard tests, this supremum occurs at the boundary of\n\\(\\Theta_0\\) (e.g., at\n\\(\\mu=0\\) for\n\\(H_0: \\mu \\le 0\\)).Let’s visualize how the p-value is calculated for a two-sided test.\nWe’ll use the Wald test as our example, where the test statistic follows\na standard normal distribution under\n\\(H_0\\).For a two-sided test (e.g.,\n\\(H_0: \\theta = \\theta_0\\) vs\n\\(H_1: \\theta \\neq \\theta_0\\)),\n“extreme” means far from zero in either direction. The p-value is the\ntotal probability in both tails beyond our observed test statistic:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Example: Observed test statistic value\nw_obs = 2.3  # Our observed Wald statistic\n\n# Create the standard normal distribution\nz_values = np.linspace(-4, 4, 300)\npdf_values = stats.norm.pdf(z_values)\n\n# Calculate p-value\np_value = 2 * stats.norm.cdf(-abs(w_obs))\n\nplt.figure(figsize=(7, 4))\n# Plot the distribution\nplt.plot(z_values, pdf_values, 'b-', linewidth=2, label='Standard Normal')\n\n# Shade the rejection regions (tails)\ntail_right = z_values[z_values &gt;= abs(w_obs)]\ntail_left = z_values[z_values &lt;= -abs(w_obs)]\n\nplt.fill_between(tail_right, 0, stats.norm.pdf(tail_right), \n                  color='red', alpha=0.3, label=f'Right tail')\nplt.fill_between(tail_left, 0, stats.norm.pdf(tail_left), \n                  color='red', alpha=0.3, label=f'Left tail')\n\n# Mark the observed values\nplt.axvline(w_obs, color='black', linestyle='--', linewidth=2, label=f'Observed: w = {w_obs}')\nplt.axvline(-w_obs, color='black', linestyle='--', linewidth=2)\n\n# Add annotations\nplt.text(w_obs + 0.1, 0.05, f'α/2 = {p_value/2:.4f}', fontsize=10)\nplt.text(-w_obs - 0.8, 0.05, f'α/2 = {p_value/2:.4f}', fontsize=10)\n\nplt.xlabel('Test Statistic Value')\nplt.ylabel('Probability Density')\nplt.title(f'p-value Calculation: p = {p_value:.4f}')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\nplt.ylim(0, 0.45)\nplt.show()\n\nprint(f\"For observed test statistic w = {w_obs}:\")\nprint(f\"p-value = 2 * P(Z &gt; |{w_obs}|) = {p_value:.4f}\")\nprint(f\"\\nInterpretation: If H₀ were true, we'd see a test statistic\")\nprint(f\"this extreme or more extreme only {p_value*100:.2f}% of the time.\")\n\n\n\n\nFor observed test statistic w = 2.3:\np-value = 2 * P(Z &gt; |2.3|) = 0.0214\n\nInterpretation: If H₀ were true, we'd see a test statistic\nthis extreme or more extreme only 2.14% of the time.\n\n\n\n\n7.4.2 How to Interpret p-values (and How Not To)\nThe p-value is one of the most misunderstood concepts in statistics. Let’s clarify what it is and isn’t.\nWhat the p-value IS:\n\nThe p-value IS the probability, computed under H_0, of observing data as extreme or more extreme than what we actually observed\nThe p-value IS a measure of evidence against H_0 – smaller values indicate stronger evidence\nThe p-value IS the answer to: “If H_0 were true, how surprising would our data be?”\nThe p-value IS useful for deciding whether to reject H_0 at any given significance level\n\nA p-value of 0.02 means: If the null hypothesis were true, we’d see data this extreme or more extreme only 2% of the time. That’s fairly surprising, suggesting the null might be false.\n\n\n\n\n\n\nCommon p-value Misinterpretations\n\n\n\nA p-value is NOT the probability that the null hypothesis is true.\n\nWrong: “p = 0.03 means there’s a 3% chance the null hypothesis is true”\nThe p-value is \\mathbb{P}(\\text{data} | H_0), not \\mathbb{P}(H_0 | \\text{data})\nComputing \\mathbb{P}(H_0 | \\text{data}) requires Bayesian methods (Chapter 8)\n\nA large p-value is NOT strong evidence that the null hypothesis is true.\n\nA large p-value could mean:\n\nH_0 is true, or\nH_0 is false but our test has low power to detect the effect\n\nNever conclude “we accept H_0” based on a large p-value\n\nStatistical significance is NOT the same as practical significance.\n\nWith enough data, tiny meaningless effects can become “statistically significant”\nAlways examine the effect size (e.g., via confidence intervals) to judge practical importance\nExample: A drug that lowers blood pressure by 0.1 mmHg might be statistically significant with n=10,000 but clinically meaningless\n\n\n\n\n\n\n\n\n\nStandard p-value Interpretation Scales\n\n\n\n\n\nThe interpretation of p-values varies significantly by field and context. Here’s a common scale used in many fields:\n\n\n\np-value\nEvidence against H_0\n\n\n\n\n&lt; 0.01\nStrong evidence\n\n\n0.01 - 0.05\nPositive evidence\n\n\n&gt; 0.05\nLittle or no evidence\n\n\n\nField-specific standards:\n\nMedicine/Psychology: Often use \\alpha = 0.05 as the standard.\nGenomics: Use much stricter thresholds (e.g., 5 × 10^{-8}) due to multiple testing, as we will see below.\nParticle Physics: Extremely strict standards for discoveries:\n\nThe Higgs boson discovery required a “5-sigma” result (p &lt; 3 × 10^{-7}).\nThis corresponds to less than 1 in 3.5 million chance of a false positive.\n\n\nThese thresholds are conventions, not laws of nature. The appropriate threshold depends on the consequences of Type I and Type II errors in your specific context.\n\n\n\n\nIf the test statistic has a continuous distribution, then under H_0: \\theta = \\theta_0, the p-value has a Uniform(0,1) distribution. Therefore, if we reject H_0 when the p-value is less than \\alpha, the probability of a Type I error is exactly \\alpha.\n\nThis property means that p-values “work correctly” – under the null hypothesis, you’ll get a p-value less than 0.05 exactly 5% of the time, a p-value less than 0.01 exactly 1% of the time, and so on.\n\n\n7.4.3 Applying p-values: Classification Algorithm Comparison\nLet’s apply the Wald test and p-value concepts to a practical problem that appears frequently in machine learning and data science: comparing the performance of two classification algorithms. This example (from AoS Example 10.7) illustrates both independent and paired testing scenarios.\n\n\n\n\n\n\nExample: Comparing Algorithms with Independent Test Sets\n\n\n\nSuppose we test two classification algorithms on different, independent test sets:\n\nAlgorithm 1: Test set of size m, makes X errors\nAlgorithm 2: Test set of size n, makes Y errors\n\nThen X \\sim \\text{Binomial}(m, p_1) and Y \\sim \\text{Binomial}(n, p_2), where p_1 and p_2 are the true error rates.\nWe want to test: H_0: p_1 = p_2 \\quad \\text{versus} \\quad H_1: p_1 \\neq p_2\nOr equivalently, H_0: \\delta = 0 versus H_1: \\delta \\neq 0, where \\delta = p_1 - p_2.\nThe Wald Test Approach:\nThe MLE is \\hat{\\delta} = \\hat{p}_1 - \\hat{p}_2 where \\hat{p}_1 = X/m and \\hat{p}_2 = Y/n.\nThe estimated standard error is: \\widehat{\\text{se}} = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{m} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n}}\nThe Wald test statistic is: W = \\frac{\\hat{\\delta} - 0}{\\widehat{\\text{se}}} = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{m} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n}}}\nFor a size \\alpha test, we reject H_0 when |W| &gt; z_{\\alpha/2}, and the p-value is 2\\Phi(-|W|).\nNumerical Example: Let m = n = 500, with Algorithm 1 making 75 errors and Algorithm 2 making 100 errors:\n\n\nShow code\nimport numpy as np\nfrom scipy import stats\n\n# Independent test sets\nm, n = 500, 500\np1_hat, p2_hat = 0.15, 0.20\n\n# Standard error\nse = np.sqrt(p1_hat*(1-p1_hat)/m + p2_hat*(1-p2_hat)/n)\n\n# Wald test statistic\nW = (p1_hat - p2_hat) / se\n\n# p-value (two-sided)\np_value = 2 * stats.norm.cdf(-abs(W))\n\nprint(f\"Error rates: Algorithm 1 = {p1_hat:.2%}, Algorithm 2 = {p2_hat:.2%}\")\nprint(f\"Difference: {p1_hat - p2_hat:.2%}\")\nprint(f\"Standard error: {se:.4f}\")\nprint(f\"Wald statistic: {W:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\nConclusion: {'Significant difference' if p_value &lt; 0.05 else 'No significant difference'} at α = 0.05\")\n\n\nError rates: Algorithm 1 = 15.00%, Algorithm 2 = 20.00%\nDifference: -5.00%\nStandard error: 0.0240\nWald statistic: -2.085\np-value: 0.0371\n\nConclusion: Significant difference at α = 0.05\n\n\nThis 5 percentage point difference is statistically significant at α = 0.05. But the significance depends critically on sample size. As a numerical illustration:\n\nWith m = n = 100: Same 5% difference gives |W| \\approx 0.93, p-value ≈ 0.35 (not significant)\nWith m = n = 500: |W| \\approx 2.09, p-value ≈ 0.037 (significant)\n\nWith m = n = 1000: |W| \\approx 2.95, p-value ≈ 0.003 (highly significant)\n\nLet’s visualize how sample size affects our ability to detect this difference:\n\n\n\n\n\n\n\n\n\nKey Insight: The same 5% difference in error rates can be:\n\nNon-significant with small samples (low power)\nHighly significant with large samples (high power)\n\nThis illustrates why reporting effect sizes (the actual difference) alongside p-values is crucial!\n\n\n\n\n\n\n\n\nExample: Comparing Algorithms with Paired Test Sets\n\n\n\nOften we test both algorithms on the same test set. This is more efficient but requires a different analysis because the results are no longer independent.\nData Structure: For each test instance i = 1, \\ldots, n:\n\n\n\n\n\n\n\n\n\nTest Case\nAlgorithm 1 (X_i)\nAlgorithm 2 (Y_i)\nDifference (D_i = X_i - Y_i)\n\n\n\n\n1\n1 (correct)\n0 (incorrect)\n1\n\n\n2\n1 (correct)\n1 (correct)\n0\n\n\n3\n0 (incorrect)\n1 (correct)\n-1\n\n\n…\n…\n…\n…\n\n\nn\nX_n\nY_n\nD_n\n\n\n\nThe key insight: We can no longer treat X and Y as independent because they’re tested on the same instances.\nThe Paired Test Approach:\nDefine D_i = X_i - Y_i for each test instance. Then: \\delta = \\mathbb{E}(D_i) = \\mathbb{E}(X_i) - \\mathbb{E}(Y_i) = \\mathbb{P}(X_i = 1) - \\mathbb{P}(Y_i = 1)\nWe test H_0: \\delta = 0 versus H_1: \\delta \\neq 0.\nThe nonparametric plug-in estimate is \\hat{\\delta} = \\bar{D} = \\frac{1}{n}\\sum_{i=1}^n D_i.\nThe standard error is \\widehat{\\text{se}}(\\hat{\\delta}) = S/\\sqrt{n}, where S^2 = \\frac{1}{n}\\sum_{i=1}^n (D_i - \\bar{D})^2.\nThe Wald test statistic is W = \\hat{\\delta}/\\widehat{\\text{se}} and we reject H_0 if |W| &gt; z_{\\alpha/2}.\nThis is called a paired comparison or paired test.\n\n\nShow code\n# Simulate paired comparison\nnp.random.seed(42)\nn = 1000  # test set size\n\n# True probabilities of correct classification\np1_true = 0.85  # Algorithm 1\np2_true = 0.80  # Algorithm 2\n\n# Simulate correlated outcomes (algorithms often agree)\ncorrelation = 0.7\nfrom scipy.stats import multivariate_normal\n\n# Generate correlated binary outcomes\ncov = [[1, correlation], [correlation, 1]]\nlatent = multivariate_normal.rvs([0, 0], cov, size=n)\nX = (latent[:, 0] &lt; stats.norm.ppf(p1_true)).astype(int)\nY = (latent[:, 1] &lt; stats.norm.ppf(p2_true)).astype(int)\n\n# Compute differences\nD = X - Y\nD_bar = np.mean(D)\nS = np.std(D, ddof=1)\nse = S / np.sqrt(n)\n\n# Wald test\nW = D_bar / se\np_value = 2 * stats.norm.cdf(-abs(W))\n\nprint(f\"Sample mean difference: {D_bar:.4f}\")\nprint(f\"Standard error: {se:.4f}\")\nprint(f\"Wald statistic: {W:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Show contingency table\nfrom pandas import crosstab, DataFrame\nct = crosstab(X, Y, rownames=['Alg1'], colnames=['Alg2'])\nprint(f\"\\nContingency table:\")\nprint(ct)\nprint(f\"\\nAlgorithms agree on {np.sum(X == Y)/n:.1%} of instances\")\n\n\nSample mean difference: 0.0580\nStandard error: 0.0125\nWald statistic: 4.632\np-value: 0.0000\n\nContingency table:\nAlg2    0    1\nAlg1          \n0      74   51\n1     109  766\n\nAlgorithms agree on 84.0% of instances\n\n\n\n\n\n\n\n\n\n\nPaired vs Independent Tests\n\n\n\nPaired tests are generally more powerful than independent tests when:\n\nThe same subjects/instances are measured twice\nThere’s positive correlation between measurements\n\nThe paired test removes the between-subject variability, focusing only on within-subject differences.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#constructing-statistical-tests",
    "href": "chapters/07-hypothesis-testing.html#constructing-statistical-tests",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.5 Constructing Statistical Tests",
    "text": "7.5 Constructing Statistical Tests\nNow that we understand p-values and have seen the Wald test in action, let’s step back and consider the general problem: How do we construct a statistical test?\nGiven a test statistic T(X^n) and null hypothesis H_0, we need to determine the distribution of T under H_0 to calculate p-values and make decisions. There are three main approaches:\n\nExact Distribution: Sometimes we can calculate the exact distribution of T under H_0\n\nExample: Fisher’s exact test for 2×2 tables\nAdvantage: Exact p-values, valid for any sample size\nDisadvantage: Only possible for simple cases\n\nAsymptotic Approximation: Use limiting distributions as n \\to \\infty\n\nExamples: Wald test (normal), likelihood ratio test (chi-squared)\nAdvantage: Widely applicable, computationally simple\nDisadvantage: May be inaccurate for small samples\n\nSimulation/Resampling: Simulate the distribution by resampling\n\nExamples: Permutation test, bootstrap test\nAdvantage: Minimal assumptions, works for any test statistic\nDisadvantage: Computationally intensive\n\n\nThe choice depends on:\n\nSample size (small samples → avoid asymptotics)\nDistributional assumptions (violated → use resampling)\nTest statistic complexity (complex → simulation may be only option)\nComputational resources (limited → prefer analytical methods)\n\nLet’s now explore specific tests that exemplify each approach: the permutation test (simulation), Fisher’s exact test (exact distribution), and the likelihood ratio test (asymptotic).\n\n7.5.1 The Permutation Test: A Simulation Approach\nThe permutation test is a powerful non-parametric method for testing whether two distributions are the same. It exemplifies the simulation approach to test construction, making minimal assumptions – only requiring that the distributions are identical under the null hypothesis.\n\nPermutation Test: A nonparametric method for testing whether two distributions are the same.\nLet X_1, \\ldots, X_m \\sim F_X and Y_1, \\ldots, Y_n \\sim F_Y be independent samples.\nHypotheses: H_0: F_X = F_Y versus H_1: F_X \\neq F_Y\nTest statistic: Choose any statistic T(x_1, \\ldots, x_m, y_1, \\ldots, y_n), such as T = |\\bar{X}_m - \\bar{Y}_n|\nKey principle: Under H_0, all N! = (m+n)! permutations of the combined data are equally likely.\nPermutation distribution: The distribution that puts mass 1/N! on each value T_j obtained from the N! permutations.\nPermutation p-value: \\text{p-value} = \\mathbb{P}_0(T \\geq t_\\text{obs}) = \\frac{1}{N!} \\sum_{j=1}^{N!} I(T_j \\geq t_\\text{obs})\nwhere t_\\text{obs} is the observed test statistic and T_j is the statistic for permutation j.\n\nThe key insight: If the null hypothesis is true (both groups come from the same distribution), then the specific assignment of observations to groups was just one random possibility among many. We can simulate the null distribution by considering all possible reassignments.\nWhen to use permutation tests:\n\nSmall sample sizes where asymptotic approximations may fail\nNon-standard or unknown distributions\nComplex test statistics without known distributions\nWhen exact p-values are needed for critical decisions\n\n\n\n\n\n\n\nPractical Permutation Test Algorithm\n\n\n\nSince evaluating all (m+n)! permutations is computationally infeasible for realistic sample sizes, we use Monte Carlo sampling:\n\nCompute observed test statistic: t_\\text{obs} = T(X_1, \\ldots, X_m, Y_1, \\ldots, Y_n)\nGenerate permutation distribution: For B iterations (typically 10,000+):\n\nRandomly permute the combined data\nSplit into groups of original sizes m and n\nRecompute the test statistic T_i\n\nCalculate p-value: \\text{p-value} \\approx \\frac{1 + \\sum_{i=1}^B I(T_i \\geq t_\\text{obs})}{B + 1}\n\nThe “+1” corrections in both numerator and denominator:\n\nPrevent p-values of exactly 0 (which would be misleading)\nMake the estimate slightly conservative but consistent (as B \\rightarrow \\infty, it converges to the true p-value)\nCan be interpreted as treating the observed data arrangement as one of B+1 total permutations (since it’s a valid permutation that always has T = t_\\text{obs})\n\n\n\nIntuitiveMathematicalComputationalThe core idea of the permutation test is quite simple: if the group\nlabels (e.g., “Treatment” vs “Control”) don’t actually matter – that is,\nif the null hypothesis is true – then shuffling these labels randomly\nshouldn’t change the distribution of our test statistic.Here’s the logic:\nUnder \\(H_0\\), the treatment and\ncontrol groups come from the same distribution\nSo the specific assignment of units to groups was just one random\npossibility\nWe can simulate other equally likely assignments by shuffling the\nlabels\nIf our observed difference is unusual compared to these shuffled\ndifferences, we have evidence against\n\\(H_0\\)\nThe mathematical foundation of the permutation test is the principle\nof exchangeability. Under the null hypothesis\n\\(H_0: F_X = F_Y\\), all observations\n\\((X_1, \\ldots, X_m, Y_1, \\ldots, Y_n)\\)\nare independent and identically distributed (IID) from the same\nunderlying distribution. This implies that the group labels\n(“Treatment”, “Control”) are arbitrary; any permutation of the combined\ndata is equally likely to have occurred.The test leverages this property to construct an exact,\nnon-parametric reference distribution for a chosen test\nstatistic \\(T\\) under\n\\(H_0\\):\nThe Permutation Distribution: Conceptually, we\nconsider the set of all \\(N! = (m+n)!\\)\npossible permutations of the combined data. For each permutation, we\ncompute the test statistic. The set of these\n\\(N!\\) values forms the exact\ndistribution of \\(T\\) under the null\nhypothesis, conditional on the observed data values.\nExactness: Because this distribution is derived\ndirectly from the data without asymptotic approximations, the\npermutation test is an exact test. This means that for\na chosen significance level \\(\\alpha\\),\nthe Type I error rate is controlled at exactly\n\\(\\alpha\\) (with proper handling of\ndiscrete data). This is a significant advantage over asymptotic tests\nlike the Wald test, which are only guaranteed to have the correct size\nas \\(n \\to \\infty\\).\nFormal p-value: The exact p-value is the\nproportion of permutations that yield a test statistic value as extreme\nor more extreme than the one observed with the original data labelling:\n\\[ \\text{p-value} = \\frac{\\#\\{\\text{permutations } \\pi : T(\\pi(\\text{data})) \\geq t_\\text{obs}\\}}{N!} \\]\nMonte Carlo Approximation: Since calculating all\n\\(N!\\) statistics is computationally\ninfeasible, the algorithm in the “Computational” tab uses Monte Carlo\nsampling to approximate this exact p-value. By drawing a large number of\nrandom permutations (\\(B\\)), we create\nan empirical distribution that converges to the true permutation\ndistribution as\n\\(B \\to \\infty\\).\nLet’s implement a permutation test and see it in action. We’ll test\nwhether two groups have different means, comparing our permutation test\nresults with the standard parametric t-test. This demonstrates both how\nthe test works and when it differs from classical approaches.Since evaluating all \\(N!\\)\npermutations is computationally infeasible for realistic sample sizes\n(recall that 20! ≈ 2.4 × 10^18), we’ll use Monte Carlo sampling to\napproximate the permutation distribution with\n\\(B = 10,000\\) random permutations.\nimport numpy as np\nfrom scipy import stats\n\ndef permutation_test(x, y, n_permutations=10000):\n    \"\"\"\n    Perform a two-sample permutation test.\n    H0: The two samples come from the same distribution.\n    Test statistic: Absolute difference in means.\n    \"\"\"\n    # Observed test statistic\n    t_obs = abs(np.mean(x) - np.mean(y))\n    \n    # Combine the data\n    combined = np.concatenate([x, y])\n    n_x = len(x)\n    \n    # Generate permutations and compute test statistics\n    t_perm = []\n    np.random.seed(42)  # For reproducibility\n    \n    for _ in range(n_permutations):\n        # Randomly permute the combined data\n        permuted = np.random.permutation(combined)\n        # Split into two groups of original sizes\n        x_perm = permuted[:n_x]\n        y_perm = permuted[n_x:]\n        # Compute test statistic for this permutation\n        t = abs(np.mean(x_perm) - np.mean(y_perm))\n        t_perm.append(t)\n    \n    # Calculate p-value (with +1 correction)\n    # Adding 1 to numerator and denominator for unbiased estimate\n    p_value = (np.sum(np.array(t_perm) &gt;= t_obs) + 1) / (n_permutations + 1)\n    \n    return t_obs, t_perm, p_value\n\n# Example: Compare two small samples\nnp.random.seed(123)\ngroup1 = np.random.normal(100, 15, 12)  # Mean 100\ngroup2 = np.random.normal(110, 15, 10)  # Mean 110\n\nt_obs, t_perm, p_value = permutation_test(group1, group2)\n\nprint(f\"Observed difference in means: {t_obs:.2f}\")\nprint(f\"Permutation p-value: {p_value:.4f}\")\n\n# For comparison, also run parametric t-test\nt_stat, p_parametric = stats.ttest_ind(group1, group2)\nprint(f\"Parametric t-test p-value: {p_parametric:.4f}\")\n\nObserved difference in means: 26.31\nPermutation p-value: 0.0016\nParametric t-test p-value: 0.0017\n\nLet’s visualize the permutation distribution to see what we’ve\ncreated:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 4))\nplt.hist(t_perm, bins=50, density=True, alpha=0.7, \n         color='blue', edgecolor='black', label='Permutation distribution')\nplt.axvline(t_obs, color='red', linestyle='--', linewidth=2, \n            label=f'Observed statistic = {t_obs:.2f}')\n\n# Add text annotation for p-value\nplt.text(t_obs + 1, plt.ylim()[1] * 0.8, \n         f'p-value = {p_value:.3f}', \n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.xlabel('Test Statistic (|difference in means|)')\nplt.ylabel('Density')\nplt.title('Permutation Test: Distribution Under H₀')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\nWhat this demonstration shows:\nThe permutation distribution shows typical chance\ndifferences: Under \\(H_0\\), we\nsee what absolute differences in means we’d expect purely by chance when\nrandomly assigning labels. Since we use the absolute difference, all\nvalues are positive, with most falling between 0 and 20.\nOur observed statistic falls in the far right\ntail: The red dashed line shows our actual observed difference\nis larger than what we’d typically see by chance. The p-value annotation\nshows that only about 0.2% of permutations produce a difference this\nlarge or larger, providing evidence against\n\\(H_0\\).\nAgreement with the parametric test: Both the\npermutation test (p ≈ 0.016) and the t-test (p ≈ 0.017) reach similar\nconclusions. This is reassuring when assumptions hold, but the\npermutation test would still be valid even if normality assumptions were\nviolated.\nThe Monte Carlo approximation works well: With\n10,000 permutations, we get a smooth estimate of the true permutation\ndistribution, making the test both computationally feasible and\nstatistically reliable. The distribution has the characteristic shape\nfor an absolute difference statistic – starting near zero and skewing\nright – but the permutation test makes no assumptions about this\nshape.\n\n\n\n\n\n\n\nUse \\geq not &gt; for discrete distributions in permutation tests\n\n\n\nWasserman (2013) uses I(T_j &gt; t_\\text{obs}) but this is incorrect for discrete distributions. The definition of p-value includes equality: the probability of observing a value “as extreme or more extreme.” For continuous distributions this makes no difference, but for discrete cases (including permutation tests) we must use \\geq.\nExample illustrating why this matters:\nConsider a tiny dataset: X = (1, 9) and Y = (3). We want to test if the means are different using T = |\\bar{X} - \\bar{Y}| as our test statistic.\n\nObserved: \\bar{X} = 5, \\bar{Y} = 3, so t_\\text{obs} = |5 - 3| = 2\n\nNow let’s enumerate all 6 possible permutations:\n\n\n\n\n\n\n\n\n\n\n\nPermutation\nX values\nY value\n\\bar{X}\n\\bar{Y}\nT = |\\bar{X} - \\bar{Y}|\n\n\n\n\nOriginal\n(1, 9)\n(3)\n5\n3\n2\n\n\n2\n(9, 1)\n(3)\n5\n3\n2\n\n\n3\n(1, 3)\n(9)\n2\n9\n7\n\n\n4\n(3, 1)\n(9)\n2\n9\n7\n\n\n5\n(3, 9)\n(1)\n6\n1\n5\n\n\n6\n(9, 3)\n(1)\n6\n1\n5\n\n\n\nComputing the p-value:\n\nCorrect (using \\geq): \\mathbb{P}(T \\geq 2) = 6/6 = 1.0 (all permutations have T \\geq 2)\nIncorrect (using &gt;): \\mathbb{P}(T &gt; 2) = 4/6 = 0.67 (only 4 permutations have T &gt; 2)\n\nThe correct p-value of 1.0 tells us our observed difference is the smallest possible – not evidence against H_0 at all! The incorrect formula would suggest some evidence against the null, which is completely wrong.\nThis example shows why the correct formula must use I(T_j \\geq t_\\text{obs}).\n\n\n\n\n7.5.2 Fisher’s Exact Test: An Exact Approach\nFisher’s exact test exemplifies the exact distribution approach. It provides the exact probability of observing data as extreme or more extreme than what we observed, given fixed marginal totals in a 2×2 contingency table. This is particularly valuable for small sample sizes where asymptotic approximations may fail.\nTo illustrate, let’s return to the motivating drug trial problem from the chapter introduction:\n\n\n\n\n\n\nApplication: The Drug Trial\n\n\n\nRecall our 2×2 table of outcomes:\n\n\n\n\nBetter\nNot Better\n\n\n\n\nTreated\n50\n50\n\n\nControl\n40\n60\n\n\n\nMany different statistical tests could be applied in this setting – we could use a two-sample test of proportions (Wald test), a chi-squared test, or a permutation test. However, for 2×2 contingency tables with modest sample sizes, Fisher’s Exact Test is one of the more attractive alternatives. It calculates the exact probability of observing a table as extreme or more extreme than this, given fixed marginal totals.\n\n\nShow code\nimport scipy.stats as stats\n\n# Create the contingency table\ntable = [[50, 50],  # Treated: 50 better, 50 not better\n         [40, 60]]  # Control: 40 better, 60 not better\n\n# Fisher's exact test\n# alternative=\"greater\" tests if treatment has higher \"better\" rate\nodds_ratio, p_value = stats.fisher_exact(table, alternative=\"greater\")\n\nprint(f\"Contingency table:\")\nprint(f\"         Better  Not Better\")\nprint(f\"Treated:   50        50\")\nprint(f\"Control:   40        60\")\nprint(f\"\\nOdds ratio: {odds_ratio:.3f}\")\nprint(f\"One-sided p-value: {p_value:.4f}\")\n\n# Also try two-sided test\n_, p_two_sided = stats.fisher_exact(table, alternative=\"two-sided\")\nprint(f\"Two-sided p-value: {p_two_sided:.4f}\")\n\nif p_value &lt; 0.05:\n    print(\"\\nConclusion: Significant evidence of treatment effect (p &lt; 0.05)\")\nelse:\n    print(\"\\nConclusion: No significant evidence of treatment effect at α = 0.05\")\n\n\nContingency table:\n         Better  Not Better\nTreated:   50        50\nControl:   40        60\n\nOdds ratio: 1.500\nOne-sided p-value: 0.1004\nTwo-sided p-value: 0.2007\n\nConclusion: No significant evidence of treatment effect at α = 0.05\n\n\nWith p = 0.10, we don’t have strong evidence to reject the null hypothesis at the conventional α = 0.05 level. The observed 10 percentage point difference could plausibly arise by chance.\nThis example illustrates a key point: even seemingly large differences (50% vs 40% success rate) may not be statistically significant with modest sample sizes. Power analysis before conducting studies is crucial!\nMany other applications share this same 2×2 structure:\n\nA/B testing: Comparing conversion rates between website designs\nDemographics: Testing differences in proportions between groups\n\nMedical screening: Comparing test accuracy between methods\nQuality control: Comparing defect rates between processes\n\nWhenever you’re comparing binary outcomes between two groups, you face the same statistical question: is the observed difference real or just chance?\n\n\n\n\n7.5.3 The Likelihood Ratio Test: A General Asymptotic Approach\nThe Wald test is useful for testing a scalar parameter. The likelihood ratio test is more general and can be used for testing a vector-valued parameter, making it one of the most important tools in statistical inference.\n\nLikelihood Ratio Test: For testing H_0: \\theta \\in \\Theta_0 versus H_1: \\theta \\notin \\Theta_0:\nThe likelihood ratio statistic is: \\lambda = 2 \\log \\left( \\frac{\\sup_{\\theta \\in \\Theta} \\mathcal{L}(\\theta)}{\\sup_{\\theta \\in \\Theta_0} \\mathcal{L}(\\theta)} \\right) = 2[\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0)]\nwhere:\n\n\\hat{\\theta} is the MLE over the entire parameter space \\Theta\n\\hat{\\theta}_0 is the MLE under the constraint \\theta \\in \\Theta_0\n\n\n\n\n\n\n\n\nWhy use \\Theta instead of \\Theta_0^c in the numerator?\n\n\n\nYou might expect to maximize over \\Theta_0^c (the alternative hypothesis space) in the numerator. However:\n\nUsing \\Theta has little practical effect on the test statistic\nThe theoretical properties are much simpler with this definition\nIt ensures the statistic is always non-negative\n\n\n\nThe LRT is most useful when \\Theta_0 is defined by constraining some parameters to fixed values. For example, if \\theta = (\\theta_1, \\ldots, \\theta_r) and we want to test that the last r-q components equal specific values: \\Theta_0 = \\{\\theta: (\\theta_{q+1}, \\ldots, \\theta_r) = (\\theta_{0,q+1}, \\ldots, \\theta_{0,r})\\}\n\nUnder H_0: \\theta \\in \\Theta_0, the likelihood ratio statistic has an asymptotic chi-squared distribution: \\lambda \\rightsquigarrow \\chi^2_{r-q}\nwhere r-q is the difference in dimensionality between \\Theta and \\Theta_0 (the number of constraints imposed by H_0).\nThe p-value is: \\mathbb{P}(\\chi^2_{r-q} \\geq \\lambda)\n\n\n\n\n\n\n\nExample: Counting Degrees of Freedom\n\n\n\nIf \\theta = (\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5) and we test: H_0: \\theta_4 = \\theta_5 = 0\nThen:\n\nDimension of \\Theta = 5 (all parameters free)\nDimension of \\Theta_0 = 3 (only \\theta_1, \\theta_2, \\theta_3 free)\nDegrees of freedom = 5 - 3 = 2\n\nThe test statistic \\lambda \\rightsquigarrow \\chi^2_2 under H_0.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#the-multiple-testing-problem-the-peril-of-many-tests",
    "href": "chapters/07-hypothesis-testing.html#the-multiple-testing-problem-the-peril-of-many-tests",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.6 The Multiple Testing Problem: The Peril of Many Tests",
    "text": "7.6 The Multiple Testing Problem: The Peril of Many Tests\n\n7.6.1 The Problem\nModern data science often involves testing many hypotheses simultaneously. Consider these scenarios:\n\nGenomics: Testing thousands of genes for association with disease\nA/B testing: Running dozens of experiments across a website\nNeuroscience: Testing brain activity at thousands of voxels\nFeature selection: Testing which of hundreds of features predict an outcome\n\nThe problem is simple but severe: If you perform many tests, you’re virtually guaranteed to get false positives by chance alone.\nLet’s quantify this:\n\nOne test at α = 0.05: 5% chance of a Type I error\n20 independent tests at α = 0.05 each:\n\nProbability of at least one Type I error = 1 - (0.95)^{20} \\approx 0.64\nThat’s a 64% chance of at least one false positive!\n\n1000 tests: Virtually certain to get many false positives\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate the multiple testing problem\nnp.random.seed(42)\nn_tests = [1, 5, 10, 20, 50, 100, 200, 500, 1000]\nalpha = 0.05\n\n# Calculate probability of at least one false positive\nprob_no_false_positive = [(1 - alpha)**m for m in n_tests]\nprob_at_least_one_fp = [1 - p for p in prob_no_false_positive]\n\n# Expected number of false positives\nexpected_fp = [m * alpha for m in n_tests]\n\nplt.figure(figsize=(7, 4))\nplt.subplot(1, 2, 1)\nplt.plot(n_tests, prob_at_least_one_fp, 'b-', linewidth=2, marker='o')\nplt.axhline(y=alpha, color='r', linestyle='--', alpha=0.7, label=f'α = {alpha}')\nplt.xlabel('Number of Tests')\nplt.ylabel('P(At least one false positive)')\nplt.title('Probability of False Discoveries')\nplt.xscale('log')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(n_tests, expected_fp, 'g-', linewidth=2, marker='s')\nplt.xlabel('Number of Tests')\nplt.ylabel('Expected # of False Positives')\nplt.title('Expected False Positives')\nplt.xscale('log')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n7.6.2 Key Concepts for Multiple Testing\nWhen conducting m hypothesis tests, we need to understand what types of errors we want to control. Consider this classification table of test outcomes:\n\n\n\n\nH_0 Not Rejected\nH_0 Rejected\nTotal\n\n\n\n\nH_0 True\nU\nV\nm_0\n\n\nH_0 False\nT\nS\nm_1\n\n\nTotal\nm-R\nR\nm\n\n\n\nWhere:\n\nV = Number of false positives (Type I errors)\n\nS = Number of true positives (correct rejections)\nR = V + S = Total rejections\nm_0 = Number of true null hypotheses\n\n\nFamily-Wise Error Rate (FWER): The probability of making at least one Type I error among all tests: \\text{FWER} = \\mathbb{P}(V \\geq 1)\nFalse Discovery Proportion (FDP): The proportion of rejections that are false: \\text{FDP} = \\begin{cases}\n\\frac{V}{R} & \\text{if } R &gt; 0 \\\\\n0 & \\text{if } R = 0\n\\end{cases}\nFalse Discovery Rate (FDR): The expected false discovery proportion: \\text{FDR} = \\mathbb{E}[\\text{FDP}]\n\nThese quantities represent different philosophies for error control:\n\nFWER control is very conservative, aiming to avoid any false positives.\nFDR control is more liberal and often more sensible in practical applications, accepting some false positives but controlling their overall proportion.\n\n\n\n7.6.3 The Bonferroni Method: Controlling FWER\nThe Bonferroni correction provides the simplest and most conservative approach to multiple testing.\n\nBonferroni Method: Given p-values P_1, \\ldots, P_m, reject null hypothesis H_{0i} if: P_i &lt; \\frac{\\alpha}{m}\n\nIntuition: We divide our total error budget \\alpha equally among all m tests. If each test has Type I error rate \\alpha/m, the total error rate can’t exceed \\alpha.\n\nUsing the Bonferroni method, the probability of falsely rejecting any null hypothesis is at most \\alpha: \\text{FWER} \\leq \\alpha\nProof: Let A_i be the event that test i rejects when H_{0i} is true. Then: \\text{FWER} = \\mathbb{P}(\\bigcup_{i \\in I_0} A_i) \\leq \\sum_{i \\in I_0} \\mathbb{P}(A_i) \\leq \\sum_{i \\in I_0} \\frac{\\alpha}{m} \\leq \\alpha where I_0 is the set of true null hypotheses.\n\nPros:\n\nSimple to implement\n\nProvides strong control of Type I errors\nWorks for any dependency structure among tests\n\nCons:\n\nExtremely conservative: Dramatically reduces power\nOften fails to detect true effects\n\nBecomes nearly useless with thousands of tests\n\n\n\n7.6.4 The Benjamini-Hochberg Method: Controlling FDR\nThe Benjamini-Hochberg (BH) procedure offers a more powerful alternative by controlling the proportion of false discoveries rather than the probability of any false discovery.\n\nBenjamini-Hochberg (BH) Procedure:\n\nLet P_{(1)} \\leq P_{(2)} \\leq \\ldots \\leq P_{(m)} denote the ordered p-values\nDefine: \\ell_i = \\frac{i \\alpha}{C_m m} \\quad \\text{and} \\quad R = \\max\\{i: P_{(i)} \\leq \\ell_i\\} where C_m is defined as:\n\nC_m = 1 if the p-values are independent\nC_m = \\sum_{i=1}^m \\frac{1}{i} otherwise (for dependent tests)\n\nLet T = P_{(R)} be the BH rejection threshold\nReject all null hypotheses H_{0i} for which P_i \\leq T\n\n\nIntuition: The procedure finds the largest set of rejections such that the expected proportion of false discoveries is controlled. In practice, C_m = 1 is almost always used (assuming independence), so the threshold for the i-th smallest p-value is \\ell_i = i\\alpha/m. The procedure looks for the rightmost p-value that falls below this sloped line.\nThe procedure can be visualized by plotting ordered p-values against their threshold line:\n\n\nShow code\n# Visualize BH procedure with a clearer example\nnp.random.seed(42)\nm = 20  # Number of tests\nalpha_fdr = 0.05\n\n# Generate p-values designed to show BH advantage\n# With α=0.05 and m=20: Bonferroni threshold = 0.0025, BH thresholds = i*0.0025\np_values = np.array([\n    0.0008, 0.0045, 0.0068,  # BH will reject these 3 (below 0.0025, 0.005, 0.0075)\n    0.012, 0.024, 0.041, 0.063, 0.089,  # Won't be rejected\n    0.115, 0.181, 0.224, 0.301,  # Medium p-values\n    0.412, 0.501, 0.578, 0.656,  # Larger p-values  \n    0.721, 0.812, 0.888, 0.951  # Very large p-values\n])\np_sorted = np.sort(p_values)\n\n# BH threshold line (using C_m = 1 for independent tests)\nk_values = np.arange(1, m+1)\nbh_threshold = k_values * alpha_fdr / m  # This is ℓ_i with C_m = 1\n\n# Find BH cutoff using the standard definition\n# R = max{i: P_(i) ≤ ℓ_i}\nbh_check = p_sorted &lt;= bh_threshold\nif np.any(bh_check):\n    R = np.max(np.where(bh_check)[0]) + 1  # +1 because Python uses 0-indexing\n    T = p_sorted[R-1]  # BH rejection threshold\nelse:\n    R = 0\n    T = 0\n\nplt.figure(figsize=(7, 4))\n\n# Plot p-values and thresholds\nplt.scatter(k_values, p_sorted, color='blue', s=60, label='Sorted p-values', zorder=3)\nplt.plot(k_values, bh_threshold, 'r--', linewidth=2, label=f'BH threshold (α={alpha_fdr})', zorder=2)\nplt.plot(k_values, [alpha_fdr/m]*m, 'g--', alpha=0.7, linewidth=1.5, label=f'Bonferroni threshold', zorder=2)\n\nif R &gt; 0:\n    # Highlight rejected p-values\n    plt.scatter(k_values[:R], p_sorted[:R], \n                color='red', s=100, marker='s', label='Rejected', zorder=4)\n    # Mark the cutoff point R\n    plt.axvline(R + 0.5, color='black', linestyle=':', alpha=0.4, linewidth=1)\n    plt.text(R + 0.7, 0.15, f'R = {R}', fontsize=9, color='black')\n\n# Formatting\nplt.xlabel('Rank i')\nplt.ylabel('p-value (log scale)')\nplt.title('Benjamini-Hochberg Procedure')\nplt.legend(loc='upper left', framealpha=0.95)\nplt.grid(True, alpha=0.3, linewidth=0.5, which='both')\n\n# Fix axes\nplt.xticks(range(0, m+1, 5))  # Show every 5th rank\nplt.xlim(0, m+1)  # Proper bounds for discrete ranks\n\n# Use log scale for y-axis\nplt.yscale('log')\nplt.ylim(0.0005, 1.0)  # Start just below smallest p-value\n\nplt.tight_layout()\nplt.show()\n\nif R &gt; 0:\n    print(f\"BH procedure:\")\n    print(f\"  R = {R} (largest i where P_(i) &lt; i·α/m)\")\n    print(f\"  Rejection threshold T = P_({R}) = {T:.4f}\")\n    print(f\"  Rejects {R} hypotheses (p-values ≤ {T:.4f})\")\n    print(f\"\\nBonferroni would reject {np.sum(p_sorted &lt; alpha_fdr/m)} hypotheses\")\n    print(f\"  (only those with p &lt; {alpha_fdr/m:.4f})\")\nelse:\n    print(\"No hypotheses rejected by either method\")\n\n\n\n\n\n\n\n\n\nBH procedure:\n  R = 3 (largest i where P_(i) &lt; i·α/m)\n  Rejection threshold T = P_(3) = 0.0068\n  Rejects 3 hypotheses (p-values ≤ 0.0068)\n\nBonferroni would reject 1 hypotheses\n  (only those with p &lt; 0.0025)\n\n\nThe visualization shows the key difference between methods:\n\nBonferroni (flat green line): Same strict threshold for all tests\nBH (sloped red line): More lenient threshold for higher-ranked p-values\nThe BH procedure finds the rightmost crossing of p-values below the sloped line\n\n\nWhen the BH procedure is applied with the appropriate C_m: \\text{FDR} = \\mathbb{E}[\\text{FDP}] \\leq \\frac{m_0}{m}\\alpha \\leq \\alpha\nwhere m_0 is the number of true null hypotheses.\nThis guarantee holds:\n\nWith C_m = 1 when the test statistics are independent\nWith C_m = \\sum_{i=1}^m \\frac{1}{i} for arbitrary dependence structures\n\nIn practice, C_m = 1 is almost always used as the procedure is remarkably robust to many forms of dependence.\n\nComparison of Methods:\n\n\n\n\n\n\n\n\n\n\nMethod\nControls\nDecision Rule\nPower\nUse Case\n\n\n\n\nBonferroni\nFWER \\leq \\alpha\nReject if P_i &lt; \\alpha/m\nLow\nCritical applications\n\n\nBenjamini-Hochberg\nFDR \\leq \\alpha\nReject if P_i \\leq T = P_{(R)}*\nHigher\nLarge-scale testing\n\n\n\n*Where R = \\max\\{i: P_{(i)} \\leq i\\alpha/(C_m m)\\} with C_m = 1 for independent tests\n\n\n\n\n\n\nAdjusted p-values: An Alternative Presentation\n\n\n\n\n\nThe BH procedure can also be presented using adjusted p-values, which is how many software packages (including scipy.stats.false_discovery_control) implement it. This might seem like a completely different method, but it’s just a reformulation of the same procedure.\nAdjusted p-values: For the BH method, the adjusted p-value is computed in two steps:\n\nFirst, scale each p-value: \\tilde{P}'_{(i)} = \\min\\left(1, \\frac{m}{i} P_{(i)}\\right)\nThen enforce monotonicity (working from largest to smallest): \\tilde{P}_{(i)} = \\min_{j \\geq i} \\tilde{P}'_{(j)}\n\nThis ensures adjusted p-values are non-decreasing: \\tilde{P}_{(1)} \\leq \\tilde{P}_{(2)} \\leq \\ldots \\leq \\tilde{P}_{(m)}.\nAfter computing adjusted p-values, reject all hypotheses where \\tilde{P}_i \\leq \\alpha.\nThis is mathematically equivalent to the threshold approach but ensures the adjusted p-values maintain proper ordering.\n\n\nShow code\nimport scipy.stats as stats\n\n# Example p-values\npvals = [0.003, 0.02, 0.04, 0.08, 0.15, 0.25]\nm = len(pvals)\n\n# Using scipy's function (returns adjusted p-values)\nadjusted = stats.false_discovery_control(pvals, method='bh')\n\nprint(\"Original vs Adjusted p-values:\")\nprint(\"-\" * 40)\nfor i, (p, adj) in enumerate(zip(pvals, adjusted)):\n    # Step 1: Scale the p-value\n    scaled = min(1.0, (m/(i+1)) * p)\n    print(f\"p[{i+1}] = {p:.3f} → scaled = {scaled:.3f} → adjusted = {adj:.3f}\")\n\nprint(\"\\nNote: adjusted values enforce monotonicity\")\nprint(\"(each adjusted p-value ≥ previous one)\")\n\nprint(f\"\\nAt α = 0.05:\")\nprint(f\"Reject hypotheses where adjusted p-value ≤ 0.05\")\nprint(f\"Result: Reject first {sum(adjusted &lt;= 0.05)} hypotheses\")\n\n\nOriginal vs Adjusted p-values:\n----------------------------------------\np[1] = 0.003 → scaled = 0.018 → adjusted = 0.018\np[2] = 0.020 → scaled = 0.060 → adjusted = 0.060\np[3] = 0.040 → scaled = 0.080 → adjusted = 0.080\np[4] = 0.080 → scaled = 0.120 → adjusted = 0.120\np[5] = 0.150 → scaled = 0.180 → adjusted = 0.180\np[6] = 0.250 → scaled = 0.250 → adjusted = 0.250\n\nNote: adjusted values enforce monotonicity\n(each adjusted p-value ≥ previous one)\n\nAt α = 0.05:\nReject hypotheses where adjusted p-value ≤ 0.05\nResult: Reject first 1 hypotheses\n\n\nThe adjusted p-values tell you: “This is the smallest α level at which this hypothesis would be rejected by the BH procedure.”\n\n\n\n\n\n7.6.5 Practical Recommendations\n\n\n\n\n\n\nWhen to Use Which Method\n\n\n\nUse Bonferroni when:\n\nYou have a small number of tests (&lt; 20)\nThe cost of any false positive is very high\nYou need to convince skeptical reviewers\n\nUse Benjamini-Hochberg when:\n\nYou have many tests (genomics, neuroimaging, etc.)\nSome false positives are acceptable\nYou want to generate hypotheses for follow-up\n\nUse no correction when:\n\nTests are explicitly exploratory\nYou’re doing hypothesis generation, not confirmation\nBut always report that no correction was applied!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#nhst-in-practice-a-critical-view",
    "href": "chapters/07-hypothesis-testing.html#nhst-in-practice-a-critical-view",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.7 NHST in Practice: A Critical View",
    "text": "7.7 NHST in Practice: A Critical View\nDespite its ubiquity in scientific research, NHST has fundamental limitations that have contributed to what many call the replication crisis – the inability to reproduce many published scientific findings.\n\n7.7.1 Fundamental Problems with NHST\nThe Focus on the Null: NHST tells us whether to reject H_0, but doesn’t require us to specify a realistic alternative. We test against “no effect” without having to say what effect we actually expect. This leads to vague research questions and post-hoc rationalization of any significant finding.\nIgnoring Prior Plausibility: NHST treats all hypotheses equally – the same p &lt; 0.05 is required whether testing a well-established biological mechanism or claiming telepathy exists. As the saying goes, “extraordinary claims require extraordinary evidence,” but NHST doesn’t account for this. This is a key limitation addressed by Bayesian methods (Chapter 8).\nThe Dichotomy Problem: The division into “significant” and “non-significant” at α = 0.05 is entirely arbitrary. As statistician Andrew Gelman notes:\n\n“The difference between ‘significant’ and ‘not significant’ is not itself statistically significant!”\n\nA result with p = 0.049 is treated fundamentally differently from p = 0.051, despite being practically identical.\n\n\n7.7.2 The Low Power Trap\nLow-powered studies create a vicious cycle:\n\nThey often fail to detect true effects (high Type II error)\nWhen they do find “significant” results, these are more likely to be false positives\nCrucially: In low-power settings, most published “significant” findings are false positives\n\nThis counterintuitive result occurs because with low power, the few significant results that emerge are disproportionately likely to be statistical flukes rather than real effects.\n\n\n7.7.3 Common Misuses\np-hacking (Data Dredging):\n\nTesting multiple hypotheses until finding p &lt; 0.05\nStopping data collection when significance is reached\nTrying different analyses until one “works”\nExcluding “outliers” post-hoc to achieve significance\n\nHARKing (Hypothesizing After Results are Known): Looking at the data first, then pretending the observed pattern was the hypothesis all along.\nPublication Bias: Journals preferentially publish “significant” results, creating a distorted scientific literature where negative results disappear. This file-drawer problem means the published record overestimates effect sizes and underestimates uncertainty.\n\n\n7.7.4 Moving Forward: Better Practices\nPractical Recommendations:\n\nAlways report effect sizes and confidence intervals, not just p-values\nConduct power analyses before collecting data\nPre-register hypotheses and analysis plans to prevent p-hacking\nConsider Bayesian methods when prior information exists\nUse appropriate multiple testing corrections when testing many hypotheses\nReport all analyses attempted, not just the “significant” ones\n\nAlternative Approaches:\n\nBayesian inference (Chapter 8): Incorporates prior knowledge and provides probability statements about hypotheses\nEstimation-focused analysis: Emphasize effect sizes and uncertainty rather than binary decisions\nReplication studies: The ultimate test of a finding’s validity",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#chapter-summary",
    "href": "chapters/07-hypothesis-testing.html#chapter-summary",
    "title": "7  Hypothesis Testing and p-values",
    "section": "7.8 Chapter Summary",
    "text": "7.8 Chapter Summary\n\n7.8.1 Key Concepts Review\nWe’ve explored the foundations of null-hypothesis significance testing (NHST):\nThe Framework:\n\nNull and alternative hypotheses: H_0 (no effect) vs H_1 (effect exists)\nType I and Type II errors: False positives vs false negatives\nPower and size: Probability of detecting true effects vs controlling false positives\nTest statistic and rejection region: Summarizing evidence and decision rules\n\nThe p-value:\n\nMeasures how surprising data would be under H_0\nNOT the probability that H_0 is true\nSmall p-value = evidence against H_0, not proof\nStatistical significance ≠ practical significance\n\nKey Tests:\n\nWald test: Uses asymptotic normality of estimators\nPermutation test: Non-parametric alternative requiring minimal assumptions\nFisher’s exact test: Exact test for contingency tables\nLikelihood ratio test: General framework for testing constraints on parameters\n\nMultiple Testing:\n\nRunning many tests inflates Type I error rate\nBonferroni: Controls FWER (conservative but safe)\nBenjamini-Hochberg: Controls FDR (more powerful, modern standard)\n\n\n\n7.8.2 Common Pitfalls to Avoid\n\nMisinterpreting p-values: Remember, p-value ≠ P(H_0 is true)\nMultiple testing without correction: Always consider how many tests you’re running\nConfusing statistical and practical significance: A tiny effect can be “significant” with enough data\nIgnoring assumptions: Tests like the Wald test require large samples\nPost-hoc hypothesis formulation: Don’t look at data, then formulate hypotheses to test\n\n\n\n7.8.3 Chapter Connections\n\nPrevious chapters:\n\nChapters 5-6 gave us estimators and their properties; now we test hypotheses about them\nChapter 4 (Bootstrap) provides an alternative to asymptotic tests\n\nThis chapter: Core framework for statistical inference and decision-making\nNext chapter: Bayesian inference offers an alternative paradigm that:\n\nProvides probabilities for hypotheses\nIncorporates prior information\nAvoids some NHST pitfalls\n\n\n\n\n7.8.4 Self-Test Problems\n\nUnderstanding Type I and Type II Errors\nA medical test for a disease has the following properties:\n\nIf a person has the disease, the test is positive 95% of the time\nIf a person doesn’t have the disease, the test is negative 98% of the time\n\nIn hypothesis testing terms (where H_0: person is healthy):\n\nWhat is the Type I error rate?\nWhat is the Type II error rate?\nWhat is the power of the test?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nType I error = rejecting H_0 when true = false positive = 1 - 0.98 = 0.02\nType II error = failing to reject H_0 when false = false negative = 1 - 0.95 = 0.05\nPower = 1 - Type II error = 0.95\n\n\n\n\n\nWald Test Calculation\nDeath times around Passover (from AoS Exercise 10.6): Of 1919 deaths, 922 occurred the week before Passover and 997 the week after. Test H_0: p = 0.5 where p is the probability of death in the week before.\nCalculate the Wald test statistic and p-value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy import stats\n\nn = 1919\nx = 922\np_hat = x / n\np_0 = 0.5\n\n# Wald test statistic\nse = np.sqrt(p_0 * (1 - p_0) / n)\nW = (p_hat - p_0) / se\n\n# p-value (two-sided)\np_value = 2 * stats.norm.cdf(-abs(W))\n\nprint(f\"Sample proportion: {p_hat:.4f}\")\nprint(f\"Wald statistic: {W:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"Conclusion: {'Reject H₀' if p_value &lt; 0.05 else 'Fail to reject H₀'}\")\n\n\nSample proportion: 0.4805\nWald statistic: -1.712\np-value: 0.0869\nConclusion: Fail to reject H₀\n\n\n\n\n\n\nMultiple Testing Correction\nYou run 10 hypothesis tests and get these p-values:\n0.001, 0.004, 0.012, 0.025, 0.041, 0.053, 0.074, 0.135, 0.246, 0.531\nAt α = 0.05, which hypotheses are rejected using:\n\nNo correction?\nBonferroni correction?\nBenjamini-Hochberg correction?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nShow code\np_values = np.array([0.001, 0.004, 0.012, 0.025, 0.041, 0.053, 0.074, 0.135, 0.246, 0.531])\nalpha = 0.05\nm = len(p_values)\n\n# No correction\nno_correction = p_values &lt;= alpha\nprint(f\"No correction: Reject hypotheses {np.where(no_correction)[0] + 1}\")\nprint(f\"  ({np.sum(no_correction)} rejections)\")\n\n# Bonferroni\nbonferroni_threshold = alpha / m\nbonferroni = p_values &lt;= bonferroni_threshold\nprint(f\"\\nBonferroni (threshold = {bonferroni_threshold:.4f}):\")\nprint(f\"  Reject hypotheses {np.where(bonferroni)[0] + 1}\")\nprint(f\"  ({np.sum(bonferroni)} rejections)\")\n\n# Benjamini-Hochberg\nsorted_idx = np.argsort(p_values)\nsorted_p = p_values[sorted_idx]\nbh_threshold = (np.arange(1, m+1) / m) * alpha\nbh_reject_sorted = sorted_p &lt;= bh_threshold\nif np.any(bh_reject_sorted):\n    k_max = np.max(np.where(bh_reject_sorted)[0])\n    bh_reject = np.zeros(m, dtype=bool)\n    bh_reject[sorted_idx[:k_max+1]] = True\nelse:\n    bh_reject = np.zeros(m, dtype=bool)\n\nprint(f\"\\nBenjamini-Hochberg:\")\nprint(f\"  Reject hypotheses {np.where(bh_reject)[0] + 1}\")\nprint(f\"  ({np.sum(bh_reject)} rejections)\")\n\n\nNo correction: Reject hypotheses [1 2 3 4 5]\n  (5 rejections)\n\nBonferroni (threshold = 0.0050):\n  Reject hypotheses [1 2]\n  (2 rejections)\n\nBenjamini-Hochberg:\n  Reject hypotheses [1 2 3]\n  (3 rejections)\n\n\n\n\n\n\nPermutation Test vs Parametric Test\nWhen would you prefer a permutation test over a Wald test? Give at least three scenarios.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPrefer permutation tests when:\n\nSmall sample size: Asymptotic approximations may not hold\nNon-standard distributions: Data is heavily skewed or has outliers\nComplex test statistics: No known distribution for the statistic\nExact p-values needed: Critical decisions requiring exact inference\nAssumptions violated: Independence or normality assumptions fail\n\n\n\n\n\n\n7.8.5 Python and R Reference\nPythonR#| eval: false\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.stats.multitest as multitest\n\n# Wald test for single proportion\ndef wald_test_proportion(x, n, p0):\n    \"\"\"Test H0: p = p0\"\"\"\n    p_hat = x / n\n    se = np.sqrt(p0 * (1 - p0) / n)\n    W = (p_hat - p0) / se\n    p_value = 2 * stats.norm.cdf(-abs(W))\n    return W, p_value\n\n# Two-sample t-test (Wald for means)\nt_stat, p_value = stats.ttest_ind(group1, group2)\n\n# Paired t-test\nt_stat, p_value = stats.ttest_rel(sample1, sample2)\n\n# Fisher's exact test for 2x2 tables\nodds_ratio, p_value = stats.fisher_exact([[a, b], [c, d]])\n\n\n# Permutation test\ndef permutation_test(x, y, n_permutations=10000):\n    observed = np.abs(np.mean(x) - np.mean(y))\n    combined = np.concatenate([x, y])\n    count = 0\n    for _ in range(n_permutations):\n        np.random.shuffle(combined)\n        x_perm = combined[:len(x)]\n        y_perm = combined[len(x):]\n        if np.abs(np.mean(x_perm) - np.mean(y_perm)) &gt;= observed:\n            count += 1\n    return (count + 1) / (n_permutations + 1)\n\n# Multiple testing corrections\nfrom statsmodels.stats.multitest import multipletests\n\n# Bonferroni and BH corrections\nreject_bonf, pvals_bonf, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')\nreject_bh, pvals_bh, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n\n# Alternative: Using scipy (newer versions)\nfrom scipy.stats import false_discovery_control\n# This adjusts p-values using Benjamini-Hochberg\nadjusted_pvals = false_discovery_control(p_values, method='bh')\n# Reject hypotheses where adjusted p-value &lt;= alpha\nreject_scipy_bh = adjusted_pvals &lt;= 0.05#| eval: false\n# Wald test for single proportion\nprop.test(x = 50, n = 100, p = 0.5)\n\n# Two-sample t-test\nt.test(group1, group2)\n\n# Paired t-test\nt.test(sample1, sample2, paired = TRUE)\n\n# Fisher's exact test\nfisher.test(matrix(c(a, b, c, d), nrow = 2))\n\n\n# Permutation test (using coin package)\nlibrary(coin)\nindependence_test(outcome ~ group, data = df, \n                  distribution = approximate(nresample = 10000))\n\n# Multiple testing corrections\np.adjust(p_values, method = \"bonferroni\")\np.adjust(p_values, method = \"BH\")  # Benjamini-Hochberg\n\n# Or using multtest package\nlibrary(multtest)\nmt.rawp2adjp(p_values, proc = c(\"Bonferroni\", \"BH\"))\n\n\n7.8.6 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding Source(s)\n\n\n\n\nIntroduction\nLecture slides; drug trial example from slides\n\n\nFramework of Hypothesis Testing\nAoS Ch 10 Intro\n\n\n↳ Null and Alternative Hypotheses\nAoS Ch 10 Intro\n\n\n↳ Type I and Type II Errors\nAoS Table 10.1, Definition 10.1\n\n\n↳ Power and Size\nAoS Definition 10.1\n\n\nThe p-value\nAoS §10.2\n\n\n↳ Understanding the p-value\nAoS §10.2 (Definition 10.11, Theorem 10.12)\n\n\n↳ Interpretation and Misinterpretation\nAoS §10.2, expanded with modern critiques\n\n\nHypothesis Tests\n\n\n\n↳ The Wald Test\nAoS §10.1 (Definition 10.3, Theorem 10.4)\n\n\n↳ Statistical and Scientific Significance\nAoS §10.1, Theorem 10.10 (CI duality)\n\n\n↳ Comparing Proportions/Means\nAoS §10.1 (Examples 10.7, 10.8)\n\n\n↳ The Permutation Test\nAoS §10.5 (Example 10.19)\n\n\n↳ Fisher’s Exact Test\nExpanded from slides\n\n\n↳ The Likelihood Ratio Test\nAoS §10.6\n\n\nMultiple Testing Problem\nAoS §10.7\n\n\n↳ Bonferroni Correction\nAoS §10.7 (Theorem 10.24)\n\n\n↳ Benjamini-Hochberg\nAoS §10.7 (Theorem 10.26)\n\n\nNHST in Practice: A Critical View\nExpanded from slides\n\n\nSelf-Test Problems\nBased on AoS Exercise 10.6 and similar examples\n\n\n\n\n\n\n\n\n7.8.7 Further Reading\n\nModern perspective: Wasserstein & Lazar (2016), “The ASA Statement on p-Values”\nCritical view: Ioannidis (2005), “Why Most Published Research Findings Are False”\n\n\nRemember: Hypothesis testing is a tool for making decisions under uncertainty. Use it wisely – report effect sizes and confidence intervals, correct for multiple testing, and never forget that statistical significance is not the same as practical importance!\n\n\n\n\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/07-hypothesis-testing.html#footnotes",
    "href": "chapters/07-hypothesis-testing.html#footnotes",
    "title": "7  Hypothesis Testing and p-values",
    "section": "",
    "text": "NHST has been central to scientific research for decades, but has also been subject to criticism regarding its misuse and misinterpretation. We’ll explore both its strengths and limitations throughout this chapter, particularly in the sections on interpreting p-values and the critical view of NHST in practice.↩︎\nIn classical hypothesis testing, we never “accept” the null hypothesis – we can only keep it because we haven’t found enough evidence to reject it.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing and p-values</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html",
    "href": "chapters/08-bayesian-inference-decision-theory.html",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "",
    "text": "8.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#learning-objectives",
    "href": "chapters/08-bayesian-inference-decision-theory.html#learning-objectives",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "",
    "text": "Apply Bayes’ theorem to compute posterior distributions from prior and likelihood, and interpret credible intervals vs. confidence intervals.\nWork with conjugate models (Beta-Bernoulli, Normal-Normal) to derive posteriors and understand how data and prior beliefs combine.\nChoose appropriate priors and explain their impact on inference, particularly as sample size increases.\nUse decision theory to compare estimators via loss functions and risk, understanding why we need scalar summaries (Bayes risk, maximum risk).\nIdentify optimal estimators by connecting posterior summaries to Bayes estimators, finding minimax estimators via constant risk, and determining admissibility.\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter introduces two deeply connected topics: Bayesian inference, which provides a principled way to update beliefs with data, and statistical decision theory, which gives us a formal framework for comparing any statistical procedure. The material is adapted from Chapters 11 and 12 of Wasserman (2013) and supplemented with modern perspectives and computational examples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#introduction-a-different-way-of-thinking",
    "href": "chapters/08-bayesian-inference-decision-theory.html#introduction-a-different-way-of-thinking",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.2 Introduction: A Different Way of Thinking",
    "text": "8.2 Introduction: A Different Way of Thinking\n\n8.2.1 The Search for Air France Flight 447\nOn June 1, 2009, Air France Flight 447 vanished over the Atlantic Ocean. The Airbus A330, carrying 228 people, disappeared from radar while flying from Rio de Janeiro to Paris, leaving behind only automated messages indicating system failures. What followed was one of the most challenging search operations in aviation history – and ultimately, a powerful demonstration of how Bayesian inference succeeds by integrating multiple sources of uncertain information. This remarkable story is documented in detail in Stone et al. (2014), from which the figures and search details in this section are taken.\n\n\nModern airliners transmit their position every 10 minutes via satellite. When AF447’s transmissions stopped at 2:14 AM, it created a circular search area with a 40 nautical mile (74 km) radius – still covering over 5,000 square nautical miles of ocean.\nThe depth in this region reaches 14,000 feet, with underwater mountains and valleys making detection extremely challenging. The flight data and cockpit voice recorders, crucial for understanding what happened, emit acoustic beacons that function for about 40 days and have a detection range of about 2,000 meters.\nThis wasn’t just a search problem – it was a problem of combining uncertain, conflicting information from multiple sources.\n\n\n\n\nThe intended flight path of AF447 and the 40 NM radius circle centered on the last known position (LKP). The circle represents the maximum distance the aircraft could have traveled after its last transmission. Figure from Stone et al. (2014).\n\n\n\n\n\nThe Initial Search Efforts\nThroughout 2009 and 2010, search teams employed sophisticated statistical models including oceanographic drift analysis and confidence regions. However, each search operation focused on a single line of evidence rather than integrating all available information. These efforts, while extensive and expensive, all failed to locate the wreckage:\nSurface Search (June 2009)Acoustic Search (June 2009)Active Sonar (August 2009)Confidence Region (April-May 2010)The first phase focused on finding floating debris. After six days,\nsearch aircraft spotted debris and bodies approximately 38 NM north of\nthe last known position. Scientists then used reverse drift modeling\n(working backwards from where debris was found, using ocean current data\nto estimate where it originated) to predict where the wreckage might\nbe.Result: No wreckage found in the predicted\nareas.Teams deployed sensitive hydrophones to listen for the flight\nrecorders’ acoustic beacons. They concentrated along the intended flight\npath, reasoning the aircraft was likely on course when it crashed.\n\nThe vertical and horizontal search lines\nshowing the passive acoustic search paths for the flight recorder\nbeacons. The circles show the 20 and 40 NM radius from the last known\nposition. Figure from Stone et al. (2014).\nResult: No signals detected. The search assumed the\nbeacons were functioning – a reasonable but ultimately incorrect\nassumption.A limited side-scan sonar search was conducted south of the last\nknown position in areas not covered in June.\n\nRegions searched by active side-looking\nsonar. The small rectangle shows the limited August 2009 coverage, while\nthe larger areas show April-May 2010 coverage. Figure from Stone et\nal. (2014).\nResult: No wreckage found.Scientists computed a 95% confidence region by reverse drift modeling\nfrom where bodies and debris were recovered. By simulating ocean\ncurrents backwards in time, they estimated where the crash most likely\noccurred, producing a search zone north and west of the last known\nposition.\n\nThe 95% confidence zone recommended for\nthe 2010 search, located north and west of the LKP, based on reverse\ndrift modeling. Figure from Stone et al. (2014).\nResult: No wreckage found. The confidence region,\nwhile statistically valid, relied heavily on ocean current models and\ndidn’t integrate other sources of evidence like historical crash\nlocations or search effectiveness.\n\n\n\n\n\n\nWhy the Initial Approaches Failed\n\n\n\nEach search used valid and sophisticated statistical reasoning but treated evidence in isolation:\n\nDrift models didn’t account for prior crash locations\nPassive acoustic searches couldn’t distinguish between beacon failure and absence of wreckage\nSearch patterns didn’t incorporate the probability of missing the wreckage\nNo unified framework was used to combine these different sources of uncertainty\n\n\n\n\n\nThe Bayesian Strategy\nIn July 2010, after four unsuccessful search operations, the French aviation authority (BEA) assembled a new team of statisticians to design a search strategy for 2011. This team took a fundamentally different approach: instead of treating each piece of evidence separately, they used Bayesian inference to combine all sources of information into a single probability distribution.\n\n\nThe Bayesian Framework\nThe team constructed a posterior probability distribution for the wreckage location by combining:\n\nPrior Distribution: Historical data showed that aircraft are usually found close to their last known position. This gave higher prior probability to areas near the center of the circle.\nDrift Model Likelihood: Bodies found north of the LKP implied certain starting positions were more likely than others – but with significant uncertainty.\nSearch Effectiveness: Previous searches weren’t perfect. The team modeled the probability of missing the wreckage in searched areas, particularly accounting for terrain difficulty.\nBeacon Failure Possibility: The lack of acoustic signals could mean either the wreckage wasn’t in searched areas OR the beacons had failed. Bayesian analysis could incorporate both possibilities.\n\n\n\n\n\nReverse drift distribution showing the probability density of potential crash locations based on where bodies and debris were found. This was one key input to the Bayesian analysis. Figure from Stone et al. (2014).\n\n\n\n\n\n\n\n\n\n\nTechnical Detail: Computing the Posterior\n\n\n\n\n\nThe posterior distribution was computed using:\nP(\\text{location} | \\text{all evidence}) \\propto P(\\text{all evidence} | \\text{location}) \\times P(\\text{location})\nWhere the evidence included:\n\nNegative search results (no detection in searched areas)\nPositive drift data (bodies found at specific locations)\nTiming constraints (time between crash and debris discovery)\n\nThe likelihood P(\\text{all evidence} | \\text{location}) was itself a product of multiple conditional probabilities, each capturing different aspects of the search problem. Monte Carlo methods were used to integrate over unknown parameters like ocean current variations and detection probabilities.\n\n\n\n\n\nThe Breakthrough\nThe Bayesian analysis produced a surprising result: the highest probability areas were very close to the last known position. Although these areas had been covered by passive acoustic searches in 2009, the active sonar efforts in 2009-2010 had focused elsewhere based on drift models.\n\n\n\n\n\n\nThe Key Insight\n\n\n\nThe Bayesian approach revealed that multiple pieces of weak evidence all pointed to the same conclusion:\n\nHistorical data suggested searching near the LKP\nDebris drift models had high uncertainty and conflicting predictions\nThe failure to find wreckage in extensively searched areas increased relative probability elsewhere\nBeacon failure was historically more likely than initially assumed\n\nNo single piece of evidence was conclusive, but together they pointed strongly to areas near the last known position.\n\n\n\n\nDiscovery and Vindication\nThe new search began in 2011, focusing on the high-probability areas identified by the Bayesian analysis. After just one week of searching, on April 3, 2011, the wreckage was found at a depth of approximately 14,000 feet, very close to the last known position.\n\n\n\nPosterior distribution from the Bayesian analysis, showing the actual wreck location marked. The dark area near the center shows the highest probability zone, which correctly identified the area where the wreckage was ultimately found. Figure from Stone et al. (2014).\n\n\n\n\n\n\n\n\nWhy Bayesian Methods Succeeded\n\n\n\nThe Bayesian approach succeeded where the initial methods failed for three fundamental reasons:\n\nCoherent Information Integration: While the initial searches treated each piece of evidence separately, Bayesian inference combined them into a single, coherent picture.\nUncertainty Quantification: The approach explicitly modeled multiple sources of uncertainty – from ocean currents to sensor reliability – rather than assuming point estimates were correct.\nPrior Knowledge Utilization: Historical data about crash locations provided valuable information that pure data-driven approaches ignored.\n\nThis case demonstrates the power of Bayesian thinking: when faced with multiple sources of imperfect information, Bayesian methods provide the mathematical framework to combine them optimally.\n\n\n\n\n\n8.2.2 The Two Philosophies of Statistics\nIn the world of statistical inference, there are two major philosophical schools of thought about probability, parameters, and how we should make inferences from data. These aren’t just abstract philosophical debates – they lead to fundamentally different methods, interpretations, and answers. Understanding both perspectives is crucial for modern data scientists.\nFrequentist ViewBayesian ViewWe’ve been working primarily within the frequentist framework\nthroughout this course. Let’s formalize its key principles:F1. Probability as Frequency: Probability refers to\nlimiting relative frequencies in repeated experiments. Probabilities are\nobjective properties of the real world. When we say a coin has\nprobability 0.5 of landing heads, we mean that in an infinite sequence\nof flips, exactly half would be heads.F2. Fixed Parameters: Parameters are fixed, unknown\nconstants. They are not random variables. Because they don’t vary, we\ncannot make probability statements about them. We can’t say “there’s a\n95% probability that \\(\\mu\\) is between\n2 and 4” – either it is or it isn’t.F3. Long-Run Performance: Statistical methods should\nhave well-defined long-run frequency properties. A 95% confidence\ninterval should trap the true parameter in 95% of repeated experiments.\nThis is a statement about the procedure, not about any particular\ninterval.F4. Point-Conditioned Prediction: Predictions are\ntypically conditioned on a single parameter value, often an estimate\nlike the MLE. We predict future data assuming our estimate is\ncorrect.F5. Separate Theories: There’s no single,\noverarching theory unifying all aspects of frequentist inference.\nEstimation theory, hypothesis testing, and prediction each have their\nown frameworks and optimality criteria.The Bayesian approach starts from fundamentally different\nassumptions:B1. Probability as Belief: Probability describes\ndegree of belief or confidence. Probabilities can be subjective and\nrepresent our uncertainty about anything – including fixed events. We\ncan meaningfully say “I’m 70% confident it rained in Paris on January 1,\n1850” even though this is a fixed historical fact.B2. Probabilistic Parameters: We can make\nprobability statements about parameters, treating our uncertainty about\nthem as something to be described by a probability distribution. Even\nthough \\(\\theta\\) is fixed, our\nknowledge about it is uncertain, and we quantify this uncertainty with\nprobabilities.B3. Inference as Belief Updating: The core of\ninference is updating our beliefs about parameters by producing a\nposterior probability distribution after observing data. This posterior\nencapsulates everything we know about the parameter.B4. Averaged Prediction: Predictions are made by\naveraging over all parameter values, weighted by their posterior\nprobability. Instead of picking one “best” parameter value, we consider\nall plausible values.B5. Unified Theory: The framework has a strong,\nunified theoretical foundation based on the rules of probability. Bayes’\ntheorem provides a single coherent approach to all inference\nproblems.\n\n\n8.2.3 This Chapter’s Goal\nWe will explore two deeply connected topics:\n\nBayesian Inference: The machinery for updating our beliefs about parameters using data. We’ll see how prior knowledge combines with observed data to produce posterior distributions.\nStatistical Decision Theory: A formal framework for choosing the “best” estimator under any paradigm. This theory, which applies to both frequentist and Bayesian methods, gives us a rigorous way to compare different statistical procedures.\n\nThese topics are connected because Bayesian inference naturally leads to optimal estimators under decision theory, while decision theory helps us understand when and why Bayesian methods work well.\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nBayesian inference\nBayesiläinen päättely\nMain inferential framework\n\n\nPrior distribution\nPriorijakauma\nBeliefs before seeing data\n\n\nPosterior distribution\nPosteriorijakauma\nUpdated beliefs after data\n\n\nLikelihood\nUskottavuus\nProbability of data given parameters\n\n\nCredible interval\nUskottavuusväli\nBayesian confidence interval\n\n\nLoss function\nTappiofunktio\nMeasure of estimation error\n\n\nRisk\nRiski\nExpected loss\n\n\nBayes estimator\nBayes-estimaattori\nMinimizes Bayes risk\n\n\nMinimax estimator\nMinimax-estimaattori\nMinimizes maximum risk\n\n\nAdmissible\nKäypä, kelvollinen\nCannot be uniformly improved",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#the-bayesian-method-updating-beliefs-with-data",
    "href": "chapters/08-bayesian-inference-decision-theory.html#the-bayesian-method-updating-beliefs-with-data",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.3 The Bayesian Method: Updating Beliefs with Data",
    "text": "8.3 The Bayesian Method: Updating Beliefs with Data\n\n8.3.1 The Engine: Bayes’ Theorem for Inference\nThe Bayesian method centers on a fundamental question: how do we make predictions about unknown quantities when we have uncertain knowledge about the parameters that govern them?\nConsider predicting some unknown quantity x^* (which could be future data, or properties of the parameter itself) when we have:\n\nA model with unknown parameter \\theta\nObserved data x^n = (x_1, \\ldots, x_n) that provides information about \\theta\n\nUsing the rules of probability, we can write: f(x^* | x^n) = \\int f(x^* | \\theta, x^n) f(\\theta | x^n) d\\theta\nIf x^* depends on the data only through \\theta (a common assumption), this simplifies to: f(x^* | x^n) = \\int f(x^* | \\theta) f(\\theta | x^n) d\\theta\nThis equation reveals the key insight: to make predictions, we need the posterior distribution f(\\theta | x^n). The posterior tells us which parameter values are plausible given the data, and we average our predictions over all these plausible values.\n\nThe Components of Bayesian Inference\nTo compute the posterior distribution f(\\theta | x^n), we need:\n\nPrior Distribution f(\\theta): What we believe about \\theta before seeing the data. This encodes our initial knowledge or assumptions. We will see later how the prior is chosen.\nLikelihood f(x^n | \\theta) or \\mathcal{L}_n(\\theta): The probability of observing our data given different parameter values. This is the same likelihood function used in maximum likelihood estimation.\nPosterior Distribution f(\\theta | x^n): Our updated belief about \\theta after seeing the data, obtained via Bayes’ theorem.\n\n\n\nThe posterior distribution is computed as:  f(\\theta | x^n) = \\frac{f(x^n | \\theta) f(\\theta)}{\\int f(x^n | \\theta) f(\\theta) d\\theta} \nThe denominator \\int f(x^n | \\theta) f(\\theta) d\\theta is called the marginal likelihood or evidence. It’s a normalizing constant that ensures the posterior integrates to 1.1\n\nWe often do not specifically care about the normalizing constant, and write:  f(\\theta | x^n) \\propto f(x^n | \\theta) f(\\theta) \ndenoting that the posterior is proportional to Likelihood times Prior.\nWhen the observations X_1, \\ldots, X_n are IID given \\theta, the likelihood factorizes:  f(\\theta | x^n) \\propto \\mathcal{L}_n(\\theta) f(\\theta) = \\left[\\prod_{i=1}^n f(x_i | \\theta)\\right] f(\\theta) \nThis product structure is what allows evidence to accumulate across independent observations.\n\n\n\n\n\n\nWhy Do We Care About the Posterior?\n\n\n\nThe posterior distribution serves two distinct purposes:\n1. Direct Parameter Inference: Sometimes the parameters themselves are what we want to know:\n\nWhat’s the true efficacy of a vaccine?\nWhat’s the rate of climate change?\nWhat’s a manufacturing process’s defect rate?\n\nHere, we examine the posterior directly to understand the parameter values.\n2. Prediction: Other times, parameters are just a means to predict future observations:\n\nEstimating weather model parameters to forecast tomorrow’s conditions\nLearning user preferences to recommend movies\nEstimating volatility to predict financial risk\n\nFor prediction, we integrate over the posterior, incorporating parameter uncertainty into our forecasts rather than conditioning on a single estimate.\n\n\n\n\n8.3.2 Summarizing the Posterior\nThe posterior distribution f(\\theta | x^n) contains all our knowledge about \\theta after seeing the data. It’s the complete Bayesian answer to an inference problem. However, we often need to summarize this distribution with a single point – a point estimate – for communication or decision-making.\nPoint Estimates:\n\nPosterior Mean: \\bar{\\theta}_n = \\mathbb{E}[\\theta | x^n] = \\int \\theta f(\\theta | x^n) d\\theta\nThe center of our posterior beliefs, weighting all possible values by their posterior probability.\nPosterior Median: The value \\theta_m such that \\mathbb{P}(\\theta \\le \\theta_m | x^n) = 0.5\nThe value that splits the posterior distribution in half.\nPosterior Mode (MAP): \\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} f(\\theta | x^n)\nThe most probable value according to the posterior. MAP stands for “Maximum A Posteriori.”2\n\nInterval Estimates:\n\nCredible Interval: A (1-\\alpha) credible interval3 is a range (a, b) such that: \\mathbb{P}(a &lt; \\theta &lt; b | x^n) = 1-\\alpha\nTypically computed as an equal-tailed interval by finding a and b where \\int_{-\\infty}^a f(\\theta | x^n) d\\theta = \\int_b^{\\infty} f(\\theta | x^n) d\\theta = \\alpha/2.\n\n\n\n\n\n\n\nCredible vs. Confidence Intervals\n\n\n\nA crucial distinction:\n\nCredible interval (Bayesian): “Given the data, there’s a 95% probability that \\theta lies in this interval.”\nConfidence interval (Frequentist): “This procedure produces intervals that trap the true \\theta in 95% of repeated experiments.”\n\nThe credible interval makes a direct probability statement about the parameter, which is what most people incorrectly think confidence intervals do!\n\n\nIntuitiveMathematicalComputationalImagine you’re trying to estimate the average height in a population.\nYou take a sample and compute an interval.Confidence Interval (Frequentist): “If I repeated\nthis sampling procedure 100 times, about 95 of those intervals would\ncontain the true average height.” It’s a statement about the reliability\nof the method, not about any specific interval. Once computed,\nthe true value is either in it or not – there’s no probability\ninvolved.Credible Interval (Bayesian): “Based on the data I\nobserved and my prior knowledge, I’m 95% confident the true average\nheight is in this interval.” It’s a direct probability statement about\nwhere the parameter lies, given what we’ve learned.The confidence interval is like a fishing net manufacturer’s\nguarantee: “95% of our nets catch fish.” The credible interval is like a\nweather forecast: “95% chance of rain tomorrow.” One describes a\nlong-run property of a procedure; the other describes belief about a\nspecific unknown.Let \\(\\theta\\) be the parameter and\n\\(X^n\\) the observed data.Confidence Interval: Find functions\n\\(L(X^n)\\) and\n\\(U(X^n)\\) such that:\n\\[\\mathbb{P}_\\theta(L(X^n) \\leq \\theta \\leq U(X^n)) = 1-\\alpha \\text{ for all } \\theta\\]The probability is over the random data\n\\(X^n\\), with\n\\(\\theta\\) fixed. Different data gives\ndifferent intervals.Credible Interval: Find constants\n\\(a\\) and\n\\(b\\) such that:\n\\[\\int_a^b f(\\theta|X^n) d\\theta = 1-\\alpha\\]The probability is over the parameter\n\\(\\theta\\) given fixed, observed data\n\\(X^n\\). The interval quantifies our\nposterior uncertainty about\n\\(\\theta\\).Key difference: In confidence intervals, data is random and parameter\nis fixed. In credible intervals, data is fixed (observed) and parameter\nis treated as random (uncertain).Let’s simulate both types of intervals to see their fundamental\ndifference. We’ll generate many datasets to show the frequentist\ncoverage property, then compute a single credible interval to show the\nBayesian probability statement:\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Simulate the difference between confidence and credible intervals\nnp.random.seed(42)\n\n# True parameter\ntrue_mean = 5.0\ntrue_std = 1.0\nn = 30\n\n# Generate many datasets to show confidence interval behavior\nn_simulations = 100\nconfidence_intervals = []\n\nfor i in range(n_simulations):\n    # Generate a dataset\n    data = np.random.normal(true_mean, true_std, n)\n    sample_mean = np.mean(data)\n    sample_se = true_std / np.sqrt(n)  # Known variance case\n    \n    # 95% Confidence interval\n    ci_lower = sample_mean - 1.96 * sample_se\n    ci_upper = sample_mean + 1.96 * sample_se\n    confidence_intervals.append((ci_lower, ci_upper))\n\n# Count how many contain the true parameter\ncoverage = sum(1 for (l, u) in confidence_intervals if l &lt;= true_mean &lt;= u)\nprint(f\"Confidence Interval Coverage: {coverage}/{n_simulations} = {coverage/n_simulations:.2%}\")\nprint(\"This demonstrates the frequentist guarantee: ~95% coverage in repeated sampling\")\n\n# Visualize all 100 confidence intervals\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n\n# Top panel: Show all confidence intervals\nax1.axhline(true_mean, color='black', linestyle='-', label='True mean', linewidth=1.5, zorder=5)\n\n# Plot all intervals, colored by whether they contain the true mean\nfor i in range(n_simulations):\n    l, u = confidence_intervals[i]\n    contains_true = l &lt;= true_mean &lt;= u\n    color = '#0173B2' if contains_true else '#DE8F05'  # Blue vs Orange (high contrast, colorblind safe)\n    # Use thinner lines and transparency for better visualization\n    ax1.plot([i, i], [l, u], color=color, linewidth=0.8, alpha=0.7)\n    # Small dots for interval centers\n    ax1.plot(i, (l+u)/2, '.', color=color, markersize=2, alpha=0.8)\n\n# Add summary statistics\nn_containing = sum(1 for (l,u) in confidence_intervals if l &lt;= true_mean &lt;= u)\nax1.set_xlabel('Dataset number')\nax1.set_ylabel('Parameter value')\nax1.set_title(f'All {n_simulations} Confidence Intervals\\n'\n             f'{n_containing}/{n_simulations} ({n_containing/n_simulations:.1%}) contain true mean • '\n             f'Blue = contains true mean, Orange = misses')\nax1.grid(True, alpha=0.3)\nax1.set_xlim(-1, n_simulations)\n\n# Now show a single Bayesian credible interval\nsingle_dataset = np.random.normal(true_mean, true_std, n)\nsample_mean = np.mean(single_dataset)\n\n# With a Normal prior N(0, 10) and known variance\nprior_mean, prior_var = 0, 10\nposterior_var = 1 / (1/prior_var + n/true_std**2)\nposterior_mean = posterior_var * (prior_mean/prior_var + n*sample_mean/true_std**2)\nposterior_std = np.sqrt(posterior_var)\n\n# 95% Credible interval\ncred_lower = posterior_mean - 1.96 * posterior_std\ncred_upper = posterior_mean + 1.96 * posterior_std\n\n# Bottom panel: Show posterior distribution\nx_range = np.linspace(posterior_mean - 4*posterior_std, posterior_mean + 4*posterior_std, 200)\nposterior_density = stats.norm.pdf(x_range, posterior_mean, posterior_std)\n\nax2.plot(x_range, posterior_density, 'b-', linewidth=2, label='Posterior')\nax2.fill_between(x_range, posterior_density, \n                 where=(x_range &gt;= cred_lower) & (x_range &lt;= cred_upper),\n                 alpha=0.3, color='blue', label='95% Credible Interval')\nax2.axvline(true_mean, color='red', linestyle='--', label='True mean', linewidth=2)\nax2.axvline(sample_mean, color='green', linestyle=':', label='Sample mean', linewidth=2)\nax2.set_xlabel('Parameter value')\nax2.set_ylabel('Posterior density')\nax2.set_title(f'Posterior Distribution for One Dataset\\nP({cred_lower:.2f} &lt; θ &lt; {cred_upper:.2f} | data) = 0.95')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFor this specific dataset:\")\nprint(f\"  Sample mean: {sample_mean:.2f}\")\nprint(f\"  95% Credible Interval: [{cred_lower:.2f}, {cred_upper:.2f}]\")\nprint(f\"  This is a direct probability statement about the parameter!\")\n\nConfidence Interval Coverage: 97/100 = 97.00%\nThis demonstrates the frequentist guarantee: ~95% coverage in repeated sampling\n\nFor this specific dataset:\n  Sample mean: 4.72\n  95% Credible Interval: [4.35, 5.06]\n  This is a direct probability statement about the parameter!\n\n\n\n\nKey Takeaway: Confidence intervals achieve 95%\ncoverage across many experiments (a procedure property), while\ncredible intervals give 95% probability for this specific\ndataset (a parameter property). Same numbers, fundamentally\ndifferent meanings.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#bayesian-inference-in-action",
    "href": "chapters/08-bayesian-inference-decision-theory.html#bayesian-inference-in-action",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.4 Bayesian Inference in Action",
    "text": "8.4 Bayesian Inference in Action\n\n8.4.1 Conjugate Models and Conjugate Priors\nIn principle, Bayesian inference requires us to compute integrals to normalize the posterior distribution. In practice, these integrals are often intractable. However, for certain combinations of priors and likelihoods, the posterior has a nice closed form. These special cases are called conjugate models.\n\n\n\n\n\n\nWhat is a Conjugate Prior?\n\n\n\nA prior is conjugate to a likelihood if the resulting posterior distribution is in the same family as the prior. This means:\n\nIf the prior is Beta, the posterior is also Beta\nIf the prior is Normal, the posterior is also Normal\n\nConjugacy provides a convenient analytical shortcut, though modern computational methods have reduced its importance.\n\n\n\nExample: The Bernoulli-Beta Model\nConsider the fundamental problem of estimating a probability from binary data. Let X_i \\sim \\text{Bernoulli}(p) for i = 1, ..., n, where we observe s successes out of n trials.\nStarting with a Uniform Prior:\nSince p is a probability, it must lie in [0,1]. If we have no prior information, a natural choice is the uniform prior: f(p) = 1 for p \\in [0,1].\nLikelihood: With s successes in n trials, the likelihood is: \\mathcal{L}_n(p) \\propto p^s(1-p)^{n-s}\nPosterior calculation: f(p | x^n) \\propto f(p) \\times \\mathcal{L}_n(p) = 1 \\times p^s(1-p)^{n-s} = p^{(s+1)-1}(1-p)^{(n-s+1)-1}\nThis has the form of a Beta distribution! Specifically, if we match the parameters: p | x^n \\sim \\text{Beta}(s+1, n-s+1)\nThe mean of \\text{Beta}(\\alpha, \\beta) is \\alpha / (\\alpha + \\beta), so the posterior mean here is \\bar{p} = \\frac{s+1}{n+2}, which can be written as: \\bar{p} = \\frac{n}{n+2} \\cdot \\frac{s}{n} + \\frac{2}{n+2} \\cdot \\frac{1}{2}\nThis is a weighted average of the MLE \\hat{p} = s/n and the prior mean 1/2, with the data getting more weight as n increases.\nThe General Beta Prior:\nThe uniform prior is actually a special case of the Beta distribution. In general, if we use a \\text{Beta}(\\alpha, \\beta) prior: f(p) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\nThen the posterior is: p | x^n \\sim \\text{Beta}(\\alpha + s, \\beta + n - s)\nKey insights:\n\nThe Beta distribution is conjugate to the Bernoulli likelihood - the posterior stays in the Beta family\nThe parameters \\alpha and \\beta act as “pseudo-counts”: \\alpha prior successes, \\beta prior failures\nThe uniform prior is \\text{Beta}(1, 1) - one pseudo-success and one pseudo-failure\nThe posterior mean \\bar{p} = \\frac{\\alpha + s}{\\alpha + \\beta + n} combines prior pseudo-counts with observed counts\nAs n \\to \\infty, the data dominates and the prior’s influence vanishes\n\nLet’s visualize how the posterior evolves with data:\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set up the figure\nfig, axes = plt.subplots(2, 1, figsize=(7, 7))\n\n# Top panel: Effect of sample size\nax1 = axes[0]\np_true = 0.7  # True probability\nalpha_prior, beta_prior = 1, 1  # Uniform prior\n\n# Different sample sizes\nsample_sizes = [0, 10, 50, 200]\ncolors = ['gray', 'blue', 'green', 'red']\n\np_range = np.linspace(0, 1, 200)\n\n# Set seed once outside the loop for reproducible results\nnp.random.seed(42)\n\nfor n, color in zip(sample_sizes, colors):\n    if n == 0:\n        # Just the prior\n        y = stats.beta.pdf(p_range, alpha_prior, beta_prior)\n        label = 'Prior'\n    else:\n        # Simulate data\n        s = np.random.binomial(n, p_true)\n        # Posterior parameters\n        alpha_post = alpha_prior + s\n        beta_post = beta_prior + n - s\n        y = stats.beta.pdf(p_range, alpha_post, beta_post)\n        label = f'n={n}, s={s}'\n    \n    ax1.plot(p_range, y, color=color, linewidth=2, label=label)\n\nax1.axvline(p_true, color='black', linestyle='--', alpha=0.5, label='True p')\nax1.set_xlabel('p')\nax1.set_ylabel('Density')\nax1.set_title('Posterior Becomes More Concentrated with More Data')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Bottom panel: Effect of different priors\nax2 = axes[1]\nn = 20\ns = 10  # 50% success rate in data\n\npriors = [\n    (1, 1, 'Uniform: Beta(1,1)'),\n    (10, 10, 'Informative: Beta(10,10)'),\n    (1, 10, 'Skeptical: Beta(1,10)')\n]\n\nfor (alpha, beta, label) in priors:\n    # Prior\n    prior_y = stats.beta.pdf(p_range, alpha, beta)\n    ax2.plot(p_range, prior_y, linestyle=':', alpha=0.5)\n    \n    # Posterior\n    alpha_post = alpha + s\n    beta_post = beta + n - s\n    post_y = stats.beta.pdf(p_range, alpha_post, beta_post)\n    ax2.plot(p_range, post_y, linewidth=2, label=label)\n\nax2.axvline(0.5, color='black', linestyle='--', alpha=0.5, label='MLE')\nax2.set_xlabel('p')\nax2.set_ylabel('Density')\nax2.set_title(f'Different Priors, Same Data (n={n}, s={s})')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plots illustrate two key principles:\n\nTop panel: As we collect more data, the posterior becomes increasingly concentrated around the true value, regardless of the prior.\nBottom panel: Different priors lead to different posteriors (but this effect diminishes with larger sample sizes).\n\n\n\nExample: The Normal-Normal Model\nNow consider estimating the mean of a Normal distribution with known variance. Let X_i \\sim \\mathcal{N}(\\theta, \\sigma^2) where \\sigma^2 is known (this assumption simplifies the math and is commonly used in introductory examples).\nPrior: \\theta \\sim \\mathcal{N}(\\theta_0, \\sigma_0^2)\nLikelihood: For IID data, the sufficient statistic is the sample mean \\bar{x}, and: \\bar{x} | \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2/n)\n\nGiven a likelihood X_i | \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2) (known \\sigma^2) and a prior \\theta \\sim \\mathcal{N}(\\theta_0, \\sigma_0^2), the posterior is:\n\\theta | x^n \\sim \\mathcal{N}(\\theta_*, \\sigma_*^2)\nwhere:\n\nPosterior Precision: \\frac{1}{\\sigma_*^2} = \\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}\nPosterior Mean: \\theta_* = \\sigma_*^2 \\left( \\frac{\\theta_0}{\\sigma_0^2} + \\frac{n \\bar{x}}{\\sigma^2} \\right)\n\n\nInterpretation:\n\nPrecision (inverse variance) is additive: posterior precision = prior precision + data precision\nThe posterior mean is a precision-weighted average of prior mean and sample mean\nMore precise information gets more weight\nThe posterior mean interpolates between the prior mean and sample mean, with the weight given to the data increasing as n increases\nPosterior is always more precise than either prior or likelihood alone\n\n\n\nShow code for Normal-Normal posterior calculation\ndef normal_normal_posterior(prior_mean, prior_var, data_mean, data_var, n):\n    \"\"\"\n    Calculate posterior parameters for Normal-Normal conjugate model.\n    \n    Parameters:\n    -----------\n    prior_mean : Prior mean θ₀\n    prior_var : Prior variance σ₀²\n    data_mean : Sample mean x̄\n    data_var : Known data variance σ²\n    n : Sample size\n    \n    Returns:\n    --------\n    post_mean : Posterior mean θ*\n    post_var : Posterior variance σ*²\n    \"\"\"\n    # Convert to precisions (inverse variances)\n    prior_precision = 1 / prior_var\n    data_precision = n / data_var\n    \n    # Posterior precision is sum of precisions\n    post_precision = prior_precision + data_precision\n    post_var = 1 / post_precision\n    \n    # Posterior mean is precision-weighted average\n    post_mean = post_var * (prior_precision * prior_mean + \n                            data_precision * data_mean)\n    \n    return post_mean, post_var\n\n# Example calculation\nprior_mean, prior_var = 0, 4  # Prior: N(0, 4)\ndata_var = 1  # Known variance\nn = 10\ndata_mean = 2.3  # Observed sample mean\n\npost_mean, post_var = normal_normal_posterior(\n    prior_mean, prior_var, data_mean, data_var, n\n)\n\nprint(f\"Prior: N({prior_mean}, {prior_var})\")\nprint(f\"Data: n={n}, x̄={data_mean}, σ²={data_var}\")\nprint(f\"Posterior: N({post_mean:.3f}, {post_var:.3f})\")\nprint(f\"\\nPosterior mean is {post_mean:.3f}, between prior mean {prior_mean} and MLE {data_mean}\")\n\n\nPrior: N(0, 4)\nData: n=10, x̄=2.3, σ²=1\nPosterior: N(2.244, 0.098)\n\nPosterior mean is 2.244, between prior mean 0 and MLE 2.3\n\n\n\n\n\n8.4.2 The Art and Science of Choosing Priors\nOne of the most debated topics in Bayesian statistics is how to choose the prior distribution. Critics argue that priors introduce subjectivity; advocates counter that they make assumptions explicit. The reality is nuanced: prior choice is both an art requiring judgment and a science with established principles.\nConjugate Priors: We’ve seen these in action – Beta for Bernoulli, Normal for Normal. They’re computationally convenient and have nice interpretations (like pseudo-counts), but they may not reflect genuine prior beliefs. Using them just for convenience can lead to misleading results.\nNon-Informative Priors: These attempt to be “objective” by letting the data speak for itself. Common choices include:\n\nUniform priors: f(\\theta) = \\text{constant}\nJeffreys’ prior: f(\\theta) \\propto \\sqrt{I(\\theta)} where I(\\theta) is the Fisher information\n\n\n\n\n\n\n\nThe Flat Prior Fallacy\n\n\n\nA uniform prior is not “uninformative”! Consider:\n\nScale matters: A uniform prior on [-10^6, 10^6] says |\\theta| is almost certainly large\nNot transformation invariant: If p \\sim \\text{Uniform}(0,1), then \\log(p/(1-p)) is not uniform\nCan encode strong beliefs: Uniform on [0, 1000] for a rate parameter implies most mass is on very large values (highly informative!)\n\nThe notion of “no information” is not well-defined mathematically.\n\n\n\n\n\n\n\n\nAdvanced: Jeffreys’ Prior\n\n\n\n\n\nJeffreys proposed using f(\\theta) \\propto \\sqrt{I(\\theta)} where I(\\theta) is the Fisher information. This prior has a key property: it’s invariant to reparameterization. If we transform \\theta to \\varphi = g(\\theta), the Jeffreys prior for \\varphi is what we’d get by transforming the Jeffreys prior for \\theta.\nFor \\text{Bernoulli}(p), the Jeffreys prior is \\text{Beta}(1/2, 1/2), which is U-shaped, putting more mass near 0 and 1 than at 0.5 – hardly “uninformative”!\n\n\n\nWeakly Informative Priors: This is the recommended approach nowadays which balances several goals:\n\nWide enough to not exclude plausible values\nTight enough to exclude absurd values\nRegularize estimation to prevent overfitting\n\nFor example, for a logistic regression coefficient, a \\mathcal{N}(0, 2.5^2) prior (mean 0, standard deviation 2.5) allows large effects but prevents numerical instability. Note: We use the \\mathcal{N}(\\mu, \\sigma^2) parameterization throughout these notes.\n\n\n\n\n\n\nThe Challenge of High-Dimensional Priors\n\n\n\nPlacing sensible priors becomes increasingly difficult as dimensionality grows:\n\nThe Gaussian bubble: In high dimensions, a multivariate standard normal \\mathcal{N}(0, I) concentrates its mass in a thin shell at radius \\sqrt{d} from the origin – almost no mass is near zero despite this being the “center” of the distribution. This concentration of measure phenomenon means our intuitions about priors break down (see this blog post).\nDeep learning: Specifying priors for millions of neural network weights remains an open problem in Bayesian deep learning. Most practitioners resort to simple priors like \\mathcal{N}(0, \\sigma^2 I) that don’t capture the true structure, or avoid fully Bayesian approaches altogether.\n\nHigh-dimensional Bayesian inference requires careful thought about what the prior actually implies when there are many parameters.\n\n\n\n\n\n\n\n\nAdvanced: The Bayesian Central Limit Theorem\n\n\n\n\n\nA remarkable result shows that Bayesian and frequentist methods converge with enough data.\nUnder suitable regularity conditions, as n \\to \\infty, the posterior distribution can be approximated by: f(\\theta | x^n) \\approx \\mathcal{N}(\\hat{\\theta}_{MLE}, \\widehat{se}^2)\nwhere \\hat{\\theta}_{MLE} is the maximum likelihood estimate and \\widehat{se} is its standard error.\nWhy this matters:\n\nWith enough data, the prior becomes irrelevant\nBayesian credible intervals ≈ Frequentist confidence intervals\nBoth approaches give essentially the same answer\nThe likelihood dominates both approaches in large samples\n\nThis is reassuring: two philosophically different approaches converge to the same practical conclusions when we have sufficient evidence.\n\n\n\n\n\n8.4.3 Implementing Bayesian Inference\nFor the conjugate models in this chapter, we can solve for the posterior distribution analytically. But what happens in more complex, real-world models where this is not possible?\nThe modern Bayesian workflow relies on powerful computational algorithms, most commonly Markov chain Monte Carlo (MCMC). These algorithms allow us to generate a large collection of samples that are representative of the posterior distribution, even when we cannot solve for it directly. Once we have these samples, we can approximate any summary we need (like the mean or a credible interval) and easily get posteriors for transformed parameters.\nModern probabilistic programming frameworks such as Stan or PyMC allow users to perform Bayesian inference relatively easily, exploiting modern machinery. This computational approach is incredibly powerful and flexible, and we will explore it in detail in Chapter 10. For now, the key takeaway is that the goal of Bayesian inference is always to obtain the posterior; the methods in this chapter do it with math, while later methods will do it with computation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#statistical-decision-theory-a-framework-for-best",
    "href": "chapters/08-bayesian-inference-decision-theory.html#statistical-decision-theory-a-framework-for-best",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.5 Statistical Decision Theory: A Framework for “Best”",
    "text": "8.5 Statistical Decision Theory: A Framework for “Best”\nWe now shift from Bayesian inference to a more general question: given multiple ways to estimate a parameter, how do we choose the best one? Statistical decision theory provides a formal framework for comparing any estimators – Bayesian, frequentist, or otherwise.\n\n8.5.1 The Ingredients: Loss and Risk\nTo compare estimators formally, we need to quantify how “wrong” an estimate is.\n\nLoss Function L(\\theta, \\hat{\\theta}): Quantifies the penalty for estimating \\theta with \\hat{\\theta}.\nCommon examples:\n\nSquared Error: L_2(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2\nAbsolute Error: L_1(\\theta, \\hat{\\theta}) = |\\theta - \\hat{\\theta}|\nL_p Loss: L_p(\\theta, \\hat{\\theta}) = |\\theta - \\hat{\\theta}|^p for p \\geq 1 (generalizes the above)\nZero-One Loss: L_{0-1}(\\theta, \\hat{\\theta}) = \\begin{cases} 0 & \\text{if } \\theta = \\hat{\\theta} \\\\ 1 & \\text{otherwise} \\end{cases}\n\n\nLoss tells us the cost of a specific estimate for a specific parameter value. But estimators are random – they depend on data. So we need to average:\n\nRisk R(\\theta, \\hat{\\theta}): The expected loss over all possible datasets, for a given loss function \\mathcal{L} and fixed \\theta. R(\\theta, \\hat{\\theta}) = \\mathbb{E}_{\\theta}[L(\\theta, \\hat{\\theta}(X))] = \\int L(\\theta, \\hat{\\theta}(x)) f(x;\\theta) dx\n\nFor squared error loss, the risk equals the MSE: R(\\theta, \\hat{\\theta}) = \\mathbb{E}_{\\theta}[(\\theta - \\hat{\\theta})^2] = \\text{MSE} = \\mathbb{V}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta}) \nIntuitiveMathematicalComputationalThink of risk as the “average wrongness” of an estimator, for a\nspecific definition of “wrongness” (loss function). Imagine you could\nrepeat your experiment many times with the same true parameter value.\nEach time, you’d get different data and thus a different estimate. The\nrisk tells you the average penalty you’d pay across all these\nrepetitions.It’s like evaluating a weather forecaster: you don’t judge them on\none prediction, but on their average performance over many days. An\nestimator with low risk is consistently good, even if it’s not perfect\non any single dataset.Risk is the frequentist expectation of the loss function, treating\nthe estimator as a random variable (through its dependence on random\ndata) while holding the parameter fixed:\\[R(\\theta, \\hat{\\theta}) = \\mathbb{E}_{X \\sim f(x;\\theta)}[L(\\theta, \\hat{\\theta}(X))]\\]This contrasts with Bayes risk, which averages over\n\\(\\theta\\) according to a prior. The\nrisk function \\(R(\\theta, \\cdot)\\) maps\neach parameter value to a real number, creating a curve that\ncharacterizes the estimator’s performance across the parameter\nspace.For squared error loss, the bias-variance decomposition shows that\nrisk combines systematic error (bias) with variability (variance),\nrevealing the fundamental tradeoff in estimation.We can understand risk concretely through simulation. The following\ncode demonstrates what risk actually means by repeatedly generating\ndatasets from the same distribution and computing the squared loss for\neach one:\nimport numpy as np\n\ndef estimate_risk_by_simulation(true_theta, estimator_func, \n                                n_samples=20, n_simulations=10000):\n    \"\"\"\n    Estimate risk via Monte Carlo simulation.\n    \n    This shows what risk really means: the average loss\n    over many possible datasets.\n    \"\"\"\n    losses = []\n    \n    for _ in range(n_simulations):\n        # Generate a dataset (example: Normal distribution)\n        data = np.random.normal(true_theta, 1, n_samples)\n        \n        # Compute the estimate for this dataset\n        estimate = estimator_func(data)\n        \n        # Compute the loss (using squared error)\n        loss = (estimate - true_theta)**2\n        losses.append(loss)\n    \n    # Risk is the average loss\n    risk = np.mean(losses)\n    \n    print(f\"True parameter: {true_theta}\")\n    print(f\"Estimated risk: {risk:.4f}\")\n    print(f\"Min loss seen: {np.min(losses):.4f}\")\n    print(f\"Max loss seen: {np.max(losses):.4f}\")\n    \n    return risk\n\n# Example: Risk of sample mean estimator\nrisk = estimate_risk_by_simulation(\n    true_theta=5.0,\n    estimator_func=lambda data: np.mean(data),\n    n_samples=20\n)\n\nTrue parameter: 5.0\nEstimated risk: 0.0503\nMin loss seen: 0.0000\nMax loss seen: 0.8506\n\nKey Takeaway: Risk is the average loss\nacross all possible datasets. The simulation shows that while individual\nlosses vary widely (min to max), risk captures the expected performance\nof an estimator.\n\n\n8.5.2 The Challenge of Comparing Risk Functions\nRisk functions are curves – one risk value for each possible \\theta. This creates a fundamental problem: estimators rarely dominate uniformly.\n\n\n\n\n\n\nExample: A Simple Case\n\n\n\nLet X \\sim \\mathcal{N}(\\theta, 1) and let’s assume squared error loss. Compare:\n\n\\hat{\\theta}_1 = X (the sensible estimator)\n\\hat{\\theta}_2 = 3 (a silly constant estimator)\n\nRisk calculations:\n\nFor \\hat{\\theta}_1 = X: R(\\theta, \\hat{\\theta}_1) = \\mathbb{E}_\\theta[(X - \\theta)^2] = \\text{Var}(X) = 1 This is constant for all \\theta.\nFor \\hat{\\theta}_2 = 3: R(\\theta, \\hat{\\theta}_2) = \\mathbb{E}_\\theta[(3 - \\theta)^2] = (3-\\theta)^2 This depends on \\theta since the estimator is non-random.\n\n\n\n\n\n\n\n\n\n\nThe constant estimator is actually better when \\theta is near 3 (“a broken clock is right twice a day”), but terrible elsewhere. Still, neither uniformly dominates the other.\n\n\n\n\n\n\n\n\nExample: Bernoulli Estimation\n\n\n\nConsider X_1, \\ldots, X_n \\sim \\text{Bernoulli}(p) with squared error loss. Let S = \\sum_{i=1}^n X_i be the number of successes.\nWe’ll compare two natural estimators by computing their risk functions.\nEstimator 1: MLE\nThe MLE is \\hat{p}_1 = \\bar{X} = \\frac{S}{n}\nSince S \\sim \\text{Binomial}(n, p), we have:\n\n\\mathbb{E}[S] = np, so \\mathbb{E}[\\hat{p}_1] = p (unbiased)\n\\text{Var}(S) = np(1-p), so \\text{Var}(\\hat{p}_1) = \\frac{p(1-p)}{n}\n\nThe risk under squared error loss is: R(p, \\hat{p}_1) = \\mathbb{E}[(\\hat{p}_1 - p)^2] = \\text{Var}(\\hat{p}_1) + \\text{Bias}^2(\\hat{p}_1) = \\frac{p(1-p)}{n} + 0 = \\frac{p(1-p)}{n}\nThis is a parabola with maximum at p = 1/2.\nEstimator 2: Bayesian posterior mean with Beta(α, β) prior\nUsing Bayes’ theorem with prior p \\sim \\text{Beta}(\\alpha, \\beta) and observing S successes: p | S \\sim \\text{Beta}(\\alpha + S, \\beta + n - S)\nThe posterior mean4 is: \\hat{p}_2 = \\frac{\\alpha + S}{\\alpha + \\beta + n}\nTo find the risk, we compute bias and variance:\n\nExpected value: \\mathbb{E}[\\hat{p}_2] = \\frac{\\alpha + np}{\\alpha + \\beta + n}\nBias: \\text{Bias}(\\hat{p}_2) = \\frac{\\alpha + np}{\\alpha + \\beta + n} - p = \\frac{\\alpha - p(\\alpha + \\beta)}{\\alpha + \\beta + n}\nVariance: \\text{Var}(\\hat{p}_2) = \\text{Var}\\left(\\frac{S}{\\alpha + \\beta + n}\\right) = \\frac{np(1-p)}{(\\alpha + \\beta + n)^2}\n\nTherefore, the general risk formula is: R(p, \\hat{p}_2) = \\frac{np(1-p)}{(\\alpha + \\beta + n)^2} + \\left(\\frac{\\alpha - p(\\alpha + \\beta)}{\\alpha + \\beta + n}\\right)^2\nSpecial case: Uniform prior Beta(1, 1)\nFor \\alpha = \\beta = 1, the posterior mean becomes: \\hat{p}_2 = \\frac{1 + S}{2 + n} = \\frac{n}{n+2} \\cdot \\frac{S}{n} + \\frac{2}{n+2} \\cdot \\frac{1}{2}\nThis is a weighted average of the MLE and the prior mean 1/2.\nThe risk specializes to: R(p, \\hat{p}_2) = \\frac{np(1-p)}{(n+2)^2} + \\left(\\frac{1 - 2p}{n + 2}\\right)^2\nLet’s plot both risk functions to see how they compare:\n\n\n\n\n\n\n\n\n\nThe risk functions cross! The MLE is better near the extremes (p near 0 or 1), while the Bayes estimator is better near the middle (p near 1/2). Neither estimator uniformly dominates the other.\n\n\nBoth examples above illustrate a fundamental challenge in decision theory: when risk functions cross, we cannot declare one estimator uniformly better than another. Different estimators excel in different regions of the parameter space.\nTo make a decision on the estimator to use, we must reduce these risk curves to single numbers that we can compare. But how should we summarize an entire function? Should we care most about average performance or worst-case performance? Different answers to this question lead to two distinct optimality criteria: Bayes estimators (optimizing average risk) and minimax estimators (optimizing worst-case risk), detailed in the next section.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#optimal-estimators-bayes-and-minimax-rules",
    "href": "chapters/08-bayesian-inference-decision-theory.html#optimal-estimators-bayes-and-minimax-rules",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.6 Optimal Estimators: Bayes and Minimax Rules",
    "text": "8.6 Optimal Estimators: Bayes and Minimax Rules\n\n8.6.1 The Bayesian Approach: Minimizing Average Risk\nThe Bayesian approach to decision theory averages the risk over a prior distribution, giving us a single number to minimize.\n\nBayes Risk: The expected risk, averaged over the prior distribution f(\\theta): r(f, \\hat{\\theta}) = \\int R(\\theta, \\hat{\\theta}) f(\\theta) d\\theta\n\n\nBayes Estimator: The estimator \\hat{\\theta}^B that minimizes the Bayes risk: \\hat{\\theta}^B = \\arg\\min_{\\hat{\\theta}} r(f, \\hat{\\theta})\n\nThe remarkable connection between Bayesian inference and decision theory:\n\nThe Bayes estimator can be found by minimizing the posterior expected loss for each observed x. Specifically:\n\nFor Squared Error Loss: The Bayes estimator is the Posterior Mean\nFor Absolute Error Loss: The Bayes estimator is the Posterior Median\n\nFor Zero-One Loss: The Bayes estimator is the Posterior Mode (MAP)\n\n\n\n\n\n\n\n\nProof for Different Loss Functions\n\n\n\n\n\nFor any loss function L(\\theta, \\hat{\\theta}), the Bayes estimator minimizes the posterior expected loss: \\hat{\\theta}^B(x) = \\arg\\min_a \\mathbb{E}[L(\\theta, a) | X = x] = \\arg\\min_a \\int L(\\theta, a) f(\\theta|x) d\\theta\nSquared Error Loss: L(\\theta, a) = (\\theta - a)^2\nWe need to minimize: \\int (\\theta - a)^2 f(\\theta|x) d\\theta\nTaking the derivative with respect to a and setting to zero: \\frac{d}{da} \\int (\\theta - a)^2 f(\\theta|x) d\\theta = -2 \\int (\\theta - a) f(\\theta|x) d\\theta = 0\nThis gives: \\int \\theta f(\\theta|x) d\\theta = a \\int f(\\theta|x) d\\theta = a\nTherefore: \\hat{\\theta}^B(x) = \\int \\theta f(\\theta|x) d\\theta = \\mathbb{E}[\\theta | X = x] (posterior mean)\nAbsolute Error Loss: L(\\theta, a) = |\\theta - a|\nWe need to minimize: \\int |\\theta - a| f(\\theta|x) d\\theta = \\int_{-\\infty}^a (a - \\theta) f(\\theta|x) d\\theta + \\int_a^{\\infty} (\\theta - a) f(\\theta|x) d\\theta\nTaking the derivative with respect to a: \\frac{d}{da} = \\int_{-\\infty}^a f(\\theta|x) d\\theta - \\int_a^{\\infty} f(\\theta|x) d\\theta = F(a|x) - (1 - F(a|x)) = 2F(a|x) - 1\nSetting to zero: F(a|x) = 1/2, so \\hat{\\theta}^B(x) is the posterior median.\nZero-One Loss: L(\\theta, a) = \\mathbb{1}\\{\\theta \\neq a\\}\nThe expected loss is: \\mathbb{E}[L(\\theta, a) | X = x] = P(\\theta \\neq a | X = x) = 1 - P(\\theta = a | X = x)\nThis is minimized when P(\\theta = a | X = x) is maximized, which occurs at the posterior mode.\n\n\n\nThis theorem reveals a profound insight: Bayesian inference naturally produces optimal estimators! The posterior summaries we compute for inference are exactly the estimators that minimize expected loss.\n\n\n8.6.2 The Frequentist Approach: Minimizing Worst-Case Risk\nThe minimax approach takes a pessimistic view: prepare for the worst case.\n\nMaximum Risk: The worst-case risk over the entire parameter space \\Theta: \\bar{R}(\\hat{\\theta}) = \\sup_{\\theta \\in \\Theta} R(\\theta, \\hat{\\theta})\n\n\nMinimax Estimator: The estimator \\hat{\\theta}^{MM} with the smallest maximum risk: \\hat{\\theta}^{MM} = \\arg\\min_{\\hat{\\theta}} \\sup_{\\theta} R(\\theta, \\hat{\\theta})\nIt’s the “best of the worst-case” estimators.\n\nFinding minimax estimators directly is usually difficult. However, there’s a powerful connection to Bayes estimators:\n\nIf a Bayes estimator has constant risk (the same risk for all \\theta), then it is minimax.\n\nThis gives us a recipe: find a prior such that the resulting Bayes estimator has constant risk.\n\n\n\n\n\n\nExample: Minimax Estimator for Bernoulli\n\n\n\nConsider X_1, \\ldots, X_n \\sim \\text{Bernoulli}(p) with squared error loss. We know from the previous example that the Bayes estimator with prior \\text{Beta}(\\alpha, \\beta) has risk:\nR(p, \\hat{p}) = \\frac{np(1-p)}{(\\alpha + \\beta + n)^2} + \\left(\\frac{\\alpha - p(\\alpha + \\beta)}{\\alpha + \\beta + n}\\right)^2\nThe key insight: Can we choose \\alpha and \\beta to make this risk constant (independent of p)? If so, the constant risk theorem tells us the resulting estimator would be minimax.\nIt turns out that setting \\alpha = \\beta = \\sqrt{n/4} does exactly this!\n\n\n\n\n\n\nDerivation of the minimax prior\n\n\n\n\n\nIf we set \\alpha = \\beta, the risk becomes: R(p, \\hat{p}) = \\frac{1}{(2\\alpha + n)^2}\\left[np(1-p) + \\alpha^2(1 - 2p)^2\\right]\nExpanding the term in brackets: np(1-p) + \\alpha^2(1 - 2p)^2 = np - np^2 + \\alpha^2(1 - 4p + 4p^2) = \\alpha^2 + (n - 4\\alpha^2)p + (4\\alpha^2 - n)p^2\nThis is constant if and only if the coefficients of p and p^2 are both zero: - Coefficient of p: n - 4\\alpha^2 = 0 \\Rightarrow \\alpha^2 = n/4 - Coefficient of p^2: 4\\alpha^2 - n = 0 \\Rightarrow \\alpha^2 = n/4 ✓\nBoth conditions give the same answer: \\alpha = \\sqrt{n/4}.\n\n\n\nWith \\alpha = \\beta = \\sqrt{n/4}:\n\nThe Bayes estimator is: \\hat{p} = \\frac{S + \\sqrt{n/4}}{n + \\sqrt{n}}\nThe risk is constant: R(p, \\hat{p}) = \\frac{n}{4(n + \\sqrt{n})^2} for all p\nBy the theorem above, this makes it minimax!\n\nHow does this minimax estimator compare to the MLE?\nBy maximum risk (worst-case criterion):\n\nMLE: Maximum risk is \\frac{1}{4n} (at p = 1/2)\nMinimax: Constant risk \\frac{n}{4(n+\\sqrt{n})^2} &lt; \\frac{1}{4n}\n\nThe minimax estimator wins on worst-case performance - that’s what it was designed for!\nBut here’s the interesting part: even though the minimax estimator was derived from a Beta(\\sqrt{n/4}, \\sqrt{n/4}) prior, we can ask how it performs on average under any prior. For instance, under a uniform prior:\n\nMLE: Bayes risk = \\frac{1}{6n}\nMinimax: Bayes risk = \\frac{n}{4(n+\\sqrt{n})^2}\n\nFor n \\geq 20, the MLE has lower average risk under the uniform prior. This illustrates a key principle: the minimax estimator optimizes worst-case performance, but may sacrifice average-case performance to achieve this robustness.\n\n\n\n\n\n\n\n\nExample: Minimax Estimator for Normal Mean\n\n\n\nFor X_1, \\ldots, X_n \\sim \\mathcal{N}(\\theta, 1), the sample mean \\bar{X} has risk: R(\\theta, \\bar{X}) = \\mathbb{E}[(\\bar{X} - \\theta)^2] = \\text{Var}(\\bar{X}) = \\frac{1}{n}\nThis risk is constant (doesn’t depend on \\theta). Furthermore, \\bar{X} can be shown to be admissible (as it is the limit of admissible Bayes estimators for Normal priors). An admissible estimator with constant risk is minimax. Therefore, \\bar{X} is minimax.\nThis result holds for any “well-behaved” loss function (convex and symmetric about the origin).\n\n\n\n\n\n\n\n\nExample: Large Sample MLE\n\n\n\nIn most parametric models with large n, the MLE is approximately minimax. The intuition: as n \\rightarrow \\infty, the MLE becomes approximately Normal with variance 1/(nI(\\theta)) where I(\\theta) is the Fisher information. In many regular models, this leads to approximately constant risk.\nImportant caveat: This breaks down when the number of parameters grows with n. For example, in the “many Normal means” problem where we estimate n means from n observations, the MLE is far from minimax.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#admissibility-ruling-out-bad-estimators",
    "href": "chapters/08-bayesian-inference-decision-theory.html#admissibility-ruling-out-bad-estimators",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.7 Admissibility: Ruling Out Bad Estimators",
    "text": "8.7 Admissibility: Ruling Out Bad Estimators\nMinimax and Bayes estimators tell us about optimality according to specific criteria. But there’s a more basic requirement: an estimator shouldn’t be uniformly worse than another.\n\n8.7.1 Defining Admissibility\n\nAn estimator \\hat{\\theta} is inadmissible if there exists another estimator \\hat{\\theta}' such that:\n\nR(\\theta, \\hat{\\theta}') \\le R(\\theta, \\hat{\\theta}) for all \\theta\nR(\\theta, \\hat{\\theta}') &lt; R(\\theta, \\hat{\\theta}) for at least one \\theta\n\nOtherwise, \\hat{\\theta} is admissible.\n\nAn inadmissible estimator is dominated – there’s another estimator that’s never worse and sometimes better. Using an inadmissible estimator is irrational.\n\n\n\n\n\n\nAdmissibility ≠ Good\n\n\n\nThe constant estimator \\hat{\\theta} = 3 for X \\sim \\mathcal{N}(\\theta, 1) is admissible! Why? Any estimator that beats it at \\theta = 3 must be worse elsewhere. But it’s still a terrible estimator for most purposes.\nAdmissibility is a necessary but not sufficient condition for a good estimator.\n\n\n\n\n8.7.2 Key Properties and Connections\n\nA Bayes estimator for a prior with full support (positive density everywhere) is always admissible.\n\nThis is powerful: Bayesian methods automatically avoid inadmissible estimators.\nOther connections:\n\nConstant Risk and Admissibility: An admissible estimator with constant risk is minimax\nMinimax and Admissibility: Minimax estimators are usually admissible or “nearly” admissible\nMLE and Admissibility: The MLE is not always admissible, especially in high dimensions\n\n\n\n\n\n\n\nAdvanced: Stein’s Paradox and Shrinkage\n\n\n\n\n\nConsider estimating k \\geq 3 Normal means simultaneously. Let Y_i \\sim \\mathcal{N}(\\theta_i, 1) for i = 1, ..., k.\nThe Setup: We want to estimate \\theta = (\\theta_1, ..., \\theta_k) with total squared error loss: L(\\theta, \\hat{\\theta}) = \\sum_{i=1}^k (\\theta_i - \\hat{\\theta}_i)^2\nThe Paradox: The “obvious” estimator \\hat{\\theta}_i = Y_i (using each observation to estimate its own mean) is inadmissible when k \\geq 3!\nThe Solution: The James-Stein estimator \\hat{\\theta}_i^{JS} = \\left(1 - \\frac{k-2}{\\sum_j Y_j^2}\\right)^+ Y_i “shrinks” estimates toward zero and has uniformly lower risk than the MLE.\nThe Importance: This counterintuitive result revolutionized high-dimensional statistics. It shows that when estimating many parameters simultaneously, we can improve by “borrowing strength” across parameters. This is the foundation of modern regularization methods in machine learning.\nThe key insight: in high dimensions, the MLE can be improved by shrinkage toward a common value.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#chapter-summary-and-connections",
    "href": "chapters/08-bayesian-inference-decision-theory.html#chapter-summary-and-connections",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "8.8 Chapter Summary and Connections",
    "text": "8.8 Chapter Summary and Connections\n\n8.8.1 Key Concepts Review\nBayesian Inference:\n\nPosterior ∝ Likelihood × Prior: Bayes’ theorem provides the recipe for updating beliefs\nConjugate models: Beta-Bernoulli and Normal-Normal give closed-form posteriors\nPrior choice matters: Conjugate (convenient), non-informative (problematic), weakly informative (recommended)\nCredible intervals: Direct probability statements about parameters\n\nStatistical Decision Theory:\n\nLoss functions: Quantify the cost of estimation errors\nRisk functions: Expected loss – curves that are hard to compare\nBayes estimators: Minimize average risk over a prior\nMinimax estimators: Minimize worst-case risk\n\nKey Connections:\n\nPosterior mean = Bayes estimator for squared error loss\nConstant risk Bayes rules are minimax\nBayes rules are admissible\nIn large samples, Bayesian and frequentist methods converge\n\n\n\n8.8.2 The Big Picture\nThis chapter revealed two fundamental insights:\n\nBayesian inference provides a unified, probabilistic framework for learning from data. By treating parameters as random variables, we can make direct probability statements and naturally incorporate prior knowledge.\nDecision theory provides a formal language for evaluating and comparing any statistical procedure. The posterior mean is not just an arbitrary summary – it’s the optimal estimator under squared error loss.\n\nThe connection runs deeper: Bayesian methods naturally produce optimal estimators, while decision theory helps us understand when and why different approaches work well. Even frequentist stalwarts use decision theory, and the best frequentist estimators often have Bayesian interpretations.\n\n\n8.8.3 Common Pitfalls to Avoid\n\nConfusing credible and confidence intervals: A 95% credible interval contains \\theta with probability 0.95 given the data.5 A 95% confidence interval is produced by a procedure that traps \\theta in 95% of repeated experiments.\nThinking uniform priors are “uninformative”: They encode specific beliefs and aren’t transformation invariant.\nUsing conjugate priors blindly: Convenience shouldn’t override reasonable prior beliefs.\nForgetting the prior’s influence diminishes: With enough data, different reasonable priors lead to similar posteriors.6\nAssuming admissible = good: The constant estimator \\hat{\\theta} = 3 is admissible but useless.\n\n\n\n8.8.4 Chapter Connections\n\nPrevious (Ch. 5-7): We learned frequentist methods for finding and evaluating estimators. Now we have a completely different paradigm (Bayesian) and a unified theory (decision theory) for comparing estimators from any paradigm.\nThis Chapter: Introduced Bayesian thinking and formal decision theory. These provide alternative and complementary approaches to the frequentist methods we’ve studied.\nNext (Ch. 10): We’ll see how modern computational methods (MCMC, Stan) make Bayesian inference practical for complex models where conjugacy doesn’t help.\nApplications: Bayesian methods shine in hierarchical models, missing data problems, and anywhere prior information is valuable.\n\n\n\n8.8.5 Self-Test Problems\n\nBayesian Calculation: Given n=10 observations from a \\text{Poisson}(\\lambda) distribution with \\sum x_i = 30, and a prior \\lambda \\sim \\text{Gamma}(2, 1), find the posterior distribution for \\lambda. What is the Bayes estimator under squared error loss?\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nThe Gamma distribution is conjugate to the Poisson. Using the shape-rate parameterization (where \\beta is the rate parameter), if \\lambda \\sim \\text{Gamma}(\\alpha, \\beta) and we observe data with sum S, then \\lambda | \\text{data} \\sim \\text{Gamma}(\\alpha + S, \\beta + n). The posterior mean (Bayes estimator) is (\\alpha + S)/(\\beta + n).\n\n\n\nDecision Theory Concepts: Let X_1, \\ldots, X_n \\sim \\mathcal{N}(\\mu, 1). The MLE \\hat{\\mu} = \\bar{X} has risk 1/n under squared error loss.\n\n\nIs this risk constant?\n\n\nHow does \\hat{\\mu} relate to the Bayes estimator under a Normal prior? (One sentence.)\n\n\nIs \\hat{\\mu} minimax? Give a one-line justification.\n\n\nIs \\hat{\\mu} admissible? Give a one-line justification.\n\n\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\n\nYes, R(\\mu, \\bar{X}) = \\text{Var}(\\bar{X}) = 1/n is constant.\nWith prior \\mu \\sim \\mathcal{N}(a, b^2), the Bayes estimator is the posterior mean w\\bar{X} + (1-w)a, where w = \\frac{b^2}{b^2 + 1/n}; as b^2 \\to \\infty (very diffuse prior), this approaches \\bar{X}.\nConstant risk + admissibility ⇒ minimax (by results in the notes).\nYes, in the 1D Normal-mean problem with squared error, \\bar{X} is admissible (classical result), even though it’s a limit of Bayes rules.\n\n\n\n\nPrior Choice: You’re estimating the probability p that a new medical treatment works. You’re skeptical because most new treatments fail. What would be:\n\nA weakly informative prior for p?\nA strong prior reflecting your skepticism?\n\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nWeakly informative: Beta(1, 3) or Beta(1, 5) - allows all values but slightly favors lower success rates. Strong skeptical prior: Beta(1, 10) or Beta(1, 20) - strongly concentrates mass near 0, reflecting belief that the treatment likely doesn’t work. Remember: Beta parameters can be interpreted as pseudo-counts of successes and failures.\n\n\n\nConceptual Understanding: Why is the James-Stein estimator’s improvement over the MLE considered paradoxical? What does it tell us about estimating multiple parameters simultaneously?\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nThe paradox: When estimating three or more unrelated means (e.g., baseball batting average, physics constants, and rainfall), using information from all of them together (via shrinkage) gives better estimates than treating them separately. This violates our intuition that unrelated problems should be solved independently. The lesson: In high dimensions, “borrowing strength” across parameters through shrinkage reduces overall risk, even for unrelated parameters.\n\n\n\n\n\n\n8.8.6 Python and R Reference\nPythonR#| eval: false\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Conjugate Bayesian Inference\n\n## Beta-Bernoulli Model\ndef beta_bernoulli_posterior(n_successes, n_trials, alpha_prior=1, beta_prior=1):\n    \"\"\"\n    Compute posterior parameters for Beta-Bernoulli model.\n    Prior: Beta(alpha_prior, beta_prior)\n    Data: n_successes in n_trials\n    Posterior: Beta(alpha_post, beta_post)\n    \"\"\"\n    alpha_post = alpha_prior + n_successes\n    beta_post = beta_prior + (n_trials - n_successes)\n    return alpha_post, beta_post\n\n# Example usage\nn, s = 20, 12  # 12 successes in 20 trials\nalpha_post, beta_post = beta_bernoulli_posterior(s, n)\n\n# Posterior mean (Bayes estimator for squared error loss)\nposterior_mean = alpha_post / (alpha_post + beta_post)\n\n# 95% credible interval\nci_lower, ci_upper = stats.beta.ppf([0.025, 0.975], alpha_post, beta_post)\n\n# Visualize posterior\np_range = np.linspace(0, 1, 200)\nposterior = stats.beta.pdf(p_range, alpha_post, beta_post)\nplt.plot(p_range, posterior)\nplt.fill_between(p_range, posterior, \n                 where=(p_range &gt;= ci_lower) & (p_range &lt;= ci_upper),\n                 alpha=0.3, label='95% Credible Interval')\nplt.xlabel('p')\nplt.ylabel('Posterior density')\nplt.title(f'Beta({alpha_post}, {beta_post}) Posterior')\nplt.legend()\n\n## Normal-Normal Model  \ndef normal_normal_posterior(data, prior_mean=0, prior_var=1, data_var=1):\n    \"\"\"\n    Compute posterior for Normal-Normal conjugate model.\n    Prior: N(prior_mean, prior_var)\n    Likelihood: N(theta, data_var) for each observation\n    \"\"\"\n    n = len(data)\n    data_mean = np.mean(data)\n    \n    # Precision (1/variance) is additive\n    prior_precision = 1/prior_var\n    data_precision = n/data_var\n    post_precision = prior_precision + data_precision\n    \n    # Posterior parameters\n    post_var = 1/post_precision\n    post_mean = post_var * (prior_precision * prior_mean + \n                            data_precision * data_mean)\n    \n    return post_mean, post_var\n\n# Decision Theory\n\ndef compute_risk(estimator_func, true_theta, n_simulations=10000, n_samples=20):\n    \"\"\"\n    Estimate risk via simulation for squared error loss.\n    \"\"\"\n    losses = []\n    for _ in range(n_simulations):\n        # Generate data\n        data = np.random.normal(true_theta, 1, n_samples)\n        # Compute estimate\n        estimate = estimator_func(data)\n        # Compute loss\n        loss = (estimate - true_theta)**2\n        losses.append(loss)\n    \n    return np.mean(losses)  # Risk = expected loss\n\n# Example: Compare MLE and a shrinkage estimator\ndef mle_estimator(data):\n    return np.mean(data)\n\ndef shrinkage_estimator(data, shrink_target=0, shrink_factor=0.8):\n    mle = np.mean(data)\n    return shrink_factor * mle + (1 - shrink_factor) * shrink_target\n\n# Compare risks\ntheta_values = np.linspace(-3, 3, 50)\nrisk_mle = []\nrisk_shrink = []\n\nfor theta in theta_values:\n    risk_mle.append(compute_risk(mle_estimator, theta))\n    risk_shrink.append(compute_risk(shrinkage_estimator, theta))\n\nplt.plot(theta_values, risk_mle, label='MLE')\nplt.plot(theta_values, risk_shrink, label='Shrinkage')\nplt.xlabel('True θ')\nplt.ylabel('Risk')\nplt.title('Risk Functions Comparison')\nplt.legend()#| eval: false\nlibrary(ggplot2)\n\n# Conjugate Bayesian Inference\n\n## Beta-Bernoulli Model\nbeta_bernoulli_posterior &lt;- function(n_successes, n_trials, \n                                    alpha_prior = 1, beta_prior = 1) {\n  # Compute posterior parameters\n  alpha_post &lt;- alpha_prior + n_successes\n  beta_post &lt;- beta_prior + (n_trials - n_successes)\n  \n  list(alpha = alpha_post, beta = beta_post,\n       mean = alpha_post / (alpha_post + beta_post))\n}\n\n# Example usage\nn &lt;- 20\ns &lt;- 12\nposterior &lt;- beta_bernoulli_posterior(s, n)\n\n# 95% credible interval\nci &lt;- qbeta(c(0.025, 0.975), posterior$alpha, posterior$beta)\n\n# Visualize posterior\np_seq &lt;- seq(0, 1, length.out = 200)\nposterior_density &lt;- dbeta(p_seq, posterior$alpha, posterior$beta)\n\ndf &lt;- data.frame(p = p_seq, density = posterior_density)\nggplot(df, aes(x = p, y = density)) +\n  geom_line(size = 1.2) +\n  geom_area(data = subset(df, p &gt;= ci[1] & p &lt;= ci[2]),\n            alpha = 0.3, fill = \"blue\") +\n  labs(x = \"p\", y = \"Posterior density\",\n       title = sprintf(\"Beta(%g, %g) Posterior\", \n                      posterior$alpha, posterior$beta)) +\n  theme_minimal()\n\n## Normal-Normal Model\nnormal_normal_posterior &lt;- function(data, prior_mean = 0, prior_var = 1, \n                                   data_var = 1) {\n  n &lt;- length(data)\n  data_mean &lt;- mean(data)\n  \n  # Precision is additive\n  prior_precision &lt;- 1/prior_var\n  data_precision &lt;- n/data_var\n  post_precision &lt;- prior_precision + data_precision\n  \n  # Posterior parameters\n  post_var &lt;- 1/post_precision\n  post_mean &lt;- post_var * (prior_precision * prior_mean + \n                           data_precision * data_mean)\n  \n  list(mean = post_mean, var = post_var, sd = sqrt(post_var))\n}\n\n# Decision Theory\n\ncompute_risk &lt;- function(estimator_func, true_theta, \n                         n_simulations = 10000, n_samples = 20) {\n  # Estimate risk via simulation\n  losses &lt;- replicate(n_simulations, {\n    data &lt;- rnorm(n_samples, true_theta, 1)\n    estimate &lt;- estimator_func(data)\n    (estimate - true_theta)^2  # Squared error loss\n  })\n  \n  mean(losses)  # Risk = expected loss\n}\n\n# Example estimators\nmle_estimator &lt;- function(data) mean(data)\n\nshrinkage_estimator &lt;- function(data, target = 0, factor = 0.8) {\n  factor * mean(data) + (1 - factor) * target\n}\n\n# Compare risk functions\ntheta_seq &lt;- seq(-3, 3, length.out = 50)\nrisk_mle &lt;- sapply(theta_seq, function(theta) \n  compute_risk(mle_estimator, theta))\nrisk_shrink &lt;- sapply(theta_seq, function(theta) \n  compute_risk(shrinkage_estimator, theta))\n\ndf_risk &lt;- data.frame(\n  theta = rep(theta_seq, 2),\n  risk = c(risk_mle, risk_shrink),\n  estimator = rep(c(\"MLE\", \"Shrinkage\"), each = length(theta_seq))\n)\n\nggplot(df_risk, aes(x = theta, y = risk, color = estimator)) +\n  geom_line(size = 1.2) +\n  labs(x = \"True θ\", y = \"Risk\", \n       title = \"Risk Functions Comparison\") +\n  theme_minimal()\n\n\n8.8.7 Connections to Source Material\n\n\n\n\n\n\nMapping to “All of Statistics”\n\n\n\n\n\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding Source(s)\n\n\n\n\nIntroduction: A Different Way of Thinking\nFrom slides and AF447 case study from Stone et al. (2014)\n\n\n↳ The Two Philosophies of Statistics\nAoS §11.1 and slides\n\n\nThe Bayesian Method: Updating Beliefs with Data\nAoS §11.2\n\n\n↳ The Engine: Bayes’ Theorem for Inference\nAoS §11.2 plus slides\n\n\n↳ Summarizing the Posterior\nAoS §11.2; AoS §12.3 (for median/mode)\n\n\n↳ Credible vs. Confidence Intervals\nAoS §11.9 and expanded from lecture material\n\n\nBayesian Inference in Action\n\n\n\n↳ Conjugate Models and Conjugate Priors\nAoS §11.2\n\n\n↳ Example: The Bernoulli-Beta Model\nAoS Example 11.1 and slides\n\n\n↳ Example: The Normal-Normal Model\nAoS Example 11.2 and slides\n\n\n↳ The Art and Science of Choosing Priors\nAoS §11.6 expanded with modern views\n\n\n↳ Implementing Bayesian Inference\nAoS §11.4 expanded with modern tools\n\n\nStatistical Decision Theory\nAoS Ch 12\n\n\n↳ The Ingredients: Loss and Risk\nAoS §12.1\n\n\n↳ The Challenge of Comparing Risk Functions\nAoS §12.2 (Examples 12.2, 12.3)\n\n\nOptimal Estimators: Bayes and Minimax Rules\n\n\n\n↳ The Bayesian Approach: Minimizing Average Risk\nAoS §12.3\n\n\n↳ The Frequentist Approach: Minimizing Worst-Case Risk\nAoS §12.4\n\n\n↳ Example: Minimax Estimator for Bernoulli\nAoS Example 12.12\n\n\n↳ Example: Minimax Estimator for Normal Mean\nAoS Theorem 12.14, Theorem 12.22\n\n\n↳ Example: Large Sample MLE\nAoS §12.5\n\n\nAdmissibility: Ruling Out Bad Estimators\nAoS §12.6\n\n\n↳ Defining Admissibility\nAoS Definition 12.17\n\n\n↳ Key Properties and Connections\nAoS Theorem 12.19, Theorem 12.21\n\n\n↳ Advanced: Stein’s Paradox and Shrinkage\nAoS §12.7\n\n\nSelf-Test Problems\nBased on AoS Ch 11/12 exercises and concepts\n\n\n\n\n\n\n\n\n8.8.8 Further Materials\n\nFoundational Text: Gelman et al., “Bayesian Data Analysis” (3rd ed.)\nThe Air France Search Case Study: Stone et al. (2014).\nPrior Choice: See the Stan wiki.\n\n\nRemember: Bayesian inference is about updating beliefs with data. Decision theory is about choosing the best estimator. Together, they provide a complete framework for statistical inference that complements and enriches the frequentist approach. Master both paradigms – they each have their place in the modern statistician’s toolkit!\n\n\n\n\n\n\nStone, Lawrence D, Colleen M Keller, Thomas M Kratzke, and Johan P Strumpfer. 2014. “Search for the Wreckage of Air France Flight AF 447.” Statistical Science 29 (1): 69–80. https://doi.org/10.1214/13-STS420.\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/08-bayesian-inference-decision-theory.html#footnotes",
    "href": "chapters/08-bayesian-inference-decision-theory.html#footnotes",
    "title": "8  Bayesian Inference and Statistical Decision Theory",
    "section": "",
    "text": "The marginal likelihood is not just a constant. Since it encodes the probability of the data under a specific statistical model, it can be used as a metric for comparing different models.↩︎\nThe term “a posteriori” is Latin meaning “from what comes after” or “from the latter,” referring to knowledge that comes after observing evidence. This contrasts with “a priori” meaning “from what comes before” – knowledge before seeing data.↩︎\nAlso called a posterior interval in some texts, particularly older or more theoretical works. Both terms are correct and refer to the same concept.↩︎\nThe posterior mean is the Bayes estimator under squared error loss, as we will see in the following section.↩︎\nFor a given model and prior.↩︎\nIn regular, identifiable parametric models; this can fail in high dimensions or weakly identified settings.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Inference and Statistical Decision Theory</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html",
    "href": "chapters/09-linear-logistic-regression.html",
    "title": "9  Linear and Logistic Regression",
    "section": "",
    "text": "9.1 Learning Objectives\nAfter completing this chapter, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#learning-objectives",
    "href": "chapters/09-linear-logistic-regression.html#learning-objectives",
    "title": "9  Linear and Logistic Regression",
    "section": "",
    "text": "Build and interpret linear regression models to understand relationships between variables, connecting the geometric intuition of least squares with the statistical framework of maximum likelihood estimation.\nEvaluate regression coefficients meaningfully, including their practical interpretation, statistical significance via confidence intervals and hypothesis tests, and the crucial distinction between association and causation.\nDiagnose and address model inadequacies using residual plots to check assumptions, applying transformations when relationships are nonlinear, and selecting predictors using principled criteria (AIC, BIC, cross-validation).\nExtend regression to binary outcomes through logistic regression, understanding how the logit link enables probability modeling and interpreting coefficients as odds ratios.\nApply regression for both prediction and explanation, recognizing when linear models excel (interpretability needs, moderate sample sizes) versus when more complex methods are warranted.\n\n\n\n\n\n\n\nNote\n\n\n\nThis chapter introduces the two most fundamental and widely used models in statistics: linear and logistic regression. While modern machine learning offers powerful black-box predictors, linear models remain essential for their interpretability and foundational role in statistical inference. The material is adapted from Chapter 13 of Wasserman (2013), supplemented with modern perspectives on model interpretability and practical examples.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#introduction-why-linear-models-still-matter",
    "href": "chapters/09-linear-logistic-regression.html#introduction-why-linear-models-still-matter",
    "title": "9  Linear and Logistic Regression",
    "section": "9.2 Introduction: Why Linear Models Still Matter",
    "text": "9.2 Introduction: Why Linear Models Still Matter\n\n9.2.1 The Power of Interpretability\nIn an age dominated by complex machine learning models – neural networks with millions of parameters, random forests with thousands of trees, gradient boosting machines with intricate interactions – one might wonder: why dedicate an entire chapter to something as simple as linear regression?\nThe answer lies in a fundamental trade-off in statistical modeling: complexity versus interpretability. While a deep neural network might achieve better prediction accuracy on a complex dataset, it operates as a “black box”. We feed in inputs, we get outputs, but the mechanism connecting them remains opaque. This opacity becomes problematic when we need to:\n\nExplain predictions to stakeholders: “Why was this loan application denied?”\nIdentify which features matter most: “Which factors most strongly predict patient readmission?”\nEnsure fairness and avoid bias: “Is our model discriminating based on protected attributes?”\nDebug unexpected behavior: “Why did the model fail on this particular case?”\n\nLinear models excel at all these tasks. Every coefficient has a clear interpretation: it tells us exactly how a one-unit change in a predictor affects the outcome, holding all else constant. This interpretability has made linear models the reference tools in fields where understanding relationships is as important as making predictions – from economics to medicine to social sciences.\n\n\n\n\n\n\nAdvanced: Are Neural Networks Black Boxes?\n\n\n\n\n\nThe story we presented above of neural networks being completely black boxes is a simplification nowadays. The field of interpretability research has developed sophisticated methods to understand neural networks, including techniques like mechanistic interpretability, activation analysis, and circuit discovery. These approaches have yielded notable insights, for example for large language models.\nStill, there’s a crucial distinction: linear models are transparent by construction – each coefficient directly tells us how a unit change in input affects output. Neural networks must be reverse-engineered through complex analysis requiring specialized tools and expertise. Think of it as the difference between reading a recipe versus doing forensic analysis on a finished cake.\nIn short, while interpretability research continues to advance, linear models remain uniquely valuable when interpretability is a primary requirement rather than an afterthought – and linear models can be used as tools to understand more complex models, as seen in the next paragraph.\n\n\n\n\n\n9.2.2 Linear Models as Building Blocks\nBeyond their direct application, linear models serve as the foundation for building and understanding more complex methods:\n\nGeneralized Linear Models (GLMs) extend linear regression to handle non-normal outcomes\nMixed Effects Models add random effects to account for hierarchical data structures\nRegularized Regression (Ridge, LASSO, Elastic Net) adds penalties to handle high-dimensional data\nLocal Linear Methods like LOESS use linear regression in small neighborhoods for flexible curve fitting\n\nEven in the realm of “black-box” machine learning, linear models play a crucial role in model interpretation. Consider LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro, Singh, and Guestrin 2016), a popular technique for explaining individual predictions from any complex model. LIME works by:\n\nTaking a prediction from a complex model (e.g., “This image is 92% likely to contain a cat”)\nGenerating perturbed samples around the input of interest\nGetting predictions from the complex model for these perturbed samples\nFitting a simple linear model to approximate the complex model’s behavior locally\nUsing the linear model’s coefficients to explain which features drove the prediction\n\nIn essence, LIME uses the interpretability of linear models to shed light on the darkness of black-box predictions. When we can’t understand the global behavior of a complex model, we can at least understand its local behavior through the lens of linear approximation.\n\n\n9.2.3 This Chapter’s Goal\nOur goal in this chapter is to master both the theory and practice of linear and logistic regression. We’ll develop the mathematical framework, explore the connections to maximum likelihood estimation, and learn how to implement and interpret these models in practice. Along the way, we’ll address critical questions like:\n\nHow do we estimate regression coefficients and quantify our uncertainty about them?\nWhat assumptions underlie linear regression, and what happens when they’re violated?\nHow do we choose which predictors to include when we have many candidates?\nHow do we extend the framework to binary outcomes?\n\nBy the end of this chapter, you’ll have a thorough understanding of these fundamental models – knowledge that will serve you whether you’re building interpretable models for scientific research or using LIME to explain neural network predictions.\n\n\n\n\n\n\nHistorical Note: The Origins of “Regression”\n\n\n\n\n\nThe term “regression” might seem odd for a method that predicts one variable from others. Its origin lies in the work of Sir Francis Galton (1822-1911), who studied the relationship between parents’ heights and their children’s heights. Galton observed that while tall parents tended to have tall children and short parents short children, the children’s heights tended to be closer to the population mean than their parents’ heights.\nHe called this phenomenon “regression towards mediocrity” (later “regression to the mean”). Though the name stuck, modern regression analysis is far more general than Galton’s original application – it’s a comprehensive framework for modeling relationships between variables.\n\n\n\n\n\n\n\n\n\nFinnish Terminology Reference\n\n\n\n\n\nFor Finnish-speaking students, here’s a concise reference table of key terms in this chapter:\n\n\n\n\n\n\n\n\nEnglish\nFinnish\nContext\n\n\n\n\nRegression\nRegressio\nGeneral statistical method\n\n\nLinear regression\nLineaarinen regressio\nPredicting continuous outcomes\n\n\nSimple linear regression\nYhden selittäjän lineaarinen regressio\nOne predictor variable\n\n\nMultiple regression\nUsean selittäjän lineaarinen regressio\nMultiple predictor variables\n\n\nLogistic regression\nLogistinen regressio\nPredicting binary outcomes\n\n\nRegression function\nRegressiofunktio\nr(x) = \\mathbb{E}(Y \\mid X=x)\n\n\nResponse variable\nVastemuuttuja\nDependent variable\n\n\nPredictor variable\nSelittävä muuttuja (myös: kovariaatti, piirre)\nIndependent variable\n\n\nLeast squares\nPienimmän neliösumman menetelmä\nParameter estimation method\n\n\nResidual Sum of Squares (RSS)\nJäännösneliösumma\nFit criterion minimized by OLS\n\n\nResidual\nResiduaali, jäännös\nDifference between observed and predicted\n\n\nFitted value\nSovitettu arvo (myös: sovite; ennustettu arvo)\nModel prediction for a data point\n\n\nFitted line\nSovitettu suora\nLine of best fit\n\n\nCoefficient\nKerroin\nParameter in regression equation\n\n\nIntercept\nVakiotermi\nConstant term in regression\n\n\nSlope\nKulmakerroin\nRate of change parameter\n\n\nStandard error\nKeskivirhe\nUncertainty of an estimate\n\n\nConditional likelihood\nEhdollinen uskottavuus\nLikelihood given covariates\n\n\nR-squared\nSelitysaste (R^2)\nProportion of variance explained\n\n\nTraining error\nOpetusvirhe\nIn-sample error\n\n\nOverfitting\nYlisovitus (tai: liikasovitus)\nModel too complex for data\n\n\nUnderfitting\nVajaasovitus\nModel too simple for data\n\n\nAIC\nAkaiken informaatiokriteeri\nModel selection criterion\n\n\nBIC\nBayes-informaatiokriteeri\nModel selection criterion\n\n\nForward search/selection\nEtenevä haku (myös: eteenpäin valinta)\nGreedy model search\n\n\nStatistical control\nVakiointi\nIncluding background variables\n\n\nCross-validation\nRistiinvalidointi\nModel evaluation technique\n\n\nLeave-one-out cross-validation\nYksi-pois-ristiinvalidointi (myös: yksittäisristiinvalidointi)\nCV with one held-out point\n\n\np-value\np-arvo\nSignificance measure for tests\n\n\nConfidence interval\nLuottamusväli\nUncertainty quantification\n\n\nOdds ratio\nVetosuhde\nEffect measure in logistic regression\n\n\nLogit (log-odds)\nLogit-muunnos (logaritminen vetosuhde)\nLink in logistic regression\n\n\nInteraction term\nVuorovaikutustermi (interaktiotermi)\nEffect modification between predictors\n\n\nHomoscedasticity / Heteroscedasticity\nHomo-/heteroskedastisuus\n(Non-)constant error variance\n\n\nMulticollinearity\nMultikollineaarisuus\nCorrelation among predictors",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#simple-linear-regression",
    "href": "chapters/09-linear-logistic-regression.html#simple-linear-regression",
    "title": "9  Linear and Logistic Regression",
    "section": "9.3 Simple Linear Regression",
    "text": "9.3 Simple Linear Regression\n\n9.3.1 Regression Models\nRegression is a method for studying the relationship between a response variable Y and a covariate X. The response variable (also called the dependent variable) is what we’re trying to understand or predict – exam scores, blood pressure, sales revenue. The covariate is also called a predictor variable or feature – these are the variables we use to explain or predict the response, such as study hours, weight, or advertising spend.\nWhen we observe pairs of these variables, we naturally ask: how does Y tend to change as X varies? The answer lies in the regression function:\nr(x) = \\mathbb{E}(Y \\mid X=x) = \\int y f(y \\mid x) \\, dy\nThis function tells us the expected (average) value of Y for any given value of X. Think of it as the systematic part of the relationship – the signal beneath the noise. If we knew r(x) perfectly, we could say “when X = x, we expect Y to be around r(x), though individual observations will vary.”\nOur goal is to estimate the regression function r(x) from data of the form: (X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n) \\sim F_{X,Y}\nwhere each pair represents one observation drawn from some joint distribution. In this chapter, we take a parametric approach and assume that r has a specific functional form – namely, that it’s linear.\n\n\n9.3.2 Regression Notation\nBefore diving into linear regression specifically, let’s establish a basic notation. Any regression model can be written as:\nY = \\underbrace{r(X)}_{\\text{signal}} + \\underbrace{\\epsilon}_{\\text{noise}}\nwhere \\mathbb{E}(\\epsilon | X) = 0. This decomposition is fundamental: the observed response equals the signal (the systematic component we can predict from X) plus noise or error (the random variation we cannot predict).1\n\n\n\n\n\n\nWhy can we always write it this way?\n\n\n\n\n\nThe proof is simple. Define \\epsilon = Y - r(X), then: Y = Y + r(X) - r(X) = r(X) + \\epsilon.\nMoreover, since r(X) = \\mathbb{E}(Y \\mid X) by definition, we have: \\mathbb{E}(\\epsilon \\mid X) = \\mathbb{E}(Y - r(X) \\mid X) = \\mathbb{E}(Y \\mid X) - r(X) = 0\nThis shows that the decomposition isn’t just a notational convenience – it’s a mathematical fact that any joint distribution of (X, Y) can be decomposed into a predictable part (the conditional expectation) and an unpredictable part (the zero-mean error).\n\n\n\nThis notation separates what’s predictable (the regression function) from what’s unpredictable (the error term). The art and science of regression lies in finding good estimates for r(x) from finite data.\n\n\n9.3.3 The Simple Linear Regression Model\nIn simple linear regression, we assume r(x) is a linear function of one-dimensional X:\nr(x) = \\beta_0 + \\beta_1 x\nThis defines the simple linear regression model:\n\nSimple Linear Regression Model\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nwhere \\mathbb{E}(\\epsilon_i \\mid X_i) = 0 and \\mathbb{V}(\\epsilon_i \\mid X_i) = \\sigma^2.\nThe parameters have specific interpretations:\n\n\\beta_0 is the intercept: the expected value of Y when X = 0\n\\beta_1 is the slope: the expected change in Y for a one-unit increase in X\n\\sigma^2 is the error variance: how much individual observations vary around the line\n\n\nThis model makes a bold claim: the relationship between X and Y can be adequately captured by a straight line, with all deviations from this line being random noise with constant variance. The assumption \\mathbb{E}(\\epsilon_i \\mid X_i) = 0 ensures the line goes through the “middle” of the data at each value of X, while \\mathbb{V}(\\epsilon_i \\mid X_i) = \\sigma^2 (homoscedasticity) means the scatter around the line is equally variable across all values of X.\n\n\n\n\n\n\nWhen is Linearity Reasonable?\n\n\n\nMore often than you might think! Many relationships are approximately linear, at least over the range of observed data. Even when the true relationship is nonlinear, linear regression often provides a useful first-order approximation – much like how we can approximate curves with tangent lines in calculus.\nConsider height and weight, income and spending, or temperature and ice cream sales. While none of these relationships are perfectly linear across all possible values, they’re often reasonably linear within the range we observe. And sometimes that’s all we need for useful predictions and insights.\n\n\nOnce we’ve estimated the parameters – which we’ll discuss next –, we can make predictions and assess our model:\n\nRegression Terminology\nLet \\hat{\\beta}_0 and \\hat{\\beta}_1 denote estimates of \\beta_0 and \\beta_1. Then:\n\nThe fitted line is: \\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\nThe predicted values or fitted values are: \\hat{Y}_i = \\hat{r}(X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\nThe residuals are: \\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i)\n\n\nThe residuals are crucial – they represent what our model doesn’t explain. Small residuals mean our line fits well; large residuals suggest either a poor fit or inherent variability in the relationship.\n\n\n9.3.4 Estimating Parameters: The Method of Least Squares\nNow comes the central question: given our data, how do we find the “best” line? What values should we choose for \\hat{\\beta}_0 and \\hat{\\beta}_1?\nThe most common answer is the method of least squares. The idea is to find the line that minimizes the Residual Sum of Squares (RSS):\n\\text{RSS} = \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2 = \\sum_{i=1}^{n} (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2\nThe RSS measures how well the line fits the data – it’s the sum of squared vertical distances from the points to the line. The least squares estimates are the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that make RSS as small as possible.\nIntuitiveMathematicalComputationalImagine you’re trying to draw a line through a cloud of points on a\nscatter plot. You want the line to be “close” to all the points\nsimultaneously. But what does “close” mean?For each point, we have an error: how far the point lies above the\nline (positive error) or below the line (negative error). We need to\naggregate these errors into a single measure of fit. We have several\noptions:\nSum of errors: Won’t work – positive and\nnegative errors cancel out. A terrible line far above half the points\nand far below the others could have zero total error!\nSum of absolute errors: This works (no\ncancellation), but absolute values are mathematically inconvenient –\nthey’re not differentiable at zero, making optimization harder.\nSum of squared errors: This is the winner!\nSquaring prevents cancellation, penalizes large errors more than small\nones (outliers matter), and is mathematically convenient allowing\nclosed-form solutions.\nThe least squares line is the one that minimizes this sum of squared\nerrors. It’s the line that best “threads through” the cloud of points in\nthe squared-error sense.To find the least squares estimates, we minimize RSS with respect to\n\\(\\hat{\\beta}_0\\) and\n\\(\\hat{\\beta}_1\\). Taking partial\nderivatives and setting them to zero:\\[\\frac{\\partial \\text{RSS}}{\\partial \\hat{\\beta}_0} = -2\\sum_{i=1}^{n} (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0\\]\\[\\frac{\\partial \\text{RSS}}{\\partial \\hat{\\beta}_1} = -2\\sum_{i=1}^{n} X_i(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0\\]These are called the normal equations. From the\nfirst equation:\n\\[\\sum_{i=1}^{n} Y_i = n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^{n} X_i\\]Dividing by \\(n\\) gives:\n\\(\\bar{Y}_n = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X}_n\\),\nwhich shows the fitted line passes through\n\\((\\bar{X}_n, \\bar{Y}_n)\\).From the second equation and some algebra (expanding the products and\nusing the first equation), we get:\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} X_i Y_i - n\\bar{X}_n\\bar{Y}_n}{\\sum_{i=1}^{n} X_i^2 - n\\bar{X}_n^2} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X}_n)(Y_i - \\bar{Y}_n)}{\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2}\\]This is the sample covariance of\n\\(X\\) and\n\\(Y\\) divided by the sample variance of\n\\(X\\).Let’s see least squares in action. We’ll generate some synthetic data\nwith a known linear relationship plus noise, then find the least squares\nline. The visualization will show two key perspectives:\nThe fitted line with residuals: How well the line\nfits the data and what the residuals look like\nThe optimization landscape: How RSS changes as we\nvary the slope, showing that our formula finds the minimum\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some example data\nnp.random.seed(42)\nn = 30\nX = np.random.uniform(0, 10, n)\ntrue_beta0, true_beta1 = 2, 1.5\nY = true_beta0 + true_beta1 * X + np.random.normal(0, 2, n)\n\n# Calculate least squares estimates\nX_mean = np.mean(X)\nY_mean = np.mean(Y)\nbeta1_hat = np.sum((X - X_mean) * (Y - Y_mean)) / np.sum((X - X_mean)**2)\nbeta0_hat = Y_mean - beta1_hat * X_mean\n\n# Create visualization with stacked subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n\n# Top panel: Show the residuals\nax1.scatter(X, Y, alpha=0.6, s=50)\nX_plot = np.linspace(0, 10, 100)\nY_plot = beta0_hat + beta1_hat * X_plot\nax1.plot(X_plot, Y_plot, 'r-', linewidth=2, label=f'Fitted line: Y = {beta0_hat:.2f} + {beta1_hat:.2f}X')\n\n# Draw residuals\nfor i in range(n):\n    Y_pred = beta0_hat + beta1_hat * X[i]\n    ax1.plot([X[i], X[i]], [Y[i], Y_pred], 'g--', alpha=0.5, linewidth=0.8)\n    \nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_title('Least Squares Minimizes Squared Residuals')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Bottom panel: Show RSS as a function of slope\nslopes = np.linspace(0.5, 2.5, 100)\nrss_values = []\nfor slope in slopes:\n    intercept = Y_mean - slope * X_mean  # Best intercept given slope\n    residuals = Y - (intercept + slope * X)\n    rss = np.sum(residuals**2)\n    rss_values.append(rss)\n\nax2.plot(slopes, rss_values, 'b-', linewidth=2)\nax2.axvline(beta1_hat, color='r', linestyle='--', linewidth=2, label=f'Optimal slope = {beta1_hat:.2f}')\nax2.scatter([beta1_hat], [min(rss_values)], color='r', s=100, zorder=5)\nax2.set_xlabel('Slope (β₁)')\nax2.set_ylabel('Residual Sum of Squares')\nax2.set_title('RSS as a Function of Slope')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Least squares estimates: β₀ = {beta0_hat:.3f}, β₁ = {beta1_hat:.3f}\")\nprint(f\"Minimum RSS = {min(rss_values):.2f}\")\n\n\n\n\nLeast squares estimates: β₀ = 2.858, β₁ = 1.221\nMinimum RSS = 80.61\n\nThe top panel shows the fitted line and the residuals (green dashed\nlines). Notice how the line “threads through” the cloud of points,\nbalancing the errors above and below. The residuals visualize what we’re\nminimizing – we want these vertical distances (squared) to be as small\nas possible in total.The bottom panel reveals the optimization landscape. As we vary the\nslope \\(\\beta_1\\) (while adjusting\n\\(\\beta_0\\) optimally for each slope to\nmaintain the constraint that the line passes through\n\\((\\bar{X}, \\bar{Y})\\)), the RSS forms\na parabola with a clear minimum. The red dashed line marks where our\nformula places us – exactly at the minimum! This confirms that the least\nsquares formula genuinely finds the best fit in terms of minimizing\nsquared errors.\n\nThe values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the RSS are:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X}_n)(Y_i - \\bar{Y}_n)}{\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2} \\quad \\text{(slope)}\n\\hat{\\beta}_0 = \\bar{Y}_n - \\hat{\\beta}_1 \\bar{X}_n \\quad \\text{(intercept)}\nwhere \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i and \\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^{n} Y_i are the sample means.\n\nThese formulas have intuitive interpretations:\n\nThe slope \\hat{\\beta}_1 is essentially the sample covariance between X and Y divided by the sample variance of X\nThe intercept \\hat{\\beta}_0 is chosen so the fitted line passes through the point (\\bar{X}_n, \\bar{Y}_n) – the center of the data\n\nLeast squares is very convenient because it has a closed-form solution. We don’t need iterative algorithms or numerical optimization2 – just plug the data into our formulas and we get the optimal answer.\n\n\n\n\n\n\nEstimating Error Variance\n\n\n\nWe also need to estimate the error variance \\sigma^2. An unbiased estimator is:\n\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2\nWhy divide by n-2 instead of n? We’ve estimated two parameters (\\beta_0 and \\beta_1), which costs us two degrees of freedom. This adjustment ensures our variance estimate is unbiased.\n\n\n\n\n9.3.5 Connection to Maximum Likelihood Estimation\nSo far, we’ve motivated least squares geometrically – it finds the line that minimizes squared distances. But there’s a deeper connection to the likelihood principle we studied in previous chapters.\nAdding the Normality Assumption\nSuppose we strengthen our assumptions by specifying that the errors are normally distributed: \\epsilon_i \\mid X_i \\sim \\mathcal{N}(0, \\sigma^2)\nThis implies that: Y_i \\mid X_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X_i, \\sigma^2)\nEach observation follows a normal distribution centered at the regression line, with constant variance \\sigma^2.\nThe Likelihood Function\nUnder these assumptions, we can write the likelihood function. The joint density of all observations is:\n\\prod_{i=1}^{n} f(X_i, Y_i) = \\prod_{i=1}^{n} f_X(X_i) \\times \\prod_{i=1}^{n} f_{Y|X}(Y_i | X_i)\nThe first term doesn’t involve our parameters \\beta_0 and \\beta_1, so we focus on the second term – the conditional likelihood:\n\\mathcal{L}(\\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^{n} f_{Y|X}(Y_i | X_i) \\propto \\sigma^{-n} \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\\right\\}\nThe conditional log-likelihood is:\n\\ell(\\beta_0, \\beta_1, \\sigma) = -n \\log \\sigma - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\n\n\n\n\n\nThe Key Insight\n\n\n\nTo maximize the log-likelihood with respect to \\beta_0 and \\beta_1, we must minimize the sum: \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\nBut this is exactly the RSS! Therefore, under the normality assumption, the least squares estimators are also the maximum likelihood estimators.\nThis is a profound connection: the simple geometric idea of minimizing squared distances coincides with the principled statistical approach of maximum likelihood, at least when errors are normal.\n\n\n\n\n\n\n\n\nWhat about estimating σ²?\n\n\n\n\n\nFrom the conditional log-likelihood above we can also derive the MLE for the error variance: \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2\nHowever, this estimator is biased – it systematically underestimates the true variance. In practice, we use the unbiased estimator: \\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2\nThe difference is small for large n, but the unbiased version provides more accurate confidence intervals and hypothesis tests, which is why it’s standard in linear regression.\n\n\n\n\n\n9.3.6 Properties of the Least Squares Estimators\nUnderstanding the statistical properties of our estimates is crucial for inference. How accurate are they? How does accuracy improve with more data? Can we quantify our uncertainty?\nFinite Sample Properties\n\nLet \\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T denote the least squares estimators. Assume that:\n\n\\mathbb{E}(\\epsilon_i \\mid X^n) = 0 for all i\n\\mathbb{V}(\\epsilon_i \\mid X^n) = \\sigma^2 for all i (homoscedasticity)\n\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X^n) = 0 for i \\neq j (uncorrelated errors)\n\nThen, conditional on X^n = (X_1, \\ldots, X_n):\n\\mathbb{E}(\\hat{\\beta} \\mid X^n) = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n\\mathbb{V}(\\hat{\\beta} \\mid X^n) = \\frac{\\sigma^2}{n s_X^2} \\begin{pmatrix} \\frac{1}{n}\\sum_{i=1}^{n} X_i^2 & -\\bar{X}_n \\\\ -\\bar{X}_n & 1 \\end{pmatrix}\nwhere s_X^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 is the sample variance of X.\n\nThis theorem tells us that the least squares estimators are unbiased – on average, they hit the true values. The variance formula allows us to compute standard errors:\n\\widehat{\\text{se}}(\\hat{\\beta}_0) = \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{\\sum_{i=1}^{n} X_i^2}{n}} \\quad \\text{and} \\quad \\widehat{\\text{se}}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}}\nThese standard errors quantify the uncertainty in our estimates. Notice that both decrease with \\sqrt{n} – more data means more precision.\n\n\n\n\n\n\nAdvanced: Derivation of Standard Errors\n\n\n\n\n\nThe variance formula comes from the fact that \\hat{\\beta}_1 is a linear combination of the Y_i’s: \\hat{\\beta}_1 = \\sum_{i=1}^{n} w_i Y_i where w_i = \\frac{X_i - \\bar{X}_n}{\\sum_{j=1}^{n}(X_j - \\bar{X}_n)^2}.\nSince the Y_i’s are independent with variance \\sigma^2: \\text{Var}(\\hat{\\beta}_1 \\mid X^n) = \\sigma^2 \\sum_{i=1}^{n} w_i^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2} = \\frac{\\sigma^2}{n s_X^2}\nThe derivation for \\hat{\\beta}_0 is similar but more involved due to its dependence on both \\bar{Y}_n and \\hat{\\beta}_1.\n\n\n\nAsymptotic Properties and Inference\n\nUnder appropriate regularity conditions, as n \\to \\infty:\n\nConsistency: \\hat{\\beta}_0 \\xrightarrow{P} \\beta_0 and \\hat{\\beta}_1 \\xrightarrow{P} \\beta_1\nAsymptotic Normality: \\frac{\\hat{\\beta}_0 - \\beta_0}{\\widehat{\\text{se}}(\\hat{\\beta}_0)} \\rightsquigarrow \\mathcal{N}(0, 1) \\quad \\text{and} \\quad \\frac{\\hat{\\beta}_1 - \\beta_1}{\\widehat{\\text{se}}(\\hat{\\beta}_1)} \\rightsquigarrow \\mathcal{N}(0, 1)\nConfidence Intervals: An approximate (1-\\alpha) confidence interval for \\beta_j is: \\hat{\\beta}_j \\pm z_{\\alpha/2} \\cdot \\widehat{\\text{se}}(\\hat{\\beta}_j)\nHypothesis Testing: The Wald test for H_0: \\beta_1 = 0 vs. H_1: \\beta_1 \\neq 0 is: reject H_0 if |W| &gt; z_{\\alpha/2} where W = \\hat{\\beta}_1 / \\widehat{\\text{se}}(\\hat{\\beta}_1)\n\n\n\n\n\n\n\n\nFinite Sample Refinement\n\n\n\n\n\nFor finite n, a more accurate test uses the Student’s t-distribution with n-2 degrees of freedom rather than the normal distribution. This accounts for the additional uncertainty from estimating \\sigma^2. Most statistical software uses the t-distribution by default for regression inference.\n\n\n\n\n\n9.3.7 Simple Linear Regression in Practice\nLet’s see how these concepts work with real data. We’ll use the classic Framingham Heart Study dataset to explore the relationship between weight and blood pressure, building our understanding step by step.\n\n\n\n\n\n\nExample: The Framingham Heart Study\n\n\n\nThe Framingham Heart Study is a long-term cardiovascular study that began in 1948 in Framingham, Massachusetts. This landmark study has provided crucial insights into cardiovascular disease risk factors. We’ll examine the relationship between relative weight (FRW - a normalized weight measure where 100 represents median weight for height) and systolic blood pressure (SBP - the pressure when the heart beats).\n\n\n\nStep 1: Loading and Exploring the Data\nFirst, let’s load the data and understand what we’re working with:\nPythonR\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom scipy import stats\n\n# Load the Framingham data\nfram = pd.read_csv('../data/fram.txt', sep='\\t', index_col=0)\n\n# Display first few rows to understand the structure\nprint(\"First 6 rows of the Framingham data:\")\nprint(fram.head(6))\nprint(f\"\\nDataset shape: {fram.shape}\")\nprint(f\"Columns: {list(fram.columns)}\")\n\nFirst 6 rows of the Framingham data:\n         SEX  AGE  FRW  SBP  SBP10  DBP  CHOL  CIG  CHD YRS_CHD  DEATH  \\\nID                                                                       \n4988  female   57  135  186    NaN  120   150    0    1     pre      7   \n3001  female   60  123  165    NaN  100   167   25    0      16     10   \n5079  female   54  115  140    NaN   90   213    5    0       8      8   \n5162  female   52  102  170    NaN  104   280   15    0      10      7   \n4672  female   45   99  185    NaN  105   326   20    0       8     10   \n5822  female   51   93  142    NaN   90   234   35    0       4      8   \n\n      YRS_DTH    CAUSE  \nID                      \n4988       11  unknown  \n3001       17  unknown  \n5079       13  unknown  \n5162       11  unknown  \n4672       17  unknown  \n5822       13  unknown  \n\nDataset shape: (1394, 13)\nColumns: ['SEX', 'AGE', 'FRW', 'SBP', 'SBP10', 'DBP', 'CHOL', 'CIG', 'CHD', 'YRS_CHD', 'DEATH', 'YRS_DTH', 'CAUSE']\n\n\n# Focus on our variables of interest\nprint(\"\\nSummary statistics for key variables:\")\nprint(fram[['FRW', 'SBP']].describe())\n\n\nSummary statistics for key variables:\n               FRW          SBP\ncount  1394.000000  1394.000000\nmean    105.365136   148.086083\nstd      17.752489    28.022062\nmin      52.000000    90.000000\n25%      94.000000   130.000000\n50%     103.000000   142.000000\n75%     114.000000   160.000000\nmax     222.000000   300.000000\n\n# Load the data\nfram &lt;- read.csv('../data/fram.txt', sep='\\t', row.names = 1)\n\n# Display first few rows to understand the structure\ncat(\"First 6 rows of the Framingham data:\\n\")\nhead(fram)\n\ncat(\"\\nDataset dimensions:\", dim(fram), \"\\n\")\ncat(\"Column names:\", names(fram), \"\\n\")\n\n# Summary statistics for key variables\nsummary(fram[c(\"FRW\", \"SBP\")])\n\n\n\n\n\n\nUnderstanding the Variables\n\n\n\n\nThe dataset contains 1,394 observations with 13 variables including demographics, behaviors, and health outcomes.\nFRW (Framingham Relative Weight): A normalized measure where 100 represents the median weight for a given height. Values above 100 indicate above-median weight.\nSBP (Systolic Blood Pressure): Measured in mmHg, normal range is typically below 120. Values ≥140 indicate hypertension.\n\n\n\n\n\nStep 2: Initial Visualization\nBefore fitting any model, let’s visualize the relationship between weight and blood pressure:\nPythonR\n# Create scatter plot to explore the relationship\nplt.figure(figsize=(7, 5))\nplt.scatter(fram['FRW'], fram['SBP'], alpha=0.5, s=20)\nplt.xlabel('Relative Weight (FRW)')\nplt.ylabel('Systolic Blood Pressure (SBP)')\nplt.title('Relationship between Relative Weight and Blood Pressure')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n# Create scatter plot to explore the relationship\nplot(fram$FRW, fram$SBP, \n     xlab = \"Relative Weight (FRW)\",\n     ylab = \"Systolic Blood Pressure (SBP)\",\n     main = \"Relationship between Relative Weight and Blood Pressure\",\n     pch = 16, col = rgb(0, 0, 0, 0.3))\ngrid()\n\n\n\n\n\n\nWhat the Visualization Tells Us\n\n\n\nLooking at this scatter plot, we can observe:\n\nThere appears to be a positive relationship: As relative weight increases, blood pressure tends to increase.\nThere’s substantial variation: The wide spread of points suggests that weight alone won’t perfectly predict blood pressure - other factors must also be important.\nThe pattern looks compatible with the linearity hypothesis, in the sense that there’s no obvious curve or nonlinear pattern that would suggest a straight line is inappropriate.\n\nThis visualization motivates our next step: fitting a linear model to quantify this relationship.\n\n\n\n\nStep 3: Fitting the Linear Regression Model\nNow let’s fit the simple linear regression model: \\text{SBP} = \\beta_0 + \\beta_1 \\cdot \\text{FRW} + \\epsilon.\nLinear regression with a quadratic loss is also called “ordinary least squares” (OLS), a term which can be found in statistical software like in Python’s statsmodel package.\nPythonR\n# Fit a linear regression model using statsmodels\nmodel = smf.ols('SBP ~ FRW', data=fram)\nresults = model.fit()\n\n# Show the fitted equation\nprint(f\"Fitted equation: SBP = {results.params['Intercept']:.2f} + {results.params['FRW']:.3f} * FRW\")\n\nFitted equation: SBP = 92.87 + 0.524 * FRW\n\n# Fit the linear regression model\nfit &lt;- lm(SBP ~ FRW, data = fram)\n\n# Show the fitted equation\ncat(sprintf(\"Fitted equation: SBP = %.2f + %.3f * FRW\\n\", \n            coef(fit)[1], coef(fit)[2]))\nWe’ve now fitted our linear regression model. The fitted equation shows the estimated relationship between relative weight and blood pressure. In the following steps, we’ll use this model to make predictions, visualize the fit, check our assumptions, and interpret the statistical significance of our findings.\n\n\nStep 4: Making Predictions\nLet’s use our model to make predictions for specific weight values:\nPythonR\n# Make predictions for specific FRW values\ntest_weights = pd.DataFrame({'FRW': [80, 100, 120]})\npredictions = results.predict(test_weights)\n\nprint(\"Predictions for new data points:\")\nprint(\"=\"*40)\nfor frw, sbp in zip(test_weights['FRW'], predictions):\n    print(f\"FRW = {frw:3d} → Predicted SBP = {sbp:.1f}\")\n\n# Manual calculation to show the mechanics\nprint(\"\\nManual calculation (verifying our understanding):\")\nbeta0 = results.params['Intercept']\nbeta1 = results.params['FRW']\nfor frw in [80, 100, 120]:\n    manual_pred = beta0 + beta1 * frw\n    print(f\"FRW = {frw:3d} → {beta0:.3f} + {beta1:.3f} × {frw} = {manual_pred:.1f}\")\n\nPredictions for new data points:\n========================================\nFRW =  80 → Predicted SBP = 134.8\nFRW = 100 → Predicted SBP = 145.3\nFRW = 120 → Predicted SBP = 155.8\n\nManual calculation (verifying our understanding):\nFRW =  80 → 92.866 + 0.524 × 80 = 134.8\nFRW = 100 → 92.866 + 0.524 × 100 = 145.3\nFRW = 120 → 92.866 + 0.524 × 120 = 155.8\n\n# Make predictions for specific FRW values\ntest_weights &lt;- data.frame(FRW = c(80, 100, 120))\npredictions &lt;- predict(fit, newdata = test_weights)\n\ncat(\"Predictions for new data points:\\n\")\ncat(paste(rep(\"=\", 40), collapse=\"\"), \"\\n\")\nfor(i in 1:nrow(test_weights)) {\n    cat(sprintf(\"FRW = %3d → Predicted SBP = %.1f\\n\", \n                test_weights$FRW[i], predictions[i]))\n}\n\n# Manual calculation\nbeta0 &lt;- coef(fit)[1]\nbeta1 &lt;- coef(fit)[2]\ncat(\"\\nManual calculation (verifying our understanding):\\n\")\nfor(frw in c(80, 100, 120)) {\n    manual_pred &lt;- beta0 + beta1 * frw\n    cat(sprintf(\"FRW = %3d → %.3f + %.3f × %d = %.1f\\n\", \n                frw, beta0, beta1, frw, manual_pred))\n}\n\n\nStep 5: Visualizing the Fitted Model\nLet’s plot the fitted model:\nPythonR\n# Basic fitted line visualization\nplt.figure(figsize=(7, 5))\nplt.scatter(fram['FRW'], fram['SBP'], alpha=0.4, s=20)\nx_range = np.linspace(fram['FRW'].min(), fram['FRW'].max(), 100)\ny_pred = results.params['Intercept'] + results.params['FRW'] * x_range\nplt.plot(x_range, y_pred, 'r-', linewidth=2, label='Fitted line')\nplt.xlabel('Relative Weight (FRW)')\nplt.ylabel('Systolic Blood Pressure (SBP)')\nplt.title('Fitted Regression Line')\n# Add equation to the plot\nequation = f'SBP = {results.params[\"Intercept\"]:.1f} + {results.params[\"FRW\"]:.3f} × FRW'\nplt.text(0.05, 0.95, equation, transform=plt.gca().transAxes, fontsize=11,\n         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n# Basic fitted line visualization\nplot(fram$FRW, fram$SBP, \n     xlab = \"Relative Weight (FRW)\",\n     ylab = \"Systolic Blood Pressure (SBP)\",\n     main = \"Fitted Regression Line\",\n     pch = 16, col = rgb(0, 0, 0, 0.4))\nabline(fit, col = \"red\", lwd = 2)\n# Add equation to the plot\nequation &lt;- paste(\"SBP =\", round(coef(fit)[1], 1), \"+\", \n                  round(coef(fit)[2], 3), \"× FRW\")\nlegend(\"topleft\", legend = equation, \n       bty = \"n\", cex = 1.1,\n       text.col = \"black\", bg = rgb(1, 0.96, 0.8, 0.8))\ngrid()\nThe plot above shows our fitted regression line. The equation in the box gives us the specific relationship: for each unit increase in relative weight (FRW), systolic blood pressure increases by approximately half a mmHg on average (see the exact coefficient in the equation above).\nThe plot below shows the geometric interpretation of intercept (\\beta_0) and slope (\\beta_1) parameters:\n\n\nShow code\n# Geometric interpretation of intercept and slope\nplt.figure(figsize=(7, 6))\nplt.scatter(fram['FRW'], fram['SBP'], alpha=0.4, s=20)\n\n# Extend x range to show intercept\nx_extended = np.linspace(0, fram['FRW'].max(), 100)\ny_extended = results.params['Intercept'] + results.params['FRW'] * x_extended\nplt.plot(x_extended, y_extended, 'r-', linewidth=2)\n\n# Show intercept (β₀) at x=0\nplt.plot([0, 0], [0, results.params['Intercept']], 'green', linewidth=3)\nplt.plot(0, results.params['Intercept'], 'go', markersize=8)\nplt.annotate(r'$\\beta_0$' + f' = {results.params[\"Intercept\"]:.1f}', \n            xy=(0, results.params['Intercept']), xytext=(10, results.params['Intercept'] - 10),\n            fontsize=11, color='green', fontweight='bold',\n            arrowprops=dict(arrowstyle='-&gt;', color='green', lw=1))\n\n# Show slope (β₁) interpretation\nx_demo = [40, 60]\ny_demo = [results.params['Intercept'] + results.params['FRW'] * x for x in x_demo]\nplt.plot(x_demo, [y_demo[0], y_demo[0]], 'b-', linewidth=3)\nplt.plot([x_demo[1], x_demo[1]], y_demo, 'b-', linewidth=3)\nplt.annotate('Δx = 20', xy=(50, y_demo[0] - 15), fontsize=10, color='blue', ha='center')\nplt.annotate(f'Δy = {20 * results.params[\"FRW\"]:.1f}', xy=(x_demo[1] + 2, np.mean(y_demo)), \n            fontsize=10, color='blue', rotation=90, va='center')\nplt.text(20, y_demo[0] + 25, r'$\\beta_1$' + f' = {results.params[\"FRW\"]:.3f}', \n         fontsize=11, color='blue', fontweight='bold',\n         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n\nplt.xlim(-5, fram['FRW'].max() + 5)\nplt.ylim(0, fram['SBP'].max() + 10)\nplt.xlabel('Relative Weight (FRW)')\nplt.ylabel('Systolic Blood Pressure (SBP)')\nplt.title(r'Geometric Interpretation: $\\beta_0$ (intercept) and $\\beta_1$ (slope)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 6: Model Diagnostics\nBefore interpreting our regression results, let’s check if our model assumptions are satisfied using diagnostic plots:\nPythonR\n# Create diagnostic plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n\n# Get residuals and fitted values\nresiduals = results.resid\nfitted = results.fittedvalues\n\n# Plot 1: Residuals vs Fitted Values\nax1.scatter(fitted, residuals, alpha=0.5, s=20)\nax1.axhline(y=0, color='r', linestyle='--', linewidth=1)\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Residuals')\nax1.set_title('Residuals vs Fitted Values')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Q-Q plot for normality check\nstats.probplot(residuals, dist=\"norm\", plot=ax2)\nax2.set_title('Normal Q-Q Plot')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n# Create diagnostic plots\npar(mfrow = c(2, 1))\n\n# Plot 1: Residuals vs Fitted\nplot(fit, which = 1)\n\n# Plot 2: Q-Q plot\nplot(fit, which = 2)\n\n# Reset plot layout\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nWhat the Diagnostic Plots Tell Us\n\n\n\nResiduals vs Fitted: The residuals are approximately centered around zero with reasonably random scatter, though we can see some evidence of skew (asymmetry) in the distribution.\nQ-Q Plot (Quantile-Quantile Plot): This plot compares the distribution of our residuals to a normal distribution. If residuals were perfectly normal, all points would fall exactly on the diagonal line. Here we see the points roughly follow the line but with some deviation at the extremes (the tails), confirming that the residuals are not perfectly normally distributed.\nImportant: These minor violations of assumptions are common with real data and not necessarily dealbreakers. Linear regression is quite robust to modest departures from normality, especially with large samples like ours (n=1,394). However, we should keep these limitations in mind when interpreting results and consider transformations or robust methods if violations become severe.\n\n\n\n\nStep 7: Model Interpretation\nNow let’s examine and interpret the full regression output:\nPythonR\n# Display the full regression summary\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    SBP   R-squared:                       0.110\nModel:                            OLS   Adj. R-squared:                  0.110\nMethod:                 Least Squares   F-statistic:                     172.5\nDate:                Sun, 07 Sep 2025   Prob (F-statistic):           3.18e-37\nTime:                        17:33:30   Log-Likelihood:                -6542.3\nNo. Observations:                1394   AIC:                         1.309e+04\nDf Residuals:                    1392   BIC:                         1.310e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     92.8658      4.264     21.778      0.000      84.501     101.231\nFRW            0.5241      0.040     13.132      0.000       0.446       0.602\n==============================================================================\nOmnibus:                      338.464   Durbin-Watson:                   1.756\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              883.998\nSkew:                           1.271   Prob(JB):                    1.10e-192\nKurtosis:                       5.959   Cond. No.                         643.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nNote: Python’s statsmodels output is more verbose\nthan R’s. Focus on these key sections:\nCoefficients table (middle): Shows estimates,\nstandard errors, t-statistics, and p-values\nR-squared (top-right): Proportion of variance\nexplained\nF-statistic (top-right): Tests overall model\nsignificance\n# Display the regression summary\nsummary(fit)\n\n\n\n\n\n\nInterpreting the Regression Output\n\n\n\nKey Statistical Tests:\n\nThe reported p-values for individual coefficients use the Wald Test:\n\nH_0: \\beta_j = 0 (coefficient equals zero, no effect)\nTest statistic: t = \\hat{\\beta}_j / \\text{SE}(\\hat{\\beta}_j)\nOur result: FRW p-value &lt; 0.001 → reject H_0 → weight significantly affects blood pressure\n\nF-Test is used for overall model significance\n\nH_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_k = 0 (no predictors have any effect)\nTests if any coefficient is non-zero\nOur result: F = 172.5, p &lt; 0.001 → reject H_0 → model is useful\n\nR-Squared (R^2, coefficient of determination)\n\nProportion of variance in Y explained by the model (typically between 0 and 1)\nMeasures accuracy on training data\nIn our example: model explains roughly 11% of blood pressure variation (see exact value in output above)\nLow R^2 common when many unmeasured factors affect outcome, which happens for many real-world data\n\n\nReading the Coefficients Table:\n\nIntercept (\\beta_0): Expected SBP when FRW = 0 (extrapolation - not meaningful here)\nFRW coefficient (\\beta_1): Each unit increase in relative weight increases SBP by approximately 0.5 mmHg on average (see exact value in output)\nStandard errors: Quantify uncertainty in estimates\n95% CI for \\beta_1: Check the confidence interval in the output - it quantifies our uncertainty about the true slope\n\nModel Quality Assessment:\n\nResidual Standard Error: Typical prediction error (see output for exact value)\nStatistical vs. Practical Significance: While statistically significant (p &lt; 0.001), the effect size is modest\nPredictive Power: The relatively low R^2 indicates that weight alone is not a strong predictor of blood pressure\n\nKey Takeaway: The regression confirms a statistically significant positive relationship between weight and blood pressure. However, the modest R^2 reminds us that blood pressure is multifactorial – diet, exercise, genetics, stress, and many other factors play important roles.\n\n\n\n\nSummary: What We’ve Learned\nThrough this comprehensive example, we’ve followed a complete regression workflow:\n\nData Exploration: Understood our variables and their context\nInitial Visualization: Examined the relationship visually before modeling\nModel Fitting: Applied least squares to find the best-fitting line\nMaking Predictions: Used the model to predict new values\nVisualizing the Fit: Showed the fitted line\nModel Diagnostics: Checked assumptions through residual plots\nModel Interpretation: Understood the statistical tests and what they tell us\n\nThe Framingham data reveals an important finding: weight has a statistically significant effect on blood pressure (p &lt; 0.001), with each unit increase in relative weight associated with approximately a 0.5 mmHg increase in systolic blood pressure (see the exact coefficient in the regression output above).\nThe modest R^2 (around 11% of variance explained) doesn’t diminish the medical importance of this relationship. Rather, it reminds us that:\n\nBlood pressure is multifactorial - weight is one important factor among many (age, diet, exercise, genetics, stress)\nEven when individual predictors explain modest variance, they can still be clinically meaningful\nSimple linear regression effectively quantifies real-world relationships and their uncertainty",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#multiple-linear-regression",
    "href": "chapters/09-linear-logistic-regression.html#multiple-linear-regression",
    "title": "9  Linear and Logistic Regression",
    "section": "9.4 Multiple Linear Regression",
    "text": "9.4 Multiple Linear Regression\n\n9.4.1 Extending the Model to Multiple Predictors\nReal-world phenomena rarely depend on just one predictor. A person’s blood pressure isn’t determined solely by their weight – age, diet, exercise, genetics, and countless other factors play roles. Multiple linear regression extends our framework to handle multiple predictors simultaneously.\n\nThe Model\nWith k predictors (covariates), the multiple linear regression model becomes:\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik} + \\epsilon_i, \\quad i = 1, \\ldots, n\nwhere:\n\nY_i is the response for observation i\nX_{ij} is the value of the j-th covariate for observation i\n\\beta_0 is the intercept\n\\beta_j is the coefficient for the j-th covariate (for j = 1, \\ldots, k)\n\\epsilon_i is the error term with \\mathbb{E}(\\epsilon_i \\mid X_i) = 0 and \\mathbb{V}(\\epsilon_i \\mid X_i) = \\sigma^2\n\n\n\n\n\n\n\nConvention: Incorporating the Intercept\n\n\n\nThe intercept \\beta_0 is often incorporated directly into the covariate notation by defining X_{i0} = 1 for all observations i = 1, \\ldots, n. This allows us to write the model more compactly as: Y_i = \\sum_{j=0}^{k} \\beta_j X_{ij} + \\epsilon_i\nThis convention simplifies matrix notation and many derivations. When you see design matrices or covariate vectors, check whether the intercept is handled explicitly (with a column of ones) or implicitly (assumed but not shown).\n\n\n\n\n\n\n\n\nIndexing Confusion: Starting from 0 vs 1\n\n\n\n\n\nYou’ll encounter two indexing conventions in the literature:\nConvention 1 (0-indexed, used here in the lecture notes):\n\nX_{i0} = 1 for the intercept\nX_{i1}, X_{i2}, \\ldots, X_{ik} for the k actual covariates\n\nDesign matrix \\mathbf{X} is n \\times (k+1)\nModel: Y_i = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_k X_{ik} + \\epsilon_i\n\nConvention 2 (1-indexed, common in some texts):\n\nX_{i1} = 1 for the intercept\nX_{i2}, X_{i3}, \\ldots, X_{i,k+1} for the k actual covariates\nDesign matrix \\mathbf{X} is still n \\times (k+1)\nModel: Y_i = \\beta_1 + \\beta_2 X_{i2} + \\cdots + \\beta_{k+1} X_{i,k+1} + \\epsilon_i\n\nBoth are correct! The key is consistency within a given analysis. Software typically handles this transparently – R’s lm() and Python’s statsmodels automatically add the intercept column regardless of your indexing preference.\n\n\n\n\nMultiple Linear Regression in Matrix Form\nThe model is more elegantly expressed using matrix notation: \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\nwhere:\n\n\\mathbf{Y} is an n \\times 1 vector of responses\n\\mathbf{X} is an n \\times (k+1) design matrix (often written as n \\times k when the intercept is implicit)\n\\boldsymbol{\\beta} is a (k+1) \\times 1 vector of coefficients (or k \\times 1 when intercept is implicit)\n\\boldsymbol{\\epsilon} is an n \\times 1 vector of errors\n\n\nExplicitly (showing the column of 1s separately from the actual covariates): \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix} 1 & X_{11} & X_{12} & \\cdots & X_{1k} \\\\ 1 & X_{21} & X_{22} & \\cdots & X_{2k} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & X_{n1} & X_{n2} & \\cdots & X_{nk} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\nHere:\n\nThe first column of 1s corresponds to what we defined as X_{i0} = 1 for the intercept\nX_{i1}, X_{i2}, \\ldots, X_{ik} are the values of the k actual covariates for observation i\nThe n rows correspond to different observations\nThe k+1 columns correspond to the intercept and k covariates\n\n\n\nLeast Squares in Matrix Form\nThe least squares criterion remains the same – minimize RSS – but the solution is now expressed in matrix form:\n\nAssuming \\mathbf{X}^T\\mathbf{X} is invertible, the least squares estimate is:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\nwith variance-covariance matrix:\n\\mathbb{V}(\\hat{\\boldsymbol{\\beta}} \\mid \\mathbf{X}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nKey Results from the Least Squares Solution:\n\nThe estimated regression function is: \\hat{r}(x) = \\hat{\\beta}_0 + \\sum_{j=1}^{k} \\hat{\\beta}_j x_j\nAn unbiased estimate of the error variance \\sigma^2 is: \\hat{\\sigma}^2 = \\frac{1}{n-k-1} \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2 = \\frac{1}{n-k-1} ||\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}||^2\nWe divide by n-k-1 because we’ve estimated k+1 parameters (including the intercept).\nConfidence intervals for individual coefficients: \\hat{\\beta}_j \\pm z_{\\alpha/2} \\cdot \\widehat{\\text{se}}(\\hat{\\beta}_j)\nwhere \\widehat{\\text{se}}^2(\\hat{\\beta}_j) is the j-th diagonal element of \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}.\n\n\n\n\n\n\n\nThe Meaning of a Coefficient\n\n\n\nIn multiple regression, the interpretation of coefficients becomes more subtle. The coefficient \\beta_j represents the expected change in Y for a one-unit change in X_j, holding all other covariates constant.\nThis is the crucial concept of statistical control or adjustment. We’re estimating the partial effect of each predictor, after accounting for the linear effects of all other predictors in the model.\nThis interpretation assumes:\n\nThe other variables can actually be held constant (may not be realistic)\nThe relationship is truly linear\nNo important interactions exist between predictors\n\n\n\n\n\n\n9.4.2 Multiple Regression in Practice\nWe’ll now add two biologically relevant predictors to our weight-only model:\n\nSEX: Categorical variable (“female”/“male” in the data)\n\nR and Python automatically encode with female = 0 (reference), male = 1\nOutput shows as “SEX[T.male]” (Python) or “SEXmale” (R)\nCoefficient interpretation: difference in mean SBP for males vs females\n\nCHOL: Total serum cholesterol in mg/dL (typical range: 150-250 mg/dL)\n\nA known cardiovascular risk factor\nCoefficient interpretation: change in SBP per 1 mg/dL increase in cholesterol\n\n\nWe’ll build three models:\n\nModel 1: SBP ~ FRW (baseline simple regression)\nModel 2: SBP ~ FRW + SEX + CHOL (multiple regression)\n\nAdds SEX and CHOL predictors\n\nModel 3: SBP ~ FRW + SEX + CHOL + FRW:SEX (with interaction/cross term)\n\nFRW:SEX is an interaction term - that is the product FRW × SEX (weight × 0/1 for female/male)\nThis tests: “Does weight affect blood pressure differently for males vs females?”\nFor females (SEX = 0): Effect of weight = \\beta_{\\text{FRW}}\nFor males (SEX = 1): Effect of weight = \\beta_{\\text{FRW}} + \\beta_{\\text{FRW:SEX}}\nIf \\beta_{\\text{FRW:SEX}} &gt; 0: weight increases BP more for males than females\n\n\n\n\n\n\n\n\nExample: From Simple to Multiple Regression\n\n\n\nLet’s build on our simple regression model by adding sex and cholesterol as predictors:\nPythonR\n# Load the same Framingham data\nfram = pd.read_csv('../data/fram.txt', sep='\\t', index_col=0)\n\n# Model 1: Simple regression (for comparison)\nmodel1 = smf.ols('SBP ~ FRW', data=fram).fit()\n\n# Model 2: Multiple regression\nmodel2 = smf.ols('SBP ~ FRW + SEX + CHOL', data=fram).fit()\n\n# Model 3: With interaction (does weight affect BP differently for males/females?)\nmodel3 = smf.ols('SBP ~ FRW + SEX + CHOL + FRW:SEX', data=fram).fit()\n\nprint(\"Simple Regression (Model 1):\")\nprint(f\"  FRW coefficient: {model1.params['FRW']:.3f} (SE={model1.bse['FRW']:.3f})\")\nprint(f\"  R-squared: {model1.rsquared:.3f}\")\n\nprint(\"\\nMultiple Regression (Model 2):\")\nprint(f\"  FRW coefficient: {model2.params['FRW']:.3f} (SE={model2.bse['FRW']:.3f})\")\nprint(f\"  SEX[T.male] coefficient: {model2.params['SEX[T.male]']:.3f}\")\nprint(f\"  CHOL coefficient: {model2.params['CHOL']:.3f}\")\nprint(f\"  R-squared: {model2.rsquared:.3f}\")\n\nprint(\"\\nWith Interaction (Model 3):\")\nif 'FRW:SEX[T.male]' in model3.params:\n    interaction_coef = model3.params['FRW:SEX[T.male]']\n    interaction_pval = model3.pvalues['FRW:SEX[T.male]']\n    print(f\"  FRW:SEX interaction: {interaction_coef:.3f} (p={interaction_pval:.3f})\")\n    if interaction_pval &gt; 0.05:\n        print(f\"  Interpretation: No significant interaction - weight affects BP similarly for both sexes\")\n    else:\n        print(f\"  Interpretation: Weight has {'stronger' if interaction_coef &gt; 0 else 'weaker'} effect for males\")\n\nSimple Regression (Model 1):\n  FRW coefficient: 0.524 (SE=0.040)\n  R-squared: 0.110\n\nMultiple Regression (Model 2):\n  FRW coefficient: 0.499 (SE=0.040)\n  SEX[T.male] coefficient: -4.066\n  CHOL coefficient: 0.053\n  R-squared: 0.125\n\nWith Interaction (Model 3):\n  FRW:SEX interaction: -0.004 (p=0.967)\n  Interpretation: No significant interaction - weight affects BP similarly for both sexes\n\n# Load data\nfram &lt;- read.csv('../data/fram.txt', sep='\\t', row.names = 1)\n\n# Build models\nmodel1 &lt;- lm(SBP ~ FRW, data = fram)\nmodel2 &lt;- lm(SBP ~ FRW + SEX + CHOL, data = fram)\nmodel3 &lt;- lm(SBP ~ FRW + SEX + CHOL + FRW:SEX, data = fram)\n\n# Compare coefficients\ncat(\"Simple Regression (Model 1):\\n\")\ncat(sprintf(\"  FRW coefficient: %.3f (SE=%.3f)\\n\", \n            coef(model1)[\"FRW\"], summary(model1)$coef[\"FRW\", \"Std. Error\"]))\ncat(sprintf(\"  R-squared: %.3f\\n\", summary(model1)$r.squared))\n\ncat(\"\\nMultiple Regression (Model 2):\\n\")\ncat(sprintf(\"  FRW coefficient: %.3f (SE=%.3f)\\n\", \n            coef(model2)[\"FRW\"], summary(model2)$coef[\"FRW\", \"Std. Error\"]))\ncat(sprintf(\"  SEXmale coefficient: %.3f\\n\", coef(model2)[\"SEXmale\"]))\ncat(sprintf(\"  CHOL coefficient: %.3f\\n\", coef(model2)[\"CHOL\"]))\ncat(sprintf(\"  R-squared: %.3f\\n\", summary(model2)$r.squared))\n\ncat(\"\\nWith Interaction (Model 3):\\n\")\nif(\"FRW:SEXmale\" %in% names(coef(model3))) {\n  interaction_coef &lt;- coef(model3)[\"FRW:SEXmale\"]\n  interaction_pval &lt;- summary(model3)$coef[\"FRW:SEXmale\", \"Pr(&gt;|t|)\"]\n  cat(sprintf(\"  FRW:SEX interaction: %.3f (p=%.3f)\\n\", interaction_coef, interaction_pval))\n  if(interaction_pval &gt; 0.05) {\n    cat(\"  Interpretation: No significant interaction - weight affects BP similarly for both sexes\\n\")\n  } else {\n    cat(sprintf(\"  Interpretation: Weight has %s effect for males\\n\", \n                ifelse(interaction_coef &gt; 0, \"stronger\", \"weaker\")))\n  }\n}\n\n\nKey Observations from the Multiple Regression Results:\n\nMinimal coefficient change: The FRW coefficient changes only slightly when adding other predictors (compare Model 1 vs Model 2 outputs above). This stability suggests weight has a robust relationship with blood pressure that isn’t confounded by sex or cholesterol.\nSex effect in this cohort: In this Framingham cohort, the model shows males have somewhat lower blood pressure than females (see the SEX coefficient in Model 2 output). This pattern in the 1950s-60s data (mean age ≈ 52 years) may reflect post-menopausal effects in women.\nModest R-squared improvement: Adding sex and cholesterol only marginally improves R^2 (compare the R-squared values between Model 1 and Model 2). This teaches us that more predictors don’t always mean much better predictions.\nNo meaningful interaction: The interaction term in Model 3 is near zero (see FRW:SEX coefficient), suggesting weight affects blood pressure similarly for both sexes. Not all hypothesized interactions turn out to be important!\n\n\n\n\n\n\n\nReal Data Lessons\n\n\n\n\n\nThese Framingham results illustrate important realities of data analysis:\n\nEffects can be counterintuitive: Women having higher blood pressure in this 1950s-60s cohort (mean age 52) surprises many, but it’s consistent across all age groups in the data. Historical context and demographics matter!\nMore predictors ≠ much better fit: Despite adding two predictors and an interaction, we only explained an additional 1.5% of variance.\nMost interactions are null: We often hypothesize interactions that don’t materialize. That’s fine – testing and rejecting hypotheses is part of science.\nBiological systems are complex: Even our best model explains only 12.5% of blood pressure variation. The remaining 87.5% comes from genetics, lifestyle, measurement error, and countless other factors.\n\n\n\n\n\n\n9.4.3 Model Selection: Choosing the Right Predictors\nWith many potential predictors, a critical question arises: which ones should we include? Including too few predictors (underfitting) leads to bias; including too many (overfitting) increases variance and reduces interpretability. Model selection seeks the sweet spot.\n\nThe Core Problem\nWhen we have k potential predictors, there are 2^k possible models (each predictor is either in or out). With just 10 predictors, that’s 1,024 models; with 20 predictors, over a million! We need:\n\nA way to score each model’s quality\nAn efficient search strategy to find the best model\n\n\n\nScoring Models: The Bias-Variance Trade-off\nThe fundamental challenge is that training error – how well the model fits the data used to build it – is a bad guide to how well it will predict new data. Complex models always fit training data better, but they may perform poorly on new data. This is a manifestation of the bias-variance tradeoff we studied in Chapter 3: as we add more predictors to a regression, bias decreases (better fit to the true relationship) but variance increases (more sensitivity to the particular sample). Too few predictors leads to underfitting (high bias), while too many leads to overfitting (high variance).\n\nPrediction Risk (Quadratic Loss)\nFor a model S with predictors \\mathcal{X}_S, the prediction risk under quadratic loss is: R(S) = \\sum_{i=1}^{n} \\mathbb{E}[(\\hat{Y}_i(S) - Y_i^*)^2]\nwhere Y_i^* is a future observation at covariate value X_i, and \\hat{Y}_i(S) is the prediction from model S.\n\n\n\n\n\n\n\nWhy Squared Error Loss?\n\n\n\nWe use quadratic loss throughout model selection because:\n\nIt matches our least squares estimation method (consistency across model fitting and model evaluation)\nIt leads to tractable bias-variance decompositions\n\nIt penalizes large errors more than small ones (often desirable in practice)\n\nOther loss functions (absolute error, 0-1 loss) are valid but lead to different optimal models.\n\n\nSince we can’t directly compute prediction risk (we don’t have future data!), we need estimates. Many model selection criteria follow a similar form which we want to maximize:\n\\text{Model Score} = \\underbrace{\\text{Goodness of Fit}}_{\\text{how well model fits data}} - \\underbrace{\\text{Complexity Penalty}}_{\\text{penalty for too many parameters}}\nEquivalently, some model selection metrics aim to minimize:\n\\text{Model Score Loss} = \\text{Training Error} + \\text{Complexity Penalty}\nThis fundamental trade-off appears in different guises across the methods we’ll examine. The key insight is that we must balance how well we fit the current data against the danger of overfitting.\nMallow’s CpAICBICCross-ValidationMallow’s \\(C_p\\)\nStatistic provides an estimate of prediction risk:\\[\\hat{R}(S) = \\text{RSS}(S) + 2|S|\\hat{\\sigma}^2\\]where:\n\\(\\text{RSS}(S)\\) = residual sum of\nsquares (training error)\n\\(|S|\\) = number of parameters in\nmodel \\(S\\)\n\\(\\hat{\\sigma}^2\\) = error variance\nestimate from the full model\nInterpretation: The first term measures lack of fit,\nthe second penalizes complexity. Named after statistician Colin\nMallows who developed it.When to use: Linear regression with normal errors\nwhen you want an unbiased estimate of prediction risk.AIC (Akaike Information Criterion) takes an\ninformation-theoretic approach. The standard definition used in\nstatistical software is:\\[\\text{AIC}(S) = -2\\ell_S + 2|S|\\]where: - \\(\\ell_S\\) is the\nlog-likelihood at the MLE - \\(|S|\\) is\nthe number of parameters in model \\(S\\)\n(including the intercept)We minimize AIC to select the best model.\nConceptually, this is equivalent to maximizing “goodness of fit minus\ncomplexity penalty” since minimizing\n\\(-2\\ell_S + 2|S|\\) is the same as\nmaximizing \\(\\ell_S - |S|\\).Key insight: For linear regression with normal\nerrors, AIC is equivalent to Mallow’s\n\\(C_p\\) (they select the same\nmodel).Philosophy: AIC was developed by statistician Hirotugu Akaike\nto approximate the Kullback-Leibler divergence between the true and\nfitted models. It aims to minimize prediction error, not find the “true”\nmodel – recognizing that the true model might be too complex to express\nmathematically.When to use: When prediction accuracy is the primary\ngoal and you believe the true model may be complex.BIC (Bayesian Information Criterion) adds a stronger\ncomplexity penalty. The standard definition is:\\[\\text{BIC}(S) = -2\\ell_S + |S|\\log n\\]where \\(n\\) is the sample size. We\nminimize BIC to select the best model.Note that the penalty \\(|S|\\log n\\)\ngrows with sample size, making BIC more conservative than AIC (which has\na fixed penalty of \\(2|S|\\)). For\n\\(n &gt; e^2 \\approx 7.4\\), BIC penalizes\ncomplexity more heavily than AIC.Philosophy: BIC, developed by statistician Gideon E.\nSchwarz, has a Bayesian interpretation – it approximates the log\nposterior probability of the model. As\n\\(n \\to \\infty\\), BIC selects the true\nmodel with probability 1 (consistency), if the true model belongs to\nthe candidate model set.Key difference from AIC: Stronger penalty leads to\nsimpler models. BIC assumes a true, relatively simple model exists among\nthe candidates.When to use: When you believe a relatively simple\ntrue model exists and want consistency.Alternatively, we can approximate prediction error by training our\nmodel on a subset of the data, and testing on the\nremaining (held-out) set. Repeating this procedure multiple times for\ndifferent partitions of the data is called\ncross-validation (CV).Leave-one-out Cross-Validation (LOO-CV) directly\nestimates prediction error on one held-out data point at a time:\\[\\hat{R}_{CV}(S) = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_{(i)})^2\\]where \\(\\hat{Y}_{(i)}\\) is the\nprediction for observation \\(i\\) from a\nmodel fit without observation \\(i\\).\nLOO-CV can be very expensive (requires refitting the model\n\\(n\\) times!) but for linear models it\ncan be computed efficiently.\\(k\\)-fold CV:\nDivide data into \\(k\\) groups called\n“folds” (often \\(k=5\\) or\n\\(k=10\\)), train on\n\\(k-1\\) folds, test on the held-out\nfold, repeat and average. This only requires retraining the model\n\\(k\\) times.When to use: When you want a direct, model-agnostic\nestimate of prediction performance. Essential for complex models where\nAIC/BIC aren’t available. CV is a common evaluation technique in machine\nlearning.\n\n\nSearch Strategies\nEven with a scoring criterion, we can’t check all 2^k models when k is large. Common search strategies include:\n\nForward Stepwise Selection: Start with no predictors, add the best one at each step\nBackward Stepwise Selection: Start with all predictors, remove the worst one at each step\nBest Subset Selection: Check all models of each size (computationally intensive)\n\n\n\n\n\n\n\nGreedy Search Limitations\n\n\n\nStepwise methods are greedy algorithms – they make locally optimal choices without considering the global picture. They may miss the best model. For example, two predictors might be useless alone but powerful together due to interaction effects.\n\n\n\n\nComparing Predictor Importance\nOnce we’ve selected a model, we often want to know: which predictors have the most impact? The raw regression coefficients can be misleading because predictors are on different scales. A predictor measured in millimeters will have a much smaller coefficient than one measured in kilometers, even if they have the same actual importance.\nThe solution is to standardize predictors before comparing coefficients. After standardization:\n\nAll predictors have mean 0 and the same spread\nCoefficients become directly comparable\nThe coefficient magnitude indicates relative importance\n\nGelman & Hill’s recommendation: Gelman and Hill (2007) recommend dividing continuous predictors by 2 standard deviations (not 1). This makes binary and continuous predictors more comparable, since a binary predictor’s standard deviation is at most 0.5, so dividing by 2 SDs puts it on a similar scale.\nAfter standardization, predictors with larger coefficient magnitudes have stronger effects on the outcome (in standard deviation units). However, when predictors are correlated, standardized coefficients don’t directly measure the unique variance explained by each predictor. For that, consider partial R^2 or other variance decomposition methods.\n\n\n\n\n\n\nHeuristics for Model Selection\n\n\n\nBeyond automated criteria, Gelman and Hill (2007) suggest these practical guidelines:\n\nInclude predictors you expect to be important based on subject knowledge\nConsider creating composite predictors: Not all related variables need separate inclusion – you can combine multiple covariates into meaningful composites (e.g., a socioeconomic index from income, education, and occupation)\nAdd interaction terms for strong predictors: When predictors have large effects, their interactions often matter too\nUse statistical significance and sign to guide decisions:\n\nSignificant with expected sign: Keep these predictors\nSignificant with unexpected sign: Investigate further – may indicate model misspecification, confounding, or data issues\nNon-significant with expected sign: Often worth keeping if theoretically important\nNon-significant with unexpected sign: Generally drop these\n\n\nRemember: statistical significance isn’t everything. A predictor’s theoretical importance and practical significance matter too.\n\n\n\n\nControlling for Background Variables\nMany studies control for background variables (age, sex, education, socioeconomic status, etc.). This simply means including these variables as predictors in the model to “remove their impact” on the relationship of interest.\nFor example, in our earlier Framingham analysis, Model 2 controlled for sex and cholesterol when examining the weight-blood pressure relationship:\n\nModel 1: SBP ~ FRW (simple regression with weight only)\n\nModel 2: SBP ~ FRW + SEX + CHOL (controlling for sex and cholesterol)\n\nThe weight coefficient changed only slightly between models (see the outputs above), suggesting the relationship isn’t confounded by these variables.\n\n\n\n\n\n\nLimitation of Statistical Control\n\n\n\nControlling only captures linear effects of the control variables. If age has a nonlinear effect on the outcome (e.g., quadratic), simply including age as a linear term won’t fully control for it. Background variables can still affect inferences if their effects are nonlinear.\nConsider including polynomial terms or splines for control variables when you suspect nonlinear relationships.\n\n\n\n\n\n9.4.4 Regression Assumptions and Diagnostics\nLinear regression makes strong assumptions. When violated, our inferences may be invalid.\n\n\n\n\n\n\nThe Five Assumptions of Linear Regression\n\n\n\nIn decreasing order of importance (per Gelman and Hill 2007):\n\nValidity: Are the data relevant to your research question? Does the outcome measure what you think it measures? Are all important predictors included? Missing key variables can invalidate all conclusions.\nAdditivity and Linearity: The model assumes Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... If relationships are nonlinear, consider transformations (\\log, square root), polynomial terms, or interaction terms.\nIndependence of Errors: Each observation’s error should be independent of others. Watch out for time series (temporal correlation), spatial data (geographic clustering), or grouped data (students within schools).\nEqual Variance (Homoscedasticity): Error variance should be constant across all predictor values. Violation makes standard errors and confidence intervals unreliable.\nNormality of Errors: Errors should follow a normal distribution. This is the least important – with large samples, the Central Limit Theorem ensures valid inference even with non-normal errors. Outliers matter more than the exact distribution shape.\n\n\n\n\nChecking Assumptions: Residual Plots\nSince the statistical assumptions of linear regression focus on the errors \\epsilon_i = Y_i - \\hat{Y}_i, visualizing the residuals provides our primary diagnostic tool. The most important plot is residuals versus fitted values, which can reveal multiple assumption violations at once.\nWe demonstrated this in Step 6 of our Framingham analysis, where we created two key diagnostic plots:\n\nResiduals vs Fitted: Reveals problems with linearity and constant variance assumptions.\nQ-Q Plot: Checks whether residuals follow a normal distribution.\n\n\n\n\n\n\n\nInterpreting Residual Patterns\n\n\n\nGood residuals look like random noise around zero – no patterns, just scatter. Specific patterns reveal specific problems:\n\nCurved patterns (U-shape, waves): Nonlinearity detected. The true relationship isn’t straight. Try transformations or polynomial terms.\nFunnel shape (variance changes with fitted values): Heteroscedasticity. Errors have unequal variance. Consider log-transforming Y or weighted least squares.\nOutliers or extreme points: Can dominate the entire regression. Check if they’re data errors or reveal model limitations.\n\n\n\n\n\n\n\n\n\nCorrelation vs. Causation\n\n\n\n\n\nA significant regression coefficient does not imply causation! Regression finds associations, not causal relationships. For example, ice cream sales and swimming pool drownings are positively correlated (both increase in summer), but ice cream doesn’t cause drowning.\nEstablishing causation requires theoretical justification, temporal precedence, ruling out confounders, and ideally randomized experiments. We’ll explore causal inference in detail in Chapter 11.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#logistic-regression",
    "href": "chapters/09-linear-logistic-regression.html#logistic-regression",
    "title": "9  Linear and Logistic Regression",
    "section": "9.5 Logistic Regression",
    "text": "9.5 Logistic Regression\n\n9.5.1 Modeling Binary Outcomes\nSo far, we’ve assumed the response variable Y is continuous. But what if Y is binary? Consider:\n\nDoes a patient have the disease? (Yes/No)\nWill a customer churn? (Yes/No)3\nDid the email get clicked? (Yes/No)\nWill a loan default? (Yes/No)\n\nFor these binary outcomes, we need logistic regression.\n\n\n9.5.2 The Logistic Regression Model\n\nThe Logistic Regression Model\nFor a binary outcome Y_i \\in \\{0, 1\\} and predictors X_i, the logistic regression model specifies:\np_i \\equiv \\mathbb{P}(Y_i = 1 \\mid X_i) = \\frac{e^{\\beta_0 + \\sum_{j=1}^k \\beta_j X_{ij}}}{1 + e^{\\beta_0 + \\sum_{j=1}^k \\beta_j X_{ij}}}\nEquivalently, using the logit (log-odds) transformation:\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\sum_{j=1}^k \\beta_j X_{ij}\n\nIntuitiveMathematicalComputationalWhere does this formula come from? Why “logistic”\nregression?Point is, when \\(Y\\) is binary,\nlinear regression produces continuous predictions – not the 0s and 1s we\nobserve with binary data. The natural approach is to model the\nprobability\n\\(p = \\mathbb{P}(Y = 1 \\mid X)\\)\ninstead of \\(Y\\).We could try to model the probability\n\\(p\\) with a linear model such as\n\\(p = \\beta_0 + \\beta_1 X\\). However,\nwe would immediately hit two problems:\nLinear functions are unbounded: when\n\\(X\\) is large,\n\\(\\beta_0 + \\beta_1 X\\) can exceed 1;\nwhen \\(X\\) is small, it can fall below\n0. Both give impossible “probabilities.”\nBinary data strongly violates the homoscedasticity assumption – a\nBernoulli (i.e., binary) variable with probability\n\\(p\\) has variance\n\\(p(1-p)\\), which depends on\n\\(X\\).\nThe trick is that instead of modelling directly\n\\(p\\), we model the\nlog-odds (logarithm of the probability ratio), that is\n\\(\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\),\nwhich is an unbounded quantity that lives in\n\\((-\\infty, \\infty)\\).The logistic\nfunction is then the function that allows us to map the logit values\nback to \\((0,1)\\), ensuring valid\nprobabilities regardless of predictor values.The logistic function emerges naturally from maximum likelihood with\nBernoulli data. Since\n\\(Y_i \\sim \\text{Bernoulli}(p_i)\\), the\nlikelihood is:\\[\\mathcal{L} = \\prod_{i=1}^n p_i^{Y_i}(1-p_i)^{1-Y_i}\\]The log-likelihood becomes:\n\\[\\ell = \\sum_{i=1}^n \\left[Y_i \\log p_i + (1-Y_i)\\log(1-p_i)\\right]\\]We need to connect \\(p_i\\) to the\npredictors \\(X_i\\). We could try\nvarious transformations, but the logit turns out to be\nspecial – it’s the “canonical link” for Bernoulli distributions, meaning\nit makes the mathematics particularly elegant. Specifically, if we set:\n\\[\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 X_i\\]then the log-likelihood becomes concave in the\nparameters \\(\\beta_0, \\beta_1\\). This\nis crucial: a concave function has a unique maximum, so we’re guaranteed\nto find the best fit without worrying about local maxima. Solving the\nlogit equation for \\(p_i\\) gives us the\nlogistic function.Let’s visualize the logistic function to build intuition for how it\ntransforms linear predictors into probabilities:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-6, 6, 200)\np = 1 / (1 + np.exp(-x))\n\nplt.figure(figsize=(7, 4))\nplt.plot(x, p, 'b-', linewidth=2.5)\nplt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\nplt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\nplt.xlabel('Linear predictor: β₀ + β₁X')\nplt.ylabel('Probability: P(Y=1|X)')\nplt.title('The Logistic Function')\nplt.grid(True, alpha=0.3)\nplt.ylim(-0.05, 1.05)\n\n# Mark key point\nplt.plot(0, 0.5, 'ro', markersize=8)\nplt.annotate('When β₀ + β₁X = 0,\\nprobability = 0.5', \n            xy=(0, 0.5), xytext=(2, 0.3),\n            arrowprops=dict(arrowstyle='-&gt;', color='red'))\n\nplt.tight_layout()\nplt.show()\n\n\n\nThe S-shaped curve is the logistic function in action. Notice three\ncritical features:\nBounded output: No matter how extreme the input (β₀\n+ β₁X), the output stays strictly between 0 and 1\nDecision boundary: When the linear predictor equals\n0, the probability equals 0.5 – this is the natural decision\nthreshold\nSmooth transitions: Unlike a hard step function,\nthe logistic provides gradual probability changes, reflecting\nuncertainty near the boundary\nThis smooth mapping from\n\\((-\\infty, \\infty) \\to (0,1)\\) is what\nmakes logistic regression both mathematically tractable and practically\ninterpretable.\nUnlike linear regression, logistic regression has no closed-form solution. The parameters are estimated via maximum likelihood using numerical optimization algorithms, as implemented in common statistical packages.\n\n\n\n\n\n\nInterpreting Coefficients: Odds Ratios\n\n\n\n\n\nIn logistic regression, coefficients have a specific interpretation:\n\n\\beta_j is the change in log-odds for a one-unit increase in X_j, holding other variables constant\ne^{\\beta_j} is the odds ratio: the factor by which odds are multiplied for a one-unit increase in X_j\n\nFor example, if \\beta_{\\text{age}} = 0.05, then:\n\nEach additional year of age increases log-odds by 0.05\nEach additional year multiplies odds by e^{0.05} \\approx 1.051 (5.1% increase)\n\nRemember: odds = p/(1-p). If p = 0.2, odds = 0.25. If p = 0.8, odds = 4.\n\n\n\n\n\n9.5.3 Logistic Regression in Practice\nLet’s apply logistic regression to the Framingham data to predict high blood pressure.\n\n\n\n\n\n\nExample: Predicting High Blood Pressure\n\n\n\nWe’ll create a binary outcome for high blood pressure using the clinical definition: systolic blood pressure (SBP) ≥ 140 mmHg or diastolic blood pressure (DBP) ≥ 90 mmHg. This is the standard threshold used in medical practice.\nTo model the probability of high blood pressure, we’ll:\n\nStandardize continuous predictors (weight, age, cholesterol) by dividing by 2 standard deviations – this makes coefficients comparable across predictors.\nFit logistic regression models, starting with a single predictor then adding multiple variables.\nInterpret the results through odds ratios.\n\n\n\nPythonR\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\n# Load the Framingham data\nfram = pd.read_csv('../data/fram.txt', sep='\\t', index_col=0)\n\n# Define high blood pressure (standard clinical threshold)\nfram['HIGH_BP'] = ((fram['SBP'] &gt;= 140) | (fram['DBP'] &gt;= 90)).astype(int)\n\nprint(f\"Prevalence of high BP: {fram['HIGH_BP'].mean():.1%} ({fram['HIGH_BP'].sum()} of {len(fram)})\")\n\n# Standardize predictors (following Gelman & Hill's recommendation)\n# Dividing by 2*SD makes binary and continuous predictors comparable\ndef standardize(x):\n    return (x - x.mean()) / (2 * x.std())\n\nfram['sFRW'] = standardize(fram['FRW'])\nfram['sAGE'] = standardize(fram['AGE'])\nfram['sCHOL'] = standardize(fram['CHOL'])\n\n# Fit a simple logistic regression with standardized weight\nmodel = smf.logit('HIGH_BP ~ sFRW', data=fram).fit(disp=0)\nprint(\"\\n\" + \"=\"*50)\nprint(\"Logistic Regression: HIGH_BP ~ sFRW\")\nprint(\"=\"*50)\nprint(model.summary2().tables[1])\n\n# Visualize the fitted model\nplt.figure(figsize=(7, 5))\n\n# Plot the data points (with slight jitter for visibility)\ny_jitter = fram['HIGH_BP'] + np.random.normal(0, 0.02, len(fram))\nplt.scatter(fram['sFRW'], y_jitter, alpha=0.3, s=20)\n\n# Plot the fitted probability curve\nx_range = np.linspace(fram['sFRW'].min(), fram['sFRW'].max(), 200)\nX_pred = pd.DataFrame({'sFRW': x_range})\ny_pred = model.predict(X_pred)\nplt.plot(x_range, y_pred, 'r-', linewidth=2, label='Fitted probability')\n\nplt.xlabel('Standardized Weight (sFRW)')\nplt.ylabel('P(High BP = 1)')\nplt.title('Probability of High Blood Pressure vs Weight')\nplt.ylim(-0.05, 1.05)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n# Interpret the coefficient as odds ratio\nbeta_sfrw = model.params['sFRW']\nor_sfrw = np.exp(beta_sfrw)\nprint(f\"\\nCoefficient for sFRW: {beta_sfrw:.4f}\")\nprint(f\"Odds ratio: {or_sfrw:.4f}\")\nprint(f\"Interpretation: A 2-SD increase in weight multiplies odds of high BP by {or_sfrw:.4f}\")\n\nPrevalence of high BP: 65.0% (906 of 1394)\n\n==================================================\nLogistic Regression: HIGH_BP ~ sFRW\n==================================================\n              Coef.  Std.Err.          z         P&gt;|z|    [0.025    0.975]\nIntercept  0.675629  0.059255  11.402124  4.080404e-30  0.559492  0.791766\nsFRW       1.201906  0.138943   8.650323  5.135384e-18  0.929582  1.474230\n\nCoefficient for sFRW: 1.2019\nOdds ratio: 3.3265\nInterpretation: A 2-SD increase in weight multiplies odds of high BP by 3.3265\n\n\n\n\nlibrary(arm)  # For rescale and invlogit functions\n\n# Load data\nfram &lt;- read.csv('../data/fram.txt', sep='\\t', row.names = 1)\n\n# Define high blood pressure\nfram$HIGH_BP &lt;- (fram$SBP &gt;= 140) | (fram$DBP &gt;= 90)\n\ncat(sprintf(\"Prevalence of high BP: %.1f%% (%d of %d)\\n\", \n            mean(fram$HIGH_BP)*100, sum(fram$HIGH_BP), nrow(fram)))\n\n# Standardize predictors (rescale divides by 2*SD as recommended by Gelman & Hill)\nfram$sFRW &lt;- rescale(fram$FRW)\nfram$sAGE &lt;- rescale(fram$AGE)\nfram$sCHOL &lt;- rescale(fram$CHOL)\n\n# Fit logistic regression with standardized weight\nfit &lt;- glm(HIGH_BP ~ sFRW, data = fram, family = binomial(link = 'logit'))\nsummary(fit)\n\n# Visualize the fitted model\nplot(fram$sFRW, jitter(as.numeric(fram$HIGH_BP), 0.05), \n     xlab = \"Standardized Weight (sFRW)\", \n     ylab = \"P(High BP = 1)\",\n     main = \"Probability of High Blood Pressure vs Weight\",\n     pch = 16, col = rgb(0, 0, 0, 0.3))\n\n# Add fitted curve\ncurve(invlogit(coef(fit)[1] + coef(fit)[2]*x), \n      add = TRUE, col = \"red\", lwd = 2)\nlegend(\"topleft\", \"Fitted probability\", col = \"red\", lwd = 2)\n\n# Interpret as odds ratio\nbeta_sfrw &lt;- coef(fit)[\"sFRW\"]\nor_sfrw &lt;- exp(beta_sfrw)\ncat(sprintf(\"\\nCoefficient for sFRW: %.4f\\n\", beta_sfrw))\ncat(sprintf(\"Odds ratio: %.4f\\n\", or_sfrw))\ncat(sprintf(\"Interpretation: A 2-SD increase in weight multiplies odds of high BP by %.4f\\n\", or_sfrw))\nThe fitted logistic curve shows how the probability of high blood pressure increases with weight. Unlike linear regression, the relationship is nonlinear – the effect of weight on probability is strongest in the middle range where probabilities are near 0.5.\n\nAdding Multiple Predictors\nNow let’s include age and sex to improve our model:\nPythonR\n# Fit model with multiple predictors (using standardized variables)\nmodel2 = smf.logit('HIGH_BP ~ sFRW + sAGE + SEX', data=fram).fit(disp=0)\n\nprint(\"=\"*50)\nprint(\"Multiple Logistic Regression\")\nprint(\"=\"*50)\nprint(model2.summary2().tables[1])\n\n# Calculate and display odds ratios\nprint(\"\\n\" + \"=\"*50)\nprint(\"ODDS RATIOS\")\nprint(\"=\"*50)\nfor var in model2.params.index:\n    or_val = np.exp(model2.params[var])\n    ci = model2.conf_int().loc[var]\n    ci_low, ci_high = np.exp(ci[0]), np.exp(ci[1])\n    if var != 'Intercept':\n        print(f\"{var:10s}: OR = {or_val:5.3f} (95% CI: {ci_low:5.3f}-{ci_high:5.3f})\")\n\n# Visualize the effect of sex on the weight-BP relationship\nplt.figure(figsize=(7, 5))\n\n# Plot the actual data points with jitter, separated by sex\nfemales = fram[fram['SEX'] == 'female']\nmales = fram[fram['SEX'] == 'male']\n\n# Add jitter to binary outcome for visibility\njitter_f = females['HIGH_BP'] + np.random.normal(0, 0.02, len(females))\njitter_m = males['HIGH_BP'] + np.random.normal(0, 0.02, len(males))\n\nplt.scatter(females['sFRW'], jitter_f, alpha=0.3, s=20, color='red', label='Female (data)')\nplt.scatter(males['sFRW'], jitter_m, alpha=0.3, s=20, color='blue', label='Male (data)')\n\n# Create prediction data: vary weight, hold age at mean (0 for standardized)\nweight_range = np.linspace(fram['sFRW'].min(), fram['sFRW'].max(), 100)\n\n# Predictions for females\npred_data_f = pd.DataFrame({'sFRW': weight_range, 'sAGE': 0, 'SEX': 'female'})\npred_f = model2.predict(pred_data_f)\n\n# Predictions for males  \npred_data_m = pd.DataFrame({'sFRW': weight_range, 'sAGE': 0, 'SEX': 'male'})\npred_m = model2.predict(pred_data_m)\n\n# Plot the fitted curves\nplt.plot(weight_range, pred_f, 'r-', linewidth=2.5, label='Female (fitted)')\nplt.plot(weight_range, pred_m, 'b-', linewidth=2.5, label='Male (fitted)')\n\nplt.xlabel('Standardized Weight (sFRW)')\nplt.ylabel('P(High BP = 1)')\nplt.title('Probability of High BP by Weight and Sex\\n(Age held at mean)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.ylim(0, 1)\nplt.show()\n\n==================================================\nMultiple Logistic Regression\n==================================================\n                Coef.  Std.Err.         z         P&gt;|z|    [0.025    0.975]\nIntercept    0.780125  0.083319  9.363138  7.740317e-21  0.616823  0.943426\nSEX[T.male] -0.201763  0.116912 -1.725767  8.438929e-02 -0.430907  0.027380\nsFRW         1.153445  0.140824  8.190707  2.596960e-16  0.877436  1.429454\nsAGE         0.373807  0.117219  3.188958  1.427867e-03  0.144062  0.603552\n\n==================================================\nODDS RATIOS\n==================================================\nSEX[T.male]: OR = 0.817 (95% CI: 0.650-1.028)\nsFRW      : OR = 3.169 (95% CI: 2.405-4.176)\nsAGE      : OR = 1.453 (95% CI: 1.155-1.829)\n\n\n\n\n# Fit model with multiple predictors (using standardized variables)\nfit2 &lt;- glm(HIGH_BP ~ sFRW + sAGE + SEX, data = fram, \n            family = binomial(link = 'logit'))\nsummary(fit2)\n\n# Calculate odds ratios with confidence intervals\nor_table &lt;- exp(cbind(OR = coef(fit2), confint(fit2)))\nprint(or_table)\n\n# Visualize the effect of sex on the weight-BP relationship\n# Plot the actual data points with jitter, separated by sex\nfemales &lt;- fram[fram$SEX == \"female\", ]\nmales &lt;- fram[fram$SEX == \"male\", ]\n\nplot(females$sFRW, jitter(as.numeric(females$HIGH_BP), 0.05), \n     col = rgb(1, 0, 0, 0.3), pch = 16, \n     xlab = \"Standardized Weight (sFRW)\", \n     ylab = \"P(High BP = 1)\",\n     main = \"Probability of High BP by Weight and Sex\\n(Age held at mean)\",\n     ylim = c(-0.1, 1.1))\npoints(males$sFRW, jitter(as.numeric(males$HIGH_BP), 0.05), \n       col = rgb(0, 0, 1, 0.3), pch = 16)\n\n# Add fitted curves for females and males (age at mean = 0 for standardized)\n# Female curve\ncurve(invlogit(coef(fit2)[1] + coef(fit2)[\"sFRW\"]*x + coef(fit2)[\"sAGE\"]*0), \n      col = \"red\", lwd = 2.5, add = TRUE)\n\n# Male curve  \ncurve(invlogit(coef(fit2)[1] + coef(fit2)[\"SEXmale\"] + coef(fit2)[\"sFRW\"]*x + coef(fit2)[\"sAGE\"]*0),\n      col = \"blue\", lwd = 2.5, add = TRUE)\n\nlegend(\"topleft\", \n       c(\"Female (data)\", \"Male (data)\", \"Female (fitted)\", \"Male (fitted)\"), \n       col = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5), \"red\", \"blue\"), \n       pch = c(16, 16, NA, NA),\n       lwd = c(NA, NA, 2.5, 2.5))\ngrid()\nInterpretation of Results:\n\nsFRW (Weight): A 2-SD increase in weight multiplies the odds of high BP by 3.17 (95% CI: 2.41-4.18) – a very strong effect\nsAGE (Age): A 2-SD increase in age multiplies the odds of high BP by 1.45 (95% CI: 1.16-1.83) – significant but weaker than weight\nSEX: Being male decreases the odds by about 18% (OR = 0.82), but this is not statistically significant (95% CI: 0.65-1.03 includes 1)\n\nThe visualization shows how the probability curves are parallel on the logit scale – males have consistently lower probability across all weight values (the blue curve sits below the red curve). The standardization allows direct comparison: weight has the strongest association with high blood pressure in this model.\n\n\n\n\n\n\nRemarks About Logistic Regression\n\n\n\n\n\n\nNo R-squared: The usual R^2 doesn’t apply. Pseudo-R^2 measures exist but are less interpretable.\nClassification vs. Probability: Logistic regression estimates probabilities. Classification (yes/no) requires choosing a threshold (often 0.5, but domain-specific considerations matter).\nSeparation Problem: If predictors perfectly separate the classes, the MLE doesn’t exist (coefficients go to ±∞). Regularization or Bayesian methods can help.\nSample Size Requirements: Need more data than linear regression. Rule of thumb: 10-20 events per predictor for the less common outcome.\nLink Functions: The logit is just one choice, another choice is for example the probit (normal CDF).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#chapter-summary-and-connections",
    "href": "chapters/09-linear-logistic-regression.html#chapter-summary-and-connections",
    "title": "9  Linear and Logistic Regression",
    "section": "9.6 Chapter Summary and Connections",
    "text": "9.6 Chapter Summary and Connections\n\n9.6.1 Key Concepts Review\nWe’ve covered the two fundamental models in statistical learning:\nLinear Regression:\n\nModels the expected value of a continuous response as a linear function of predictors\nEstimated via least squares, which coincides with MLE under normality\nProvides interpretable coefficients with well-understood inference procedures\nExtends naturally to multiple predictors, with matrix formulation\nRequires careful attention to assumptions and model selection\n\nLogistic Regression:\n\nExtends the linear framework to binary outcomes via the logit link\nModels probabilities, not the outcomes directly\nCoefficients represent changes in log-odds, interpretable as odds ratios\nEstimated via maximum likelihood with iterative algorithms\nShares the interpretability advantages of linear models\n\nKey Connections:\nBoth models exemplify the fundamental statistical modeling workflow:\n\nSpecify a model (assumptions about the data-generating process)\nEstimate parameters (least squares or maximum likelihood)\nQuantify uncertainty (standard errors, confidence intervals)\nCheck assumptions (diagnostic plots)\nMake predictions and interpret results\n\n\n\n9.6.2 The Big Picture\nThis chapter has taken you through the fundamentals of linear and logistic regression, from basic concepts to advanced applications. These models exemplify the core statistical modeling workflow: specify a model, estimate parameters, quantify uncertainty, check assumptions, and make predictions.\nBut why do these simple linear models remain so important in the era of deep learning? The answer lies in their unique ability to explain complex predictions. LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro, Singh, and Guestrin 2016) demonstrates this perfectly: it explains any complex model’s predictions by fitting simple linear models locally around points of interest.\n\n\n\n\n\n\nFigure 9.1: LIME: Local linear models explain complex predictions by approximating them in small neighborhoods. Figure reproduced from Ribeiro, Singh, and Guestrin (2016).\n\n\n\nThis principle – that complex functions are locally linear – makes linear models useful for understanding predictions from any model, no matter how complex. The techniques you’ve learned in this chapter (least squares, coefficient interpretation, diagnostics) aren’t just historical artifacts; they’re the foundation for both classical statistical analysis and modern interpretable machine learning.\n\n\n9.6.3 Common Pitfalls to Avoid\nWhen working with linear and logistic regression, watch out for these critical mistakes:\n1. Interpretation Errors\n\nCorrelation ≠ Causation: A significant coefficient shows association, not causation (that’s Chapter 11’s topic!)\nStatistical ≠ Practical significance: With n=10,000, even tiny effects become “significant”\nLogistic coefficients are log-odds: A coefficient of 0.5 doesn’t mean “50% increase in probability”\n\n2. Model Selection Traps\n\nOverfitting: Using training error to select models guarantees disappointment on new data\nAutomation without thinking: If your model says ice cream sales decrease temperatures, something’s wrong\nIgnoring validation: Always hold out data – a perfect training fit often means terrible generalization\n\n3. Technical Violations\n\nIgnoring diagnostic plots: That funnel-shaped residual plot? Your model needs help\nMulticollinearity chaos: When predictors correlate highly, coefficients become unstable and standard errors explode\n\n4. The Big One: Context Blindness\n\nExtrapolation: Linear trends rarely continue forever (no, humans won’t be 20 feet tall in year 3000)\nDomain knowledge matters: Statistical criteria (AIC/BIC) are guides, not gospel\n\n\n\n9.6.4 Chapter Connections\nPrevious (Chapters 5-8):\n\nChapters 5-6 introduced parametric inference – linear regression is the quintessential parametric model, with least squares achieving the Cramér-Rao bound under normality\nChapter 7’s hypothesis testing framework directly applies to testing regression coefficients via Wald tests and F-tests\nChapter 8’s Bayesian paradigm offers an alternative: Bayesian linear regression incorporates prior knowledge about parameters\n\nThis Chapter: Introduced the two workhorses of statistical modeling: linear and logistic regression. We saw how least squares connects to maximum likelihood, how to handle multiple predictors, select models, and extend to binary outcomes. The focus on interpretability makes these models essential even in the era of complex machine learning.\nNext (Ch. 10-11):\n\nChapter 10 will show Bayesian regression in practice using probabilistic programming languages, with complex priors and hierarchical structures\nChapter 11 will distinguish association from causation – regression finds associations, not causal effects\n\nApplications: Linear models remain indispensable across fields: economics (modeling market relationships), medicine (risk factor analysis), social sciences (understanding social determinants), machine learning (LIME and interpretability), and A/B testing (variance reduction through regression adjustment).\n\n\n9.6.5 Self-Test Problems\n\nCentroid Property: Show that the least squares regression line always passes through the point (\\bar{X}, \\bar{Y}).\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nUse the normal equations: from \\frac{\\partial \\text{RSS}}{\\partial \\hat{\\beta}_0} = 0 you get \\sum_i(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0, which gives \\bar{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1\\bar{X}.\n\n\n\nMulticollinearity Inflates Standard Errors: Explain why high correlation between X_j and the other predictors increases the standard error of \\hat{\\beta}_j.\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nIn multiple regression, \\text{Var}(\\hat{\\beta}_j) \\propto (1-R_j^2)^{-1}, where R_j^2 is from regressing X_j on the other X’s. As R_j^2 \\to 1, the variance (and SE) blows up.\n\n\n\nBinary Outcomes and Heteroscedasticity: For Y \\sim \\text{Bernoulli}(p), compute \\text{Var}(Y) and explain why applying linear regression to binary Y violates the constant-variance assumption.\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\n\\text{Var}(Y) = p(1-p), which depends on X if p = \\mathbb{P}(Y=1 \\mid X) changes with X. The variance is maximized at p=0.5 and approaches 0 as p \\to 0 or p \\to 1.\n\n\n\nOdds Ratio to Probability: Baseline probability is p_0 = 0.20. A predictor has an odds ratio \\text{OR} = 2.4. What is the new probability?\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nConvert baseline to odds: o_0 = \\frac{p_0}{1-p_0} = \\frac{0.20}{0.80} = 0.25. Multiply by OR to get o_1 = 2.4 \\times 0.25 = 0.6. Convert back: p_1 = \\frac{o_1}{1+o_1} = \\frac{0.6}{1.6} = 0.375.\n\n\n\nInterpreting an Interaction: In the model Y = \\beta_0 + \\beta_1 \\cdot \\text{FRW} + \\beta_2 \\cdot \\text{SEX} + \\beta_3 \\cdot (\\text{FRW} \\times \\text{SEX}) + \\epsilon with \\text{SEX}=0 (female) and \\text{SEX}=1 (male), what is the effect (slope) of FRW on Y for females vs males? What does \\beta_2 represent?\n\n\n\n\n\n\nSolution Hint\n\n\n\n\n\nPlug in \\text{SEX}=0 and \\text{SEX}=1. The FRW slope is \\beta_1 for females and \\beta_1 + \\beta_3 for males. \\beta_2 is the male-female intercept difference when \\text{FRW}=0.\n\n\n\n\n\n\n9.6.6 Python and R Reference\nThis section provides a quick reference for the main functions used in linear and logistic regression.\nPython (statsmodels)RLinear Regressionimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\n# Using formula interface (R-style)\nresults = smf.ols('Y ~ X1 + X2 + X3', data=df).fit()\nprint(results.summary())\n\n# Using arrays interface\nX = sm.add_constant(X)  # Add intercept column\nresults = sm.OLS(Y, X).fit()\n\n# Key methods\nresults.params           # Coefficients\nresults.bse             # Standard errors\nresults.conf_int()      # Confidence intervals\nresults.pvalues         # P-values\nresults.rsquared        # R-squared\nresults.fittedvalues    # Predicted values\nresults.resid           # Residuals\nresults.predict(new_df) # Predictions for new data (DataFrame with same columns)Logistic Regression# Using formula interface\nresults = smf.logit('Y ~ X1 + X2 + X3', data=df).fit()\n\n# Using arrays interface (must add intercept!)\nX = sm.add_constant(X)  # Don't forget the intercept\nresults = sm.Logit(Y, X).fit()\n\n# Key methods (similar to linear)\nresults.params          # Log-odds coefficients\nnp.exp(results.params)  # Odds ratios\nresults.predict()       # Predicted probabilities\nresults.pred_table()    # Classification table (2x2 contingency)\n\n# Model selection\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Check multicollinearity (compute on X with constant, skip reporting intercept VIF)\nX_with_const = sm.add_constant(X_features)\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) \n              for i in range(1, X_with_const.shape[1])]  # Skip intercept columnDiagnostic Plotsimport statsmodels.graphics.api as smg\n\n# Basic plots\nsmg.plot_fit(results, 'X1')  # Observed vs fitted for predictor X1\nsmg.qqplot(results.resid, line='45')  # Q-Q plot with 45° line\n\n# Residuals vs fitted (custom)\nimport matplotlib.pyplot as plt\nplt.scatter(results.fittedvalues, results.resid)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\n# Partial regression plots\nfrom statsmodels.graphics.regressionplots import plot_partregress_grid\nplot_partregress_grid(results)Linear Regression# Fit model\nmodel &lt;- lm(Y ~ X1 + X2 + X3, data = df)\nsummary(model)\n\n# Key functions\ncoef(model)         # Coefficients\nconfint(model)      # Confidence intervals\npredict(model)      # Predictions\nresiduals(model)    # Residuals\nfitted(model)       # Fitted values\nvcov(model)         # Variance-covariance matrix\nanova(model)        # ANOVA table\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)         # Standard diagnostic plots\n\n# Partial regression plots\nlibrary(car)\navPlots(model)Logistic Regression# Fit model\nmodel &lt;- glm(Y ~ X1 + X2 + X3, data = df, \n             family = binomial(link = \"logit\"))\nsummary(model)\n\n# Odds ratios\nexp(coef(model))\nexp(confint(model))  # Profile likelihood CIs (more accurate)\n# For faster Wald CIs: exp(confint.default(model))\n\n# Predictions\npredict(model, type = \"response\")  # Probabilities\npredict(model, type = \"link\")      # Log-odds\n\n# Model selection\nlibrary(MASS)\nstepAIC(model)  # Stepwise selection using AIC\n\n# Check multicollinearity\nlibrary(car)\nvif(model)Model Selection and Comparison# Compare models\nAIC(model1, model2, model3)\nBIC(model1, model2, model3)\n\n# Cross-validation for linear regression\nlibrary(caret)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\nmodel_cv &lt;- train(Y ~ ., data = df, method = \"lm\",\n                  trControl = train_control)\n\n# For logistic regression with caret:\n# train(Y ~ ., data = df, method = \"glm\", \n#       family = binomial, trControl = train_control)\n\n\n9.6.7 Connections to Source Material\n\n\n\n\n\n\nMapping to Course Materials\n\n\n\n\n\n\n\n\n\n\n\n\nLecture Note Section\nCorresponding Source(s)\n\n\n\n\nIntroduction: Why Linear Models Still Matter\nLecture 9 slides intro on interpretable ML and LIME\n\n\n↳ The Power of Interpretability\nExpanded from lecture motivation\n\n\n↳ Linear Models as Building Blocks\nNew material connecting to GLMs, mixed models, LIME\n\n\nSimple Linear Regression\nAoS §13.1\n\n\n↳ Regression Models\nAoS §13.1\n\n\n↳ The Simple Linear Regression Model\nAoS Definition 13.1\n\n\n↳ Estimating Parameters: Method of Least Squares\nAoS §13.1, Theorem 13.4\n\n\n↳ Connection to Maximum Likelihood\nAoS §13.2\n\n\n↳ Properties of the Least Squares Estimators\nAoS §13.3, Theorems 13.8-13.9\n\n\n↳ Simple Linear Regression in Practice\nNew Framingham example from lecture slides, applying concepts from AoS §13.1-13.3\n\n\nMultiple Linear Regression\nAoS §13.5\n\n\n↳ Extending the Model to Multiple Predictors\nAoS §13.5\n\n\n↳ Least Squares in Matrix Form\nAoS Theorem 13.13\n\n\n↳ Multiple Regression in Practice\nNew Framingham example from lecture slides, applying concepts from AoS §13.5\n\n\nModel Selection: Choosing the Right Predictors\nAoS §13.6\n\n\n↳ Scoring Models: The Bias-Variance Trade-off\nAoS §13.6\n\n\n↳ Mallow’s Cp, AIC, BIC, Cross-Validation\nAoS §13.6\n\n\n↳ Search Strategies\nAoS §13.6\n\n\n↳ Comparing Predictor Importance\nLecture 9 slides + Gelman & Hill\n\n\n↳ Controlling for Background Variables\nLecture 9 slides\n\n\nRegression Assumptions and Diagnostics\nLecture 9 slides (sourcing Gelman & Hill)\n\n\n↳ The Five Assumptions\nLecture 9 slides (sourcing Gelman & Hill)\n\n\n↳ Checking Assumptions: Residual Plots\nLecture 9 slides + expanded examples\n\n\nLogistic Regression\nAoS §13.7\n\n\n↳ Modeling Binary Outcomes\nAoS §13.7\n\n\n↳ The Logistic Regression Model\nAoS §13.7\n\n\n↳ Logistic Regression in Practice\nNew Framingham example from lecture slides, applying concepts from AoS §13.7\n\n\nChapter Summary and Connections\nNew comprehensive summary\n\n\nPython and R Reference\nNew - added Python alongside R implementations\n\n\n\n\n\n\n\n\n9.6.8 Further Materials\n\nGelman and Hill (2007) - Practical regression guidance with excellent intuition and real-world advice\nRibeiro, Singh, and Guestrin (2016) - The LIME paper demonstrating how local linear models can explain any classifier’s predictions (GitHub repo)\n\n\nRemember: Start with linear regression – many problems that get a neural network thrown at them could be solved with these simple models. Always establish a linear/logistic baseline first: if your complex deep network only improves accuracy by 2%, is the added complexity, computation, and loss of interpretability really worth it? Master these fundamental methods – they’re used daily by practitioners worldwide and remain indispensable as baselines, interpretable models, and building blocks for more complex systems.\n\n\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge: Cambridge University Press.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Why Should i Trust You?: Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–44. ACM. https://doi.org/10.1145/2939672.2939778.\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/09-linear-logistic-regression.html#footnotes",
    "href": "chapters/09-linear-logistic-regression.html#footnotes",
    "title": "9  Linear and Logistic Regression",
    "section": "",
    "text": "Note that the distribution of \\epsilon may depend on X, though its mean is always zero.↩︎\nThough for very large datasets, iterative methods like stochastic gradient descent may still be used for computational efficiency, even when closed-form solutions exist. Also, when we add regularization (Ridge, LASSO) or have constraints, we lose the closed-form solution and must use iterative methods.↩︎\nCustomer churn refers to when customers stop doing business with a company or cancel their subscription to a service. For example, a mobile phone customer “churns” when they switch to a different provider or cancel their contract. Predicting churn helps companies identify at-risk customers and take preventive action.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear and Logistic Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with\nComments and a Rejoinder by the Author).” Statistical\nScience 16 (3): 199–231.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge: Cambridge\nUniversity Press.\n\n\nKingma, Diederik P, and Jimmy Ba. 2015. “Adam: A Method for\nStochastic Optimization.” In International Conference on\nLearning Representations (ICLR).\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n“Why Should i Trust You?: Explaining the Predictions of Any\nClassifier.” In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\n1135–44. ACM. https://doi.org/10.1145/2939672.2939778.\n\n\nStone, Lawrence D, Colleen M Keller, Thomas M Kratzke, and Johan P\nStrumpfer. 2014. “Search for the Wreckage of Air\nFrance Flight AF 447.” Statistical\nScience 29 (1): 69–80. https://doi.org/10.1214/13-STS420.\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in\nStatistical Inference. Springer Science & Business Media.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "pdf-download.html",
    "href": "pdf-download.html",
    "title": "Download Complete PDF",
    "section": "",
    "text": "📄 PDF Version Available\n\n\n\nThe complete lecture notes are available as a PDF document.\nDownload Statistics for Data Science - Lecture Notes (PDF)\nThis PDF contains all chapters in a single document, optimized for printing or offline reading.",
    "crumbs": [
      "Download Complete PDF"
    ]
  }
]