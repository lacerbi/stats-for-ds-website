<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 5&nbsp; Parametric Inference I: Finding Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/06-parametric-inference-II.html" rel="next">
<link href="../chapters/04-nonparametric-bootstrap.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/05-parametric-inference-I.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">5.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-machine-learning-as-statistical-estimation" id="toc-introduction-machine-learning-as-statistical-estimation" class="nav-link" data-scroll-target="#introduction-machine-learning-as-statistical-estimation"><span class="header-section-number">5.2</span> Introduction: Machine Learning As Statistical Estimation</a></li>
  <li><a href="#parametric-models" id="toc-parametric-models" class="nav-link" data-scroll-target="#parametric-models"><span class="header-section-number">5.3</span> Parametric Models</a></li>
  <li><a href="#the-method-of-moments-mom" id="toc-the-method-of-moments-mom" class="nav-link" data-scroll-target="#the-method-of-moments-mom"><span class="header-section-number">5.4</span> The Method of Moments (MoM)</a>
  <ul class="collapse">
  <li><a href="#the-principle-matching-moments" id="toc-the-principle-matching-moments" class="nav-link" data-scroll-target="#the-principle-matching-moments"><span class="header-section-number">5.4.1</span> The Principle: Matching Moments</a></li>
  <li><a href="#mom-in-action-examples" id="toc-mom-in-action-examples" class="nav-link" data-scroll-target="#mom-in-action-examples"><span class="header-section-number">5.4.2</span> MoM in Action: Examples</a></li>
  <li><a href="#properties-of-method-of-moments-estimator" id="toc-properties-of-method-of-moments-estimator" class="nav-link" data-scroll-target="#properties-of-method-of-moments-estimator"><span class="header-section-number">5.4.3</span> Properties of Method of Moments Estimator</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimation-mle" id="toc-maximum-likelihood-estimation-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-mle"><span class="header-section-number">5.5</span> Maximum Likelihood Estimation (MLE)</a>
  <ul class="collapse">
  <li><a href="#the-principle-what-parameter-makes-my-data-most-probable" id="toc-the-principle-what-parameter-makes-my-data-most-probable" class="nav-link" data-scroll-target="#the-principle-what-parameter-makes-my-data-most-probable"><span class="header-section-number">5.5.1</span> The Principle: What Parameter Makes My Data Most Probable?</a></li>
  <li><a href="#the-likelihood-function" id="toc-the-likelihood-function" class="nav-link" data-scroll-target="#the-likelihood-function"><span class="header-section-number">5.5.2</span> The Likelihood Function</a></li>
  <li><a href="#finding-the-mle-analytically" id="toc-finding-the-mle-analytically" class="nav-link" data-scroll-target="#finding-the-mle-analytically"><span class="header-section-number">5.5.3</span> Finding the MLE Analytically</a></li>
  </ul></li>
  <li><a href="#mle-via-numerical-optimization" id="toc-mle-via-numerical-optimization" class="nav-link" data-scroll-target="#mle-via-numerical-optimization"><span class="header-section-number">5.6</span> MLE Via Numerical Optimization</a>
  <ul class="collapse">
  <li><a href="#the-optimization-setup-for-mle" id="toc-the-optimization-setup-for-mle" class="nav-link" data-scroll-target="#the-optimization-setup-for-mle"><span class="header-section-number">5.6.1</span> The Optimization Setup for MLE</a></li>
  <li><a href="#numerical-optimization-in-1d" id="toc-numerical-optimization-in-1d" class="nav-link" data-scroll-target="#numerical-optimization-in-1d"><span class="header-section-number">5.6.2</span> Numerical Optimization in 1D</a></li>
  <li><a href="#the-gradient-the-key-to-multidimensional-optimization" id="toc-the-gradient-the-key-to-multidimensional-optimization" class="nav-link" data-scroll-target="#the-gradient-the-key-to-multidimensional-optimization"><span class="header-section-number">5.6.3</span> The Gradient: The Key to Multidimensional Optimization</a></li>
  <li><a href="#evaluating-the-gradient" id="toc-evaluating-the-gradient" class="nav-link" data-scroll-target="#evaluating-the-gradient"><span class="header-section-number">5.6.4</span> Evaluating the Gradient</a></li>
  <li><a href="#gradient-based-optimization-methods" id="toc-gradient-based-optimization-methods" class="nav-link" data-scroll-target="#gradient-based-optimization-methods"><span class="header-section-number">5.6.5</span> Gradient-Based Optimization Methods</a></li>
  <li><a href="#stochastic-gradient-methods" id="toc-stochastic-gradient-methods" class="nav-link" data-scroll-target="#stochastic-gradient-methods"><span class="header-section-number">5.6.6</span> Stochastic Gradient Methods</a></li>
  <li><a href="#which-optimizer-should-i-use" id="toc-which-optimizer-should-i-use" class="nav-link" data-scroll-target="#which-optimizer-should-i-use"><span class="header-section-number">5.6.7</span> Which Optimizer Should I Use?</a></li>
  <li><a href="#faq-common-issues-in-numerical-optimization" id="toc-faq-common-issues-in-numerical-optimization" class="nav-link" data-scroll-target="#faq-common-issues-in-numerical-optimization"><span class="header-section-number">5.6.8</span> FAQ: Common Issues in Numerical Optimization</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">5.7</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">5.7.1</span> Key Concepts Review</a></li>
  <li><a href="#the-big-picture" id="toc-the-big-picture" class="nav-link" data-scroll-target="#the-big-picture"><span class="header-section-number">5.7.2</span> The Big Picture</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">5.7.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">5.7.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">5.7.5</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">5.7.6</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">5.7.7</span> Connections to Source Material</a></li>
  <li><a href="#further-materials" id="toc-further-materials" class="nav-link" data-scroll-target="#further-materials"><span class="header-section-number">5.7.8</span> Further Materials</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">5.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li><strong>Define a parametric model</strong> and explain its role in statistical inference, distinguishing between parameters of interest and nuisance parameters.</li>
<li><strong>Derive estimators using the Method of Moments (MoM)</strong> by equating theoretical and sample moments.</li>
<li><strong>Explain the principle of Maximum Likelihood Estimation (MLE)</strong> and formulate the likelihood and log-likelihood functions for a given model.</li>
<li><strong>Find the Maximum Likelihood Estimator (MLE) analytically</strong> in simple cases by maximizing the (log-)likelihood.</li>
<li><strong>Explain when and why numerical optimization is necessary for MLE</strong>, and apply standard optimization libraries to find estimators computationally.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter covers parametric models and two fundamental approaches to finding estimators: the Method of Moments and Maximum Likelihood Estimation. The material is adapted from Chapter 9 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> and supplemented with computational examples and optimization techniques relevant to modern data science applications.</p>
</div>
</div>
</section>
<section id="introduction-machine-learning-as-statistical-estimation" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="introduction-machine-learning-as-statistical-estimation"><span class="header-section-number">5.2</span> Introduction: Machine Learning As Statistical Estimation</h2>
<p>When fitting a machine learning model <span class="math inline">f(x, \theta)</span> for regression with inputs <span class="math inline">x_i</span> and targets <span class="math inline">y_i</span>, <span class="math inline">i = 1, \dots, n</span>, a common approach is to minimize the <strong>mean squared error (MSE)</strong>: <span class="math display"> \min_{\theta} \frac{1}{n} \sum_{i=1}^n \left( f(x_i, \theta) - y_i \right)^2. </span></p>
<p>The terms in this expression are similar to the log-probability of a normal distribution: <span class="math display"> \log \mathcal{N}(y_i;\; f(x_i, \theta), \sigma^2) = -\frac{1}{2}\log(2\pi \sigma^2) - \frac{1}{2 \sigma^2} \left( f(x_i, \theta) - y_i \right)^2. </span></p>
<p>This similarity is not a coincidence. When <span class="math inline">\sigma</span> is constant, maximizing the log-likelihood means maximizing <span class="math inline">-\frac{1}{2\sigma^2} \sum_i (f(x_i, \theta) - y_i)^2</span> plus a constant – which is equivalent to minimizing the MSE.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The ML-Statistics Connection
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you minimize MSE in machine learning, you’re performing maximum likelihood estimation under a Gaussian noise assumption! This connection reveals a fundamental truth: many machine learning algorithms are secretly (or openly) solving statistical estimation problems.</p>
</div>
</div>
<p>This chapter introduces the foundational principles of <strong>parametric inference</strong> – the engine that powers both classical statistics and modern machine learning. We’ll learn two primary methods for finding estimators for model parameters: the Method of Moments and the celebrated Maximum Likelihood Estimation.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here’s a reference table of key terms in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Parametric model</td>
<td>Parametrinen malli</td>
<td>Models with finite parameters</td>
</tr>
<tr class="even">
<td>Parameter of interest</td>
<td>Kiinnostuksen parametri</td>
<td>The quantity we want to estimate</td>
</tr>
<tr class="odd">
<td>Nuisance parameter</td>
<td>Kiusaparametri</td>
<td>Parameters not of primary interest</td>
</tr>
<tr class="even">
<td>Method of Moments (MoM)</td>
<td>Momenttimenetelmä</td>
<td>Estimation by matching moments</td>
</tr>
<tr class="odd">
<td>Moment</td>
<td>Momentti</td>
<td>Expected value of powers</td>
</tr>
<tr class="even">
<td>Sample moment</td>
<td>Otosmomentti</td>
<td>Empirical average of powers</td>
</tr>
<tr class="odd">
<td>Maximum Likelihood Estimation (MLE)</td>
<td>Suurimman uskottavuuden menetelmä</td>
<td>Most common estimation method</td>
</tr>
<tr class="even">
<td>Likelihood function</td>
<td>Uskottavuusfunktio</td>
<td>Joint density as function of parameters</td>
</tr>
<tr class="odd">
<td>Log-likelihood function</td>
<td>Log-uskottavuusfunktio</td>
<td>Logarithm of likelihood</td>
</tr>
<tr class="even">
<td>Maximum Likelihood Estimator</td>
<td>SU-estimaattori</td>
<td>Parameter maximizing likelihood</td>
</tr>
<tr class="odd">
<td>Numerical optimization</td>
<td>Numeerinen optimointi</td>
<td>Computational methods for finding optima</td>
</tr>
<tr class="even">
<td>Gradient</td>
<td>Gradientti</td>
<td>Vector of partial derivatives</td>
</tr>
<tr class="odd">
<td>Gradient descent</td>
<td>Gradienttimenetelmä</td>
<td>Iterative optimization algorithm</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="parametric-models" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="parametric-models"><span class="header-section-number">5.3</span> Parametric Models</h2>
<p>We introduced parametric models in Chapter 3 when discussing statistical inference frameworks. Recall that in the world of statistical inference, we often make assumptions about the structure of our data-generating process. A <strong>parametric model</strong> is one such assumption – it postulates that our data comes from a distribution that can be fully characterized by a finite number of parameters.</p>
<p>Now that we’re diving into estimation methods, let’s revisit this concept with a focus on how we actually <em>find</em> these parameters.</p>
<div class="definition">
<p>A <strong>parametric model</strong> is a set of distributions <span class="math display">\mathfrak{F} = \{ f(x; \theta) : \theta \in \Theta \}</span> where:</p>
<ul>
<li><span class="math inline">\theta = (\theta_1, \ldots, \theta_k)</span> is the <strong>parameter</strong> (possibly vector-valued)</li>
<li><span class="math inline">\Theta \subseteq \mathbb{R}^k</span> is the <strong>parameter space</strong> (the set of all possible parameter values)</li>
<li><span class="math inline">f(x; \theta)</span> is the density or distribution function indexed by <span class="math inline">\theta</span></li>
</ul>
</div>
<p>The key insight: We assume the data-generating process belongs to a specific family of distributions, and our job is just to find the right parameter <span class="math inline">\theta</span> within that family.</p>
<p>In other words, the problem of inference reduces to estimating the parameter(s) <span class="math inline">\theta</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
All Models Are Wrong, But Some Are Useful
</div>
</div>
<div class="callout-body-container callout-body">
<p>Parametric models are widely used although the underlying models are usually not perfect. As the statistician George Box famously said – in what is possibly the most repeated quote in statistics – “<em>All models are wrong, but some are useful.</em>”</p>
<p>When the model is good enough, parametric models can be very useful because they offer a simple representation for potentially complex phenomena. The art of statistical modeling is finding a model that is wrong in acceptable ways while still capturing the essential features of your data.</p>
</div>
</div>
<p><strong>Examples of Parametric Models</strong>:</p>
<ul>
<li><strong>Simple distributions</strong>:
<ul>
<li>Bernoulli(<span class="math inline">p</span>): One parameter determining success probability</li>
<li>Poisson(<span class="math inline">\lambda</span>): One parameter determining both mean and variance</li>
<li>Normal(<span class="math inline">\mu, \sigma^2</span>): Two parameters for location and scale</li>
</ul></li>
<li><strong>Regression models</strong>:
<ul>
<li>Linear regression: <span class="math inline">Y = \beta_0 + \beta_1 X + \epsilon</span> where <span class="math inline">\epsilon \sim N(0, \sigma^2)</span></li>
<li>Logistic regression: <span class="math inline">P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}</span></li>
</ul></li>
<li><strong>Finite mixture models</strong>:
<ul>
<li>Mixture of Gaussians: <span class="math inline">f(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x; \mu_k, \sigma_k^2)</span></li>
<li>Parameters include mixing weights <span class="math inline">\pi_k</span> and component parameters <span class="math inline">(\mu_k, \sigma_k)</span></li>
</ul></li>
<li><strong>Machine Learning models</strong>:
<ul>
<li>Deep Neural Networks: Often millions of parameters (weights and biases)</li>
<li>Despite their complexity, these are still parametric models!</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remember: Parameters of Interest vs.&nbsp;Nuisance Parameters
</div>
</div>
<div class="callout-body-container callout-body">
<p>We introduced this distinction in Chapter 3, which it’s crucial for estimation:</p>
<ul>
<li><strong>Parameter of interest</strong>: The specific quantity <span class="math inline">T(\theta)</span> we want to estimate</li>
<li><strong>Nuisance parameter</strong>: Other parameters we must estimate but don’t care about directly</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Mean Lifetime Estimation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Equipment lifetimes often follow a Gamma distribution. If <span class="math inline">X_1, \ldots, X_n \sim \text{Gamma}(\alpha, \beta)</span>, then:</p>
<p><span class="math display">f(x; \alpha, \beta) = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta}, \quad x &gt; 0</span></p>
<p>If we want to estimate the mean lifetime:</p>
<ul>
<li><strong>Parameter of interest</strong>: <span class="math inline">T(\alpha, \beta) = \mathbb{E}(X) = \alpha\beta</span></li>
<li><strong>Nuisance parameters</strong>: The individual shape (<span class="math inline">\alpha</span>) and scale (<span class="math inline">\beta</span>) parameters</li>
</ul>
<p>Note that we must estimate both parameters, but only their product matters for our question.</p>
</div>
</div>
</section>
<section id="the-method-of-moments-mom" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="the-method-of-moments-mom"><span class="header-section-number">5.4</span> The Method of Moments (MoM)</h2>
<p>The Method of Moments is a simple estimation technique that does not yield optimal estimators, but provides easy-to-compute values that can serve as good starting points for more sophisticated methods.</p>
<section id="the-principle-matching-moments" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="the-principle-matching-moments"><span class="header-section-number">5.4.1</span> The Principle: Matching Moments</h3>
<p>The Method of Moments is based on a straightforward idea: if our model is correct, then theoretical properties of the distribution (moments) should match their empirical counterparts in the data. It’s like saying, “If this really is the right distribution, then the average I calculate from my model should match the average I see in my data.”</p>
<div class="definition">
<p>For a model with parameter <span class="math inline">\theta = (\theta_1, \ldots, \theta_k)</span>:</p>
<ul>
<li>The <span class="math inline">j^{\text{th}}</span> <strong>theoretical moment</strong> is: <span class="math inline">\alpha_j(\theta) = \mathbb{E}_{\theta}(X^j)</span></li>
<li>The <span class="math inline">j^{\text{th}}</span> <strong>sample moment</strong> is: <span class="math inline">\hat{\alpha}_j = \frac{1}{n} \sum_{i=1}^n X_i^j</span></li>
</ul>
</div>
<div class="definition">
<p>The <strong>Method of Moments estimator</strong> <span class="math inline">\hat{\theta}_n</span> is the value of <span class="math inline">\theta</span> such that: <span class="math display">\begin{align}
\alpha_1(\hat{\theta}_n) &amp;= \hat{\alpha}_1 \\
\alpha_2(\hat{\theta}_n) &amp;= \hat{\alpha}_2 \\
&amp;\vdots \\
\alpha_k(\hat{\theta}_n) &amp;= \hat{\alpha}_k
\end{align}</span></p>
</div>
<p>This gives us a system of <span class="math inline">k</span> equations with <span class="math inline">k</span> unknowns – exactly what we need to solve for <span class="math inline">k</span> parameters!</p>
</section>
<section id="mom-in-action-examples" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="mom-in-action-examples"><span class="header-section-number">5.4.2</span> MoM in Action: Examples</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Bernoulli Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \text{Bernoulli}(p)</span>, we have one parameter to estimate.</p>
<ul>
<li>Theoretical first moment: <span class="math inline">\alpha_1(p) = \mathbb{E}_p(X) = p</span></li>
<li>Sample first moment: <span class="math inline">\hat{\alpha}_1 = \frac{1}{n}\sum_{i=1}^n X_i = \bar{X}_n</span></li>
</ul>
<p>Equating them: <span class="math inline">p = \bar{X}_n</span></p>
<p>Therefore, the MoM estimator is <span class="math inline">\hat{p}_{\text{MoM}} = \bar{X}_n</span> – simply the proportion of successes!</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Normal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)</span>, we have two parameters, so we need two equations.</p>
<p><strong>First moment equation:</strong></p>
<ul>
<li><span class="math inline">\alpha_1(\theta) = \mathbb{E}(X) = \mu</span></li>
<li><span class="math inline">\hat{\alpha}_1 = \bar{X}_n</span></li>
<li>Setting equal: <span class="math inline">\mu = \bar{X}_n</span></li>
</ul>
<p><strong>Second moment equation:</strong></p>
<ul>
<li><span class="math inline">\alpha_2(\theta) = \mathbb{E}(X^2) = \mathbb{V}(X) + (\mathbb{E}(X))^2 = \sigma^2 + \mu^2</span></li>
<li><span class="math inline">\hat{\alpha}_2 = \frac{1}{n}\sum_{i=1}^n X_i^2</span></li>
<li>Setting equal: <span class="math inline">\sigma^2 + \mu^2 = \frac{1}{n}\sum_{i=1}^n X_i^2</span></li>
</ul>
<p>Solving this system:</p>
<ul>
<li><span class="math inline">\hat{\mu}_{\text{MoM}} = \bar{X}_n</span></li>
<li><span class="math inline">\hat{\sigma}^2_{\text{MoM}} = \frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}_n^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}_n)^2</span></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Gamma Distribution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \text{Gamma}(\alpha, \beta)</span>, let’s derive the MoM estimators.</p>
<p>The Gamma distribution has:</p>
<ul>
<li>First moment: <span class="math inline">\mathbb{E}(X) = \alpha\beta</span></li>
<li>Second moment: <span class="math inline">\mathbb{E}(X^2) = \mathbb{V}(X) + (\mathbb{E}(X))^2 = \alpha\beta^2 + (\alpha\beta)^2</span></li>
</ul>
<p>Setting up the moment equations: <span class="math display">\alpha\beta = \bar{X}_n</span> <span class="math display">\alpha\beta^2 + (\alpha\beta)^2 = \frac{1}{n}\sum_{i=1}^n X_i^2</span></p>
<p>From the first equation: <span class="math inline">\alpha = \bar{X}_n / \beta</span></p>
<p>Substituting into the second equation: <span class="math display">\frac{\bar{X}_n}{\beta} \cdot \beta^2 + \bar{X}_n^2 = \frac{1}{n}\sum_{i=1}^n X_i^2</span></p>
<p>Simplifying: <span class="math display">\bar{X}_n \beta + \bar{X}_n^2 = \frac{1}{n}\sum_{i=1}^n X_i^2</span></p>
<p>Solving for <span class="math inline">\beta</span>: <span class="math display">\beta = \frac{\frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}_n^2}{\bar{X}_n} = \frac{\text{sample variance}}{\bar{X}_n}</span></p>
<p>Therefore, the MoM estimators are:</p>
<ul>
<li><span class="math inline">\hat{\beta}_{\text{MoM}} = \frac{\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2}{\bar{X}_n}</span></li>
<li><span class="math inline">\hat{\alpha}_{\text{MoM}} = \frac{\bar{X}_n}{\hat{\beta}_{\text{MoM}}} = \frac{\bar{X}_n^2}{\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2}</span></li>
</ul>
<div id="998d7b47" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from a Gamma distribution</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>true_alpha, true_beta <span class="op">=</span> <span class="fl">3.0</span>, <span class="fl">2.0</span>  <span class="co"># True parameters</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> stats.gamma.rvs(a<span class="op">=</span>true_alpha, scale<span class="op">=</span>true_beta, size<span class="op">=</span>n)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments for Gamma distribution</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># For Gamma(α, β): E[X] = αβ, E[X²] = α(α+1)β²</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sample_second_moment <span class="op">=</span> np.mean(data<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the system of equations</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># mean = α * β</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># second_moment = α * β² * (α + 1) = α * β² + α² * β²</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># This gives us: β = (second_moment - mean²) / mean</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>mom_beta <span class="op">=</span> (sample_second_moment <span class="op">-</span> sample_mean<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> sample_mean</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>mom_alpha <span class="op">=</span> sample_mean <span class="op">/</span> mom_beta</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"True parameters: α = </span><span class="sc">{</span>true_alpha<span class="sc">}</span><span class="ss">, β = </span><span class="sc">{</span>true_beta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MoM estimates:   α = </span><span class="sc">{</span>mom_alpha<span class="sc">:.3f}</span><span class="ss">, β = </span><span class="sc">{</span>mom_beta<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fit</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">20</span>, <span class="dv">200</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.hist(data, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Data'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.gamma.pdf(x, a<span class="op">=</span>true_alpha, scale<span class="op">=</span>true_beta), </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>         <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True distribution'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.gamma.pdf(x, a<span class="op">=</span>mom_alpha, scale<span class="op">=</span>mom_beta), </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MoM fit'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Method of Moments Estimation for Gamma Distribution'</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>True parameters: α = 3.0, β = 2.0
MoM estimates:   α = 3.869, β = 1.511</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05-parametric-inference-I_files/figure-html/cell-2-output-2.png" width="606" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="properties-of-method-of-moments-estimator" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="properties-of-method-of-moments-estimator"><span class="header-section-number">5.4.3</span> Properties of Method of Moments Estimator</h3>
<p>Under regular conditions, Method of Moments estimators have some desirable properties:</p>
<div class="theorem" name="Properties of MoM Estimator">
<p>Let <span class="math inline">\hat{\theta}_n</span> denote the method of moments estimator. Under appropriate conditions on the model, the following statements hold:</p>
<ol type="1">
<li><p><strong>Existence</strong>: The estimate <span class="math inline">\hat{\theta}_n</span> exists with probability tending to 1.</p></li>
<li><p><strong>Consistency</strong>: The estimate is consistent: <span class="math display">\hat{\theta}_n \xrightarrow{P} \theta</span></p></li>
<li><p><strong>Asymptotic Normality</strong>: The estimate is asymptotically Normal: <span class="math display">\sqrt{n}(\hat{\theta}_n - \theta) \rightsquigarrow N(0, \Sigma)</span> where <span class="math display">\Sigma = g \mathbb{E}_{\theta}(Y Y^T) g^T,</span> with <span class="math inline">Y = (X, X^2, \ldots, X^k)^T</span> and <span class="math inline">g = (g_1, \ldots, g_k)</span> where <span class="math inline">g_j = \frac{\partial \alpha_j^{-1}(\theta)}{\partial \theta}</span>.</p></li>
</ol>
</div>
<p>The last result can yield confidence intervals, but bootstrap is usually easier.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Good are MoM Estimators?
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Strengths:</strong></p>
<ul>
<li>Simple to compute – just solve algebraic equations!</li>
<li>Guaranteed existence and consistency under mild conditions</li>
<li>Asymptotically normal, enabling confidence intervals</li>
</ul>
<p><strong>Weaknesses:</strong></p>
<ul>
<li><strong>Not efficient</strong>: Other estimators (like MLE) typically have smaller variance</li>
<li><strong>May give impossible values</strong>: Can produce estimates outside the parameter space</li>
<li><strong>Arbitrary</strong>: Why use moments? Why not other features of the distribution?</li>
</ul>
<p><strong>Primary Use Case</strong>: MoM estimators are excellent starting values for more sophisticated methods like maximum likelihood estimation, which often require numerical optimization.</p>
</div>
</div>
</section>
</section>
<section id="maximum-likelihood-estimation-mle" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="maximum-likelihood-estimation-mle"><span class="header-section-number">5.5</span> Maximum Likelihood Estimation (MLE)</h2>
<section id="the-principle-what-parameter-makes-my-data-most-probable" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="the-principle-what-parameter-makes-my-data-most-probable"><span class="header-section-number">5.5.1</span> The Principle: What Parameter Makes My Data Most Probable?</h3>
<p>Maximum Likelihood Estimation is arguably the most important estimation method in statistics. While the Method of Moments asks “what parameters make the theoretical moments match the empirical ones?”, MLE asks a more direct question: “what parameter values make my observed data most plausible?”</p>
<p>The elegance of MLE is that it reverses our usual probability thinking:</p>
<ul>
<li><strong>Probability</strong>: Given parameters, what data would we expect to see?</li>
<li><strong>Likelihood</strong>: Given data, which parameters make this data most probable?</li>
</ul>
</section>
<section id="the-likelihood-function" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="the-likelihood-function"><span class="header-section-number">5.5.2</span> The Likelihood Function</h3>
<p>The mathematical object at the core of MLE is the likelihood function.</p>
<div class="definition">
<p>Let <span class="math inline">X_1, \ldots, X_n</span> be IID with PDF <span class="math inline">f(x; \theta)</span>.</p>
<p>The <strong>likelihood function</strong> is: <span class="math display">\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta)</span></p>
</div>
<p>For both mathematical and numerical convenience, in practice we often work with the <strong>log-likelihood function</strong>: <span class="math display">\ell_n(\theta) = \log \mathcal{L}_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)</span></p>
<div class="definition">
<p>The <strong>Maximum Likelihood Estimator (MLE)</strong> is: <span class="math display">\hat{\theta}_{\text{MLE}} = \arg\max_{\theta \in \Theta} \mathcal{L}_n(\theta) = \arg\max_{\theta \in \Theta} \ell_n(\theta)</span></p>
</div>
<p>Note that maximizing the likelihood is the same as maximizing the log-likelihood, since the logarithm is a strictly increasing (monotonic) function.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255600-513-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255600-513-1" role="tab" aria-controls="tabset-1757255600-513-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255600-513-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255600-513-2" role="tab" aria-controls="tabset-1757255600-513-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255600-513-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255600-513-3" role="tab" aria-controls="tabset-1757255600-513-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255600-513-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255600-513-1-tab"><p>Likelihood is a “what if” game. Imagine you have a coin and you flip
it 10 times, getting 7 heads. You ask yourself:</p><p>“What if the probability of heads were 0.5? How likely would 7 heads
in 10 flips be?” “What if the probability were 0.6? Would that make my
data more or less likely?” “What if it were 0.7? 0.8?”</p><p>For each possible value of the parameter (here, the probability of
heads), you calculate how probable your exact observed data would be.
The parameter value that maximizes this probability is your maximum
likelihood estimate.</p><p>The likelihood function is this “what if” calculation formalized –
it’s the probability (or probability density) of your observed data,
treated as a function of the unknown parameters.</p></div><div id="tabset-1757255600-513-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255600-513-2-tab"><p>Why do we often prefer working with the log-likelihood?</p><p><span class="math display">\[\ell_n(\theta) = \log \mathcal{L}_n(\theta) = \sum_{i=1}^n \log f(x_i; \theta)\]</span></p><ol type="1">
<li><p><strong>Differentiation</strong>: For <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential
family</a> distributions (normal, exponential, gamma, etc.), log
derivatives eliminate exponentials. For example, for normal:
<span class="math inline">\(\frac{d}{d\mu} e^{-(x-\mu)^2/2}\)</span>
becomes just <span class="math inline">\((x-\mu)\)</span> after taking
logs</p></li>
<li><p><strong>Numerical stability</strong>: Products of small
probabilities underflow; sums of logs don’t</p></li>
<li><p><strong>Additive structure</strong>: Log-likelihood is a sum over
observations, making derivatives and optimization more
tractable</p></li>
</ol></div><div id="tabset-1757255600-513-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255600-513-3-tab"><p>Let’s visualize the likelihood function for a simple Bernoulli
example:</p><div id="3aae136b" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate coin flips: n=20, observed 12 successes</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the log-likelihood function</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">200</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> successes <span class="op">*</span> np.log(p_values) <span class="op">+</span> (n <span class="op">-</span> successes) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> p_values)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the MLE (exact analytical solution)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>p_mle <span class="op">=</span> successes <span class="op">/</span> n  <span class="co"># Exact MLE for Bernoulli</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.plot(p_values, log_likelihood, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.axvline(p_mle, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'MLE: p̂ = </span><span class="sc">{</span>p_mle<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-likelihood ℓₙ(p)'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Log-likelihood for Bernoulli with n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, S=</span><span class="sc">{</span>successes<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Maximum likelihood estimate: p̂ = </span><span class="sc">{</span>successes<span class="op">/</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This makes intuitive sense: it's simply the observed proportion of successes!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="05-parametric-inference-I_files/figure-html/cell-3-output-1.png" width="661" height="372"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Maximum likelihood estimate: p̂ = 0.6
This makes intuitive sense: it's simply the observed proportion of successes!</code></pre>
</div>
</div><p>Notice how the log-likelihood is maximized exactly at the observed
proportion of successes. This is no coincidence – the MLE often has an
intuitive interpretation.</p></div></div></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important: Likelihood function is NOT a probability distribution!
</div>
</div>
<div class="callout-body-container callout-body">
<p>The likelihood <span class="math inline">\mathcal{L}_n(\theta)</span> is NOT a probability distribution over <span class="math inline">\theta</span>. In general: <span class="math display">\int_\Theta \mathcal{L}_n(\theta) d\theta \neq 1</span></p>
<p>Why? The same mathematical expression <span class="math inline">f(x; \theta)</span> plays two different roles:</p>
<ul>
<li><strong>As a PDF</strong>: Fix <span class="math inline">\theta</span>, vary <span class="math inline">x</span> → <span class="math inline">\int f(x; \theta) dx = 1</span> (proper probability distribution)</li>
<li><strong>As a likelihood</strong>: Fix <span class="math inline">x</span> (observed data), vary <span class="math inline">\theta</span> → <span class="math inline">\int \mathcal{L}_n(\theta) d\theta</span> is usually not 1</li>
</ul>
<p><strong>Example</strong>: Observe one coin flip with result <span class="math inline">X = 1</span> (heads) from Bernoulli(<span class="math inline">p</span>):</p>
<ul>
<li>The likelihood is <span class="math inline">\mathcal{L}(p) = p</span> for <span class="math inline">p \in [0,1]</span></li>
<li><span class="math inline">\int_0^1 p \, dp = \frac{1}{2} \neq 1</span></li>
</ul>
<p>The likelihood tells us relative plausibility of parameter values, not their probabilities. This is why we need Bayesian methods if we want actual probability distributions over parameters!</p>
</div>
</div>
</section>
<section id="finding-the-mle-analytically" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="finding-the-mle-analytically"><span class="header-section-number">5.5.3</span> Finding the MLE Analytically</h3>
<p>For simple models, we can find the MLE by taking derivatives and setting them to zero. The general recipe:</p>
<ol type="1">
<li>Write down the likelihood <span class="math inline">\mathcal{L}_n(\theta)</span></li>
<li>Take the logarithm to get <span class="math inline">\ell_n(\theta)</span> (this simplifies products to sums)</li>
<li>Take the derivative with respect to <span class="math inline">\theta</span></li>
<li>Set equal to zero and solve</li>
<li>Verify it’s a maximum (not a minimum or saddle point)</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simplifying MLE Calculations
</div>
</div>
<div class="callout-body-container callout-body">
<p>When finding the MLE, multiplying the likelihood (or log-likelihood) by a positive constant or adding a constant doesn’t change where the maximum occurs. This means we can often ignore:</p>
<ul>
<li>Normalization constants that don’t depend on <span class="math inline">\theta</span></li>
<li>Terms like <span class="math inline">\frac{1}{(2\pi)^{n/2}}</span> in the normal distribution</li>
<li>Factorials in discrete distributions</li>
</ul>
<p>This greatly simplifies calculations – focus only on terms involving <span class="math inline">\theta</span>!</p>
</div>
</div>
<p>Let’s work through some examples:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Bernoulli Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \text{Bernoulli}(p)</span>:</p>
<p><strong>Step 1</strong>: The likelihood is <span class="math display">\mathcal{L}_n(p) = \prod_{i=1}^n p^{X_i}(1-p)^{1-X_i} = p^S(1-p)^{n-S}</span> where <span class="math inline">S = \sum_{i=1}^n X_i</span> is the total number of successes.</p>
<p><strong>Step 2</strong>: The log-likelihood is <span class="math display">\ell_n(p) = S \log p + (n-S) \log(1-p)</span></p>
<p><strong>Step 3</strong>: Taking the derivative: <span class="math display">\frac{d\ell_n}{dp} = \frac{S}{p} - \frac{n-S}{1-p}</span></p>
<p><strong>Step 4</strong>: Setting to zero and solving: <span class="math display">\frac{S}{p} = \frac{n-S}{1-p} \implies S(1-p) = (n-S)p \implies S = np</span></p>
<p>Therefore, <span class="math inline">\hat{p}_{\text{MLE}} = S/n = \bar{X}_n</span>.</p>
<p><strong>Note</strong>: This is the same as the Method of Moments estimator!</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Normal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)</span>:</p>
<p>The log-likelihood (ignoring constants) is: <span class="math display">\ell_n(\mu, \sigma) = -n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2</span></p>
<p>Taking partial derivatives and setting to zero:</p>
<p><span class="math display">\frac{\partial \ell_n}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \mu) = 0</span></p>
<p><span class="math display">\frac{\partial \ell_n}{\partial \sigma} = -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^n (X_i - \mu)^2 = 0</span></p>
<p>Solving these equations:</p>
<ul>
<li><span class="math inline">\hat{\mu}_{\text{MLE}} = \bar{X}_n</span></li>
<li><span class="math inline">\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X}_n)^2</span></li>
</ul>
<p>Again, these match the Method of Moments estimators!</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: A Harder Case - Uniform(0, θ)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Not all MLEs can be found by differentiation! Consider <span class="math inline">X_1, \ldots, X_n \sim \text{Uniform}(0, \theta)</span>.</p>
<p>The PDF is: <span class="math display">f(x; \theta) = \begin{cases}
1/\theta &amp; \text{if } 0 \leq x \leq \theta \\
0 &amp; \text{otherwise}
\end{cases}</span></p>
<p>The likelihood is: <span class="math display">\mathcal{L}_n(\theta) = \begin{cases}
(1/\theta)^n &amp; \text{if } \theta \geq \max\{X_1, \ldots, X_n\} \\
0 &amp; \text{if } \theta &lt; \max\{X_1, \ldots, X_n\}
\end{cases}</span></p>
<p>This function:</p>
<ul>
<li>Is 0 when <span class="math inline">\theta</span> is less than the largest observation</li>
<li>Decreases as <span class="math inline">(1/\theta)^n</span> for larger <span class="math inline">\theta</span></li>
</ul>
<p>Therefore, the likelihood is maximized at the boundary: <span class="math inline">\hat{\theta}_{\text{MLE}} = X_{(n)} = \max\{X_1, \ldots, X_n\}</span>.</p>
<p>This example shows that not all optimization problems are solved by calculus – sometimes we need to think more carefully about the function’s behavior!</p>
<p>Let’s visualize this unusual likelihood function:</p>
<div id="f9d390ce" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="3">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Uniform(0, θ) MLE visualization</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>true_theta <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, true_theta, n)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> np.<span class="bu">max</span>(data)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create theta values</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>theta_values <span class="op">=</span> np.linspace(<span class="fl">0.1</span>, <span class="fl">3.0</span>, <span class="dv">300</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> np.zeros_like(theta_values)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate likelihood (proportional to)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, theta <span class="kw">in</span> <span class="bu">enumerate</span>(theta_values):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> theta <span class="op">&gt;=</span> x_max:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        likelihood[i] <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>theta)<span class="op">**</span>n</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        likelihood[i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot data points</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>plt.scatter(data, np.zeros_like(data), color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">50</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">'Data'</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>plt.axvline(x_max, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'max(X) = </span><span class="sc">{</span>x_max<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">3.0</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Observed Data'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, likelihood, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>plt.axvline(x_max, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'MLE = </span><span class="sc">{</span>x_max<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Likelihood (proportional to)'</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Likelihood Function'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05-parametric-inference-I_files/figure-html/cell-4-output-1.png" width="662" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="mle-via-numerical-optimization" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="mle-via-numerical-optimization"><span class="header-section-number">5.6</span> MLE Via Numerical Optimization</h2>
<p>Finding the maximum of the log-likelihood by taking a derivative and setting it to zero is clean and satisfying, but it only works for the simplest models. For most real-world problems, the log-likelihood function is a complex, high-dimensional surface, and we cannot find the peak analytically.</p>
<p>Instead, we must turn to <strong>numerical optimization</strong>. The core idea of most optimization algorithms is simple: we start with an initial guess for the parameters, <span class="math inline">\theta_0</span>, and then iteratively take steps “uphill” on the likelihood surface until we can no longer find a higher point.</p>
<p>This section explores the concepts and tools behind this fundamental process.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Need for Numerical Methods
</div>
</div>
<div class="callout-body-container callout-body">
<p>For many important models, we cannot solve for the MLE analytically:</p>
<ul>
<li><strong>Logistic regression</strong>: No closed-form solution for the regression coefficients</li>
<li><strong>Mixture models</strong>: Complex likelihood with multiple local maxima<br>
</li>
<li><strong>Most complex ML models</strong>: Neural networks, random forests, etc.</li>
</ul>
<p>We must use iterative numerical optimization algorithms to find the maximum of the likelihood function (or minimum of the negative log-likelihood).</p>
</div>
</div>
<section id="the-optimization-setup-for-mle" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="the-optimization-setup-for-mle"><span class="header-section-number">5.6.1</span> The Optimization Setup for MLE</h3>
<p>Finding the MLE requires solving an optimization problem to find the parameters <span class="math inline">\theta</span> that maximize the likelihood function <span class="math inline">\mathcal{L}_n(\theta)</span> or equivalently the log-likelihood function <span class="math inline">\ell_n(\theta)</span>.</p>
<p>Since optimization methods in software libraries are conventionally written to perform <strong>minimization</strong>, our practical goal becomes:</p>
<p><span class="math display"> \hat{\theta}_{\text{MLE}} = \arg\min_{\theta} \left[-\ell_n(\theta)\right] </span></p>
<p>This is a crucial point: we minimize the <strong>negative</strong> log-likelihood. Forgetting this minus sign is one of the most common programming errors in statistical computing!</p>
<p>Throughout this section, we’ll follow the standard convention and present algorithms for minimization. We’ll use examples from Python’s <code>scipy.optimize</code>, but other languages and libraries (R’s <code>optim</code>, Julia’s <code>Optim.jl</code>, etc.) offer similar algorithms with comparable interfaces.</p>
<p>When in optimization we say that a problem is <span class="math inline">D</span>-dimensional, we refer to the number of variables being optimized over – here, the number of elements of <span class="math inline">\theta</span>.</p>
</section>
<section id="numerical-optimization-in-1d" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="numerical-optimization-in-1d"><span class="header-section-number">5.6.2</span> Numerical Optimization in 1D</h3>
<p>To understand how numerical optimization works, it’s helpful to start with the simplest case: finding the minimum of a function with a single parameter, <span class="math inline">f(\theta)</span>.</p>
<p>Let’s take a simple example function to minimize:</p>
<p><span class="math display"> f(\theta) = \frac{\theta^2}{10} + \frac{1}{\theta}; \quad \theta &gt; 0 </span></p>
<p>whose global minimum is at <span class="math inline">\theta^\star \approx 1.71</span>.</p>
<div id="2ad50832" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05-parametric-inference-I_files/figure-html/cell-5-output-1.png" width="576" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>How can a computer find the minimum of this function? There are two main families of approaches.</p>
<section id="d-optimization-without-derivatives" class="level4">
<h4 class="anchored" data-anchor-id="d-optimization-without-derivatives">1D Optimization without Derivatives</h4>
<p>The simplest algorithms work by bracketing the minimum. If you can find three points <span class="math inline">(a, c, b)</span> such that <span class="math inline">a &lt; c &lt; b</span> and <span class="math inline">f(c) &lt; f(a)</span> and <span class="math inline">f(c) &lt; f(b)</span>, you know a minimum lies somewhere in the interval <span class="math inline">(a, b)</span>.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Golden-section_search"><strong>Golden-section search</strong></a>: A simple but robust method that is analogous to binary search for finding a root. It progressively narrows the bracket until the desired precision is reached.</li>
<li><a href="https://en.wikipedia.org/wiki/Brent%27s_method"><strong>Brent’s method</strong></a>: A more sophisticated and generally preferred method. It combines the guaranteed (but slow) progress of golden-section search with the potentially faster convergence of fitting a parabola to the three points and jumping to the minimum of the parabola.</li>
</ul>
<p>These methods are useful when the function’s derivative is unavailable or unreliable.</p>
</section>
<section id="d-optimization-with-derivatives-newtons-method" class="level4">
<h4 class="anchored" data-anchor-id="d-optimization-with-derivatives-newtons-method">1D Optimization with Derivatives: Newton’s Method</h4>
<p>If we can compute derivatives, we can often find the minimum much faster. The most famous derivative-based method is <a href="https://en.wikipedia.org/wiki/Newton%27s_method"><strong>Newton’s method</strong></a> (or Newton-Raphson).</p>
<p>The idea is to iteratively approximate the function with a quadratic and jump to the minimum of that quadratic. A second-order Taylor expansion of <span class="math inline">f(\theta)</span> around a point <span class="math inline">\theta_t</span> is: <span class="math display"> f(\theta) \approx f(\theta_t) + f'(\theta_t) (\theta - \theta_t) + \frac{1}{2} f''(\theta_t) (\theta - \theta_t)^2. </span></p>
<p>To find the minimum of this quadratic approximation, we take its derivative with respect to <span class="math inline">\theta</span> and set it to zero: <span class="math display"> \frac{d}{d\theta} (\text{approx}) = f'(\theta_t) + f''(\theta_t)(\theta - \theta_t) = 0 </span></p>
<p>Solving for <span class="math inline">\theta</span> gives us the next point in our iteration, <span class="math inline">\theta_{t+1}</span>: <span class="math display"> \theta_{t+1} = \theta_t - \frac{f'(\theta_t)}{f''(\theta_t)}. </span></p>
<p>This simple update rule is the core of Newton’s method. When close to a well-behaved minimum, it converges very rapidly (quadratically). However, it can be unstable if the function is not “nice” or if the starting point is far from the minimum.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: 1D Optimization in <code>scipy</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s find the minimum of our example function using <code>scipy.optimize</code>. We’ll use Brent’s method, which only requires the function itself and a bracket.</p>
<div id="894b25ec" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> theta<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>theta</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the minimum using Brent's method</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to provide a bracket (a, b) or (a, c, b)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># where the minimum is expected to lie.</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's use (0.1, 10.0)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> scipy.optimize.brent(f, brack<span class="op">=</span>(<span class="fl">0.1</span>, <span class="fl">10.0</span>))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The minimum found by Brent's method is at θ = </span><span class="sc">{</span>result<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The value of the function at the minimum is f(θ) = </span><span class="sc">{</span>f(result)<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Analytical minimum is at (5)^(1/3)</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>analytical_min <span class="op">=</span> <span class="dv">5</span><span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The analytical minimum is at θ = </span><span class="sc">{</span>analytical_min<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The minimum found by Brent's method is at θ = -0.000000
The value of the function at the minimum is f(θ) = -456934496164.231018
The analytical minimum is at θ = 1.709976</code></pre>
</div>
</div>
<p>The output shows that the numerical method finds the correct minimum with high precision.</p>
</div>
</div>
</section>
<section id="moving-to-multiple-dimensions" class="level4">
<h4 class="anchored" data-anchor-id="moving-to-multiple-dimensions">Moving to Multiple Dimensions</h4>
<p>The concepts from 1D optimization extend to the multi-dimensional case, but with added complexity.</p>
<p><strong>Derivative-free methods:</strong></p>
<p>Generalizing derivative-free methods to multiple dimensions, i.e., optimizing <span class="math inline">f(\theta_1, \ldots, \theta_n)</span> with respect to <span class="math inline">\theta_1, \ldots, \theta_n</span> is hard! As the dimensionality increases, the number of possible directions that need to be searched for the optimum grows very rapidly.</p>
<ul>
<li><strong>Classical approaches</strong>: Very old algorithms like <a href="https://en.wikipedia.org/wiki/Pattern_search_(optimization)">direct search</a> methods are mostly inefficient and struggle as dimensionality grows. Searching in many directions at once becomes prohibitively expensive.</li>
<li><strong>Modern approaches</strong>: Recent methods can be more effective in specific scenarios:
<ul>
<li><strong><a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a></strong>: Works well in low dimensions (typically &lt; 20) and can be very sample-efficient when function evaluations are expensive</li>
<li><strong><a href="https://en.wikipedia.org/wiki/CMA-ES">CMA-ES</a></strong> (Covariance Matrix Adaptation Evolution Strategy): If you can afford many function evaluations and don’t have gradients, this method can work in surprisingly high dimensions – up to tens of thousands of parameters</li>
</ul></li>
<li>These methods are mainly used when gradients are unavailable or unreliable (e.g., noisy simulations, black-box models)</li>
</ul>
<p><strong>Derivative-based methods:</strong></p>
<ul>
<li><strong>The standard choice</strong>: When gradients are available and the function is relatively inexpensive to evaluate, gradient-based methods dominate</li>
<li><strong>Scalability</strong>: These methods can handle problems with millions or even billions of parameters (think deep neural networks)</li>
<li><strong>Key transformation</strong>: The first derivative (slope) becomes the <strong>gradient</strong> (<span class="math inline">\nabla f</span>), and the second derivative (curvature) becomes the <strong>Hessian matrix</strong> (<span class="math inline">H</span>)</li>
<li><strong>Why they work</strong>: Gradient information provides a principled direction for improvement at each step, making the search much more efficient than blind exploration</li>
</ul>
<p>You can find visualizations of several optimization algorithms at work <a href="https://github.com/lacerbi/optimviz">here</a>.</p>
</section>
</section>
<section id="the-gradient-the-key-to-multidimensional-optimization" class="level3" data-number="5.6.3">
<h3 data-number="5.6.3" class="anchored" data-anchor-id="the-gradient-the-key-to-multidimensional-optimization"><span class="header-section-number">5.6.3</span> The Gradient: The Key to Multidimensional Optimization</h3>
<p>The main tool for optimization of multidimensional functions</p>
<p><span class="math display">f: \mathbb{R}^n \to \mathbb{R}</span></p>
<p>is the <strong>gradient</strong>:</p>
<p><span class="math display">\nabla f(\theta) = \left(\frac{\partial f}{\partial \theta_1}, \ldots, \frac{\partial f}{\partial \theta_n}\right)^T</span></p>
<p>Here <span class="math inline">\frac{\partial f}{\partial \theta_i}</span> is the <strong>partial derivative</strong> of <span class="math inline">f</span> with respect to <span class="math inline">\theta_i</span>. It can be evaluated by computing the derivative w.r.t. <span class="math inline">\theta_i</span> while keeping the other variables constant.</p>
<p><strong>Geometric interpretation</strong>: The gradient points in the direction where the function values grow fastest. Its magnitude measures the rate of change in that direction.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuition: The Gradient as a Compass
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of finding the MLE as hiking blindfolded on a hilly terrain (the negative log-likelihood surface). At each point, the gradient tells you which direction is steepest uphill. Since we want to minimize, we go in the opposite direction – downhill.</p>
</div>
</div>
<p>Let’s visualize this concept:</p>
<div id="6c4adc24" class="cell" data-fig-height="10" data-fig-width="7" data-execution_count="6">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2D function with interesting topology (negative log-likelihood surface)</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example negative log-likelihood with two local minima"""</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> np.exp(<span class="op">-</span>(x<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> y<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> np.exp(<span class="op">-</span>(x<span class="op">+</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> y<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>(x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">50</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">40</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> f(X, Y)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure with two subplots</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">10</span>))</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># First subplot: 3D surface</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax1.plot_surface(X, Y, Z, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, edgecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>ax1.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, offset<span class="op">=</span>Z.<span class="bu">min</span>(), alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the global minimum</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>ax1.scatter([<span class="dv">1</span>], [<span class="dv">0</span>], [f(<span class="dv">1</span>, <span class="dv">0</span>)], color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'Global minimum'</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'θ₁'</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'θ₂'</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>ax1.set_zlabel(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'3D Optimization Landscape'</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust viewing angle to swap visual perspective</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>ax1.view_init(elev<span class="op">=</span><span class="dv">30</span>, azim<span class="op">=-</span><span class="dv">45</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Second subplot: 2D contour with gradient field</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute gradient (numerically)</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>dy <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>grad_x <span class="op">=</span> (f(X <span class="op">+</span> dx, Y) <span class="op">-</span> f(X <span class="op">-</span> dx, Y)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dx)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>grad_y <span class="op">=</span> (f(X, Y <span class="op">+</span> dy) <span class="op">-</span> f(X, Y <span class="op">-</span> dy)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dy)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize gradient vectors for better visualization</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>grad_norm <span class="op">=</span> np.sqrt(grad_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> grad_y<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>grad_x_norm <span class="op">=</span> <span class="op">-</span>grad_x <span class="op">/</span> (grad_norm <span class="op">+</span> <span class="fl">1e-10</span>)  <span class="co"># Negative for descent</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>grad_y_norm <span class="op">=</span> <span class="op">-</span>grad_y <span class="op">/</span> (grad_norm <span class="op">+</span> <span class="fl">1e-10</span>)</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Contour plot</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax2.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>ax2.clabel(contour, inline<span class="op">=</span><span class="va">True</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient vectors</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>skip <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Show every 3rd arrow for clarity</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>ax2.quiver(X[::skip, ::skip], Y[::skip, ::skip], </span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>           grad_x_norm[::skip, ::skip], grad_y_norm[::skip, ::skip], </span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>           scale<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, width<span class="op">=</span><span class="fl">0.003</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark global minimum</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>ax2.plot([<span class="dv">1</span>], [<span class="dv">0</span>], <span class="st">'k*'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'Global minimum'</span>)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'θ₁'</span>)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'θ₂'</span>)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Gradient Field on Negative Log-Likelihood Surface'</span>)</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05-parametric-inference-I_files/figure-html/cell-7-output-1.png" width="661" height="948" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>These visualizations show:</p>
<ul>
<li><strong>Top (3D)</strong>: The optimization landscape as a surface, showing the “hills and valleys” that optimization algorithms must navigate</li>
<li><strong>Bottom (2D)</strong>: The same landscape from above with:
<ul>
<li><strong>Contour lines</strong> showing level sets of the negative log-likelihood</li>
<li><strong>Red arrows</strong> pointing in the direction of steepest descent (negative gradient)</li>
<li><strong>Black star</strong> marking the global minimum at θ₁ = 1, θ₂ = 0</li>
</ul></li>
<li>Note how the gradient vectors always point toward lower values, guiding optimization algorithms downhill</li>
</ul>
</section>
<section id="evaluating-the-gradient" class="level3" data-number="5.6.4">
<h3 data-number="5.6.4" class="anchored" data-anchor-id="evaluating-the-gradient"><span class="header-section-number">5.6.4</span> Evaluating the Gradient</h3>
<p>The first practical challenge in using gradients for optimization is how to evaluate the gradient. There are three main approaches:</p>
<ol type="1">
<li><strong>Analytically</strong>: Derive by hand and implement – fast but error-prone</li>
<li><a href="https://en.wikipedia.org/wiki/Finite_difference"><strong>Finite differences</strong></a>: Approximate numerically by evaluating nearby points – easy but expensive<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and less precise</li>
<li><strong>Automatic differentiation (autodiff)</strong>: Best of both worlds – exact, fast and automatic</li>
</ol>
<p>Modern frameworks like <a href="https://pytorch.org/">PyTorch</a> and <a href="https://docs.jax.dev/en/latest/index.html">JAX</a> make automatic differentiation the preferred choice. However, some older languages such as R still lack full autodiff support. Gradient computation usually uses <strong>backward-mode autodiff</strong>, which is suitable for computing derivatives of real-valued functions <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Gradients in Python with JAX
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here’s how to compute gradients automatically using JAX:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>theta</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> jax.grad(f)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>g(<span class="fl">1.5</span>)  <span class="co"># Evaluating at a scalar value</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># For multiple variables:</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f2(theta):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (jnp.exp(<span class="op">-</span>(theta[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span> jnp.exp(<span class="op">-</span>(theta[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> jax.grad(f2)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>g2(jnp.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="fl">0.5</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="gradient-based-optimization-methods" class="level3" data-number="5.6.5">
<h3 data-number="5.6.5" class="anchored" data-anchor-id="gradient-based-optimization-methods"><span class="header-section-number">5.6.5</span> Gradient-Based Optimization Methods</h3>
<p>Once we have gradients, we can use various algorithms to find the optimum.</p>
<p>The simplest gradient-based algorithm is <strong>gradient descent</strong> (also called <strong>steepest descent</strong>), which iteratively takes steps in the direction of the negative gradient:</p>
<p><span class="math display">\theta_{t+1} = \theta_t - \eta \nabla f(\theta_t)</span></p>
<p>where <span class="math inline">\eta &gt; 0</span> (<a href="https://en.wikipedia.org/wiki/Eta">eta</a>) is the <em>learning rate</em>, which defines the step length.</p>
<p>Gradient descent is inefficient, often zigzagging toward the optimum, but it’s the foundation for understanding more sophisticated methods.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced Gradient-Based Methods
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Beyond basic gradient descent, there are many sophisticated optimization algorithms:</p>
<p><strong>Classical improvements</strong>:</p>
<ul>
<li><p><strong><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate gradient methods</a></strong>: Choose search directions that are conjugate with respect to the Hessian matrix. For quadratic functions, this guarantees finding the exact minimum in at most <span class="math inline">n</span> steps (where <span class="math inline">n</span> is the number of dimensions). Unlike gradient descent which can take tiny steps in nearly orthogonal directions, conjugate gradient methods take larger, more efficient steps.</p></li>
<li><p><strong>Quasi-Newton methods</strong>: Approximate the Hessian matrix without computing second derivatives directly.</p>
<ul>
<li><strong><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS</a></strong> (Broyden-Fletcher-Goldfarb-Shanno): Builds up an approximation to the inverse Hessian using gradient information from previous steps</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a></strong> (Limited-memory BFGS): Stores only a few vectors instead of the full Hessian approximation, making it practical for high-dimensional problems</li>
</ul></li>
</ul>
<p><strong>Modern methods</strong>:</p>
<ul>
<li><p><strong>Momentum methods</strong> (heavy ball method): Add a fraction of the previous step to the current gradient step: <span class="math display">\theta_{t+1} = \theta_t - \eta \nabla f(\theta_t) + \beta(\theta_t - \theta_{t-1})</span> where <span class="math inline">\beta \in [0, 1)</span> is the momentum coefficient. This helps the optimizer “roll through” small local variations and reduces zigzagging in narrow valleys.</p></li>
<li><p><strong>Accelerated gradient methods</strong>: Achieve provably faster convergence rates. Nesterov’s accelerated gradient has convergence rate <span class="math inline">O(1/t^2)</span> compared to <span class="math inline">O(1/t)</span> for standard gradient descent on convex functions.</p></li>
</ul>
<p>These methods are especially important in deep learning where simple gradient descent would be too slow to navigate the complex, high-dimensional loss landscapes of neural networks.</p>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-methods" class="level3" data-number="5.6.6">
<h3 data-number="5.6.6" class="anchored" data-anchor-id="stochastic-gradient-methods"><span class="header-section-number">5.6.6</span> Stochastic Gradient Methods</h3>
<p>Many machine learning problems, especially in deep learning, involve optimizing functions of the form:</p>
<p><span class="math display">\min_\theta f(X, \theta) = \sum_{i=1}^n f(x_i, \theta)</span></p>
<p>When <span class="math inline">n</span> is large and <span class="math inline">\theta</span> is high-dimensional, evaluating the full gradient becomes computationally prohibitive.</p>
<p><strong>Stochastic gradient descent (SGD)</strong> approximates the gradient using a random subset of data:</p>
<p><span class="math display">\theta_{n+1} = \theta_n - \eta_n \nabla \sum_{i \in S_n} f(x_i, \theta_n)</span></p>
<p>where <span class="math inline">S_n \subset \{1, \ldots, n\}</span> is a randomly selected mini-batch.</p>
<p><strong>Convergence</strong>: SGD converges for well-behaved functions when the learning rate sequence satisfies: <span class="math display">\sum_{i=1}^\infty \eta_i = \infty, \quad \sum_{i=1}^\infty \eta_i^2 &lt; \infty</span></p>
<p>Note that constant learning rates don’t guarantee convergence to the exact optimum!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Popular SGD Variants
</div>
</div>
<div class="callout-body-container callout-body">
<p>The huge popularity of SGD has spawned many improved variants:</p>
<ul>
<li><strong>Adam</strong> (Adaptive Moment Estimation) <span class="citation" data-cites="kingma2014adam">(<a href="../references.html#ref-kingma2014adam" role="doc-biblioref">Kingma and Ba 2015</a>)</span>: Combines momentum with adaptive learning rates for each parameter</li>
<li><strong>AdaGrad</strong> (Adaptive Gradient): Adapts learning rate based on historical gradients - parameters with frequent updates get smaller learning rates</li>
<li><strong>RMSprop</strong> (Root Mean Square Propagation): Uses a running average of recent gradients to normalize the learning rate</li>
</ul>
</div>
</div>
</section>
<section id="which-optimizer-should-i-use" class="level3" data-number="5.6.7">
<h3 data-number="5.6.7" class="anchored" data-anchor-id="which-optimizer-should-i-use"><span class="header-section-number">5.6.7</span> Which Optimizer Should I Use?</h3>
<p><strong>For smaller datasets</strong> (&lt; 10K observations):</p>
<ul>
<li><strong>L-BFGS</strong> is usually the best first choice</li>
<li>Fast convergence, reliable for smooth problems</li>
<li>Standard choice in traditional statistical software</li>
</ul>
<p><strong>For large datasets or deep learning</strong>:</p>
<ul>
<li><strong>SGD</strong> and variants (especially <strong>Adam</strong>) are often the only practical choice</li>
<li>Require careful tuning of learning rates</li>
<li>Can handle millions and even billions of parameters</li>
</ul>
<p><strong>For black-box problems</strong> (no gradients available):</p>
<ul>
<li><strong><a href="https://en.wikipedia.org/wiki/CMA-ES">CMA-ES</a></strong>: Principled “population-based” method that can handle up to thousands of parameters (requiring many evaluations)</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian Optimization</a></strong>: Efficient for expensive functions with <span class="math inline">D &lt; 20</span> parameters</li>
<li><strong><a href="https://acerbilab.github.io/pybads/index.html">BADS</a></strong> (Bayesian Adaptive Direct Search): Combines Bayesian optimization with mesh adaptive direct search, good for noisy and mildly expensive functions with <span class="math inline">D &lt; 20</span></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced Topic: Constrained Optimization
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Often parameters have natural constraints:</p>
<ul>
<li>Standard deviations must be positive: <span class="math inline">\sigma &gt; 0</span></li>
<li>Probabilities must be in [0,1]: <span class="math inline">0 \leq p \leq 1</span></li>
<li>Correlation matrices must be positive definite</li>
</ul>
<p>How do we enforce these? There are typically three different approaches:</p>
<ol type="1">
<li><strong>Reparameterization</strong>: Optimize <span class="math inline">\phi = \log \sigma</span> and then use <span class="math inline">\sigma = e^\phi</span> to ensure positivity<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
<li><strong>Constrained optimization</strong>: Use algorithms like L-BFGS-B that allows you to specify parameter bounds</li>
<li><strong>Barrier methods</strong>: Add penalty terms that blow up at boundaries (can be unstable – not recommended)</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: 2D MLE with Numerical Optimization
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Now let’s apply the multidimensional optimization concepts to a real statistical problem. We’ll estimate both parameters of a Gamma distribution, which provides an excellent illustration of:</p>
<ul>
<li><strong>True 2D optimization</strong>: Unlike our 1D examples, we need to simultaneously find the shape parameter <span class="math inline">\alpha</span> and scale parameter <span class="math inline">\beta</span>.</li>
<li><strong>Constrained optimization in practice</strong>: Both parameters must be positive, so we’ll use L-BFGS-B with bounds.</li>
<li><strong>The importance of starting values</strong>: We’ll launch optimization from multiple starting points to see if they converge to the same solution. We choose the first starting point using the Method of Moments, while the others arbitrarily (could have been random).</li>
<li><strong>Visualizing the likelihood surface</strong>: We’ll create both 3D and contour plots to understand the optimization landscape.</li>
</ul>
<p>The Gamma distribution is particularly interesting because its two parameters interact in the likelihood function, creating a curved valley in the likelihood surface rather than a simple bowl.</p>
<div id="5a5a9359" class="cell" data-fig-height="10" data-fig-width="7" data-execution_count="7">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats, optimize</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from Gamma distribution</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>true_alpha <span class="op">=</span> <span class="fl">3.0</span>  <span class="co"># shape parameter</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>true_beta <span class="op">=</span> <span class="fl">2.0</span>   <span class="co"># scale parameter</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> stats.gamma.rvs(a<span class="op">=</span>true_alpha, scale<span class="op">=</span>true_beta, size<span class="op">=</span>n)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Define negative log-likelihood for Gamma(α, β)</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_likelihood(params, data):</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Negative log-likelihood for Gamma distribution"""</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> params</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> alpha <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">or</span> beta <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.inf  <span class="co"># Return infinity for invalid parameters</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(stats.gamma.logpdf(data, a<span class="op">=</span>alpha, scale<span class="op">=</span>beta))</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments starting values</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>sample_var <span class="op">=</span> np.var(data)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>mom_beta <span class="op">=</span> sample_var <span class="op">/</span> sample_mean</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>mom_alpha <span class="op">=</span> sample_mean <span class="op">/</span> mom_beta</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Try optimization from different starting points</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>starting_points <span class="op">=</span> [</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    [mom_alpha, mom_beta],  <span class="co"># MoM estimate</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">1.0</span>],             <span class="co"># Generic start</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">5.0</span>, <span class="fl">5.0</span>],             <span class="co"># Far from truth</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Store optimization paths</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>paths <span class="op">=</span> []</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, start <span class="kw">in</span> <span class="bu">enumerate</span>(starting_points):</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a list to store the path for this optimization</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> []</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Callback function to record each iteration</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> callback(xk):</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        path.append(xk.copy())</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> optimize.minimize(</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        fun<span class="op">=</span>neg_log_likelihood,</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>        x0<span class="op">=</span>start,</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>(data,),</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        bounds<span class="op">=</span>[(<span class="fl">0.01</span>, <span class="va">None</span>), (<span class="fl">0.01</span>, <span class="va">None</span>)],  <span class="co"># Enforce positivity</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        callback<span class="op">=</span>callback</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add starting point to path</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    full_path <span class="op">=</span> [np.array(start)] <span class="op">+</span> path</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    paths.append(np.array(full_path))</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    results.append(result)</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Start </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>start<span class="sc">}</span><span class="ss"> → MLE: [</span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">], "</span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"NLL: </span><span class="sc">{</span>result<span class="sc">.</span>fun<span class="sc">:.3f}</span><span class="ss">, Iterations: </span><span class="sc">{</span><span class="bu">len</span>(full_path)<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Best result</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>best_result <span class="op">=</span> <span class="bu">min</span>(results, key<span class="op">=</span><span class="kw">lambda</span> r: r.fun)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>mle_alpha, mle_beta <span class="op">=</span> best_result.x</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the likelihood surface</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>alpha_range <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="dv">6</span>, <span class="dv">50</span>)</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>beta_range <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="fl">5.2</span>, <span class="dv">50</span>)</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>Alpha, Beta <span class="op">=</span> np.meshgrid(alpha_range, beta_range)</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>NLL <span class="op">=</span> np.zeros_like(Alpha)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Alpha.shape[<span class="dv">0</span>]):</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(Alpha.shape[<span class="dv">1</span>]):</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>        NLL[i, j] <span class="op">=</span> neg_log_likelihood([Alpha[i, j], Beta[i, j]], data)</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">10</span>))</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a><span class="co"># 3D surface plot</span></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax1.plot_surface(Alpha, Beta, NLL, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, edgecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>ax1.contour(Alpha, Beta, NLL, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, offset<span class="op">=</span>NLL.<span class="bu">min</span>(), alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the different solutions</span></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>ax1.scatter([true_alpha], [true_beta], [neg_log_likelihood([true_alpha, true_beta], data)], </span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'True parameters'</span>)</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>ax1.scatter([mle_alpha], [mle_beta], [best_result.fun], </span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'α (shape)'</span>)</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'β (scale)'</span>)</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>ax1.set_zlabel(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Likelihood Surface for Gamma Distribution'</span>)</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>ax1.view_init(elev<span class="op">=</span><span class="dv">25</span>, azim<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D contour plot with optimization paths</span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax2.contour(Alpha, Beta, NLL, levels<span class="op">=</span><span class="dv">30</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>ax2.clabel(contour, inline<span class="op">=</span><span class="va">True</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot optimization paths</span></span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'orange'</span>]</span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (path, color) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(paths, colors)):</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the full optimization path</span></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[:, <span class="dv">0</span>], path[:, <span class="dv">1</span>], <span class="st">'-'</span>, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark start point</span></span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[<span class="dv">0</span>, <span class="dv">0</span>], path[<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'o'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'Start </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark intermediate points</span></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>], path[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">'.'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark end point</span></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>], path[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">'s'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>ax2.plot(true_alpha, true_beta, <span class="st">'k*'</span>, markersize<span class="op">=</span><span class="dv">12</span>, label<span class="op">=</span><span class="st">'True'</span>)</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>ax2.plot(mle_alpha, mle_beta, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'α (shape)'</span>)</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'β (scale)'</span>)</span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'L-BFGS Optimization Traces from Different Starting Points'</span>)</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">True parameters: α = </span><span class="sc">{</span>true_alpha<span class="sc">}</span><span class="ss">, β = </span><span class="sc">{</span>true_beta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE estimates:   α = </span><span class="sc">{</span>mle_alpha<span class="sc">:.3f}</span><span class="ss">, β = </span><span class="sc">{</span>mle_beta<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MoM estimates:   α = </span><span class="sc">{</span>mom_alpha<span class="sc">:.3f}</span><span class="ss">, β = </span><span class="sc">{</span>mom_beta<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Start 1: [3.868907522031454, 1.510860197180884] → MLE: [4.021, 1.454], NLL: 240.066, Iterations: 6
Start 2: [1.0, 1.0] → MLE: [4.021, 1.454], NLL: 240.066, Iterations: 14
Start 3: [5.0, 5.0] → MLE: [4.021, 1.454], NLL: 240.066, Iterations: 13

True parameters: α = 3.0, β = 2.0
MLE estimates:   α = 4.021, β = 1.454
MoM estimates:   α = 3.869, β = 1.511</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05-parametric-inference-I_files/figure-html/cell-8-output-2.png" width="661" height="948" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how all three starting points converged to the same MLE, demonstrating that this likelihood surface is well-behaved with a single global optimum. The actual L-BFGS traces (with intermediate points marked as dots) reveal interesting optimization behavior:</p>
<ul>
<li><strong>Blue path (MoM start)</strong>: Converges in very few iterations since it starts close to the optimum</li>
<li><strong>Green path (generic start)</strong>: Takes a curved path following the likelihood valley</li>
<li><strong>Orange path (far start)</strong>: Makes larger initial steps, then follows the contours more carefully as it approaches the optimum</li>
</ul>
<p>The 3D plot reveals the characteristic curved valley of the Gamma likelihood, explaining why the optimization paths curve rather than taking straight lines to the optimum.</p>
</div>
</div>
</div>
</section>
<section id="faq-common-issues-in-numerical-optimization" class="level3" data-number="5.6.8">
<h3 data-number="5.6.8" class="anchored" data-anchor-id="faq-common-issues-in-numerical-optimization"><span class="header-section-number">5.6.8</span> FAQ: Common Issues in Numerical Optimization</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Frequently Asked Questions
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Q: I used a minimizer, but I’m supposed to be doing <em>maximum</em> likelihood. What gives?</strong></p>
<p>A: A classic source of confusion! Maximizing a function <span class="math inline">f(x)</span> is equivalent to minimizing its negative <span class="math inline">-f(x)</span>. All standard optimization libraries are built as minimizers. Therefore, in practice, we always find the MLE by <strong>minimizing the negative log-likelihood function</strong>.</p>
<p><strong>Q: I tried different starting points and the optimizer gave different answers. Is it broken?</strong></p>
<p>A: No, this is expected behavior! The algorithms we use are <strong>local optimizers</strong>. They find the nearest valley (local minimum), but they have no guarantee of finding the deepest valley (global minimum). If your likelihood surface has multiple local optima, the result will depend on your starting point. This is why using a good initial value (like the Method of Moments estimate!) is so important.</p>
<p><strong>Q: How do I know if the algorithm has actually converged to the right answer?</strong></p>
<p>A: In general, you don’t know with 100% certainty. Optimizers use heuristics, like stopping when the change in the parameter values or the likelihood is very small. Good practices include:</p>
<ul>
<li><strong>Always try different starting points!</strong></li>
<li>Check the optimizer’s status messages</li>
<li>Verify that the gradient is near zero at the solution</li>
<li>Compare with simpler methods (like MoM) as a sanity check</li>
</ul>
</div>
</div>
<p>Let’s demonstrate the importance of starting values:</p>
<div id="8a9acb0b" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="8">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Multiple local optima in a mixture model</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll create a bimodal likelihood surface</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bimodal_nll(theta):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A negative log-likelihood with two local minima"""</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Artificial example with two valleys</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(<span class="fl">0.6</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>(theta<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>                   <span class="fl">0.4</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">*</span>(theta<span class="op">-</span><span class="dv">4</span>)<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.01</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Try optimization from different starting points</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>theta_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">200</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>nll_surface <span class="op">=</span> [bimodal_nll(t) <span class="cf">for</span> t <span class="kw">in</span> theta_range]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>starting_points <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>, <span class="fl">2.5</span>, <span class="dv">5</span>]</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'green'</span>, <span class="st">'blue'</span>]</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_range, nll_surface, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Objective function'</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> start, color <span class="kw">in</span> <span class="bu">zip</span>(starting_points, colors):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> optimize.minimize(bimodal_nll, x0<span class="op">=</span>[start], method<span class="op">=</span><span class="st">'L-BFGS-B'</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    results.append(result.x[<span class="dv">0</span>])</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    plt.scatter([start], [bimodal_nll(start)], color<span class="op">=</span>color, s<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="ss">f'Start: </span><span class="sc">{</span>start<span class="sc">:.1f}</span><span class="ss">'</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    plt.scatter([result.x[<span class="dv">0</span>]], [result.fun], color<span class="op">=</span>color, s<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="ss">f'End: </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw arrow from start to end</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    plt.annotate(<span class="st">''</span>, xy<span class="op">=</span>(result.x[<span class="dv">0</span>], result.fun), </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>                 xytext<span class="op">=</span>(start, bimodal_nll(start)),</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>                 arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span>color, lw<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Local Optimization: Different Starting Points → Different Solutions'</span>)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting points and their corresponding local optima:"</span>)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> start, end <span class="kw">in</span> <span class="bu">zip</span>(starting_points, results):</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Start: </span><span class="sc">{</span>start<span class="sc">:4.1f}</span><span class="ss"> → End: </span><span class="sc">{</span>end<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05-parametric-inference-I_files/figure-html/cell-9-output-1.png" width="679" height="372" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting points and their corresponding local optima:
  Start: -1.0 → End: 4.000
  Start:  2.5 → End: 1.000
  Start:  5.0 → End: 4.000</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">5.7</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">5.7.1</span> Key Concepts Review</h3>
<p>We’ve explored two fundamental approaches to finding estimators in parametric models:</p>
<p><strong>Parametric Models</strong>:</p>
<ul>
<li>Assume data comes from a specific family of distributions <span class="math inline">\mathfrak{F} = \{f(x; \theta): \theta \in \Theta\}</span></li>
<li>Our job reduces to estimating the finite-dimensional parameter <span class="math inline">\theta</span></li>
<li>Often have parameters of interest and nuisance parameters</li>
</ul>
<p><strong>Method of Moments (MoM)</strong>:</p>
<ul>
<li>Match theoretical moments <span class="math inline">\mathbb{E}(X^j)</span> with sample moments <span class="math inline">\frac{1}{n}\sum X_i^j</span></li>
<li>Simple to compute – just solve algebraic equations</li>
<li>Consistent and asymptotically normal</li>
<li>Not efficient, but excellent starting values for other methods</li>
</ul>
<p><strong>Maximum Likelihood Estimation (MLE)</strong>:</p>
<ul>
<li>Find parameters that make observed data most probable</li>
<li>Likelihood: <span class="math inline">\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta)</span></li>
<li>Often requires numerical optimization</li>
<li>The gold standard of parametric estimation</li>
</ul>
<p><strong>Numerical Optimization</strong>:</p>
<ul>
<li>Most MLEs require iterative algorithms</li>
<li>Gradient-based methods dominate (L-BFGS for small data, SGD/Adam for large)</li>
<li>Automatic differentiation (JAX) makes implementation easier</li>
<li>Local optima are a real concern – <strong>always try multiple starting points!</strong></li>
</ul>
</section>
<section id="the-big-picture" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="the-big-picture"><span class="header-section-number">5.7.2</span> The Big Picture</h3>
<p>This chapter revealed a fundamental connection: much of modern machine learning is secretly Maximum Likelihood Estimation:</p>
<ul>
<li><strong>Linear regression</strong>: MLE with normal errors</li>
<li><strong>Logistic regression</strong>: MLE for Bernoulli responses<br>
</li>
<li><strong>Neural networks</strong>: MLE with complex parametric models</li>
<li><strong>Deep learning</strong>: MLE with stochastic optimization</li>
</ul>
<p>The principles we’ve learned – likelihood, optimization, gradients – are the foundation of both classical statistics and modern ML.</p>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="5.7.3">
<h3 data-number="5.7.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">5.7.3</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><strong>Confusing likelihood with probability</strong>: The likelihood is NOT a probability distribution over parameters</li>
<li><strong>Forgetting the negative sign</strong>: Optimizers minimize, so use negative log-likelihood</li>
<li><strong>Assuming analytical solutions exist</strong>: Most real problems require numerical methods</li>
<li><strong>Trusting a single optimization run</strong>: Did I mention that you should <strong>always try multiple starting points</strong>?</li>
<li><strong>Ignoring convergence warnings</strong>: Check optimizer status and diagnostics</li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="5.7.4">
<h3 data-number="5.7.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">5.7.4</span> Chapter Connections</h3>
<ul>
<li><strong>Previous</strong>: Chapter 3 gave us convergence concepts and the framework for evaluating estimators. Now we know how to <em>find</em> estimators.</li>
<li><strong>Next</strong>: Chapter 6 will explore the <em>properties</em> of these estimators in detail – bias, variance, efficiency – and prove that MLEs have optimal properties.</li>
<li><strong>Bootstrap (Chapter 4)</strong>: Provides a computational alternative to analytical standard errors for our estimators</li>
<li><strong>Later chapters</strong>: These estimation principles extend to more complex models (regression, time series, etc.)</li>
</ul>
</section>
<section id="self-test-problems" class="level3" data-number="5.7.5">
<h3 data-number="5.7.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">5.7.5</span> Self-Test Problems</h3>
<ol type="1">
<li><p><strong>Method of Moments</strong>: For <span class="math inline">X_1, \ldots, X_n \sim \text{Uniform}(a, b)</span>, find the Method of Moments estimators for <span class="math inline">a</span> and <span class="math inline">b</span>.</p>
<p><em>Hint</em>: Use <span class="math inline">\mathbb{E}(X) = \frac{a+b}{2}</span> and <span class="math inline">\mathbb{V}(X) = \frac{(b-a)^2}{12}</span>.</p></li>
<li><p><strong>Maximum Likelihood</strong>: For <span class="math inline">X_1, \ldots, X_n \sim \text{Poisson}(\lambda)</span>, find the Maximum Likelihood Estimator for <span class="math inline">\lambda</span>.</p>
<p><em>Hint</em>: The Poisson PMF is <span class="math inline">f(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}</span>.</p></li>
<li><p><strong>Numerical Optimization</strong>: The log-likelihood for logistic regression with one covariate is: <span class="math display">\ell(y, x; \beta_0, \beta_1) = \sum_{i=1}^n \left[y_i(\beta_0 + \beta_1 x_i) - \log(1 + e^{\beta_0 + \beta_1 x_i})\right]</span></p>
<p>Explain why you cannot find the MLE for <span class="math inline">(\beta_0, \beta_1)</span> analytically and must use numerical optimization.</p></li>
<li><p><strong>Comparing Estimators</strong>: For <span class="math inline">X_1, \ldots, X_n \sim \text{Exponential}(\lambda)</span>:</p>
<ul>
<li>Find the MoM estimator for <span class="math inline">\lambda</span></li>
<li>Find the MLE for <span class="math inline">\lambda</span></li>
<li>Are they the same? Why or why not?</li>
</ul></li>
</ol>
</section>
<section id="python-and-r-reference" class="level3" data-number="5.7.6">
<h3 data-number="5.7.6" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">5.7.6</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255600-242-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255600-242-1" role="tab" aria-controls="tabset-1757255600-242-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255600-242-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255600-242-2" role="tab" aria-controls="tabset-1757255600-242-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255600-242-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255600-242-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats, optimize</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments estimation</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> method_of_moments_normal(data):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""MoM for Normal distribution"""</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    mu_hat <span class="op">=</span> np.mean(data)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    sigma2_hat <span class="op">=</span> np.mean((data <span class="op">-</span> mu_hat)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu_hat, np.sqrt(sigma2_hat)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Maximum Likelihood with scipy</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mle_poisson(data):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""MLE for Poisson distribution"""</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For Poisson, MLE is just the sample mean</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(data)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># General MLE with numerical optimization</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_likelihood(params, data, dist_logpdf):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generic negative log-likelihood using log-pdf for numerical stability"""</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(dist_logpdf(data, <span class="op">*</span>params))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Using scipy.optimize</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> stats.gamma.rvs(a<span class="op">=</span><span class="dv">3</span>, scale<span class="op">=</span><span class="dv">2</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> optimize.minimize(</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    fun<span class="op">=</span><span class="kw">lambda</span> params: neg_log_likelihood(params, data, </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                                         <span class="kw">lambda</span> x, a, b: stats.gamma.logpdf(x, a, scale<span class="op">=</span>b)),</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>[<span class="fl">1.0</span>, <span class="fl">1.0</span>],  <span class="co"># Initial guess</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    bounds<span class="op">=</span>[(<span class="fl">0.01</span>, <span class="va">None</span>), (<span class="fl">0.01</span>, <span class="va">None</span>)]  <span class="co"># Parameters must be positive</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Using JAX for automatic differentiation</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_lik_jax(params, data):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> params</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use JAX's scipy.stats</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>jnp.<span class="bu">sum</span>(jax.scipy.stats.gamma.logpdf(data, alpha, scale<span class="op">=</span>beta))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Get gradient automatically</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>grad_nll <span class="op">=</span> jax.grad(neg_log_lik_jax)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize with gradient</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>result_jax <span class="op">=</span> optimize.minimize(</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    fun<span class="op">=</span><span class="kw">lambda</span> p: <span class="bu">float</span>(neg_log_lik_jax(jnp.array(p), data)),</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">1.0</span>]),</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    jac<span class="op">=</span><span class="kw">lambda</span> p: np.array(grad_nll(jnp.array(p), data)),</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    bounds<span class="op">=</span>[(<span class="fl">0.01</span>, <span class="va">None</span>), (<span class="fl">0.01</span>, <span class="va">None</span>)]</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255600-242-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255600-242-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments estimation</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>method_of_moments_normal <span class="ot">&lt;-</span> <span class="cf">function</span>(data) {</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  mu_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(data)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  sigma2_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>((data <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="at">mu =</span> mu_hat, <span class="at">sigma =</span> <span class="fu">sqrt</span>(sigma2_hat))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Maximum Likelihood with built-in functions</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)  <span class="co"># For fitdistr</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">100</span>, <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">scale =</span> <span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: fitdistr estimates shape and rate (where rate = 1/scale)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>mle_fit <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(data, <span class="st">"gamma"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual MLE with optim</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(params, data) {</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> params[<span class="dv">1</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> params[<span class="dv">2</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (alpha <span class="sc">&lt;=</span> <span class="dv">0</span> <span class="sc">||</span> beta <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="cn">Inf</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dgamma</span>(data, <span class="at">shape =</span> alpha, <span class="at">scale =</span> beta, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),  <span class="co"># Initial values</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">fn =</span> neg_log_likelihood,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> data,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.01</span>)  <span class="co"># Lower bounds</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Using gradient information (numerical approximation)</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>result_with_grad <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">fn =</span> neg_log_likelihood,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>  <span class="at">gr =</span> <span class="cf">function</span>(p, data) <span class="fu">grad</span>(neg_log_likelihood, p, <span class="at">data =</span> data),</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> data,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># For more complex models, use specialized packages</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bbmle)  <span class="co"># For mle2 function</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>mle2_fit <span class="ot">&lt;-</span> <span class="fu">mle2</span>(</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>  <span class="at">minuslogl =</span> <span class="cf">function</span>(shape, scale) {</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dgamma</span>(data, <span class="at">shape =</span> shape, <span class="at">scale =</span> scale, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>  <span class="at">start =</span> <span class="fu">list</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="at">scale =</span> <span class="dv">1</span>),</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="at">shape =</span> <span class="fl">0.01</span>, <span class="at">scale =</span> <span class="fl">0.01</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="5.7.7">
<h3 data-number="5.7.7" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">5.7.7</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This table maps sections in these lecture notes to the corresponding sections in <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> (“All of Statistics” or AoS).</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding AoS Section(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction: Machine Learning As Statistical Estimation</strong></td>
<td style="text-align: left;">Expanded motivation from the slides and general context from AoS §9 introduction.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Parametric Models</strong></td>
<td style="text-align: left;">AoS §9 (Introduction), AoS §9.1 (Parameter of Interest). The Gamma distribution example is from AoS Example 9.2.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>The Method of Moments (MoM)</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Principle: Matching Moments</td>
<td style="text-align: left;">AoS §9.2 (Definition 9.3).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ MoM Examples (Bernoulli, Normal, Gamma)</td>
<td style="text-align: left;">AoS §9.2 (Examples 9.4, 9.5). The Gamma example is from AoS §9.14 (Exercise 1).</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Properties of MoM Estimator</td>
<td style="text-align: left;">AoS §9.2 (Theorem 9.6).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Maximum Likelihood Estimation (MLE)</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Principle and Likelihood Function</td>
<td style="text-align: left;">AoS §9.3 (Definitions 9.7, 9.8).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Finding the MLE Analytically</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳↳ General approach &amp; examples (Bernoulli, Normal)</td>
<td style="text-align: left;">AoS §9.3 (Remark 9.9, Examples 9.10, 9.11).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳↳ Harder case: Uniform(0, θ)</td>
<td style="text-align: left;">AoS §9.3 (Example 9.12).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>MLE Via Numerical Optimization</strong></td>
<td style="text-align: left;">Expanded material from the slides and AoS §9.13.4 (Appendix), with a modern ML focus.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Need for Numerical Methods &amp; Setup</td>
<td style="text-align: left;">General optimization concepts, related to AoS §9.13.4.</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Gradient-Based Optimization</td>
<td style="text-align: left;">Concepts related to Newton-Raphson from AoS §9.13.4, but expanded with modern methods (SGD, Adam, etc.).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Example: 2D MLE for Gamma distribution</td>
<td style="text-align: left;">Practical application. The Gamma MoM is from AoS §9.14 (Exercise 1), serving as a starting point.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Chapter Summary and Connections</strong></td>
<td style="text-align: left;">New summary material.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Self-Test Problems</td>
<td style="text-align: left;">Problems inspired by AoS §9.14 (Exercises 2a, 5).</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-materials" class="level3" data-number="5.7.8">
<h3 data-number="5.7.8" class="anchored" data-anchor-id="further-materials"><span class="header-section-number">5.7.8</span> Further Materials</h3>
<ul>
<li><strong>Bayesian Optimization</strong>: An interactive exploration on <a href="https://distill.pub/2020/bayesian-optimization/">Distill</a> (Agnihotri &amp; Batra; 2020)</li>
<li><strong>Classic reference</strong>: Casella &amp; Berger, “Statistical Inference”, Chapter 7</li>
<li><strong>Modern perspective</strong>: Efron &amp; Hastie, “Computer Age Statistical Inference”, Chapter 4</li>
</ul>
<hr>
<p><em>Remember: Parametric inference is about making assumptions (choosing a model) and then finding the best parameters within that model. The Method of Moments is simple but not optimal. Maximum Likelihood is the gold standard but often requires numerical optimization. Master these concepts – they’re the foundation of statistical modeling and machine learning!</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-kingma2014adam" class="csl-entry" role="listitem">
Kingma, Diederik P, and Jimmy Ba. 2015. <span>“Adam: A Method for Stochastic Optimization.”</span> In <em>International Conference on Learning Representations (ICLR)</em>.
</div>
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>In the next chapter, we’ll explore a special optimization algorithm for MLE called the EM (Expectation-Maximization) algorithm, which is particularly useful for models with latent variables or missing data.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Generally, evaluating a gradient in <span class="math inline">D</span> dimensions with finite differences will need at least <span class="math inline">D + 1</span> evaluations.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For numerical stability, it has become common to use the <a href="https://en.wikipedia.org/wiki/Softplus">softplus</a> instead of <span class="math inline">\exp</span> to ensure positivity.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/04-nonparametric-bootstrap.html" class="pagination-link" aria-label="Nonparametric Estimation and The Bootstrap">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/06-parametric-inference-II.html" class="pagination-link" aria-label="Parametric Inference II: Properties of Estimators">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Parametric Inference I: Finding Estimators</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define a parametric model** and explain its role in statistical inference, distinguishing between parameters of interest and nuisance parameters.</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Derive estimators using the Method of Moments (MoM)** by equating theoretical and sample moments.</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain the principle of Maximum Likelihood Estimation (MLE)** and formulate the likelihood and log-likelihood functions for a given model.</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Find the Maximum Likelihood Estimator (MLE) analytically** in simple cases by maximizing the (log-)likelihood.</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain when and why numerical optimization is necessary for MLE**, and apply standard optimization libraries to find estimators computationally.</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>This chapter covers parametric models and two fundamental approaches to finding estimators: the Method of Moments and Maximum Likelihood Estimation. The material is adapted from Chapter 9 of @wasserman2013all and supplemented with computational examples and optimization techniques relevant to modern data science applications.</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction: Machine Learning As Statistical Estimation</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>When fitting a machine learning model $f(x, \theta)$ for regression with inputs $x_i$ and targets $y_i$, $i = 1, \dots, n$, a common approach is to minimize the **mean squared error (MSE)**:</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>$$ \min_{\theta} \frac{1}{n} \sum_{i=1}^n \left( f(x_i, \theta) - y_i \right)^2. $$</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>The terms in this expression are similar to the log-probability of a normal distribution:</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>$$ \log \mathcal{N}(y_i;\; f(x_i, \theta), \sigma^2) = -\frac{1}{2}\log(2\pi \sigma^2) - \frac{1}{2 \sigma^2} \left( f(x_i, \theta) - y_i \right)^2. $$</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>This similarity is not a coincidence. When $\sigma$ is constant, maximizing the log-likelihood means maximizing $-\frac{1}{2\sigma^2} \sum_i (f(x_i, \theta) - y_i)^2$ plus a constant -- which is equivalent to minimizing the MSE.</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## The ML-Statistics Connection</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>When you minimize MSE in machine learning, you're performing maximum likelihood estimation under a Gaussian noise assumption! This connection reveals a fundamental truth: many machine learning algorithms are secretly (or openly) solving statistical estimation problems.</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>This chapter introduces the foundational principles of **parametric inference** -- the engine that powers both classical statistics and modern machine learning. We'll learn two primary methods for finding estimators for model parameters: the Method of Moments and the celebrated Maximum Likelihood Estimation.</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here's a reference table of key terms in this chapter:</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>| Parametric model | Parametrinen malli | Models with finite parameters |</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>| Parameter of interest | Kiinnostuksen parametri | The quantity we want to estimate |</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>| Nuisance parameter | Kiusaparametri | Parameters not of primary interest |</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>| Method of Moments (MoM) | Momenttimenetelmä | Estimation by matching moments |</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>| Moment | Momentti | Expected value of powers |</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>| Sample moment | Otosmomentti | Empirical average of powers |</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>| Maximum Likelihood Estimation (MLE) | Suurimman uskottavuuden menetelmä | Most common estimation method |</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>| Likelihood function | Uskottavuusfunktio | Joint density as function of parameters |</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>| Log-likelihood function | Log-uskottavuusfunktio | Logarithm of likelihood |</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>| Maximum Likelihood Estimator | SU-estimaattori | Parameter maximizing likelihood |</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>| Numerical optimization | Numeerinen optimointi | Computational methods for finding optima |</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>| Gradient | Gradientti | Vector of partial derivatives |</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>| Gradient descent | Gradienttimenetelmä | Iterative optimization algorithm |</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="fu">## Parametric Models</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>We introduced parametric models in Chapter 3 when discussing statistical inference frameworks. Recall that in the world of statistical inference, we often make assumptions about the structure of our data-generating process. A **parametric model** is one such assumption -- it postulates that our data comes from a distribution that can be fully characterized by a finite number of parameters.</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>Now that we're diving into estimation methods, let's revisit this concept with a focus on how we actually *find* these parameters.</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>A **parametric model** is a set of distributions </span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{F} = <span class="sc">\{</span> f(x; \theta) : \theta \in \Theta <span class="sc">\}</span>$$</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta = (\theta_1, \ldots, \theta_k)$ is the **parameter** (possibly vector-valued)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Theta \subseteq \mathbb{R}^k$ is the **parameter space** (the set of all possible parameter values)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f(x; \theta)$ is the density or distribution function indexed by $\theta$</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>The key insight: We assume the data-generating process belongs to a specific family of distributions, and our job is just to find the right parameter $\theta$ within that family.</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>In other words, the problem of inference reduces to estimating the parameter(s) $\theta$.</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## All Models Are Wrong, But Some Are Useful</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>Parametric models are widely used although the underlying models are usually not perfect. As the statistician George Box famously said -- in what is possibly the most repeated quote in statistics -- "*All models are wrong, but some are useful.*" </span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>When the model is good enough, parametric models can be very useful because they offer a simple representation for potentially complex phenomena. The art of statistical modeling is finding a model that is wrong in acceptable ways while still capturing the essential features of your data.</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>**Examples of Parametric Models**:</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Simple distributions**: </span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Bernoulli($p$): One parameter determining success probability</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Poisson($\lambda$): One parameter determining both mean and variance</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Normal($\mu, \sigma^2$): Two parameters for location and scale</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regression models**:</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Linear regression: $Y = \beta_0 + \beta_1 X + \epsilon$ where $\epsilon \sim N(0, \sigma^2)$</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Logistic regression: $P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}$</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Finite mixture models**:</span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Mixture of Gaussians: $f(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x; \mu_k, \sigma_k^2)$</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Parameters include mixing weights $\pi_k$ and component parameters $(\mu_k, \sigma_k)$</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Machine Learning models**:</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Deep Neural Networks: Often millions of parameters (weights and biases)</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Despite their complexity, these are still parametric models!</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a><span class="fu">## Remember: Parameters of Interest vs. Nuisance Parameters</span></span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>We introduced this distinction in Chapter 3, which it's crucial for estimation:</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameter of interest**: The specific quantity $T(\theta)$ we want to estimate</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Nuisance parameter**: Other parameters we must estimate but don't care about directly</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Mean Lifetime Estimation</span></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>Equipment lifetimes often follow a Gamma distribution. If $X_1, \ldots, X_n \sim \text{Gamma}(\alpha, \beta)$, then:</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>$$f(x; \alpha, \beta) = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta}, \quad x &gt; 0$$</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>If we want to estimate the mean lifetime:</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameter of interest**: $T(\alpha, \beta) = \mathbb{E}(X) = \alpha\beta$</span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Nuisance parameters**: The individual shape ($\alpha$) and scale ($\beta$) parameters</span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>Note that we must estimate both parameters, but only their product matters for our question.</span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Method of Moments (MoM)</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a>The Method of Moments is a simple estimation technique that does not yield optimal estimators, but provides easy-to-compute values that can serve as good starting points for more sophisticated methods.</span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Principle: Matching Moments</span></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>The Method of Moments is based on a straightforward idea: if our model is correct, then theoretical properties of the distribution (moments) should match their empirical counterparts in the data. It's like saying, "If this really is the right distribution, then the average I calculate from my model should match the average I see in my data."</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>For a model with parameter $\theta = (\theta_1, \ldots, \theta_k)$:</span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $j^{\text{th}}$ **theoretical moment** is: $\alpha_j(\theta) = \mathbb{E}_{\theta}(X^j)$</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $j^{\text{th}}$ **sample moment** is: $\hat{\alpha}_j = \frac{1}{n} \sum_{i=1}^n X_i^j$</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>The **Method of Moments estimator** $\hat{\theta}_n$ is the value of $\theta$ such that:</span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>\alpha_1(\hat{\theta}_n) &amp;= \hat{\alpha}_1 <span class="sc">\\</span></span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>\alpha_2(\hat{\theta}_n) &amp;= \hat{\alpha}_2 <span class="sc">\\</span></span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>&amp;\vdots <span class="sc">\\</span></span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>\alpha_k(\hat{\theta}_n) &amp;= \hat{\alpha}_k</span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>This gives us a system of $k$ equations with $k$ unknowns -- exactly what we need to solve for $k$ parameters!</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### MoM in Action: Examples</span></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Bernoulli Distribution</span></span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$, we have one parameter to estimate.</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Theoretical first moment: $\alpha_1(p) = \mathbb{E}_p(X) = p$</span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample first moment: $\hat{\alpha}_1 = \frac{1}{n}\sum_{i=1}^n X_i = \bar{X}_n$</span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>Equating them: $p = \bar{X}_n$</span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>Therefore, the MoM estimator is $\hat{p}_{\text{MoM}} = \bar{X}_n$ -- simply the proportion of successes!</span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Normal Distribution</span></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$, we have two parameters, so we need two equations.</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a>**First moment equation:**</span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha_1(\theta) = \mathbb{E}(X) = \mu$</span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\alpha}_1 = \bar{X}_n$</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Setting equal: $\mu = \bar{X}_n$</span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>**Second moment equation:**</span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha_2(\theta) = \mathbb{E}(X^2) = \mathbb{V}(X) + (\mathbb{E}(X))^2 = \sigma^2 + \mu^2$</span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\alpha}_2 = \frac{1}{n}\sum_{i=1}^n X_i^2$</span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Setting equal: $\sigma^2 + \mu^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$</span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a>Solving this system:</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\mu}_{\text{MoM}} = \bar{X}_n$</span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\sigma}^2_{\text{MoM}} = \frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}_n^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}_n)^2$</span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false collapse="true"}</span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Gamma Distribution</span></span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \text{Gamma}(\alpha, \beta)$, let's derive the MoM estimators.</span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a>The Gamma distribution has:</span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First moment: $\mathbb{E}(X) = \alpha\beta$</span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Second moment: $\mathbb{E}(X^2) = \mathbb{V}(X) + (\mathbb{E}(X))^2 = \alpha\beta^2 + (\alpha\beta)^2$</span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a>Setting up the moment equations:</span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a>$$\alpha\beta = \bar{X}_n$$</span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a>$$\alpha\beta^2 + (\alpha\beta)^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$$</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>From the first equation: $\alpha = \bar{X}_n / \beta$</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a>Substituting into the second equation:</span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>$$\frac{\bar{X}_n}{\beta} \cdot \beta^2 + \bar{X}_n^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$$</span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>Simplifying:</span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a>$$\bar{X}_n \beta + \bar{X}_n^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$$</span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a>Solving for $\beta$:</span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>$$\beta = \frac{\frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}_n^2}{\bar{X}_n} = \frac{\text{sample variance}}{\bar{X}_n}$$</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a>Therefore, the MoM estimators are:</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\beta}_{\text{MoM}} = \frac{\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2}{\bar{X}_n}$</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\alpha}_{\text{MoM}} = \frac{\bar{X}_n}{\hat{\beta}_{\text{MoM}}} = \frac{\bar{X}_n^2}{\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2}$</span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from a Gamma distribution</span></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>true_alpha, true_beta <span class="op">=</span> <span class="fl">3.0</span>, <span class="fl">2.0</span>  <span class="co"># True parameters</span></span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> stats.gamma.rvs(a<span class="op">=</span>true_alpha, scale<span class="op">=</span>true_beta, size<span class="op">=</span>n)</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments for Gamma distribution</span></span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a><span class="co"># For Gamma(α, β): E[X] = αβ, E[X²] = α(α+1)β²</span></span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a>sample_second_moment <span class="op">=</span> np.mean(data<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the system of equations</span></span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a><span class="co"># mean = α * β</span></span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a><span class="co"># second_moment = α * β² * (α + 1) = α * β² + α² * β²</span></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a><span class="co"># This gives us: β = (second_moment - mean²) / mean</span></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a>mom_beta <span class="op">=</span> (sample_second_moment <span class="op">-</span> sample_mean<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> sample_mean</span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a>mom_alpha <span class="op">=</span> sample_mean <span class="op">/</span> mom_beta</span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"True parameters: α = </span><span class="sc">{</span>true_alpha<span class="sc">}</span><span class="ss">, β = </span><span class="sc">{</span>true_beta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MoM estimates:   α = </span><span class="sc">{</span>mom_alpha<span class="sc">:.3f}</span><span class="ss">, β = </span><span class="sc">{</span>mom_beta<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fit</span></span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">20</span>, <span class="dv">200</span>)</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a>plt.hist(data, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Data'</span>)</span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.gamma.pdf(x, a<span class="op">=</span>true_alpha, scale<span class="op">=</span>true_beta), </span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>         <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True distribution'</span>)</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.gamma.pdf(x, a<span class="op">=</span>mom_alpha, scale<span class="op">=</span>mom_beta), </span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MoM fit'</span>)</span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Method of Moments Estimation for Gamma Distribution'</span>)</span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of Method of Moments Estimator</span></span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a>Under regular conditions, Method of Moments estimators have some desirable properties:</span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Properties of MoM Estimator"}</span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a>Let $\hat{\theta}_n$ denote the method of moments estimator. Under appropriate conditions on the model, the following statements hold:</span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Existence**: The estimate $\hat{\theta}_n$ exists with probability tending to 1.</span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Consistency**: The estimate is consistent:</span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a>   $$\hat{\theta}_n \xrightarrow{P} \theta$$</span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Asymptotic Normality**: The estimate is asymptotically Normal:</span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a>   $$\sqrt{n}(\hat{\theta}_n - \theta) \rightsquigarrow N(0, \Sigma)$$</span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a>   where</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a>   $$\Sigma = g \mathbb{E}_{\theta}(Y Y^T) g^T,$$</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a>   with $Y = (X, X^2, \ldots, X^k)^T$ and $g = (g_1, \ldots, g_k)$ where $g_j = \frac{\partial \alpha_j^{-1}(\theta)}{\partial \theta}$.</span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a>The last result can yield confidence intervals, but bootstrap is usually easier.</span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a><span class="fu">## How Good are MoM Estimators?</span></span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a>**Strengths:**</span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple to compute -- just solve algebraic equations!</span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Guaranteed existence and consistency under mild conditions</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Asymptotically normal, enabling confidence intervals</span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a>**Weaknesses:**</span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Not efficient**: Other estimators (like MLE) typically have smaller variance</span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**May give impossible values**: Can produce estimates outside the parameter space</span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Arbitrary**: Why use moments? Why not other features of the distribution?</span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a>**Primary Use Case**: MoM estimators are excellent starting values for more sophisticated methods like maximum likelihood estimation, which often require numerical optimization.</span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a><span class="fu">## Maximum Likelihood Estimation (MLE)</span></span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Principle: What Parameter Makes My Data Most Probable?</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>Maximum Likelihood Estimation is arguably the most important estimation method in statistics. While the Method of Moments asks "what parameters make the theoretical moments match the empirical ones?", MLE asks a more direct question: "what parameter values make my observed data most plausible?"</span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a>The elegance of MLE is that it reverses our usual probability thinking:</span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Probability**: Given parameters, what data would we expect to see?</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Likelihood**: Given data, which parameters make this data most probable?</span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Likelihood Function</span></span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>The mathematical object at the core of MLE is the likelihood function.</span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a>Let $X_1, \ldots, X_n$ be IID with PDF $f(x; \theta)$. </span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a>The **likelihood function** is:</span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta)$$</span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>For both mathematical and numerical convenience, in practice we often work with the **log-likelihood function**:</span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a>$$\ell_n(\theta) = \log \mathcal{L}_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)$$</span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a>The **Maximum Likelihood Estimator (MLE)** is:</span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_{\text{MLE}} = \arg\max_{\theta \in \Theta} \mathcal{L}_n(\theta) = \arg\max_{\theta \in \Theta} \ell_n(\theta)$$</span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a>Note that maximizing the likelihood is the same as maximizing the log-likelihood, since the logarithm is a strictly increasing (monotonic) function.</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a>Likelihood is a "what if" game. Imagine you have a coin and you flip it 10 times, getting 7 heads. You ask yourself: </span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a>"What if the probability of heads were 0.5? How likely would 7 heads in 10 flips be?"</span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a>"What if the probability were 0.6? Would that make my data more or less likely?"</span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a>"What if it were 0.7? 0.8?"</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a>For each possible value of the parameter (here, the probability of heads), you calculate how probable your exact observed data would be. The parameter value that maximizes this probability is your maximum likelihood estimate.</span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>The likelihood function is this "what if" calculation formalized -- it's the probability (or probability density) of your observed data, treated as a function of the unknown parameters.</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a>Why do we often prefer working with the log-likelihood?</span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a>$$\ell_n(\theta) = \log \mathcal{L}_n(\theta) = \sum_{i=1}^n \log f(x_i; \theta)$$</span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Differentiation**: For <span class="co">[</span><span class="ot">exponential family</span><span class="co">](https://en.wikipedia.org/wiki/Exponential_family)</span> distributions (normal, exponential, gamma, etc.), log derivatives eliminate exponentials. For example, for normal: $\frac{d}{d\mu} e^{-(x-\mu)^2/2}$ becomes just $(x-\mu)$ after taking logs</span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Numerical stability**: Products of small probabilities underflow; sums of logs don't</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Additive structure**: Log-likelihood is a sum over observations, making derivatives and optimization more tractable</span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>Let's visualize the likelihood function for a simple Bernoulli example:</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate coin flips: n=20, observed 12 successes</span></span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the log-likelihood function</span></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">200</span>)</span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> successes <span class="op">*</span> np.log(p_values) <span class="op">+</span> (n <span class="op">-</span> successes) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> p_values)</span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the MLE (exact analytical solution)</span></span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a>p_mle <span class="op">=</span> successes <span class="op">/</span> n  <span class="co"># Exact MLE for Bernoulli</span></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a>plt.plot(p_values, log_likelihood, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a>plt.axvline(p_mle, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'MLE: p̂ = </span><span class="sc">{</span>p_mle<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p'</span>)</span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-likelihood ℓₙ(p)'</span>)</span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Log-likelihood for Bernoulli with n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, S=</span><span class="sc">{</span>successes<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Maximum likelihood estimate: p̂ = </span><span class="sc">{</span>successes<span class="op">/</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This makes intuitive sense: it's simply the observed proportion of successes!"</span>)</span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a>Notice how the log-likelihood is maximized exactly at the observed proportion of successes. This is no coincidence -- the MLE often has an intuitive interpretation.</span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## Important: Likelihood function is NOT a probability distribution!</span></span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a>The likelihood $\mathcal{L}_n(\theta)$ is NOT a probability distribution over $\theta$. In general:</span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>$$\int_\Theta \mathcal{L}_n(\theta) d\theta \neq 1$$</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a>Why? The same mathematical expression $f(x; \theta)$ plays two different roles:</span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**As a PDF**: Fix $\theta$, vary $x$ → $\int f(x; \theta) dx = 1$ (proper probability distribution)</span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**As a likelihood**: Fix $x$ (observed data), vary $\theta$ → $\int \mathcal{L}_n(\theta) d\theta$ is usually not 1</span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>**Example**: Observe one coin flip with result $X = 1$ (heads) from Bernoulli($p$):</span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The likelihood is $\mathcal{L}(p) = p$ for $p \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$</span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\int_0^1 p \, dp = \frac{1}{2} \neq 1$</span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a>The likelihood tells us relative plausibility of parameter values, not their probabilities. This is why we need Bayesian methods if we want actual probability distributions over parameters!</span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### Finding the MLE Analytically</span></span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>For simple models, we can find the MLE by taking derivatives and setting them to zero. The general recipe:</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Write down the likelihood $\mathcal{L}_n(\theta)$</span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Take the logarithm to get $\ell_n(\theta)$ (this simplifies products to sums)</span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Take the derivative with respect to $\theta$</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Set equal to zero and solve</span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Verify it's a maximum (not a minimum or saddle point)</span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simplifying MLE Calculations</span></span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>When finding the MLE, multiplying the likelihood (or log-likelihood) by a positive constant or adding a constant doesn't change where the maximum occurs. This means we can often ignore:</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Normalization constants that don't depend on $\theta$ </span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Terms like $\frac{1}{(2\pi)^{n/2}}$ in the normal distribution</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Factorials in discrete distributions</span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>This greatly simplifies calculations -- focus only on terms involving $\theta$!</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>Let's work through some examples:</span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Bernoulli Distribution</span></span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$:</span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a>**Step 1**: The likelihood is</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(p) = \prod_{i=1}^n p^{X_i}(1-p)^{1-X_i} = p^S(1-p)^{n-S}$$</span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a>where $S = \sum_{i=1}^n X_i$ is the total number of successes.</span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a>**Step 2**: The log-likelihood is</span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a>$$\ell_n(p) = S \log p + (n-S) \log(1-p)$$</span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a>**Step 3**: Taking the derivative:</span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>$$\frac{d\ell_n}{dp} = \frac{S}{p} - \frac{n-S}{1-p}$$</span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>**Step 4**: Setting to zero and solving:</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a>$$\frac{S}{p} = \frac{n-S}{1-p} \implies S(1-p) = (n-S)p \implies S = np$$</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>Therefore, $\hat{p}_{\text{MLE}} = S/n = \bar{X}_n$.</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a>**Note**: This is the same as the Method of Moments estimator!</span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Normal Distribution</span></span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$:</span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>The log-likelihood (ignoring constants) is:</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a>$$\ell_n(\mu, \sigma) = -n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2$$</span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a>Taking partial derivatives and setting to zero:</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \ell_n}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \mu) = 0$$</span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \ell_n}{\partial \sigma} = -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^n (X_i - \mu)^2 = 0$$</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a>Solving these equations:</span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\mu}_{\text{MLE}} = \bar{X}_n$</span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X}_n)^2$</span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a>Again, these match the Method of Moments estimators!</span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: A Harder Case - Uniform(0, θ)</span></span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a>Not all MLEs can be found by differentiation! Consider $X_1, \ldots, X_n \sim \text{Uniform}(0, \theta)$.</span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a>The PDF is:</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>$$f(x; \theta) = \begin{cases}</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a>1/\theta &amp; \text{if } 0 \leq x \leq \theta <span class="sc">\\</span></span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{otherwise}</span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a>The likelihood is:</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(\theta) = \begin{cases}</span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>(1/\theta)^n &amp; \text{if } \theta \geq \max<span class="sc">\{</span>X_1, \ldots, X_n<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{if } \theta &lt; \max<span class="sc">\{</span>X_1, \ldots, X_n<span class="sc">\}</span></span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a>This function:</span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Is 0 when $\theta$ is less than the largest observation</span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decreases as $(1/\theta)^n$ for larger $\theta$</span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a>Therefore, the likelihood is maximized at the boundary: $\hat{\theta}_{\text{MLE}} = X_{(n)} = \max<span class="sc">\{</span>X_1, \ldots, X_n<span class="sc">\}</span>$.</span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a>This example shows that not all optimization problems are solved by calculus -- sometimes we need to think more carefully about the function's behavior!</span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>Let's visualize this unusual likelihood function:</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a><span class="co"># Uniform(0, θ) MLE visualization</span></span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>true_theta <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, true_theta, n)</span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> np.<span class="bu">max</span>(data)</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a><span class="co"># Create theta values</span></span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a>theta_values <span class="op">=</span> np.linspace(<span class="fl">0.1</span>, <span class="fl">3.0</span>, <span class="dv">300</span>)</span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> np.zeros_like(theta_values)</span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate likelihood (proportional to)</span></span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, theta <span class="kw">in</span> <span class="bu">enumerate</span>(theta_values):</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> theta <span class="op">&gt;=</span> x_max:</span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a>        likelihood[i] <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>theta)<span class="op">**</span>n</span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a>        likelihood[i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot data points</span></span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>plt.scatter(data, np.zeros_like(data), color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">50</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">'Data'</span>)</span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a>plt.axvline(x_max, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'max(X) = </span><span class="sc">{</span>x_max<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">3.0</span>)</span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>)</span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Observed Data'</span>)</span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, likelihood, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a>plt.axvline(x_max, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'MLE = </span><span class="sc">{</span>x_max<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Likelihood (proportional to)'</span>)</span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Likelihood Function'</span>)</span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a><span class="fu">## MLE Via Numerical Optimization</span></span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>Finding the maximum of the log-likelihood by taking a derivative and setting it to zero is clean and satisfying, but it only works for the simplest models. For most real-world problems, the log-likelihood function is a complex, high-dimensional surface, and we cannot find the peak analytically.</span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>Instead, we must turn to **numerical optimization**. The core idea of most optimization algorithms is simple: we start with an initial guess for the parameters, $\theta_0$, and then iteratively take steps "uphill" on the likelihood surface until we can no longer find a higher point.</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a>This section explores the concepts and tools behind this fundamental process.^<span class="co">[</span><span class="ot">In the next chapter, we'll explore a special optimization algorithm for MLE called the EM (Expectation-Maximization) algorithm, which is particularly useful for models with latent variables or missing data.</span><span class="co">]</span></span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Need for Numerical Methods</span></span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a>For many important models, we cannot solve for the MLE analytically:</span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Logistic regression**: No closed-form solution for the regression coefficients</span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mixture models**: Complex likelihood with multiple local maxima  </span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Most complex ML models**: Neural networks, random forests, etc.</span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a>We must use iterative numerical optimization algorithms to find the maximum of the likelihood function (or minimum of the negative log-likelihood).</span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Optimization Setup for MLE</span></span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a>Finding the MLE requires solving an optimization problem to find the parameters $\theta$ that maximize the likelihood function $\mathcal{L}_n(\theta)$ or equivalently the log-likelihood function $\ell_n(\theta)$.</span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a>Since optimization methods in software libraries are conventionally written to perform **minimization**, our practical goal becomes:</span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>$$ \hat{\theta}_{\text{MLE}} = \arg\min_{\theta} \left<span class="co">[</span><span class="ot">-\ell_n(\theta)\right</span><span class="co">]</span> $$</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a>This is a crucial point: we minimize the **negative** log-likelihood. Forgetting this minus sign is one of the most common programming errors in statistical computing!</span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a>Throughout this section, we'll follow the standard convention and present algorithms for minimization. We'll use examples from Python's <span class="in">`scipy.optimize`</span>, but other languages and libraries (R's <span class="in">`optim`</span>, Julia's <span class="in">`Optim.jl`</span>, etc.) offer similar algorithms with comparable interfaces.</span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a>When in optimization we say that a problem is $D$-dimensional, we refer to the number of variables being optimized over -- here, the number of elements of $\theta$.</span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a><span class="fu">### Numerical Optimization in 1D</span></span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a>To understand how numerical optimization works, it's helpful to start with the simplest case: finding the minimum of a function with a single parameter, $f(\theta)$.</span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a>Let's take a simple example function to minimize:</span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a>$$ f(\theta) = \frac{\theta^2}{10} + \frac{1}{\theta}; \quad \theta &gt; 0 $$</span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a>whose global minimum is at $\theta^\star \approx 1.71$.</span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-644"><a href="#cb12-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-645"><a href="#cb12-645" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb12-646"><a href="#cb12-646" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>theta</span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a>theta_vals <span class="op">=</span> np.linspace(<span class="fl">0.2</span>, <span class="dv">5</span>, <span class="dv">200</span>)</span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a>f_vals <span class="op">=</span> f(theta_vals)</span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a>min_theta_analytical <span class="op">=</span> (<span class="dv">5</span>)<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a>min_f <span class="op">=</span> f(min_theta_analytical)</span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_vals, f_vals, label<span class="op">=</span><span class="vs">r'$f(\theta) = \frac{\theta^2}</span><span class="sc">{10}</span><span class="vs"> + \frac</span><span class="sc">{1}</span><span class="vs">{\theta}$'</span>)</span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a>plt.axvline(min_theta_analytical, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Minimum at θ ≈ </span><span class="sc">{</span>min_theta_analytical<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a>plt.axhline(min_f, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb12-658"><a href="#cb12-658" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(θ)'</span>)</span>
<span id="cb12-659"><a href="#cb12-659" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'A Simple 1D Optimization Problem'</span>)</span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a>How can a computer find the minimum of this function? There are two main families of approaches.</span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1D Optimization without Derivatives</span></span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-670"><a href="#cb12-670" aria-hidden="true" tabindex="-1"></a>The simplest algorithms work by bracketing the minimum. If you can find three points $(a, c, b)$ such that $a &lt; c &lt; b$ and $f(c) &lt; f(a)$ and $f(c) &lt; f(b)$, you know a minimum lies somewhere in the interval $(a, b)$.</span>
<span id="cb12-671"><a href="#cb12-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">**Golden-section search**</span><span class="co">](https://en.wikipedia.org/wiki/Golden-section_search)</span>: A simple but robust method that is analogous to binary search for finding a root. It progressively narrows the bracket until the desired precision is reached.</span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">**Brent's method**</span><span class="co">](https://en.wikipedia.org/wiki/Brent%27s_method)</span>: A more sophisticated and generally preferred method. It combines the guaranteed (but slow) progress of golden-section search with the potentially faster convergence of fitting a parabola to the three points and jumping to the minimum of the parabola.</span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a>These methods are useful when the function's derivative is unavailable or unreliable.</span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1D Optimization with Derivatives: Newton's Method</span></span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a>If we can compute derivatives, we can often find the minimum much faster. The most famous derivative-based method is <span class="co">[</span><span class="ot">**Newton's method**</span><span class="co">](https://en.wikipedia.org/wiki/Newton%27s_method)</span> (or Newton-Raphson).</span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-681"><a href="#cb12-681" aria-hidden="true" tabindex="-1"></a>The idea is to iteratively approximate the function with a quadratic and jump to the minimum of that quadratic. A second-order Taylor expansion of $f(\theta)$ around a point $\theta_t$ is:</span>
<span id="cb12-682"><a href="#cb12-682" aria-hidden="true" tabindex="-1"></a>$$ f(\theta) \approx f(\theta_t) + f'(\theta_t) (\theta - \theta_t) + \frac{1}{2} f''(\theta_t) (\theta - \theta_t)^2. $$</span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a>To find the minimum of this quadratic approximation, we take its derivative with respect to $\theta$ and set it to zero:</span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a>$$ \frac{d}{d\theta} (\text{approx}) = f'(\theta_t) + f''(\theta_t)(\theta - \theta_t) = 0 $$</span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a>Solving for $\theta$ gives us the next point in our iteration, $\theta_{t+1}$:</span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>$$ \theta_{t+1} = \theta_t - \frac{f'(\theta_t)}{f''(\theta_t)}. $$</span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a>This simple update rule is the core of Newton's method. When close to a well-behaved minimum, it converges very rapidly (quadratically). However, it can be unstable if the function is not "nice" or if the starting point is far from the minimum.</span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb12-693"><a href="#cb12-693" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: 1D Optimization in `scipy`</span></span>
<span id="cb12-694"><a href="#cb12-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a>Let's find the minimum of our example function using <span class="in">`scipy.optimize`</span>. We'll use Brent's method, which only requires the function itself and a bracket.</span>
<span id="cb12-696"><a href="#cb12-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize</span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-704"><a href="#cb12-704" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb12-705"><a href="#cb12-705" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> theta<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>theta</span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-707"><a href="#cb12-707" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the minimum using Brent's method</span></span>
<span id="cb12-708"><a href="#cb12-708" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to provide a bracket (a, b) or (a, c, b)</span></span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a><span class="co"># where the minimum is expected to lie.</span></span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's use (0.1, 10.0)</span></span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> scipy.optimize.brent(f, brack<span class="op">=</span>(<span class="fl">0.1</span>, <span class="fl">10.0</span>))</span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The minimum found by Brent's method is at θ = </span><span class="sc">{</span>result<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The value of the function at the minimum is f(θ) = </span><span class="sc">{</span>f(result)<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a><span class="co"># Analytical minimum is at (5)^(1/3)</span></span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a>analytical_min <span class="op">=</span> <span class="dv">5</span><span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The analytical minimum is at θ = </span><span class="sc">{</span>analytical_min<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a>The output shows that the numerical method finds the correct minimum with high precision.</span>
<span id="cb12-722"><a href="#cb12-722" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-723"><a href="#cb12-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-724"><a href="#cb12-724" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Moving to Multiple Dimensions</span></span>
<span id="cb12-725"><a href="#cb12-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-726"><a href="#cb12-726" aria-hidden="true" tabindex="-1"></a>The concepts from 1D optimization extend to the multi-dimensional case, but with added complexity.</span>
<span id="cb12-727"><a href="#cb12-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-728"><a href="#cb12-728" aria-hidden="true" tabindex="-1"></a>**Derivative-free methods:**</span>
<span id="cb12-729"><a href="#cb12-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-730"><a href="#cb12-730" aria-hidden="true" tabindex="-1"></a>Generalizing derivative-free methods to multiple dimensions, i.e., optimizing $f(\theta_1, \ldots, \theta_n)$ with respect to $\theta_1, \ldots, \theta_n$ is hard!</span>
<span id="cb12-731"><a href="#cb12-731" aria-hidden="true" tabindex="-1"></a>As the dimensionality increases, the number of possible directions that need to be searched for the optimum grows very rapidly.</span>
<span id="cb12-732"><a href="#cb12-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-733"><a href="#cb12-733" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classical approaches**: Very old algorithms like <span class="co">[</span><span class="ot">direct search</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Pattern_search_(optimization)) methods are mostly inefficient and struggle as dimensionality grows. Searching in many directions at once becomes prohibitively expensive.</span>
<span id="cb12-734"><a href="#cb12-734" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Modern approaches**: Recent methods can be more effective in specific scenarios:</span>
<span id="cb12-735"><a href="#cb12-735" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**[Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)**: Works well in low dimensions (typically &lt; 20) and can be very sample-efficient when function evaluations are expensive</span>
<span id="cb12-736"><a href="#cb12-736" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**[CMA-ES](https://en.wikipedia.org/wiki/CMA-ES)** (Covariance Matrix Adaptation Evolution Strategy): If you can afford many function evaluations and don't have gradients, this method can work in surprisingly high dimensions -- up to tens of thousands of parameters</span>
<span id="cb12-737"><a href="#cb12-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These methods are mainly used when gradients are unavailable or unreliable (e.g., noisy simulations, black-box models)</span>
<span id="cb12-738"><a href="#cb12-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-739"><a href="#cb12-739" aria-hidden="true" tabindex="-1"></a>**Derivative-based methods:**</span>
<span id="cb12-740"><a href="#cb12-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-741"><a href="#cb12-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The standard choice**: When gradients are available and the function is relatively inexpensive to evaluate, gradient-based methods dominate</span>
<span id="cb12-742"><a href="#cb12-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scalability**: These methods can handle problems with millions or even billions of parameters (think deep neural networks)</span>
<span id="cb12-743"><a href="#cb12-743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key transformation**: The first derivative (slope) becomes the **gradient** ($\nabla f$), and the second derivative (curvature) becomes the **Hessian matrix** ($H$)</span>
<span id="cb12-744"><a href="#cb12-744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Why they work**: Gradient information provides a principled direction for improvement at each step, making the search much more efficient than blind exploration</span>
<span id="cb12-745"><a href="#cb12-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-746"><a href="#cb12-746" aria-hidden="true" tabindex="-1"></a>You can find visualizations of several optimization algorithms at work <span class="co">[</span><span class="ot">here</span><span class="co">](https://github.com/lacerbi/optimviz)</span>.</span>
<span id="cb12-747"><a href="#cb12-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-748"><a href="#cb12-748" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Gradient: The Key to Multidimensional Optimization</span></span>
<span id="cb12-749"><a href="#cb12-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-750"><a href="#cb12-750" aria-hidden="true" tabindex="-1"></a>The main tool for optimization of multidimensional functions </span>
<span id="cb12-751"><a href="#cb12-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-752"><a href="#cb12-752" aria-hidden="true" tabindex="-1"></a>$$f: \mathbb{R}^n \to \mathbb{R}$$ </span>
<span id="cb12-753"><a href="#cb12-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-754"><a href="#cb12-754" aria-hidden="true" tabindex="-1"></a>is the **gradient**:</span>
<span id="cb12-755"><a href="#cb12-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-756"><a href="#cb12-756" aria-hidden="true" tabindex="-1"></a>$$\nabla f(\theta) = \left(\frac{\partial f}{\partial \theta_1}, \ldots, \frac{\partial f}{\partial \theta_n}\right)^T$$</span>
<span id="cb12-757"><a href="#cb12-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-758"><a href="#cb12-758" aria-hidden="true" tabindex="-1"></a>Here $\frac{\partial f}{\partial \theta_i}$ is the **partial derivative** of $f$ with respect to $\theta_i$. It can be evaluated by computing the derivative w.r.t. $\theta_i$ while keeping the other variables constant.</span>
<span id="cb12-759"><a href="#cb12-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-760"><a href="#cb12-760" aria-hidden="true" tabindex="-1"></a>**Geometric interpretation**: The gradient points in the direction where the function values grow fastest. Its magnitude measures the rate of change in that direction.</span>
<span id="cb12-761"><a href="#cb12-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-762"><a href="#cb12-762" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-763"><a href="#cb12-763" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuition: The Gradient as a Compass</span></span>
<span id="cb12-764"><a href="#cb12-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-765"><a href="#cb12-765" aria-hidden="true" tabindex="-1"></a>Think of finding the MLE as hiking blindfolded on a hilly terrain (the negative log-likelihood surface). At each point, the gradient tells you which direction is steepest uphill. Since we want to minimize, we go in the opposite direction -- downhill.</span>
<span id="cb12-766"><a href="#cb12-766" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-767"><a href="#cb12-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-768"><a href="#cb12-768" aria-hidden="true" tabindex="-1"></a>Let's visualize this concept:</span>
<span id="cb12-769"><a href="#cb12-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-772"><a href="#cb12-772" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-773"><a href="#cb12-773" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb12-774"><a href="#cb12-774" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 10</span></span>
<span id="cb12-775"><a href="#cb12-775" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-776"><a href="#cb12-776" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-777"><a href="#cb12-777" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb12-778"><a href="#cb12-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-779"><a href="#cb12-779" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2D function with interesting topology (negative log-likelihood surface)</span></span>
<span id="cb12-780"><a href="#cb12-780" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb12-781"><a href="#cb12-781" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example negative log-likelihood with two local minima"""</span></span>
<span id="cb12-782"><a href="#cb12-782" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> np.exp(<span class="op">-</span>(x<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> y<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> np.exp(<span class="op">-</span>(x<span class="op">+</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> y<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>(x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-783"><a href="#cb12-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-784"><a href="#cb12-784" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb12-785"><a href="#cb12-785" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">50</span>)</span>
<span id="cb12-786"><a href="#cb12-786" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">40</span>)</span>
<span id="cb12-787"><a href="#cb12-787" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb12-788"><a href="#cb12-788" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> f(X, Y)</span>
<span id="cb12-789"><a href="#cb12-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-790"><a href="#cb12-790" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure with two subplots</span></span>
<span id="cb12-791"><a href="#cb12-791" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">10</span>))</span>
<span id="cb12-792"><a href="#cb12-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-793"><a href="#cb12-793" aria-hidden="true" tabindex="-1"></a><span class="co"># First subplot: 3D surface</span></span>
<span id="cb12-794"><a href="#cb12-794" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb12-795"><a href="#cb12-795" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax1.plot_surface(X, Y, Z, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, edgecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb12-796"><a href="#cb12-796" aria-hidden="true" tabindex="-1"></a>ax1.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, offset<span class="op">=</span>Z.<span class="bu">min</span>(), alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb12-797"><a href="#cb12-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-798"><a href="#cb12-798" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the global minimum</span></span>
<span id="cb12-799"><a href="#cb12-799" aria-hidden="true" tabindex="-1"></a>ax1.scatter([<span class="dv">1</span>], [<span class="dv">0</span>], [f(<span class="dv">1</span>, <span class="dv">0</span>)], color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'Global minimum'</span>)</span>
<span id="cb12-800"><a href="#cb12-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-801"><a href="#cb12-801" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'θ₁'</span>)</span>
<span id="cb12-802"><a href="#cb12-802" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'θ₂'</span>)</span>
<span id="cb12-803"><a href="#cb12-803" aria-hidden="true" tabindex="-1"></a>ax1.set_zlabel(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb12-804"><a href="#cb12-804" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'3D Optimization Landscape'</span>)</span>
<span id="cb12-805"><a href="#cb12-805" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust viewing angle to swap visual perspective</span></span>
<span id="cb12-806"><a href="#cb12-806" aria-hidden="true" tabindex="-1"></a>ax1.view_init(elev<span class="op">=</span><span class="dv">30</span>, azim<span class="op">=-</span><span class="dv">45</span>)</span>
<span id="cb12-807"><a href="#cb12-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-808"><a href="#cb12-808" aria-hidden="true" tabindex="-1"></a><span class="co"># Second subplot: 2D contour with gradient field</span></span>
<span id="cb12-809"><a href="#cb12-809" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb12-810"><a href="#cb12-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-811"><a href="#cb12-811" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute gradient (numerically)</span></span>
<span id="cb12-812"><a href="#cb12-812" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb12-813"><a href="#cb12-813" aria-hidden="true" tabindex="-1"></a>dy <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb12-814"><a href="#cb12-814" aria-hidden="true" tabindex="-1"></a>grad_x <span class="op">=</span> (f(X <span class="op">+</span> dx, Y) <span class="op">-</span> f(X <span class="op">-</span> dx, Y)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dx)</span>
<span id="cb12-815"><a href="#cb12-815" aria-hidden="true" tabindex="-1"></a>grad_y <span class="op">=</span> (f(X, Y <span class="op">+</span> dy) <span class="op">-</span> f(X, Y <span class="op">-</span> dy)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dy)</span>
<span id="cb12-816"><a href="#cb12-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-817"><a href="#cb12-817" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize gradient vectors for better visualization</span></span>
<span id="cb12-818"><a href="#cb12-818" aria-hidden="true" tabindex="-1"></a>grad_norm <span class="op">=</span> np.sqrt(grad_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> grad_y<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-819"><a href="#cb12-819" aria-hidden="true" tabindex="-1"></a>grad_x_norm <span class="op">=</span> <span class="op">-</span>grad_x <span class="op">/</span> (grad_norm <span class="op">+</span> <span class="fl">1e-10</span>)  <span class="co"># Negative for descent</span></span>
<span id="cb12-820"><a href="#cb12-820" aria-hidden="true" tabindex="-1"></a>grad_y_norm <span class="op">=</span> <span class="op">-</span>grad_y <span class="op">/</span> (grad_norm <span class="op">+</span> <span class="fl">1e-10</span>)</span>
<span id="cb12-821"><a href="#cb12-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-822"><a href="#cb12-822" aria-hidden="true" tabindex="-1"></a><span class="co"># Contour plot</span></span>
<span id="cb12-823"><a href="#cb12-823" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax2.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb12-824"><a href="#cb12-824" aria-hidden="true" tabindex="-1"></a>ax2.clabel(contour, inline<span class="op">=</span><span class="va">True</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb12-825"><a href="#cb12-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-826"><a href="#cb12-826" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient vectors</span></span>
<span id="cb12-827"><a href="#cb12-827" aria-hidden="true" tabindex="-1"></a>skip <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Show every 3rd arrow for clarity</span></span>
<span id="cb12-828"><a href="#cb12-828" aria-hidden="true" tabindex="-1"></a>ax2.quiver(X[::skip, ::skip], Y[::skip, ::skip], </span>
<span id="cb12-829"><a href="#cb12-829" aria-hidden="true" tabindex="-1"></a>           grad_x_norm[::skip, ::skip], grad_y_norm[::skip, ::skip], </span>
<span id="cb12-830"><a href="#cb12-830" aria-hidden="true" tabindex="-1"></a>           scale<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, width<span class="op">=</span><span class="fl">0.003</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb12-831"><a href="#cb12-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-832"><a href="#cb12-832" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark global minimum</span></span>
<span id="cb12-833"><a href="#cb12-833" aria-hidden="true" tabindex="-1"></a>ax2.plot([<span class="dv">1</span>], [<span class="dv">0</span>], <span class="st">'k*'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'Global minimum'</span>)</span>
<span id="cb12-834"><a href="#cb12-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-835"><a href="#cb12-835" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'θ₁'</span>)</span>
<span id="cb12-836"><a href="#cb12-836" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'θ₂'</span>)</span>
<span id="cb12-837"><a href="#cb12-837" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Gradient Field on Negative Log-Likelihood Surface'</span>)</span>
<span id="cb12-838"><a href="#cb12-838" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb12-839"><a href="#cb12-839" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-840"><a href="#cb12-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-841"><a href="#cb12-841" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-842"><a href="#cb12-842" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-843"><a href="#cb12-843" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-844"><a href="#cb12-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-845"><a href="#cb12-845" aria-hidden="true" tabindex="-1"></a>These visualizations show:</span>
<span id="cb12-846"><a href="#cb12-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-847"><a href="#cb12-847" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Top (3D)**: The optimization landscape as a surface, showing the "hills and valleys" that optimization algorithms must navigate</span>
<span id="cb12-848"><a href="#cb12-848" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bottom (2D)**: The same landscape from above with:</span>
<span id="cb12-849"><a href="#cb12-849" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Contour lines** showing level sets of the negative log-likelihood</span>
<span id="cb12-850"><a href="#cb12-850" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Red arrows** pointing in the direction of steepest descent (negative gradient)</span>
<span id="cb12-851"><a href="#cb12-851" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Black star** marking the global minimum at θ₁ = 1, θ₂ = 0</span>
<span id="cb12-852"><a href="#cb12-852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note how the gradient vectors always point toward lower values, guiding optimization algorithms downhill</span>
<span id="cb12-853"><a href="#cb12-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-854"><a href="#cb12-854" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluating the Gradient</span></span>
<span id="cb12-855"><a href="#cb12-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-856"><a href="#cb12-856" aria-hidden="true" tabindex="-1"></a>The first practical challenge in using gradients for optimization is how to evaluate the gradient. There are three main approaches:</span>
<span id="cb12-857"><a href="#cb12-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-858"><a href="#cb12-858" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Analytically**: Derive by hand and implement -- fast but error-prone</span>
<span id="cb12-859"><a href="#cb12-859" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="co">[</span><span class="ot">**Finite differences**</span><span class="co">](https://en.wikipedia.org/wiki/Finite_difference)</span>: Approximate numerically by evaluating nearby points -- easy but expensive^<span class="co">[</span><span class="ot">Generally, evaluating a gradient in $D$ dimensions with finite differences will need at least $D + 1$ evaluations.</span><span class="co">]</span> and less precise</span>
<span id="cb12-860"><a href="#cb12-860" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Automatic differentiation (autodiff)**: Best of both worlds -- exact, fast and automatic</span>
<span id="cb12-861"><a href="#cb12-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-862"><a href="#cb12-862" aria-hidden="true" tabindex="-1"></a>Modern frameworks like <span class="co">[</span><span class="ot">PyTorch</span><span class="co">](https://pytorch.org/)</span> and <span class="co">[</span><span class="ot">JAX</span><span class="co">](https://docs.jax.dev/en/latest/index.html)</span> make automatic differentiation the preferred choice. However, some older languages such as R still lack full autodiff support.</span>
<span id="cb12-863"><a href="#cb12-863" aria-hidden="true" tabindex="-1"></a>Gradient computation usually uses **backward-mode autodiff**, which is suitable for computing derivatives of real-valued functions $f: \mathbb{R}^n \to \mathbb{R}$.</span>
<span id="cb12-864"><a href="#cb12-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-865"><a href="#cb12-865" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-866"><a href="#cb12-866" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Gradients in Python with JAX</span></span>
<span id="cb12-867"><a href="#cb12-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-868"><a href="#cb12-868" aria-hidden="true" tabindex="-1"></a>Here's how to compute gradients automatically using JAX:</span>
<span id="cb12-869"><a href="#cb12-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-870"><a href="#cb12-870" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-871"><a href="#cb12-871" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb12-872"><a href="#cb12-872" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb12-873"><a href="#cb12-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-874"><a href="#cb12-874" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb12-875"><a href="#cb12-875" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>theta</span>
<span id="cb12-876"><a href="#cb12-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-877"><a href="#cb12-877" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> jax.grad(f)</span>
<span id="cb12-878"><a href="#cb12-878" aria-hidden="true" tabindex="-1"></a>g(<span class="fl">1.5</span>)  <span class="co"># Evaluating at a scalar value</span></span>
<span id="cb12-879"><a href="#cb12-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-880"><a href="#cb12-880" aria-hidden="true" tabindex="-1"></a><span class="co"># For multiple variables:</span></span>
<span id="cb12-881"><a href="#cb12-881" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f2(theta):</span>
<span id="cb12-882"><a href="#cb12-882" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (jnp.exp(<span class="op">-</span>(theta[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-883"><a href="#cb12-883" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span> jnp.exp(<span class="op">-</span>(theta[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb12-884"><a href="#cb12-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-885"><a href="#cb12-885" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> jax.grad(f2)</span>
<span id="cb12-886"><a href="#cb12-886" aria-hidden="true" tabindex="-1"></a>g2(jnp.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="fl">0.5</span>]))</span>
<span id="cb12-887"><a href="#cb12-887" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-888"><a href="#cb12-888" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-889"><a href="#cb12-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-890"><a href="#cb12-890" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient-Based Optimization Methods</span></span>
<span id="cb12-891"><a href="#cb12-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-892"><a href="#cb12-892" aria-hidden="true" tabindex="-1"></a>Once we have gradients, we can use various algorithms to find the optimum.</span>
<span id="cb12-893"><a href="#cb12-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-894"><a href="#cb12-894" aria-hidden="true" tabindex="-1"></a>The simplest gradient-based algorithm is **gradient descent** (also called **steepest descent**), which iteratively takes steps in the direction of the negative gradient:</span>
<span id="cb12-895"><a href="#cb12-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-896"><a href="#cb12-896" aria-hidden="true" tabindex="-1"></a>$$\theta_{t+1} = \theta_t - \eta \nabla f(\theta_t)$$</span>
<span id="cb12-897"><a href="#cb12-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-898"><a href="#cb12-898" aria-hidden="true" tabindex="-1"></a>where $\eta &gt; 0$ (<span class="co">[</span><span class="ot">eta</span><span class="co">](https://en.wikipedia.org/wiki/Eta)</span>) is the *learning rate*, which defines the step length.</span>
<span id="cb12-899"><a href="#cb12-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-900"><a href="#cb12-900" aria-hidden="true" tabindex="-1"></a>Gradient descent is inefficient, often zigzagging toward the optimum, but it's the foundation for understanding more sophisticated methods.</span>
<span id="cb12-901"><a href="#cb12-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-902"><a href="#cb12-902" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-903"><a href="#cb12-903" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Gradient-Based Methods</span></span>
<span id="cb12-904"><a href="#cb12-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-905"><a href="#cb12-905" aria-hidden="true" tabindex="-1"></a>Beyond basic gradient descent, there are many sophisticated optimization algorithms:</span>
<span id="cb12-906"><a href="#cb12-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-907"><a href="#cb12-907" aria-hidden="true" tabindex="-1"></a>**Classical improvements**:</span>
<span id="cb12-908"><a href="#cb12-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-909"><a href="#cb12-909" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Conjugate gradient methods](https://en.wikipedia.org/wiki/Conjugate_gradient_method)**: Choose search directions that are conjugate with respect to the Hessian matrix. For quadratic functions, this guarantees finding the exact minimum in at most $n$ steps (where $n$ is the number of dimensions). Unlike gradient descent which can take tiny steps in nearly orthogonal directions, conjugate gradient methods take larger, more efficient steps.</span>
<span id="cb12-910"><a href="#cb12-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-911"><a href="#cb12-911" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Quasi-Newton methods**: Approximate the Hessian matrix without computing second derivatives directly. </span>
<span id="cb12-912"><a href="#cb12-912" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**[BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)** (Broyden-Fletcher-Goldfarb-Shanno): Builds up an approximation to the inverse Hessian using gradient information from previous steps</span>
<span id="cb12-913"><a href="#cb12-913" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)** (Limited-memory BFGS): Stores only a few vectors instead of the full Hessian approximation, making it practical for high-dimensional problems</span>
<span id="cb12-914"><a href="#cb12-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-915"><a href="#cb12-915" aria-hidden="true" tabindex="-1"></a>**Modern methods**:</span>
<span id="cb12-916"><a href="#cb12-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-917"><a href="#cb12-917" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Momentum methods** (heavy ball method): Add a fraction of the previous step to the current gradient step:</span>
<span id="cb12-918"><a href="#cb12-918" aria-hidden="true" tabindex="-1"></a>  $$\theta_{t+1} = \theta_t - \eta \nabla f(\theta_t) + \beta(\theta_t - \theta_{t-1})$$</span>
<span id="cb12-919"><a href="#cb12-919" aria-hidden="true" tabindex="-1"></a>  where $\beta \in [0, 1)$ is the momentum coefficient. This helps the optimizer "roll through" small local variations and reduces zigzagging in narrow valleys.</span>
<span id="cb12-920"><a href="#cb12-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-921"><a href="#cb12-921" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accelerated gradient methods**: Achieve provably faster convergence rates. Nesterov's accelerated gradient has convergence rate $O(1/t^2)$ compared to $O(1/t)$ for standard gradient descent on convex functions.</span>
<span id="cb12-922"><a href="#cb12-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-923"><a href="#cb12-923" aria-hidden="true" tabindex="-1"></a>These methods are especially important in deep learning where simple gradient descent would be too slow to navigate the complex, high-dimensional loss landscapes of neural networks.</span>
<span id="cb12-924"><a href="#cb12-924" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-925"><a href="#cb12-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-926"><a href="#cb12-926" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stochastic Gradient Methods</span></span>
<span id="cb12-927"><a href="#cb12-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-928"><a href="#cb12-928" aria-hidden="true" tabindex="-1"></a>Many machine learning problems, especially in deep learning, involve optimizing functions of the form:</span>
<span id="cb12-929"><a href="#cb12-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-930"><a href="#cb12-930" aria-hidden="true" tabindex="-1"></a>$$\min_\theta f(X, \theta) = \sum_{i=1}^n f(x_i, \theta)$$</span>
<span id="cb12-931"><a href="#cb12-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-932"><a href="#cb12-932" aria-hidden="true" tabindex="-1"></a>When $n$ is large and $\theta$ is high-dimensional, evaluating the full gradient becomes computationally prohibitive.</span>
<span id="cb12-933"><a href="#cb12-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-934"><a href="#cb12-934" aria-hidden="true" tabindex="-1"></a>**Stochastic gradient descent (SGD)** approximates the gradient using a random subset of data:</span>
<span id="cb12-935"><a href="#cb12-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-936"><a href="#cb12-936" aria-hidden="true" tabindex="-1"></a>$$\theta_{n+1} = \theta_n - \eta_n \nabla \sum_{i \in S_n} f(x_i, \theta_n)$$</span>
<span id="cb12-937"><a href="#cb12-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-938"><a href="#cb12-938" aria-hidden="true" tabindex="-1"></a>where $S_n \subset <span class="sc">\{</span>1, \ldots, n<span class="sc">\}</span>$ is a randomly selected mini-batch.</span>
<span id="cb12-939"><a href="#cb12-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-940"><a href="#cb12-940" aria-hidden="true" tabindex="-1"></a>**Convergence**: SGD converges for well-behaved functions when the learning rate sequence satisfies:</span>
<span id="cb12-941"><a href="#cb12-941" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^\infty \eta_i = \infty, \quad \sum_{i=1}^\infty \eta_i^2 &lt; \infty$$</span>
<span id="cb12-942"><a href="#cb12-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-943"><a href="#cb12-943" aria-hidden="true" tabindex="-1"></a>Note that constant learning rates don't guarantee convergence to the exact optimum!</span>
<span id="cb12-944"><a href="#cb12-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-945"><a href="#cb12-945" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-946"><a href="#cb12-946" aria-hidden="true" tabindex="-1"></a><span class="fu">## Popular SGD Variants</span></span>
<span id="cb12-947"><a href="#cb12-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-948"><a href="#cb12-948" aria-hidden="true" tabindex="-1"></a>The huge popularity of SGD has spawned many improved variants:</span>
<span id="cb12-949"><a href="#cb12-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-950"><a href="#cb12-950" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Adam** (Adaptive Moment Estimation) <span class="co">[</span><span class="ot">@kingma2014adam</span><span class="co">]</span>: Combines momentum with adaptive learning rates for each parameter</span>
<span id="cb12-951"><a href="#cb12-951" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AdaGrad** (Adaptive Gradient): Adapts learning rate based on historical gradients - parameters with frequent updates get smaller learning rates</span>
<span id="cb12-952"><a href="#cb12-952" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSprop** (Root Mean Square Propagation): Uses a running average of recent gradients to normalize the learning rate</span>
<span id="cb12-953"><a href="#cb12-953" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-954"><a href="#cb12-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-955"><a href="#cb12-955" aria-hidden="true" tabindex="-1"></a><span class="fu">### Which Optimizer Should I Use?</span></span>
<span id="cb12-956"><a href="#cb12-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-957"><a href="#cb12-957" aria-hidden="true" tabindex="-1"></a>**For smaller datasets** (&lt; 10K observations):</span>
<span id="cb12-958"><a href="#cb12-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-959"><a href="#cb12-959" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**L-BFGS** is usually the best first choice</span>
<span id="cb12-960"><a href="#cb12-960" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fast convergence, reliable for smooth problems</span>
<span id="cb12-961"><a href="#cb12-961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standard choice in traditional statistical software</span>
<span id="cb12-962"><a href="#cb12-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-963"><a href="#cb12-963" aria-hidden="true" tabindex="-1"></a>**For large datasets or deep learning**:</span>
<span id="cb12-964"><a href="#cb12-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-965"><a href="#cb12-965" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SGD** and variants (especially **Adam**) are often the only practical choice</span>
<span id="cb12-966"><a href="#cb12-966" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Require careful tuning of learning rates</span>
<span id="cb12-967"><a href="#cb12-967" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can handle millions and even billions of parameters</span>
<span id="cb12-968"><a href="#cb12-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-969"><a href="#cb12-969" aria-hidden="true" tabindex="-1"></a>**For black-box problems** (no gradients available):</span>
<span id="cb12-970"><a href="#cb12-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-971"><a href="#cb12-971" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[CMA-ES](https://en.wikipedia.org/wiki/CMA-ES)**: Principled "population-based" method that can handle up to thousands of parameters (requiring many evaluations)</span>
<span id="cb12-972"><a href="#cb12-972" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)**: Efficient for expensive functions with $D &lt; 20$ parameters</span>
<span id="cb12-973"><a href="#cb12-973" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[BADS](https://acerbilab.github.io/pybads/index.html)** (Bayesian Adaptive Direct Search): Combines Bayesian optimization with mesh adaptive direct search, good for noisy and mildly expensive functions with $D &lt; 20$</span>
<span id="cb12-974"><a href="#cb12-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-975"><a href="#cb12-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-976"><a href="#cb12-976" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-977"><a href="#cb12-977" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Topic: Constrained Optimization</span></span>
<span id="cb12-978"><a href="#cb12-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-979"><a href="#cb12-979" aria-hidden="true" tabindex="-1"></a>Often parameters have natural constraints:</span>
<span id="cb12-980"><a href="#cb12-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-981"><a href="#cb12-981" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standard deviations must be positive: $\sigma &gt; 0$</span>
<span id="cb12-982"><a href="#cb12-982" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Probabilities must be in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>: $0 \leq p \leq 1$</span>
<span id="cb12-983"><a href="#cb12-983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Correlation matrices must be positive definite</span>
<span id="cb12-984"><a href="#cb12-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-985"><a href="#cb12-985" aria-hidden="true" tabindex="-1"></a>How do we enforce these? There are typically three different approaches:</span>
<span id="cb12-986"><a href="#cb12-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-987"><a href="#cb12-987" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Reparameterization**: Optimize $\phi = \log \sigma$ and then use $\sigma = e^\phi$ to ensure positivity^<span class="co">[</span><span class="ot">For numerical stability, it has become common to use the [softplus](https://en.wikipedia.org/wiki/Softplus) instead of $\exp$ to ensure positivity.</span><span class="co">]</span></span>
<span id="cb12-988"><a href="#cb12-988" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Constrained optimization**: Use algorithms like L-BFGS-B that allows you to specify parameter bounds</span>
<span id="cb12-989"><a href="#cb12-989" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Barrier methods**: Add penalty terms that blow up at boundaries (can be unstable -- not recommended)</span>
<span id="cb12-990"><a href="#cb12-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-991"><a href="#cb12-991" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-992"><a href="#cb12-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-993"><a href="#cb12-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-994"><a href="#cb12-994" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-995"><a href="#cb12-995" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: 2D MLE with Numerical Optimization</span></span>
<span id="cb12-996"><a href="#cb12-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-997"><a href="#cb12-997" aria-hidden="true" tabindex="-1"></a>Now let's apply the multidimensional optimization concepts to a real statistical problem. We'll estimate both parameters of a Gamma distribution, which provides an excellent illustration of:</span>
<span id="cb12-998"><a href="#cb12-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-999"><a href="#cb12-999" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**True 2D optimization**: Unlike our 1D examples, we need to simultaneously find the shape parameter $\alpha$ and scale parameter $\beta$.</span>
<span id="cb12-1000"><a href="#cb12-1000" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Constrained optimization in practice**: Both parameters must be positive, so we'll use L-BFGS-B with bounds.</span>
<span id="cb12-1001"><a href="#cb12-1001" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The importance of starting values**: We'll launch optimization from multiple starting points to see if they converge to the same solution. We choose the first starting point using the Method of Moments, while the others arbitrarily (could have been random).</span>
<span id="cb12-1002"><a href="#cb12-1002" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Visualizing the likelihood surface**: We'll create both 3D and contour plots to understand the optimization landscape.</span>
<span id="cb12-1003"><a href="#cb12-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1004"><a href="#cb12-1004" aria-hidden="true" tabindex="-1"></a>The Gamma distribution is particularly interesting because its two parameters interact in the likelihood function, creating a curved valley in the likelihood surface rather than a simple bowl.</span>
<span id="cb12-1005"><a href="#cb12-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1008"><a href="#cb12-1008" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-1009"><a href="#cb12-1009" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb12-1010"><a href="#cb12-1010" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 10</span></span>
<span id="cb12-1011"><a href="#cb12-1011" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-1012"><a href="#cb12-1012" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-1013"><a href="#cb12-1013" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats, optimize</span>
<span id="cb12-1014"><a href="#cb12-1014" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb12-1015"><a href="#cb12-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1016"><a href="#cb12-1016" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from Gamma distribution</span></span>
<span id="cb12-1017"><a href="#cb12-1017" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb12-1018"><a href="#cb12-1018" aria-hidden="true" tabindex="-1"></a>true_alpha <span class="op">=</span> <span class="fl">3.0</span>  <span class="co"># shape parameter</span></span>
<span id="cb12-1019"><a href="#cb12-1019" aria-hidden="true" tabindex="-1"></a>true_beta <span class="op">=</span> <span class="fl">2.0</span>   <span class="co"># scale parameter</span></span>
<span id="cb12-1020"><a href="#cb12-1020" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-1021"><a href="#cb12-1021" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> stats.gamma.rvs(a<span class="op">=</span>true_alpha, scale<span class="op">=</span>true_beta, size<span class="op">=</span>n)</span>
<span id="cb12-1022"><a href="#cb12-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1023"><a href="#cb12-1023" aria-hidden="true" tabindex="-1"></a><span class="co"># Define negative log-likelihood for Gamma(α, β)</span></span>
<span id="cb12-1024"><a href="#cb12-1024" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_likelihood(params, data):</span>
<span id="cb12-1025"><a href="#cb12-1025" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Negative log-likelihood for Gamma distribution"""</span></span>
<span id="cb12-1026"><a href="#cb12-1026" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> params</span>
<span id="cb12-1027"><a href="#cb12-1027" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> alpha <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">or</span> beta <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb12-1028"><a href="#cb12-1028" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.inf  <span class="co"># Return infinity for invalid parameters</span></span>
<span id="cb12-1029"><a href="#cb12-1029" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(stats.gamma.logpdf(data, a<span class="op">=</span>alpha, scale<span class="op">=</span>beta))</span>
<span id="cb12-1030"><a href="#cb12-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1031"><a href="#cb12-1031" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments starting values</span></span>
<span id="cb12-1032"><a href="#cb12-1032" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb12-1033"><a href="#cb12-1033" aria-hidden="true" tabindex="-1"></a>sample_var <span class="op">=</span> np.var(data)</span>
<span id="cb12-1034"><a href="#cb12-1034" aria-hidden="true" tabindex="-1"></a>mom_beta <span class="op">=</span> sample_var <span class="op">/</span> sample_mean</span>
<span id="cb12-1035"><a href="#cb12-1035" aria-hidden="true" tabindex="-1"></a>mom_alpha <span class="op">=</span> sample_mean <span class="op">/</span> mom_beta</span>
<span id="cb12-1036"><a href="#cb12-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1037"><a href="#cb12-1037" aria-hidden="true" tabindex="-1"></a><span class="co"># Try optimization from different starting points</span></span>
<span id="cb12-1038"><a href="#cb12-1038" aria-hidden="true" tabindex="-1"></a>starting_points <span class="op">=</span> [</span>
<span id="cb12-1039"><a href="#cb12-1039" aria-hidden="true" tabindex="-1"></a>    [mom_alpha, mom_beta],  <span class="co"># MoM estimate</span></span>
<span id="cb12-1040"><a href="#cb12-1040" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">1.0</span>],             <span class="co"># Generic start</span></span>
<span id="cb12-1041"><a href="#cb12-1041" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">5.0</span>, <span class="fl">5.0</span>],             <span class="co"># Far from truth</span></span>
<span id="cb12-1042"><a href="#cb12-1042" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-1043"><a href="#cb12-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1044"><a href="#cb12-1044" aria-hidden="true" tabindex="-1"></a><span class="co"># Store optimization paths</span></span>
<span id="cb12-1045"><a href="#cb12-1045" aria-hidden="true" tabindex="-1"></a>paths <span class="op">=</span> []</span>
<span id="cb12-1046"><a href="#cb12-1046" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb12-1047"><a href="#cb12-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1048"><a href="#cb12-1048" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, start <span class="kw">in</span> <span class="bu">enumerate</span>(starting_points):</span>
<span id="cb12-1049"><a href="#cb12-1049" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a list to store the path for this optimization</span></span>
<span id="cb12-1050"><a href="#cb12-1050" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> []</span>
<span id="cb12-1051"><a href="#cb12-1051" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-1052"><a href="#cb12-1052" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Callback function to record each iteration</span></span>
<span id="cb12-1053"><a href="#cb12-1053" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> callback(xk):</span>
<span id="cb12-1054"><a href="#cb12-1054" aria-hidden="true" tabindex="-1"></a>        path.append(xk.copy())</span>
<span id="cb12-1055"><a href="#cb12-1055" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-1056"><a href="#cb12-1056" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> optimize.minimize(</span>
<span id="cb12-1057"><a href="#cb12-1057" aria-hidden="true" tabindex="-1"></a>        fun<span class="op">=</span>neg_log_likelihood,</span>
<span id="cb12-1058"><a href="#cb12-1058" aria-hidden="true" tabindex="-1"></a>        x0<span class="op">=</span>start,</span>
<span id="cb12-1059"><a href="#cb12-1059" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>(data,),</span>
<span id="cb12-1060"><a href="#cb12-1060" aria-hidden="true" tabindex="-1"></a>        method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb12-1061"><a href="#cb12-1061" aria-hidden="true" tabindex="-1"></a>        bounds<span class="op">=</span>[(<span class="fl">0.01</span>, <span class="va">None</span>), (<span class="fl">0.01</span>, <span class="va">None</span>)],  <span class="co"># Enforce positivity</span></span>
<span id="cb12-1062"><a href="#cb12-1062" aria-hidden="true" tabindex="-1"></a>        callback<span class="op">=</span>callback</span>
<span id="cb12-1063"><a href="#cb12-1063" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-1064"><a href="#cb12-1064" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-1065"><a href="#cb12-1065" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add starting point to path</span></span>
<span id="cb12-1066"><a href="#cb12-1066" aria-hidden="true" tabindex="-1"></a>    full_path <span class="op">=</span> [np.array(start)] <span class="op">+</span> path</span>
<span id="cb12-1067"><a href="#cb12-1067" aria-hidden="true" tabindex="-1"></a>    paths.append(np.array(full_path))</span>
<span id="cb12-1068"><a href="#cb12-1068" aria-hidden="true" tabindex="-1"></a>    results.append(result)</span>
<span id="cb12-1069"><a href="#cb12-1069" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-1070"><a href="#cb12-1070" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Start </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>start<span class="sc">}</span><span class="ss"> → MLE: [</span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">], "</span></span>
<span id="cb12-1071"><a href="#cb12-1071" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"NLL: </span><span class="sc">{</span>result<span class="sc">.</span>fun<span class="sc">:.3f}</span><span class="ss">, Iterations: </span><span class="sc">{</span><span class="bu">len</span>(full_path)<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-1072"><a href="#cb12-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1073"><a href="#cb12-1073" aria-hidden="true" tabindex="-1"></a><span class="co"># Best result</span></span>
<span id="cb12-1074"><a href="#cb12-1074" aria-hidden="true" tabindex="-1"></a>best_result <span class="op">=</span> <span class="bu">min</span>(results, key<span class="op">=</span><span class="kw">lambda</span> r: r.fun)</span>
<span id="cb12-1075"><a href="#cb12-1075" aria-hidden="true" tabindex="-1"></a>mle_alpha, mle_beta <span class="op">=</span> best_result.x</span>
<span id="cb12-1076"><a href="#cb12-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1077"><a href="#cb12-1077" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the likelihood surface</span></span>
<span id="cb12-1078"><a href="#cb12-1078" aria-hidden="true" tabindex="-1"></a>alpha_range <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="dv">6</span>, <span class="dv">50</span>)</span>
<span id="cb12-1079"><a href="#cb12-1079" aria-hidden="true" tabindex="-1"></a>beta_range <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="fl">5.2</span>, <span class="dv">50</span>)</span>
<span id="cb12-1080"><a href="#cb12-1080" aria-hidden="true" tabindex="-1"></a>Alpha, Beta <span class="op">=</span> np.meshgrid(alpha_range, beta_range)</span>
<span id="cb12-1081"><a href="#cb12-1081" aria-hidden="true" tabindex="-1"></a>NLL <span class="op">=</span> np.zeros_like(Alpha)</span>
<span id="cb12-1082"><a href="#cb12-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1083"><a href="#cb12-1083" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Alpha.shape[<span class="dv">0</span>]):</span>
<span id="cb12-1084"><a href="#cb12-1084" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(Alpha.shape[<span class="dv">1</span>]):</span>
<span id="cb12-1085"><a href="#cb12-1085" aria-hidden="true" tabindex="-1"></a>        NLL[i, j] <span class="op">=</span> neg_log_likelihood([Alpha[i, j], Beta[i, j]], data)</span>
<span id="cb12-1086"><a href="#cb12-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1087"><a href="#cb12-1087" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">10</span>))</span>
<span id="cb12-1088"><a href="#cb12-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1089"><a href="#cb12-1089" aria-hidden="true" tabindex="-1"></a><span class="co"># 3D surface plot</span></span>
<span id="cb12-1090"><a href="#cb12-1090" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb12-1091"><a href="#cb12-1091" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax1.plot_surface(Alpha, Beta, NLL, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, edgecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb12-1092"><a href="#cb12-1092" aria-hidden="true" tabindex="-1"></a>ax1.contour(Alpha, Beta, NLL, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, offset<span class="op">=</span>NLL.<span class="bu">min</span>(), alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb12-1093"><a href="#cb12-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1094"><a href="#cb12-1094" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the different solutions</span></span>
<span id="cb12-1095"><a href="#cb12-1095" aria-hidden="true" tabindex="-1"></a>ax1.scatter([true_alpha], [true_beta], [neg_log_likelihood([true_alpha, true_beta], data)], </span>
<span id="cb12-1096"><a href="#cb12-1096" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'True parameters'</span>)</span>
<span id="cb12-1097"><a href="#cb12-1097" aria-hidden="true" tabindex="-1"></a>ax1.scatter([mle_alpha], [mle_beta], [best_result.fun], </span>
<span id="cb12-1098"><a href="#cb12-1098" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb12-1099"><a href="#cb12-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1100"><a href="#cb12-1100" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'α (shape)'</span>)</span>
<span id="cb12-1101"><a href="#cb12-1101" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'β (scale)'</span>)</span>
<span id="cb12-1102"><a href="#cb12-1102" aria-hidden="true" tabindex="-1"></a>ax1.set_zlabel(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb12-1103"><a href="#cb12-1103" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Likelihood Surface for Gamma Distribution'</span>)</span>
<span id="cb12-1104"><a href="#cb12-1104" aria-hidden="true" tabindex="-1"></a>ax1.view_init(elev<span class="op">=</span><span class="dv">25</span>, azim<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb12-1105"><a href="#cb12-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1106"><a href="#cb12-1106" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D contour plot with optimization paths</span></span>
<span id="cb12-1107"><a href="#cb12-1107" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb12-1108"><a href="#cb12-1108" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax2.contour(Alpha, Beta, NLL, levels<span class="op">=</span><span class="dv">30</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb12-1109"><a href="#cb12-1109" aria-hidden="true" tabindex="-1"></a>ax2.clabel(contour, inline<span class="op">=</span><span class="va">True</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb12-1110"><a href="#cb12-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1111"><a href="#cb12-1111" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot optimization paths</span></span>
<span id="cb12-1112"><a href="#cb12-1112" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'orange'</span>]</span>
<span id="cb12-1113"><a href="#cb12-1113" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (path, color) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(paths, colors)):</span>
<span id="cb12-1114"><a href="#cb12-1114" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the full optimization path</span></span>
<span id="cb12-1115"><a href="#cb12-1115" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[:, <span class="dv">0</span>], path[:, <span class="dv">1</span>], <span class="st">'-'</span>, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb12-1116"><a href="#cb12-1116" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark start point</span></span>
<span id="cb12-1117"><a href="#cb12-1117" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[<span class="dv">0</span>, <span class="dv">0</span>], path[<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'o'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb12-1118"><a href="#cb12-1118" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'Start </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-1119"><a href="#cb12-1119" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark intermediate points</span></span>
<span id="cb12-1120"><a href="#cb12-1120" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>], path[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">'.'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb12-1121"><a href="#cb12-1121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark end point</span></span>
<span id="cb12-1122"><a href="#cb12-1122" aria-hidden="true" tabindex="-1"></a>    ax2.plot(path[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>], path[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">'s'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-1123"><a href="#cb12-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1124"><a href="#cb12-1124" aria-hidden="true" tabindex="-1"></a>ax2.plot(true_alpha, true_beta, <span class="st">'k*'</span>, markersize<span class="op">=</span><span class="dv">12</span>, label<span class="op">=</span><span class="st">'True'</span>)</span>
<span id="cb12-1125"><a href="#cb12-1125" aria-hidden="true" tabindex="-1"></a>ax2.plot(mle_alpha, mle_beta, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb12-1126"><a href="#cb12-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1127"><a href="#cb12-1127" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'α (shape)'</span>)</span>
<span id="cb12-1128"><a href="#cb12-1128" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'β (scale)'</span>)</span>
<span id="cb12-1129"><a href="#cb12-1129" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'L-BFGS Optimization Traces from Different Starting Points'</span>)</span>
<span id="cb12-1130"><a href="#cb12-1130" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb12-1131"><a href="#cb12-1131" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-1132"><a href="#cb12-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1133"><a href="#cb12-1133" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-1134"><a href="#cb12-1134" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-1135"><a href="#cb12-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1136"><a href="#cb12-1136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">True parameters: α = </span><span class="sc">{</span>true_alpha<span class="sc">}</span><span class="ss">, β = </span><span class="sc">{</span>true_beta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-1137"><a href="#cb12-1137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE estimates:   α = </span><span class="sc">{</span>mle_alpha<span class="sc">:.3f}</span><span class="ss">, β = </span><span class="sc">{</span>mle_beta<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-1138"><a href="#cb12-1138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MoM estimates:   α = </span><span class="sc">{</span>mom_alpha<span class="sc">:.3f}</span><span class="ss">, β = </span><span class="sc">{</span>mom_beta<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-1139"><a href="#cb12-1139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1140"><a href="#cb12-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1141"><a href="#cb12-1141" aria-hidden="true" tabindex="-1"></a>Notice how all three starting points converged to the same MLE, demonstrating that this likelihood surface is well-behaved with a single global optimum. The actual L-BFGS traces (with intermediate points marked as dots) reveal interesting optimization behavior:</span>
<span id="cb12-1142"><a href="#cb12-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1143"><a href="#cb12-1143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Blue path (MoM start)**: Converges in very few iterations since it starts close to the optimum</span>
<span id="cb12-1144"><a href="#cb12-1144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Green path (generic start)**: Takes a curved path following the likelihood valley</span>
<span id="cb12-1145"><a href="#cb12-1145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Orange path (far start)**: Makes larger initial steps, then follows the contours more carefully as it approaches the optimum</span>
<span id="cb12-1146"><a href="#cb12-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1147"><a href="#cb12-1147" aria-hidden="true" tabindex="-1"></a>The 3D plot reveals the characteristic curved valley of the Gamma likelihood, explaining why the optimization paths curve rather than taking straight lines to the optimum.</span>
<span id="cb12-1148"><a href="#cb12-1148" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1149"><a href="#cb12-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1150"><a href="#cb12-1150" aria-hidden="true" tabindex="-1"></a><span class="fu">### FAQ: Common Issues in Numerical Optimization</span></span>
<span id="cb12-1151"><a href="#cb12-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1152"><a href="#cb12-1152" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-1153"><a href="#cb12-1153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Frequently Asked Questions</span></span>
<span id="cb12-1154"><a href="#cb12-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1155"><a href="#cb12-1155" aria-hidden="true" tabindex="-1"></a>**Q: I used a minimizer, but I'm supposed to be doing *maximum* likelihood. What gives?**</span>
<span id="cb12-1156"><a href="#cb12-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1157"><a href="#cb12-1157" aria-hidden="true" tabindex="-1"></a>A: A classic source of confusion! Maximizing a function $f(x)$ is equivalent to minimizing its negative $-f(x)$. All standard optimization libraries are built as minimizers. Therefore, in practice, we always find the MLE by **minimizing the negative log-likelihood function**.</span>
<span id="cb12-1158"><a href="#cb12-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1159"><a href="#cb12-1159" aria-hidden="true" tabindex="-1"></a>**Q: I tried different starting points and the optimizer gave different answers. Is it broken?**</span>
<span id="cb12-1160"><a href="#cb12-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1161"><a href="#cb12-1161" aria-hidden="true" tabindex="-1"></a>A: No, this is expected behavior! The algorithms we use are **local optimizers**. They find the nearest valley (local minimum), but they have no guarantee of finding the deepest valley (global minimum). If your likelihood surface has multiple local optima, the result will depend on your starting point. This is why using a good initial value (like the Method of Moments estimate!) is so important.</span>
<span id="cb12-1162"><a href="#cb12-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1163"><a href="#cb12-1163" aria-hidden="true" tabindex="-1"></a>**Q: How do I know if the algorithm has actually converged to the right answer?**</span>
<span id="cb12-1164"><a href="#cb12-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1165"><a href="#cb12-1165" aria-hidden="true" tabindex="-1"></a>A: In general, you don't know with 100% certainty. Optimizers use heuristics, like stopping when the change in the parameter values or the likelihood is very small. Good practices include:</span>
<span id="cb12-1166"><a href="#cb12-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1167"><a href="#cb12-1167" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Always try different starting points!**</span>
<span id="cb12-1168"><a href="#cb12-1168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Check the optimizer's status messages</span>
<span id="cb12-1169"><a href="#cb12-1169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Verify that the gradient is near zero at the solution</span>
<span id="cb12-1170"><a href="#cb12-1170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compare with simpler methods (like MoM) as a sanity check</span>
<span id="cb12-1171"><a href="#cb12-1171" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1172"><a href="#cb12-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1173"><a href="#cb12-1173" aria-hidden="true" tabindex="-1"></a>Let's demonstrate the importance of starting values:</span>
<span id="cb12-1174"><a href="#cb12-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1177"><a href="#cb12-1177" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-1178"><a href="#cb12-1178" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb12-1179"><a href="#cb12-1179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb12-1180"><a href="#cb12-1180" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Multiple local optima in a mixture model</span></span>
<span id="cb12-1181"><a href="#cb12-1181" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll create a bimodal likelihood surface</span></span>
<span id="cb12-1182"><a href="#cb12-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1183"><a href="#cb12-1183" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bimodal_nll(theta):</span>
<span id="cb12-1184"><a href="#cb12-1184" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A negative log-likelihood with two local minima"""</span></span>
<span id="cb12-1185"><a href="#cb12-1185" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Artificial example with two valleys</span></span>
<span id="cb12-1186"><a href="#cb12-1186" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(<span class="fl">0.6</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>(theta<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> </span>
<span id="cb12-1187"><a href="#cb12-1187" aria-hidden="true" tabindex="-1"></a>                   <span class="fl">0.4</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">*</span>(theta<span class="op">-</span><span class="dv">4</span>)<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.01</span>)</span>
<span id="cb12-1188"><a href="#cb12-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1189"><a href="#cb12-1189" aria-hidden="true" tabindex="-1"></a><span class="co"># Try optimization from different starting points</span></span>
<span id="cb12-1190"><a href="#cb12-1190" aria-hidden="true" tabindex="-1"></a>theta_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">200</span>)</span>
<span id="cb12-1191"><a href="#cb12-1191" aria-hidden="true" tabindex="-1"></a>nll_surface <span class="op">=</span> [bimodal_nll(t) <span class="cf">for</span> t <span class="kw">in</span> theta_range]</span>
<span id="cb12-1192"><a href="#cb12-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1193"><a href="#cb12-1193" aria-hidden="true" tabindex="-1"></a>starting_points <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>, <span class="fl">2.5</span>, <span class="dv">5</span>]</span>
<span id="cb12-1194"><a href="#cb12-1194" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'green'</span>, <span class="st">'blue'</span>]</span>
<span id="cb12-1195"><a href="#cb12-1195" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb12-1196"><a href="#cb12-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1197"><a href="#cb12-1197" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb12-1198"><a href="#cb12-1198" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_range, nll_surface, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Objective function'</span>)</span>
<span id="cb12-1199"><a href="#cb12-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1200"><a href="#cb12-1200" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> start, color <span class="kw">in</span> <span class="bu">zip</span>(starting_points, colors):</span>
<span id="cb12-1201"><a href="#cb12-1201" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> optimize.minimize(bimodal_nll, x0<span class="op">=</span>[start], method<span class="op">=</span><span class="st">'L-BFGS-B'</span>)</span>
<span id="cb12-1202"><a href="#cb12-1202" aria-hidden="true" tabindex="-1"></a>    results.append(result.x[<span class="dv">0</span>])</span>
<span id="cb12-1203"><a href="#cb12-1203" aria-hidden="true" tabindex="-1"></a>    plt.scatter([start], [bimodal_nll(start)], color<span class="op">=</span>color, s<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb12-1204"><a href="#cb12-1204" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="ss">f'Start: </span><span class="sc">{</span>start<span class="sc">:.1f}</span><span class="ss">'</span>)</span>
<span id="cb12-1205"><a href="#cb12-1205" aria-hidden="true" tabindex="-1"></a>    plt.scatter([result.x[<span class="dv">0</span>]], [result.fun], color<span class="op">=</span>color, s<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb12-1206"><a href="#cb12-1206" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="ss">f'End: </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb12-1207"><a href="#cb12-1207" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-1208"><a href="#cb12-1208" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw arrow from start to end</span></span>
<span id="cb12-1209"><a href="#cb12-1209" aria-hidden="true" tabindex="-1"></a>    plt.annotate(<span class="st">''</span>, xy<span class="op">=</span>(result.x[<span class="dv">0</span>], result.fun), </span>
<span id="cb12-1210"><a href="#cb12-1210" aria-hidden="true" tabindex="-1"></a>                 xytext<span class="op">=</span>(start, bimodal_nll(start)),</span>
<span id="cb12-1211"><a href="#cb12-1211" aria-hidden="true" tabindex="-1"></a>                 arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span>color, lw<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb12-1212"><a href="#cb12-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1213"><a href="#cb12-1213" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb12-1214"><a href="#cb12-1214" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb12-1215"><a href="#cb12-1215" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Local Optimization: Different Starting Points → Different Solutions'</span>)</span>
<span id="cb12-1216"><a href="#cb12-1216" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb12-1217"><a href="#cb12-1217" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-1218"><a href="#cb12-1218" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-1219"><a href="#cb12-1219" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-1220"><a href="#cb12-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1221"><a href="#cb12-1221" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting points and their corresponding local optima:"</span>)</span>
<span id="cb12-1222"><a href="#cb12-1222" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> start, end <span class="kw">in</span> <span class="bu">zip</span>(starting_points, results):</span>
<span id="cb12-1223"><a href="#cb12-1223" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Start: </span><span class="sc">{</span>start<span class="sc">:4.1f}</span><span class="ss"> → End: </span><span class="sc">{</span>end<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-1224"><a href="#cb12-1224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1225"><a href="#cb12-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1226"><a href="#cb12-1226" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb12-1227"><a href="#cb12-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1228"><a href="#cb12-1228" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb12-1229"><a href="#cb12-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1230"><a href="#cb12-1230" aria-hidden="true" tabindex="-1"></a>We've explored two fundamental approaches to finding estimators in parametric models:</span>
<span id="cb12-1231"><a href="#cb12-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1232"><a href="#cb12-1232" aria-hidden="true" tabindex="-1"></a>**Parametric Models**: </span>
<span id="cb12-1233"><a href="#cb12-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1234"><a href="#cb12-1234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Assume data comes from a specific family of distributions $\mathfrak{F} = <span class="sc">\{</span>f(x; \theta): \theta \in \Theta<span class="sc">\}</span>$</span>
<span id="cb12-1235"><a href="#cb12-1235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Our job reduces to estimating the finite-dimensional parameter $\theta$</span>
<span id="cb12-1236"><a href="#cb12-1236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often have parameters of interest and nuisance parameters</span>
<span id="cb12-1237"><a href="#cb12-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1238"><a href="#cb12-1238" aria-hidden="true" tabindex="-1"></a>**Method of Moments (MoM)**:</span>
<span id="cb12-1239"><a href="#cb12-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1240"><a href="#cb12-1240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Match theoretical moments $\mathbb{E}(X^j)$ with sample moments $\frac{1}{n}\sum X_i^j$</span>
<span id="cb12-1241"><a href="#cb12-1241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple to compute -- just solve algebraic equations</span>
<span id="cb12-1242"><a href="#cb12-1242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consistent and asymptotically normal</span>
<span id="cb12-1243"><a href="#cb12-1243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Not efficient, but excellent starting values for other methods</span>
<span id="cb12-1244"><a href="#cb12-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1245"><a href="#cb12-1245" aria-hidden="true" tabindex="-1"></a>**Maximum Likelihood Estimation (MLE)**:</span>
<span id="cb12-1246"><a href="#cb12-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1247"><a href="#cb12-1247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Find parameters that make observed data most probable</span>
<span id="cb12-1248"><a href="#cb12-1248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Likelihood: $\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta)$</span>
<span id="cb12-1249"><a href="#cb12-1249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often requires numerical optimization</span>
<span id="cb12-1250"><a href="#cb12-1250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The gold standard of parametric estimation</span>
<span id="cb12-1251"><a href="#cb12-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1252"><a href="#cb12-1252" aria-hidden="true" tabindex="-1"></a>**Numerical Optimization**:</span>
<span id="cb12-1253"><a href="#cb12-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1254"><a href="#cb12-1254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Most MLEs require iterative algorithms</span>
<span id="cb12-1255"><a href="#cb12-1255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gradient-based methods dominate (L-BFGS for small data, SGD/Adam for large)</span>
<span id="cb12-1256"><a href="#cb12-1256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatic differentiation (JAX) makes implementation easier</span>
<span id="cb12-1257"><a href="#cb12-1257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Local optima are a real concern -- **always try multiple starting points!**</span>
<span id="cb12-1258"><a href="#cb12-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1259"><a href="#cb12-1259" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Big Picture</span></span>
<span id="cb12-1260"><a href="#cb12-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1261"><a href="#cb12-1261" aria-hidden="true" tabindex="-1"></a>This chapter revealed a fundamental connection: much of modern machine learning is secretly Maximum Likelihood Estimation:</span>
<span id="cb12-1262"><a href="#cb12-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1263"><a href="#cb12-1263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear regression**: MLE with normal errors</span>
<span id="cb12-1264"><a href="#cb12-1264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Logistic regression**: MLE for Bernoulli responses  </span>
<span id="cb12-1265"><a href="#cb12-1265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Neural networks**: MLE with complex parametric models</span>
<span id="cb12-1266"><a href="#cb12-1266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep learning**: MLE with stochastic optimization</span>
<span id="cb12-1267"><a href="#cb12-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1268"><a href="#cb12-1268" aria-hidden="true" tabindex="-1"></a>The principles we've learned -- likelihood, optimization, gradients -- are the foundation of both classical statistics and modern ML.</span>
<span id="cb12-1269"><a href="#cb12-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1270"><a href="#cb12-1270" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb12-1271"><a href="#cb12-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1272"><a href="#cb12-1272" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Confusing likelihood with probability**: The likelihood is NOT a probability distribution over parameters</span>
<span id="cb12-1273"><a href="#cb12-1273" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Forgetting the negative sign**: Optimizers minimize, so use negative log-likelihood</span>
<span id="cb12-1274"><a href="#cb12-1274" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Assuming analytical solutions exist**: Most real problems require numerical methods</span>
<span id="cb12-1275"><a href="#cb12-1275" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Trusting a single optimization run**: Did I mention that you should **always try multiple starting points**?</span>
<span id="cb12-1276"><a href="#cb12-1276" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Ignoring convergence warnings**: Check optimizer status and diagnostics</span>
<span id="cb12-1277"><a href="#cb12-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1278"><a href="#cb12-1278" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb12-1279"><a href="#cb12-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1280"><a href="#cb12-1280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Previous**: Chapter 3 gave us convergence concepts and the framework for evaluating estimators. Now we know how to *find* estimators.</span>
<span id="cb12-1281"><a href="#cb12-1281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next**: Chapter 6 will explore the *properties* of these estimators in detail -- bias, variance, efficiency -- and prove that MLEs have optimal properties.</span>
<span id="cb12-1282"><a href="#cb12-1282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bootstrap (Chapter 4)**: Provides a computational alternative to analytical standard errors for our estimators</span>
<span id="cb12-1283"><a href="#cb12-1283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Later chapters**: These estimation principles extend to more complex models (regression, time series, etc.)</span>
<span id="cb12-1284"><a href="#cb12-1284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1285"><a href="#cb12-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1286"><a href="#cb12-1286" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb12-1287"><a href="#cb12-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1288"><a href="#cb12-1288" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Method of Moments**: For $X_1, \ldots, X_n \sim \text{Uniform}(a, b)$, find the Method of Moments estimators for $a$ and $b$.</span>
<span id="cb12-1289"><a href="#cb12-1289" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb12-1290"><a href="#cb12-1290" aria-hidden="true" tabindex="-1"></a>   *Hint*: Use $\mathbb{E}(X) = \frac{a+b}{2}$ and $\mathbb{V}(X) = \frac{(b-a)^2}{12}$.</span>
<span id="cb12-1291"><a href="#cb12-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1292"><a href="#cb12-1292" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Maximum Likelihood**: For $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$, find the Maximum Likelihood Estimator for $\lambda$.</span>
<span id="cb12-1293"><a href="#cb12-1293" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb12-1294"><a href="#cb12-1294" aria-hidden="true" tabindex="-1"></a>   *Hint*: The Poisson PMF is $f(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}$.</span>
<span id="cb12-1295"><a href="#cb12-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1296"><a href="#cb12-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Numerical Optimization**: The log-likelihood for logistic regression with one covariate is:</span>
<span id="cb12-1297"><a href="#cb12-1297" aria-hidden="true" tabindex="-1"></a>   $$\ell(y, x; \beta_0, \beta_1) = \sum_{i=1}^n \left<span class="co">[</span><span class="ot">y_i(\beta_0 + \beta_1 x_i) - \log(1 + e^{\beta_0 + \beta_1 x_i})\right</span><span class="co">]</span>$$</span>
<span id="cb12-1298"><a href="#cb12-1298" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb12-1299"><a href="#cb12-1299" aria-hidden="true" tabindex="-1"></a>   Explain why you cannot find the MLE for $(\beta_0, \beta_1)$ analytically and must use numerical optimization.</span>
<span id="cb12-1300"><a href="#cb12-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1301"><a href="#cb12-1301" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Comparing Estimators**: For $X_1, \ldots, X_n \sim \text{Exponential}(\lambda)$:</span>
<span id="cb12-1302"><a href="#cb12-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1303"><a href="#cb12-1303" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Find the MoM estimator for $\lambda$</span>
<span id="cb12-1304"><a href="#cb12-1304" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Find the MLE for $\lambda$</span>
<span id="cb12-1305"><a href="#cb12-1305" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Are they the same? Why or why not?</span>
<span id="cb12-1306"><a href="#cb12-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1307"><a href="#cb12-1307" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb12-1308"><a href="#cb12-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1309"><a href="#cb12-1309" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb12-1310"><a href="#cb12-1310" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb12-1311"><a href="#cb12-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1312"><a href="#cb12-1312" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb12-1313"><a href="#cb12-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1314"><a href="#cb12-1314" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-1315"><a href="#cb12-1315" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb12-1316"><a href="#cb12-1316" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-1317"><a href="#cb12-1317" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats, optimize</span>
<span id="cb12-1318"><a href="#cb12-1318" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb12-1319"><a href="#cb12-1319" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb12-1320"><a href="#cb12-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1321"><a href="#cb12-1321" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments estimation</span></span>
<span id="cb12-1322"><a href="#cb12-1322" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> method_of_moments_normal(data):</span>
<span id="cb12-1323"><a href="#cb12-1323" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""MoM for Normal distribution"""</span></span>
<span id="cb12-1324"><a href="#cb12-1324" aria-hidden="true" tabindex="-1"></a>    mu_hat <span class="op">=</span> np.mean(data)</span>
<span id="cb12-1325"><a href="#cb12-1325" aria-hidden="true" tabindex="-1"></a>    sigma2_hat <span class="op">=</span> np.mean((data <span class="op">-</span> mu_hat)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-1326"><a href="#cb12-1326" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu_hat, np.sqrt(sigma2_hat)</span>
<span id="cb12-1327"><a href="#cb12-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1328"><a href="#cb12-1328" aria-hidden="true" tabindex="-1"></a><span class="co"># Maximum Likelihood with scipy</span></span>
<span id="cb12-1329"><a href="#cb12-1329" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mle_poisson(data):</span>
<span id="cb12-1330"><a href="#cb12-1330" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""MLE for Poisson distribution"""</span></span>
<span id="cb12-1331"><a href="#cb12-1331" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For Poisson, MLE is just the sample mean</span></span>
<span id="cb12-1332"><a href="#cb12-1332" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(data)</span>
<span id="cb12-1333"><a href="#cb12-1333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1334"><a href="#cb12-1334" aria-hidden="true" tabindex="-1"></a><span class="co"># General MLE with numerical optimization</span></span>
<span id="cb12-1335"><a href="#cb12-1335" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_likelihood(params, data, dist_logpdf):</span>
<span id="cb12-1336"><a href="#cb12-1336" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generic negative log-likelihood using log-pdf for numerical stability"""</span></span>
<span id="cb12-1337"><a href="#cb12-1337" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(dist_logpdf(data, <span class="op">*</span>params))</span>
<span id="cb12-1338"><a href="#cb12-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1339"><a href="#cb12-1339" aria-hidden="true" tabindex="-1"></a><span class="co"># Using scipy.optimize</span></span>
<span id="cb12-1340"><a href="#cb12-1340" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> stats.gamma.rvs(a<span class="op">=</span><span class="dv">3</span>, scale<span class="op">=</span><span class="dv">2</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb12-1341"><a href="#cb12-1341" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> optimize.minimize(</span>
<span id="cb12-1342"><a href="#cb12-1342" aria-hidden="true" tabindex="-1"></a>    fun<span class="op">=</span><span class="kw">lambda</span> params: neg_log_likelihood(params, data, </span>
<span id="cb12-1343"><a href="#cb12-1343" aria-hidden="true" tabindex="-1"></a>                                         <span class="kw">lambda</span> x, a, b: stats.gamma.logpdf(x, a, scale<span class="op">=</span>b)),</span>
<span id="cb12-1344"><a href="#cb12-1344" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>[<span class="fl">1.0</span>, <span class="fl">1.0</span>],  <span class="co"># Initial guess</span></span>
<span id="cb12-1345"><a href="#cb12-1345" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb12-1346"><a href="#cb12-1346" aria-hidden="true" tabindex="-1"></a>    bounds<span class="op">=</span>[(<span class="fl">0.01</span>, <span class="va">None</span>), (<span class="fl">0.01</span>, <span class="va">None</span>)]  <span class="co"># Parameters must be positive</span></span>
<span id="cb12-1347"><a href="#cb12-1347" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-1348"><a href="#cb12-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1349"><a href="#cb12-1349" aria-hidden="true" tabindex="-1"></a><span class="co"># Using JAX for automatic differentiation</span></span>
<span id="cb12-1350"><a href="#cb12-1350" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_lik_jax(params, data):</span>
<span id="cb12-1351"><a href="#cb12-1351" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> params</span>
<span id="cb12-1352"><a href="#cb12-1352" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use JAX's scipy.stats</span></span>
<span id="cb12-1353"><a href="#cb12-1353" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>jnp.<span class="bu">sum</span>(jax.scipy.stats.gamma.logpdf(data, alpha, scale<span class="op">=</span>beta))</span>
<span id="cb12-1354"><a href="#cb12-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1355"><a href="#cb12-1355" aria-hidden="true" tabindex="-1"></a><span class="co"># Get gradient automatically</span></span>
<span id="cb12-1356"><a href="#cb12-1356" aria-hidden="true" tabindex="-1"></a>grad_nll <span class="op">=</span> jax.grad(neg_log_lik_jax)</span>
<span id="cb12-1357"><a href="#cb12-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1358"><a href="#cb12-1358" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize with gradient</span></span>
<span id="cb12-1359"><a href="#cb12-1359" aria-hidden="true" tabindex="-1"></a>result_jax <span class="op">=</span> optimize.minimize(</span>
<span id="cb12-1360"><a href="#cb12-1360" aria-hidden="true" tabindex="-1"></a>    fun<span class="op">=</span><span class="kw">lambda</span> p: <span class="bu">float</span>(neg_log_lik_jax(jnp.array(p), data)),</span>
<span id="cb12-1361"><a href="#cb12-1361" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">1.0</span>]),</span>
<span id="cb12-1362"><a href="#cb12-1362" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb12-1363"><a href="#cb12-1363" aria-hidden="true" tabindex="-1"></a>    jac<span class="op">=</span><span class="kw">lambda</span> p: np.array(grad_nll(jnp.array(p), data)),</span>
<span id="cb12-1364"><a href="#cb12-1364" aria-hidden="true" tabindex="-1"></a>    bounds<span class="op">=</span>[(<span class="fl">0.01</span>, <span class="va">None</span>), (<span class="fl">0.01</span>, <span class="va">None</span>)]</span>
<span id="cb12-1365"><a href="#cb12-1365" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-1366"><a href="#cb12-1366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1367"><a href="#cb12-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1368"><a href="#cb12-1368" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb12-1369"><a href="#cb12-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1370"><a href="#cb12-1370" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb12-1371"><a href="#cb12-1371" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb12-1372"><a href="#cb12-1372" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of Moments estimation</span></span>
<span id="cb12-1373"><a href="#cb12-1373" aria-hidden="true" tabindex="-1"></a>method_of_moments_normal <span class="ot">&lt;-</span> <span class="cf">function</span>(data) {</span>
<span id="cb12-1374"><a href="#cb12-1374" aria-hidden="true" tabindex="-1"></a>  mu_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(data)</span>
<span id="cb12-1375"><a href="#cb12-1375" aria-hidden="true" tabindex="-1"></a>  sigma2_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>((data <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb12-1376"><a href="#cb12-1376" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="at">mu =</span> mu_hat, <span class="at">sigma =</span> <span class="fu">sqrt</span>(sigma2_hat))</span>
<span id="cb12-1377"><a href="#cb12-1377" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-1378"><a href="#cb12-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1379"><a href="#cb12-1379" aria-hidden="true" tabindex="-1"></a><span class="co"># Maximum Likelihood with built-in functions</span></span>
<span id="cb12-1380"><a href="#cb12-1380" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)  <span class="co"># For fitdistr</span></span>
<span id="cb12-1381"><a href="#cb12-1381" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">100</span>, <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">scale =</span> <span class="dv">2</span>)</span>
<span id="cb12-1382"><a href="#cb12-1382" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: fitdistr estimates shape and rate (where rate = 1/scale)</span></span>
<span id="cb12-1383"><a href="#cb12-1383" aria-hidden="true" tabindex="-1"></a>mle_fit <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(data, <span class="st">"gamma"</span>)</span>
<span id="cb12-1384"><a href="#cb12-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1385"><a href="#cb12-1385" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual MLE with optim</span></span>
<span id="cb12-1386"><a href="#cb12-1386" aria-hidden="true" tabindex="-1"></a>neg_log_likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(params, data) {</span>
<span id="cb12-1387"><a href="#cb12-1387" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> params[<span class="dv">1</span>]</span>
<span id="cb12-1388"><a href="#cb12-1388" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> params[<span class="dv">2</span>]</span>
<span id="cb12-1389"><a href="#cb12-1389" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (alpha <span class="sc">&lt;=</span> <span class="dv">0</span> <span class="sc">||</span> beta <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="cn">Inf</span>)</span>
<span id="cb12-1390"><a href="#cb12-1390" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dgamma</span>(data, <span class="at">shape =</span> alpha, <span class="at">scale =</span> beta, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb12-1391"><a href="#cb12-1391" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-1392"><a href="#cb12-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1393"><a href="#cb12-1393" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize</span></span>
<span id="cb12-1394"><a href="#cb12-1394" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb12-1395"><a href="#cb12-1395" aria-hidden="true" tabindex="-1"></a>  <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),  <span class="co"># Initial values</span></span>
<span id="cb12-1396"><a href="#cb12-1396" aria-hidden="true" tabindex="-1"></a>  <span class="at">fn =</span> neg_log_likelihood,</span>
<span id="cb12-1397"><a href="#cb12-1397" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> data,</span>
<span id="cb12-1398"><a href="#cb12-1398" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb12-1399"><a href="#cb12-1399" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.01</span>)  <span class="co"># Lower bounds</span></span>
<span id="cb12-1400"><a href="#cb12-1400" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-1401"><a href="#cb12-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1402"><a href="#cb12-1402" aria-hidden="true" tabindex="-1"></a><span class="co"># Using gradient information (numerical approximation)</span></span>
<span id="cb12-1403"><a href="#cb12-1403" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span>
<span id="cb12-1404"><a href="#cb12-1404" aria-hidden="true" tabindex="-1"></a>result_with_grad <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb12-1405"><a href="#cb12-1405" aria-hidden="true" tabindex="-1"></a>  <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb12-1406"><a href="#cb12-1406" aria-hidden="true" tabindex="-1"></a>  <span class="at">fn =</span> neg_log_likelihood,</span>
<span id="cb12-1407"><a href="#cb12-1407" aria-hidden="true" tabindex="-1"></a>  <span class="at">gr =</span> <span class="cf">function</span>(p, data) <span class="fu">grad</span>(neg_log_likelihood, p, <span class="at">data =</span> data),</span>
<span id="cb12-1408"><a href="#cb12-1408" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> data,</span>
<span id="cb12-1409"><a href="#cb12-1409" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb12-1410"><a href="#cb12-1410" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.01</span>)</span>
<span id="cb12-1411"><a href="#cb12-1411" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-1412"><a href="#cb12-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1413"><a href="#cb12-1413" aria-hidden="true" tabindex="-1"></a><span class="co"># For more complex models, use specialized packages</span></span>
<span id="cb12-1414"><a href="#cb12-1414" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bbmle)  <span class="co"># For mle2 function</span></span>
<span id="cb12-1415"><a href="#cb12-1415" aria-hidden="true" tabindex="-1"></a>mle2_fit <span class="ot">&lt;-</span> <span class="fu">mle2</span>(</span>
<span id="cb12-1416"><a href="#cb12-1416" aria-hidden="true" tabindex="-1"></a>  <span class="at">minuslogl =</span> <span class="cf">function</span>(shape, scale) {</span>
<span id="cb12-1417"><a href="#cb12-1417" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dgamma</span>(data, <span class="at">shape =</span> shape, <span class="at">scale =</span> scale, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb12-1418"><a href="#cb12-1418" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb12-1419"><a href="#cb12-1419" aria-hidden="true" tabindex="-1"></a>  <span class="at">start =</span> <span class="fu">list</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="at">scale =</span> <span class="dv">1</span>),</span>
<span id="cb12-1420"><a href="#cb12-1420" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb12-1421"><a href="#cb12-1421" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="at">shape =</span> <span class="fl">0.01</span>, <span class="at">scale =</span> <span class="fl">0.01</span>)</span>
<span id="cb12-1422"><a href="#cb12-1422" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-1423"><a href="#cb12-1423" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1424"><a href="#cb12-1424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1425"><a href="#cb12-1425" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1426"><a href="#cb12-1426" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1427"><a href="#cb12-1427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1428"><a href="#cb12-1428" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb12-1429"><a href="#cb12-1429" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-1430"><a href="#cb12-1430" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb12-1431"><a href="#cb12-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1432"><a href="#cb12-1432" aria-hidden="true" tabindex="-1"></a>Python and R code examples for this chapter can be found in the HTML version of these notes.</span>
<span id="cb12-1433"><a href="#cb12-1433" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1434"><a href="#cb12-1434" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1435"><a href="#cb12-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1436"><a href="#cb12-1436" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb12-1437"><a href="#cb12-1437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1438"><a href="#cb12-1438" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-1439"><a href="#cb12-1439" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb12-1440"><a href="#cb12-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1441"><a href="#cb12-1441" aria-hidden="true" tabindex="-1"></a>This table maps sections in these lecture notes to the corresponding sections in @wasserman2013all ("All of Statistics" or AoS).</span>
<span id="cb12-1442"><a href="#cb12-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1443"><a href="#cb12-1443" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding AoS Section(s) |</span>
<span id="cb12-1444"><a href="#cb12-1444" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb12-1445"><a href="#cb12-1445" aria-hidden="true" tabindex="-1"></a>| **Introduction: Machine Learning As Statistical Estimation** | Expanded motivation from the slides and general context from AoS §9 introduction. |</span>
<span id="cb12-1446"><a href="#cb12-1446" aria-hidden="true" tabindex="-1"></a>| **Parametric Models** | AoS §9 (Introduction), AoS §9.1 (Parameter of Interest). The Gamma distribution example is from AoS Example 9.2. |</span>
<span id="cb12-1447"><a href="#cb12-1447" aria-hidden="true" tabindex="-1"></a>| **The Method of Moments (MoM)** | |</span>
<span id="cb12-1448"><a href="#cb12-1448" aria-hidden="true" tabindex="-1"></a>| ↳ The Principle: Matching Moments | AoS §9.2 (Definition 9.3). |</span>
<span id="cb12-1449"><a href="#cb12-1449" aria-hidden="true" tabindex="-1"></a>| ↳ MoM Examples (Bernoulli, Normal, Gamma) | AoS §9.2 (Examples 9.4, 9.5). The Gamma example is from AoS §9.14 (Exercise 1). |</span>
<span id="cb12-1450"><a href="#cb12-1450" aria-hidden="true" tabindex="-1"></a>| ↳ Properties of MoM Estimator | AoS §9.2 (Theorem 9.6). |</span>
<span id="cb12-1451"><a href="#cb12-1451" aria-hidden="true" tabindex="-1"></a>| **Maximum Likelihood Estimation (MLE)** | |</span>
<span id="cb12-1452"><a href="#cb12-1452" aria-hidden="true" tabindex="-1"></a>| ↳ The Principle and Likelihood Function | AoS §9.3 (Definitions 9.7, 9.8). |</span>
<span id="cb12-1453"><a href="#cb12-1453" aria-hidden="true" tabindex="-1"></a>| ↳ Finding the MLE Analytically | |</span>
<span id="cb12-1454"><a href="#cb12-1454" aria-hidden="true" tabindex="-1"></a>| ↳↳ General approach &amp; examples (Bernoulli, Normal) | AoS §9.3 (Remark 9.9, Examples 9.10, 9.11). |</span>
<span id="cb12-1455"><a href="#cb12-1455" aria-hidden="true" tabindex="-1"></a>| ↳↳ Harder case: Uniform(0, θ) | AoS §9.3 (Example 9.12). |</span>
<span id="cb12-1456"><a href="#cb12-1456" aria-hidden="true" tabindex="-1"></a>| **MLE Via Numerical Optimization** | Expanded material from the slides and AoS §9.13.4 (Appendix), with a modern ML focus. |</span>
<span id="cb12-1457"><a href="#cb12-1457" aria-hidden="true" tabindex="-1"></a>| ↳ The Need for Numerical Methods &amp; Setup | General optimization concepts, related to AoS §9.13.4. |</span>
<span id="cb12-1458"><a href="#cb12-1458" aria-hidden="true" tabindex="-1"></a>| ↳ Gradient-Based Optimization | Concepts related to Newton-Raphson from AoS §9.13.4, but expanded with modern methods (SGD, Adam, etc.). |</span>
<span id="cb12-1459"><a href="#cb12-1459" aria-hidden="true" tabindex="-1"></a>| ↳ Example: 2D MLE for Gamma distribution | Practical application. The Gamma MoM is from AoS §9.14 (Exercise 1), serving as a starting point. |</span>
<span id="cb12-1460"><a href="#cb12-1460" aria-hidden="true" tabindex="-1"></a>| **Chapter Summary and Connections** | New summary material. |</span>
<span id="cb12-1461"><a href="#cb12-1461" aria-hidden="true" tabindex="-1"></a>| ↳ Self-Test Problems | Problems inspired by AoS §9.14 (Exercises 2a, 5). |</span>
<span id="cb12-1462"><a href="#cb12-1462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1463"><a href="#cb12-1463" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1464"><a href="#cb12-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1465"><a href="#cb12-1465" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Materials</span></span>
<span id="cb12-1466"><a href="#cb12-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1467"><a href="#cb12-1467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bayesian Optimization**: An interactive exploration on <span class="co">[</span><span class="ot">Distill</span><span class="co">](https://distill.pub/2020/bayesian-optimization/)</span> (Agnihotri &amp; Batra; 2020)</span>
<span id="cb12-1468"><a href="#cb12-1468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classic reference**: Casella &amp; Berger, "Statistical Inference", Chapter 7</span>
<span id="cb12-1469"><a href="#cb12-1469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Modern perspective**: Efron &amp; Hastie, "Computer Age Statistical Inference", Chapter 4</span>
<span id="cb12-1470"><a href="#cb12-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1471"><a href="#cb12-1471" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1472"><a href="#cb12-1472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1473"><a href="#cb12-1473" aria-hidden="true" tabindex="-1"></a>*Remember: Parametric inference is about making assumptions (choosing a model) and then finding the best parameters within that model. The Method of Moments is simple but not optimal. Maximum Likelihood is the gold standard but often requires numerical optimization. Master these concepts -- they're the foundation of statistical modeling and machine learning!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>