<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 2&nbsp; Expectation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/03-convergence-inference.html" rel="next">
<link href="../chapters/01-probability-foundations.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-expectation.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">2.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-and-motivation" id="toc-introduction-and-motivation" class="nav-link" data-scroll-target="#introduction-and-motivation"><span class="header-section-number">2.2</span> Introduction and Motivation</a>
  <ul class="collapse">
  <li><a href="#the-essence-of-supervised-machine-learning" id="toc-the-essence-of-supervised-machine-learning" class="nav-link" data-scroll-target="#the-essence-of-supervised-machine-learning"><span class="header-section-number">2.2.1</span> The Essence of Supervised Machine Learning</a></li>
  <li><a href="#why-expectation-matters-in-ml-and-beyond" id="toc-why-expectation-matters-in-ml-and-beyond" class="nav-link" data-scroll-target="#why-expectation-matters-in-ml-and-beyond"><span class="header-section-number">2.2.2</span> Why Expectation Matters in ML and Beyond</a></li>
  </ul></li>
  <li><a href="#foundations-of-expectation" id="toc-foundations-of-expectation" class="nav-link" data-scroll-target="#foundations-of-expectation"><span class="header-section-number">2.3</span> Foundations of Expectation</a>
  <ul class="collapse">
  <li><a href="#definition-and-basic-properties" id="toc-definition-and-basic-properties" class="nav-link" data-scroll-target="#definition-and-basic-properties"><span class="header-section-number">2.3.1</span> Definition and Basic Properties</a></li>
  <li><a href="#existence-of-expectation" id="toc-existence-of-expectation" class="nav-link" data-scroll-target="#existence-of-expectation"><span class="header-section-number">2.3.2</span> Existence of Expectation</a></li>
  <li><a href="#expectation-of-functions" id="toc-expectation-of-functions" class="nav-link" data-scroll-target="#expectation-of-functions"><span class="header-section-number">2.3.3</span> Expectation of Functions</a></li>
  </ul></li>
  <li><a href="#properties-of-expectation" id="toc-properties-of-expectation" class="nav-link" data-scroll-target="#properties-of-expectation"><span class="header-section-number">2.4</span> Properties of Expectation</a>
  <ul class="collapse">
  <li><a href="#the-linearity-property" id="toc-the-linearity-property" class="nav-link" data-scroll-target="#the-linearity-property"><span class="header-section-number">2.4.1</span> The Linearity Property</a></li>
  <li><a href="#applications-of-linearity" id="toc-applications-of-linearity" class="nav-link" data-scroll-target="#applications-of-linearity"><span class="header-section-number">2.4.2</span> Applications of Linearity</a></li>
  <li><a href="#independence-and-products" id="toc-independence-and-products" class="nav-link" data-scroll-target="#independence-and-products"><span class="header-section-number">2.4.3</span> Independence and Products</a></li>
  </ul></li>
  <li><a href="#variance-and-its-properties" id="toc-variance-and-its-properties" class="nav-link" data-scroll-target="#variance-and-its-properties"><span class="header-section-number">2.5</span> Variance and Its Properties</a>
  <ul class="collapse">
  <li><a href="#measuring-spread" id="toc-measuring-spread" class="nav-link" data-scroll-target="#measuring-spread"><span class="header-section-number">2.5.1</span> Measuring Spread</a></li>
  <li><a href="#sec-properties-of-variance" id="toc-sec-properties-of-variance" class="nav-link" data-scroll-target="#sec-properties-of-variance"><span class="header-section-number">2.5.2</span> Properties of Variance</a></li>
  </ul></li>
  <li><a href="#sample-mean-and-variance" id="toc-sample-mean-and-variance" class="nav-link" data-scroll-target="#sample-mean-and-variance"><span class="header-section-number">2.6</span> Sample Mean and Variance</a></li>
  <li><a href="#covariance-and-correlation" id="toc-covariance-and-correlation" class="nav-link" data-scroll-target="#covariance-and-correlation"><span class="header-section-number">2.7</span> Covariance and Correlation</a>
  <ul class="collapse">
  <li><a href="#linear-relationships" id="toc-linear-relationships" class="nav-link" data-scroll-target="#linear-relationships"><span class="header-section-number">2.7.1</span> Linear Relationships</a></li>
  <li><a href="#properties-of-covariance-and-correlation" id="toc-properties-of-covariance-and-correlation" class="nav-link" data-scroll-target="#properties-of-covariance-and-correlation"><span class="header-section-number">2.7.2</span> Properties of Covariance and Correlation</a></li>
  <li><a href="#variance-of-sums-general-case" id="toc-variance-of-sums-general-case" class="nav-link" data-scroll-target="#variance-of-sums-general-case"><span class="header-section-number">2.7.3</span> Variance of Sums (General Case)</a></li>
  </ul></li>
  <li><a href="#expectation-with-matrices" id="toc-expectation-with-matrices" class="nav-link" data-scroll-target="#expectation-with-matrices"><span class="header-section-number">2.8</span> Expectation with Matrices</a>
  <ul class="collapse">
  <li><a href="#random-vectors" id="toc-random-vectors" class="nav-link" data-scroll-target="#random-vectors"><span class="header-section-number">2.8.1</span> Random Vectors</a></li>
  <li><a href="#covariance-matrix-properties" id="toc-covariance-matrix-properties" class="nav-link" data-scroll-target="#covariance-matrix-properties"><span class="header-section-number">2.8.2</span> Covariance Matrix Properties</a></li>
  <li><a href="#linear-transformations" id="toc-linear-transformations" class="nav-link" data-scroll-target="#linear-transformations"><span class="header-section-number">2.8.3</span> Linear Transformations</a></li>
  <li><a href="#interpreting-the-covariance-matrix" id="toc-interpreting-the-covariance-matrix" class="nav-link" data-scroll-target="#interpreting-the-covariance-matrix"><span class="header-section-number">2.8.4</span> Interpreting the Covariance Matrix</a></li>
  </ul></li>
  <li><a href="#conditional-expectation" id="toc-conditional-expectation" class="nav-link" data-scroll-target="#conditional-expectation"><span class="header-section-number">2.9</span> Conditional Expectation</a>
  <ul class="collapse">
  <li><a href="#expectation-given-information" id="toc-expectation-given-information" class="nav-link" data-scroll-target="#expectation-given-information"><span class="header-section-number">2.9.1</span> Expectation Given Information</a></li>
  <li><a href="#properties-of-conditional-expectation" id="toc-properties-of-conditional-expectation" class="nav-link" data-scroll-target="#properties-of-conditional-expectation"><span class="header-section-number">2.9.2</span> Properties of Conditional Expectation</a></li>
  <li><a href="#conditional-variance" id="toc-conditional-variance" class="nav-link" data-scroll-target="#conditional-variance"><span class="header-section-number">2.9.3</span> Conditional Variance</a></li>
  </ul></li>
  <li><a href="#more-about-the-normal-distribution" id="toc-more-about-the-normal-distribution" class="nav-link" data-scroll-target="#more-about-the-normal-distribution"><span class="header-section-number">2.10</span> More About the Normal Distribution</a>
  <ul class="collapse">
  <li><a href="#quick-recap" id="toc-quick-recap" class="nav-link" data-scroll-target="#quick-recap"><span class="header-section-number">2.10.1</span> Quick Recap</a></li>
  <li><a href="#entropy-of-the-normal-distribution" id="toc-entropy-of-the-normal-distribution" class="nav-link" data-scroll-target="#entropy-of-the-normal-distribution"><span class="header-section-number">2.10.2</span> Entropy of the Normal Distribution</a></li>
  <li><a href="#multivariate-normal-properties" id="toc-multivariate-normal-properties" class="nav-link" data-scroll-target="#multivariate-normal-properties"><span class="header-section-number">2.10.3</span> Multivariate Normal Properties</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">2.11</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">2.11.1</span> Key Concepts Review</a></li>
  <li><a href="#why-these-concepts-matter" id="toc-why-these-concepts-matter" class="nav-link" data-scroll-target="#why-these-concepts-matter"><span class="header-section-number">2.11.2</span> Why These Concepts Matter</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">2.11.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">2.11.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">2.11.5</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">2.11.6</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">2.11.7</span> Connections to Source Material</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">2.11.8</span> Further Reading</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">2.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li>Explain the concept of expectation and its role in summarizing distributions and machine learning.</li>
<li>Apply key properties of expectation, especially its linearity, to simplify complex calculations.</li>
<li>Calculate and interpret variance, covariance, and correlation as measures of spread and linear dependence.</li>
<li>Extend expectation concepts to random vectors, including mean vectors and covariance matrices.</li>
<li>Define and apply conditional expectation and understand its key properties.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter covers expectation, variance, and related concepts essential for statistical inference. The material is adapted and expanded from Chapter 3 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span>, which interested readers are encouraged to consult directly.</p>
</div>
</div>
</section>
<section id="introduction-and-motivation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="introduction-and-motivation"><span class="header-section-number">2.2</span> Introduction and Motivation</h2>
<section id="the-essence-of-supervised-machine-learning" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="the-essence-of-supervised-machine-learning"><span class="header-section-number">2.2.1</span> The Essence of Supervised Machine Learning</h3>
<p>The fundamental goal of supervised machine learning is seemingly simple: train a model that makes successful predictions on new, unseen data. However, this goal hides a deeper challenge that lies at the heart of statistics.</p>
<p>When we train a model, we work with a finite training set: <span class="math display">X_1, \ldots, X_n \sim F_X</span></p>
<p>where <span class="math inline">F_X</span> is the data generating distribution. Our true objective is to find a model that minimizes the <strong>expected loss</strong>: <span class="math display">\mathbb{E}[L(X)]</span></p>
<p>over the entire distribution <span class="math inline">F_X</span>, where <span class="math inline">L(\cdot)</span> is some suitable loss function. But we can only compute the <strong>empirical loss</strong>: <span class="math display">\frac{1}{n} \sum_{i=1}^n L(X_i),</span></p>
<p>which is the loss function summed over the training data.</p>
<p>This gap between what we want (<em>expected</em> loss) and what we can calculate (<em>empirical</em> loss) is the central challenge of machine learning. The concept of expectation provides the mathematical framework to understand and bridge this gap.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255586-486-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-486-1" role="tab" aria-controls="tabset-1757255586-486-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-486-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-486-2" role="tab" aria-controls="tabset-1757255586-486-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-486-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-486-3" role="tab" aria-controls="tabset-1757255586-486-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255586-486-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255586-486-1-tab"><p><strong>Goal</strong>: Imagine training a neural network to classify
cat and dog images.</p><p>You have 10,000 training images, but your model needs to work on
millions of future images it’s never seen. When your model achieves 98%
accuracy on training data, that’s just the average over your specific
10,000 images. What you really care about is the accuracy over <em>all
possible</em> cat and dog images that exist or could exist.</p><p>This gap—between what we can measure (training performance) and what
we want (real-world performance)—is why expectation is central to
machine learning. Every loss function is secretly an expectation!</p><p>The <strong>cross-entropy loss</strong> used for classification tasks
measures how “surprised” your model is by the true labels. Lower
surprise = better predictions. The key insight: we minimize the
<em>average surprise</em> over our training data, hoping it approximates
the <em>expected surprise</em> over all possible data.</p></div><div id="tabset-1757255586-486-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-486-2-tab"><p><strong>Setup</strong>: We want to classify images as cats
(<span class="math inline">\(y=1\)</span>) or dogs
(<span class="math inline">\(y=0\)</span>).</p><p>Our model outputs:
<span class="math display">\[ \hat{p}(x) = \text{predicted probability that image } x \text{ is a cat.} \]</span></p><p><strong>Step 1: Define the loss for one example</strong></p><p>For a single image-label pair
<span class="math inline">\((x, y)\)</span>, the cross-entropy loss is:
<span class="math display">\[L(x, y) = -[y \log(\hat{p}(x)) + (1-y) \log(1-\hat{p}(x))]\]</span></p><p>This penalizes wrong predictions:</p><ul>
<li>If <span class="math inline">\(y = 1\)</span> (cat) but
<span class="math inline">\(\hat{p}(x) \approx 0\)</span>: large
loss</li>
<li>If <span class="math inline">\(y = 0\)</span> (dog) but
<span class="math inline">\(\hat{p}(x) \approx 1\)</span>: large
loss</li>
<li>Correct predictions → small loss</li>
</ul><p><strong>Step 2: The fundamental problem</strong></p><p>What we want to minimize (expected loss over all possible images):
<span class="math display">\[R_{\text{true}} = \mathbb{E}_{(X,Y)}[L(X, Y)]\]</span></p><p>What we can compute (average loss over training data):
<span class="math display">\[R_{\text{train}} = \frac{1}{n} \sum_{i=1}^n L(x_i, y_i)\]</span></p><p><strong>Step 3: The connection to expectation</strong></p><p>Notice that <span class="math inline">\(R_{\text{train}}\)</span> is
just the <em>sample mean</em> of the losses, while
<span class="math inline">\(R_{\text{true}}\)</span> is the
<em>expectation</em> of the loss. By the Law of Large Numbers:
<span class="math display">\[R_{\text{train}} \xrightarrow{n \to \infty} R_{\text{true}}\]</span></p><p><strong>This is why machine learning is fundamentally about
expectation!</strong></p></div><div id="tabset-1757255586-486-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-486-3-tab"><p>Let’s see cross-entropy loss in action with a simple cat/dog
classifier. We’ll simulate predictions and compute both the loss on a
small training set and the true expected loss over the entire
population.</p><p>Note that in this example we are not <em>training</em> a model. We
are given a model, and we want to compute its loss. What we see is how
close the empirical loss is to the expected loss as we change the
dataset size over which we compute the loss.</p><div id="2a47fb80" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a simple "classifier" that predicts cat probability based on </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># a single feature (e.g., "ear pointiness" from 0 to 1)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># True probabilities: cats have pointier ears</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> true_cat_probability(ear_pointiness):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Logistic function: more pointy → more likely cat</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">5</span> <span class="op">*</span> (ear_pointiness <span class="op">-</span> <span class="fl">0.5</span>)))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate population data</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>n_population <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>ear_pointiness <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, n_population)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>true_probs <span class="op">=</span> true_cat_probability(ear_pointiness)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample actual labels based on true probabilities</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> (np.random.random(n_population) <span class="op">&lt;</span> true_probs).astype(<span class="bu">int</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Our (imperfect) model's predictions</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_prediction(x):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Slightly wrong sigmoid (shifted and less steep)</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">3</span> <span class="op">*</span> (x <span class="op">-</span> <span class="fl">0.45</span>)))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute cross-entropy loss</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_loss(y_true, y_pred):</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Avoid log(0) with small epsilon</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.clip(y_pred, eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(y_true <span class="op">*</span> np.log(y_pred) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y_true) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> y_pred))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Show what training sees vs reality</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Left: Model predictions vs truth</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>x_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_plot, true_cat_probability(x_plot), <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'True P(cat|x)'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_plot, model_prediction(x_plot), <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'Model P̂(cat|x)'</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>ax1.scatter(ear_pointiness[:<span class="dv">100</span>], labels[:<span class="dv">100</span>], alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>[<span class="st">'red'</span> <span class="cf">if</span> l <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'blue'</span> <span class="cf">for</span> l <span class="kw">in</span> labels[:<span class="dv">100</span>]])</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Ear Pointiness'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Probability of Cat'</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Model vs Reality'</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: Empirical vs Expected Loss</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>training_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>]</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>empirical_losses <span class="op">=</span> []</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> training_sizes:</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample n training examples</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.random.choice(n_population, n, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> ear_pointiness[idx]</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    train_y <span class="op">=</span> labels[idx]</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    train_pred <span class="op">=</span> model_prediction(train_x)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Empirical loss on training set</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    emp_loss <span class="op">=</span> np.mean(cross_entropy_loss(train_y, train_pred))</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    empirical_losses.append(emp_loss)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co"># True expected loss over entire population</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>all_predictions <span class="op">=</span> model_prediction(ear_pointiness)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>expected_loss <span class="op">=</span> np.mean(cross_entropy_loss(labels, all_predictions))</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>ax2.semilogx(training_sizes, empirical_losses, <span class="st">'bo-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>             markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'Empirical Loss (Training)'</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span>expected_loss, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Expected Loss = </span><span class="sc">{</span>expected_loss<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Training Set Size'</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Cross-Entropy Loss'</span>)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Convergence to Expected Loss'</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"With just 10 samples: empirical loss = </span><span class="sc">{</span>empirical_losses[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"With 5000 samples: empirical loss = </span><span class="sc">{</span>empirical_losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"True expected loss: </span><span class="sc">{</span>expected_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">As we get more training data to calculate the loss, our empirical"</span>)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"loss estimate gets closer to the true expected loss we care about!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="02-expectation_files/figure-html/cell-2-output-1.png" width="661" height="467"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>With just 10 samples: empirical loss = 0.680
With 5000 samples: empirical loss = 0.535
True expected loss: 0.542

As we get more training data to calculate the loss, our empirical
loss estimate gets closer to the true expected loss we care about!</code></pre>
</div>
</div></div></div></div>
</section>
<section id="why-expectation-matters-in-ml-and-beyond" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="why-expectation-matters-in-ml-and-beyond"><span class="header-section-number">2.2.2</span> Why Expectation Matters in ML and Beyond</h3>
<p>The concept of expectation appears throughout data science and statistics:</p>
<ol type="1">
<li><strong>Statistical Inference</strong>: Estimating population parameters from samples</li>
<li><strong>Decision Theory</strong>: Maximizing expected utility or minimizing expected risk</li>
<li><strong>A/B Testing</strong>: Measuring expected treatment effects</li>
<li><strong>Financial Modeling</strong>: Expected returns and risk assessment</li>
<li><strong>Loss Functions in Deep Learning</strong>: Cross-entropy loss and ELBO in VAEs</li>
</ol>
<p>In each case, we’re trying to understand average behavior over some distribution, which is precisely what expectation captures.</p>
</section>
</section>
<section id="foundations-of-expectation" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="foundations-of-expectation"><span class="header-section-number">2.3</span> Foundations of Expectation</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here’s a reference table of key terms in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Expected value/Expectation</td>
<td>Odotusarvo</td>
<td>The mean of a distribution</td>
</tr>
<tr class="even">
<td>Mean</td>
<td>Keskiarvo</td>
<td>Same as expectation</td>
</tr>
<tr class="odd">
<td>Variance</td>
<td>Varianssi</td>
<td>Measure of spread</td>
</tr>
<tr class="even">
<td>Standard deviation</td>
<td>Keskihajonta</td>
<td>Square root of variance</td>
</tr>
<tr class="odd">
<td>Covariance</td>
<td>Kovarianssi</td>
<td>Linear relationship between variables</td>
</tr>
<tr class="even">
<td>Correlation</td>
<td>Korrelaatio</td>
<td>Standardized covariance</td>
</tr>
<tr class="odd">
<td>Sample mean</td>
<td>Otoskeskiarvo</td>
<td>Average of data points</td>
</tr>
<tr class="even">
<td>Sample variance</td>
<td>Otosvarianssi</td>
<td>Empirical measure of spread</td>
</tr>
<tr class="odd">
<td>Conditional expectation</td>
<td>Ehdollinen odotusarvo</td>
<td>Mean given information</td>
</tr>
<tr class="even">
<td>Moment</td>
<td>Momentti</td>
<td>Powers of random variable</td>
</tr>
<tr class="odd">
<td>Random vector</td>
<td>Satunnaisvektori</td>
<td>Vector of random variables</td>
</tr>
<tr class="even">
<td>Covariance matrix</td>
<td>Kovarianssimatriisi</td>
<td>Matrix of covariances</td>
</tr>
<tr class="odd">
<td>Precision matrix</td>
<td>Tarkkuusmatriisi</td>
<td>Inverse of covariance matrix</td>
</tr>
<tr class="even">
<td>Moment generating function</td>
<td>Momenttigeneroiva funktio</td>
<td>Transform for finding moments</td>
</tr>
<tr class="odd">
<td>Central moment</td>
<td>Keskusmomentti</td>
<td>Moment about the mean</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<section id="definition-and-basic-properties" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="definition-and-basic-properties"><span class="header-section-number">2.3.1</span> Definition and Basic Properties</h3>
<div class="definition">
<p>The <strong>expected value</strong>, or <strong>mean</strong>, or <strong>first moment</strong> of a random variable <span class="math inline">X</span> is defined to be: <span class="math display">\mathbb{E}(X) = \begin{cases}
\sum_x x \mathbb{P}(X = x) &amp; \text{if } X \text{ is discrete} \\
\int_{\mathbb{R}} x f_X(x) \, dx &amp; \text{if } X \text{ is continuous}
\end{cases}</span> assuming the sum (or integral) is well defined. We use the following notation interchangeably: <span class="math display">\mathbb{E}(X) = \mathbb{E} X = \mu = \mu_X</span></p>
</div>
<p>The expectation represents the average value of the distribution – the balance point where the distribution would balance if it were a physical object.</p>
<p><strong>Notation:</strong> The lowercase Greek letter <span class="math inline">\mu</span> (<a href="https://en.wikipedia.org/wiki/Mu_(letter)">mu</a>; pronounced <em>mju</em>) is universally used to denote the mean.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simplified Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this course, we will write the expectation using the simplified notation:</p>
<p><span class="math display">\mathbb{E}(X) = \int x f_X(x) dx</span></p>
<p>when the type of random variable is unspecified and could be either continuous or discrete.</p>
<p>For a discrete random variable, you would substitute the integral with a sum, and the PDF <span class="math inline">f_X(x)</span> (probability density function) with the PMF <span class="math inline">\mathbb{P}_X(x)</span> (probability mass function), as seen in Chapter 1 of the lecture notes.</p>
<p>Note that this is an abuse of notation and is not mathematically correct, but we found it to be more intuitive in previous iterations of the course.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Simple Expectations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s calculate expectations for some basic distributions:</p>
<ol type="1">
<li><p><strong>Bernoulli(0.3)</strong>: <span class="math display">\mathbb{E}(X) = 0 \times 0.7 + 1 \times 0.3 = 0.3</span></p></li>
<li><p><strong>Fair six-sided die</strong>: <span class="math display">\mathbb{E}(X) = \frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = \frac{21}{6} = 3.5</span></p></li>
<li><p><strong>Two coin flips</strong> (X = number of heads): <span class="math display">\mathbb{E}(X) = 0 \times \frac{1}{4} + 1 \times \frac{1}{2} + 2 \times \frac{1}{4} = 1</span></p></li>
</ol>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255586-948-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-948-1" role="tab" aria-controls="tabset-1757255586-948-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-948-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-948-2" role="tab" aria-controls="tabset-1757255586-948-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-948-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-948-3" role="tab" aria-controls="tabset-1757255586-948-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255586-948-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255586-948-1-tab"><p>Expectation is the “center of mass” of a distribution. Imagine:</p><ul>
<li><p><strong>Physical analogy</strong>: If you made a <a href="https://johncanning.net/wp/?p=1863">histogram out of metal</a>,
the expected value is where you’d place a fulcrum to balance it
perfectly.</p></li>
<li><p><strong>Long-run average</strong>: If you repeat an experiment
millions of times and average the results, you’ll get very close to the
expectation. This isn’t just intuition—it’s a theorem (the Law of Large
Numbers) we’ll prove in Chapter 3.</p></li>
<li><p><strong>Fair price</strong>: In gambling, the expectation tells
you the fair price to pay for a game. If a lottery ticket has expected
winnings of €2, then €2 is the break-even price.</p></li>
</ul><p>Think of expectation as answering: “If I had to summarize this entire
distribution with a single number pointing at its <em>center</em>, what
would it be?” The expectation or mean is not the <em>only</em> number we
could use to represent the center of a distribution, but it is a very
common choice suitable for most situations.</p></div><div id="tabset-1757255586-948-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-948-2-tab"><p>The expectation is a linear functional on the space of random
variables. For a random variable <span class="math inline">\(X\)</span>
with distribution function <span class="math inline">\(F\)</span>, the
correct mathematical notation would be:</p><p><span class="math display">\[\mathbb{E}(X) = \int x \, dF(x)\]</span></p><p>This notation correctly unifies the discrete and continuous
cases:</p><ul>
<li>For discrete <span class="math inline">\(X\)</span>: the integral
becomes a sum over the jump points of
<span class="math inline">\(F\)</span></li>
<li>For continuous <span class="math inline">\(X\)</span>: we have
<span class="math inline">\(dF(x) = f_X(x)dx\)</span></li>
</ul><p>This notation is particularly useful when dealing with mixed
distributions or when stating results that apply to both discrete and
continuous random variables without writing separate formulas. We won’t
be using this notation in the course, but you may find it in
mathematical or statistical textbooks, including Wasserman (2013).</p></div><div id="tabset-1757255586-948-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-948-3-tab"><p>Let’s demonstrate expectation through simulation, showing how sample
averages converge to the true expectation. We’ll also show a case where
expectation doesn’t exist (see next section).</p><div id="ac324a2f" class="cell" data-fig-height="8" data-fig-width="7" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> cauchy</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up figure with subplots</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convergence for Bernoulli(0.3)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>n_flips <span class="op">=</span> <span class="dv">40000</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> np.random.choice([<span class="dv">0</span>, <span class="dv">1</span>], size<span class="op">=</span>n_flips, p<span class="op">=</span>[<span class="dv">1</span><span class="op">-</span>p, p])</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>running_mean <span class="op">=</span> np.cumsum(flips) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_flips <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>ax1.plot(running_mean, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span>p, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'True E[X] = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Number of trials'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Sample Mean Converges to Expectation: Bernoulli(0.3)'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="fl">0.2</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Cauchy distribution - no expectation exists</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">40000</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>cauchy_samples <span class="op">=</span> cauchy.rvs(size<span class="op">=</span>n_samples, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>cauchy_running_mean <span class="op">=</span> np.cumsum(cauchy_samples) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_samples <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>ax2.plot(cauchy_running_mean, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Number of samples'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Cauchy Distribution: Sample Mean Does Not Converge (No Expectation)'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bernoulli: After </span><span class="sc">{</span>n_flips<span class="sc">}</span><span class="ss"> flips, sample mean = </span><span class="sc">{</span>running_mean[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cauchy: After </span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss"> samples, sample mean = </span><span class="sc">{</span>cauchy_running_mean[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Notice how Cauchy's sample mean keeps eventually jumping around,"</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"even when you think it's converging to zero!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="02-expectation_files/figure-html/cell-3-output-1.png" width="663" height="756"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Bernoulli: After 40000 flips, sample mean = 0.2969
Cauchy: After 40000 samples, sample mean = -2.9548
Notice how Cauchy's sample mean keeps eventually jumping around,
even when you think it's converging to zero!</code></pre>
</div>
</div></div></div></div>
</section>
<section id="existence-of-expectation" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="existence-of-expectation"><span class="header-section-number">2.3.2</span> Existence of Expectation</h3>
<p>Not all random variables have well-defined expectations.</p>
<div class="definition">
<p>The expectation <span class="math inline">\mathbb{E}(X)</span> exists if and only if: <span class="math display">\mathbb{E}(|X|) = \int |x| f_X(x) \, dx &lt; \infty</span></p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Cauchy Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> is a classic example of probability density with no expectation:</p>
<p><span class="math display">f_X(x) = \frac{1}{\pi(1 + x^2)}</span></p>
<p>To check if expectation exists: <span class="math display">\int_{-\infty}^{\infty} |x| \cdot \frac{1}{\pi(1 + x^2)} \, dx = \frac{2}{\pi} \int_0^{\infty} \frac{x}{1 + x^2} \, dx = \infty</span></p>
<p>The integral diverges! This means:</p>
<ul>
<li>Sample averages don’t converge to any value</li>
<li>The Law of Large Numbers doesn’t apply</li>
<li>Extreme observations are common due to heavy tails</li>
</ul>
</div>
</div>
</section>
<section id="expectation-of-functions" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="expectation-of-functions"><span class="header-section-number">2.3.3</span> Expectation of Functions</h3>
<p>Often we need the expectation of a function of a random variable. The “Rule of the Lazy Statistician” saves us from finding the distribution of the transformed variable.</p>
<div class="theorem" name="Rule of the Lazy Statistician">
<p>Let <span class="math inline">Y = r(X)</span>. Then: <span class="math display">\mathbb{E}(Y) = \mathbb{E}(r(X)) = \int r(x) f_X(x) \, dx</span></p>
</div>
<p>This result is incredibly useful—we can find <span class="math inline">\mathbb{E}(Y)</span> without determining <span class="math inline">f_Y(y)</span>!</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Breaking a Stick
</div>
</div>
<div class="callout-body-container callout-body">
<p>A stick of unit length is broken at a random point. What’s the expected length of the longer piece?</p>
<p>Let <span class="math inline">X \sim \text{Uniform}(0,1)</span> be the break point. The longer piece has length: <span class="math display">Y = r(X) = \max\{X, 1-X\}</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can identify that:</p>
<ul>
<li>If <span class="math inline">X &lt; 1/2</span>: longer piece = <span class="math inline">1-X</span></li>
<li>If <span class="math inline">X \geq 1/2</span>: longer piece = <span class="math inline">X</span></li>
</ul>
<p>Therefore:</p>
<p><span class="math display">\mathbb{E}(Y) = \int_0^{1/2} (1-x) \cdot 1 \, dx + \int_{1/2}^1 x \cdot 1 \, dx</span> <span class="math display">= \left[x - \frac{x^2}{2}\right]_0^{1/2} + \left[\frac{x^2}{2}\right]_{1/2}^1</span> <span class="math display">= \left(\frac{1}{2} - \frac{1}{8}\right) + \left(\frac{1}{2} - \frac{1}{8}\right) = \frac{3}{4}</span></p>
<div id="399cb8de" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="3">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the breaking stick problem</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>longer_piece <span class="op">=</span> np.maximum(x, <span class="dv">1</span><span class="op">-</span>x)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, longer_piece, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Length of longer piece'</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, <span class="dv">0</span>, longer_piece, alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'lightblue'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.75</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="st">'E[longer piece] = 3/4'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Break point (X)'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Length of longer piece'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Breaking a Unit Stick: Expected Length of Longer Piece'</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-expectation_files/figure-html/cell-4-output-1.png" width="661" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Exponential Prize Game
</div>
</div>
<div class="callout-body-container callout-body">
<p>A game show offers a prize based on rolling a die: if you roll <span class="math inline">X</span>, you win <span class="math inline">2^X</span> euros. What’s your expected winnings?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using the lazy statistician’s rule with <span class="math inline">X \sim \text{DiscreteUniform}(1,6)</span>: <span class="math display">\mathbb{E}(2^X) = \sum_{x=1}^6 2^x \cdot \frac{1}{6} = \frac{1}{6}(2^1 + 2^2 + \cdots + 2^6)</span></p>
<p>Direct calculation: <span class="math display">= \frac{1}{6}(2 + 4 + 8 + 16 + 32 + 64) = \frac{126}{6} = 21</span></p>
<p>So you expect to win €21 on average.</p>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Special case</strong>: Probability as expectation of indicator functions.</p>
<p>If <span class="math inline">A</span> is an event and the indicator function is defined as: <span class="math display">I_A(x) = \begin{cases}
1 &amp; \text{if } x \in A \\
0 &amp; \text{if } x \notin A
\end{cases}</span> then: <span class="math display">\mathbb{E}(I_A(X)) = \mathbb{P}(X \in A)</span></p>
<p>This shows that probability is just a special case of expectation!</p>
<p>This trick of using the indicator function with expectations is used commonly in probability, statistics and machine learning.</p>
</div>
</div>
</section>
</section>
<section id="properties-of-expectation" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="properties-of-expectation"><span class="header-section-number">2.4</span> Properties of Expectation</h2>
<section id="the-linearity-property" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="the-linearity-property"><span class="header-section-number">2.4.1</span> The Linearity Property</h3>
<div class="theorem">
<p>If <span class="math inline">X_1, \ldots, X_n</span> are random variables and <span class="math inline">a_1, \ldots, a_n</span> are constants, then: <span class="math display">\mathbb{E}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i \mathbb{E}(X_i)</span></p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Possibly The Most Important Result in This Course
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Expectation is LINEAR!</strong></p>
<p>This property:</p>
<ul>
<li>Works WITHOUT independence (unlike the product rule)</li>
<li>Simplifies hard calculations</li>
<li>Is the key to understanding sampling distributions</li>
<li>Will be used in almost every proof and application</li>
</ul>
<p>If you remember only one thing from this chapter, remember that expectation is linear. You’ll use it constantly throughout statistics and machine learning!</p>
</div>
</div>
</section>
<section id="applications-of-linearity" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="applications-of-linearity"><span class="header-section-number">2.4.2</span> Applications of Linearity</h3>
<p>The power of linearity becomes clear when we use it to solve problems that would be difficult otherwise.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Binomial Mean via Indicator Decomposition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X \sim \text{Binomial}(n, p)</span>. Finding <span class="math inline">\mathbb{E}(X)</span> directly requires evaluating: <span class="math display">\mathbb{E}(X) = \sum_{x=0}^n x \binom{n}{x} p^x (1-p)^{n-x}</span></p>
<p>Have fun calculating this! But with linearity, it’s trivial.</p>
<p>Remember that <span class="math inline">\text{Binomial}(n, p)</span> is the distribution of the sum of <span class="math inline">n</span> <span class="math inline">\text{Bernoulli}(p)</span> random variables.</p>
<p>Thus, we can write <span class="math inline">X = \sum_{i=1}^n X_i</span>, where <span class="math inline">X_i</span> are independent Bernoulli(<span class="math inline">p</span>) indicators.</p>
<p>With this, we have: <span class="math display">\mathbb{E}(X) = \mathbb{E}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathbb{E}(X_i) = \sum_{i=1}^n p = np</span></p>
<p>Done!</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Linearity Works Even with Dependent Variables
</div>
</div>
<div class="callout-body-container callout-body">
<p>A common misconception is that linearity of expectation requires independence. It doesn’t! Let’s demonstrate this crucial fact by computing <span class="math inline">\mathbb{E}[2X + 3Y]</span> where <span class="math inline">X</span> and <span class="math inline">Y</span> are strongly correlated.</p>
<p>We’ll generate <span class="math inline">X</span> and <span class="math inline">Y</span> with correlation 0.8 (highly dependent!) and verify that linearity still holds:</p>
<div id="1b33233a" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="4">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrating linearity even with dependence</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>n_sims <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate correlated X and Y</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">0.8</span>], [<span class="fl">0.8</span>, <span class="dv">1</span>]]  <span class="co"># Correlation = 0.8</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.multivariate_normal(mean, cov, n_sims)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> samples[:, <span class="dv">0</span>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> samples[:, <span class="dv">1</span>]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute E[2X + 3Y] empirically</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>X <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>Y</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>empirical_mean <span class="op">=</span> np.mean(Z)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical value using linearity</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>theoretical_mean <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>mean[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>mean[<span class="dv">1</span>]</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>plt.hist(Z, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'green'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>plt.axvline(empirical_mean, color<span class="op">=</span><span class="st">'blue'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Empirical: </span><span class="sc">{</span>empirical_mean<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>plt.axvline(theoretical_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Theoretical: </span><span class="sc">{</span>theoretical_mean<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'2X + 3Y'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Linearity of Expectation Works Even with Dependent Variables!'</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X and Y are dependent (correlation = 0.8)"</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"But E[2X + 3Y] = 2E[X] + 3E[Y] still holds!"</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical: 2×</span><span class="sc">{</span>mean[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> + 3×</span><span class="sc">{</span>mean[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>theoretical_mean<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical: </span><span class="sc">{</span>empirical_mean<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-expectation_files/figure-html/cell-5-output-1.png" width="661" height="372" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>X and Y are dependent (correlation = 0.8)
But E[2X + 3Y] = 2E[X] + 3E[Y] still holds!
Theoretical: 2×2 + 3×3 = 13
Empirical: 12.985</code></pre>
</div>
</div>
<p><strong>Key takeaway</strong>: Despite the strong correlation between <span class="math inline">X</span> and <span class="math inline">Y</span>, the empirical mean of <span class="math inline">2X + 3Y</span> matches the theoretical value <span class="math inline">2\mathbb{E}[X] + 3\mathbb{E}[Y]</span> perfectly. This is why linearity of expectation is so powerful—it works unconditionally!</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Additional Examples: More Applications of Linearity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Expected Number of Fixed Points in Random Permutation:</strong></p>
<p>In a random permutation of {1, 2, …, n}, what’s the expected number of elements that stay in their original position?</p>
<p>Let <span class="math inline">X_i = 1</span> if element <span class="math inline">i</span> stays in position <span class="math inline">i</span>, and 0 otherwise. The total number of fixed points is <span class="math inline">X = \sum_{i=1}^n X_i</span>.</p>
<p>For any position <span class="math inline">i</span>: <span class="math inline">\mathbb{P}(X_i = 1) = \frac{1}{n}</span> (element <span class="math inline">i</span> has probability <span class="math inline">1/n</span> of being in position <span class="math inline">i</span>).</p>
<p>Therefore: <span class="math inline">\mathbb{E}(X_i) = \frac{1}{n}</span></p>
<p>By linearity: <span class="math display">\mathbb{E}(X) = \sum_{i=1}^n \mathbb{E}(X_i) = \sum_{i=1}^n \frac{1}{n} = 1</span></p>
<p>Amazing! No matter how large <span class="math inline">n</span> is, we expect exactly 1 fixed point on average.</p>
<p><strong>Fortune Doubling Game (from Wasserman Exercise 3.1):</strong></p>
<p>You start with <span class="math inline">c</span> dollars. On each play, you either double your money or halve it, each with probability 1/2. What’s your expected fortune after <span class="math inline">n</span> plays?</p>
<p>Let <span class="math inline">X_i</span> be your fortune after <span class="math inline">i</span> plays. Then: - <span class="math inline">X_0 = c</span> - <span class="math inline">X_{i+1} = 2X_i</span> with probability 1/2 - <span class="math inline">X_{i+1} = X_i/2</span> with probability 1/2</p>
<p>Using conditional expectation: <span class="math display">\mathbb{E}(X_{i+1} | X_i) = \frac{1}{2}(2X_i) + \frac{1}{2}\left(\frac{X_i}{2}\right) = X_i + \frac{X_i}{4} = X_i</span></p>
<p>By the law of iterated expectations: <span class="math display">\mathbb{E}(X_{i+1}) = \mathbb{E}[\mathbb{E}(X_{i+1} | X_i)] = \mathbb{E}(X_i)</span></p>
<p>Therefore, by induction: <span class="math inline">\mathbb{E}(X_n) = \mathbb{E}(X_0) = c</span></p>
<p>Your expected fortune never changes! This is an example of a <em>martingale</em>—a fair game where the expected future value equals the current value.</p>
</div>
</div>
</div>
</section>
<section id="independence-and-products" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="independence-and-products"><span class="header-section-number">2.4.3</span> Independence and Products</h3>
<p>While expectation is linear for all random variables, products require independence.</p>
<div class="theorem">
<p>If <span class="math inline">X_1, \ldots, X_n</span> are <strong>independent</strong> random variables, then: <span class="math display">\mathbb{E}\left(\prod_{i=1}^n X_i\right) = \prod_{i=1}^n \mathbb{E}(X_i)</span></p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This ONLY works with <strong>independent</strong> random variables! As a clear counterexample, <span class="math inline">\mathbb{E}(X^2) \neq (\mathbb{E}(X))^2</span> in general, since <span class="math inline">X</span> and <span class="math inline">X</span> are clearly not independent.</p>
</div>
</div>
</section>
</section>
<section id="variance-and-its-properties" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="variance-and-its-properties"><span class="header-section-number">2.5</span> Variance and Its Properties</h2>
<section id="measuring-spread" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="measuring-spread"><span class="header-section-number">2.5.1</span> Measuring Spread</h3>
<p>While expectation tells us the center of a distribution, variance measures how “spread out” it is.</p>
<div class="definition">
<p>Let <span class="math inline">X</span> be a random variable with mean <span class="math inline">\mu</span>. The <strong>variance</strong> of <span class="math inline">X</span> – denoted by <span class="math inline">\sigma^2</span>, <span class="math inline">\sigma_X^2</span>, <span class="math inline">\mathbb{V}(X)</span>, <span class="math inline">\mathbb{V}X</span> or <span class="math inline">\text{Var}(X)</span> – is defined as: <span class="math display">\sigma^2 = \mathbb{V}(X) = \mathbb{E}[(X - \mu)^2]</span> assuming this expectation exists. The <strong>standard deviation</strong> is</p>
<p><span class="math display">\mathrm{sd}(X) = \sqrt{\mathbb{V}(X)}</span></p>
<p>and is also denoted by <span class="math inline">\sigma</span> and <span class="math inline">\sigma_X</span>.</p>
</div>
<p><strong>Notation:</strong> The lowercase Greek letter <span class="math inline">\sigma</span> (<a href="https://en.wikipedia.org/wiki/Sigma">sigma</a>) is almost universally used to denote the standard deviation (and more generally the “scale” of a distribution, related to its spread).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Both Variance and Standard Deviation?
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Variance</strong> (<span class="math inline">\sigma^2</span>) is in squared units—if <span class="math inline">X</span> measures height in cm, then <span class="math inline">\mathbb{V}(X)</span> is in cm². This makes it hard to interpret directly.</p>
<p><strong>Standard deviation</strong> (<span class="math inline">\sigma</span>) is in the same units as <span class="math inline">X</span>, making it more interpretable: “typical deviation from the mean.”</p>
<p>So why use variance at all? Variance works better for doing math because:</p>
<ul>
<li>It has nicer properties (like additivity for independent variables, as we will see later)</li>
<li>It appears naturally in formulas and proofs</li>
<li>It’s easier to manipulate algebraically</li>
</ul>
<p>In short: we do calculations with variance, then take the square root for interpretation.</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255586-595-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-595-1" role="tab" aria-controls="tabset-1757255586-595-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-595-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-595-2" role="tab" aria-controls="tabset-1757255586-595-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-595-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-595-3" role="tab" aria-controls="tabset-1757255586-595-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255586-595-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255586-595-1-tab"><p>Think of variance as measuring <strong>how wrong your guess will
typically be</strong> if you always guess the mean.</p><p>Imagine predicting tomorrow’s temperature. If you live in Nice or
Lisbon (low variance), guessing the average temperature works well
year-round. If you live in Helsinki or Berlin (high variance), that same
strategy leads to large errors – you’ll be way off in both summer and
winter.</p><p><strong>Standard deviation</strong> puts this in interpretable
units:</p><ul>
<li>Low <span class="math inline">\(\sigma\)</span>: Your guesses are
usually close (precise manufacturing, stable processes)</li>
<li>High <span class="math inline">\(\sigma\)</span>: Your guesses are
often far off (volatile stocks, unpredictable weather)</li>
</ul><p>The famous <strong>68-95-99.7 rule</strong> tells us that for
bell-shaped data:</p><ul>
<li>68% of observations fall within
1<span class="math inline">\(\sigma\)</span> of the mean</li>
<li>95% fall within 2<span class="math inline">\(\sigma\)</span><br>
</li>
<li>99.7% fall within 3<span class="math inline">\(\sigma\)</span></li>
</ul><p>This is why “3-sigma events” are considered rare outliers in quality
control.</p><p>(This rule is <em>exactly</em> true for normally-distributed
data.)</p></div><div id="tabset-1757255586-595-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-595-2-tab"><p>Variance has an elegant mathematical interpretation as the
<strong>expected squared distance from the mean</strong>:
<span class="math display">\[\mathbb{V}(X) = \mathbb{E}[(X - \mu)^2]\]</span></p><p>This squared distance has deep connections:</p><ol type="1">
<li><p><strong>Minimization property</strong>: The mean
<span class="math inline">\(\mu\)</span> minimizes
<span class="math inline">\(\mathbb{E}[(X - c)^2]\)</span> over all
constants <span class="math inline">\(c\)</span></p></li>
<li><p><strong>Pythagorean theorem</strong>: For independent
<span class="math inline">\(X, Y\)</span>:
<span class="math display">\[\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y)\]</span>
Just like <span class="math inline">\(|a + b|^2 = |a|^2 + |b|^2\)</span>
for perpendicular vectors!</p></li>
<li><p><strong>Information theory</strong>: Variance of a Gaussian
determines its entropy (uncertainty)</p></li>
</ol><p>The quadratic nature (<span class="math inline">\(a^2\)</span>
scaling) reflects that variance measures <em>squared deviations</em>:
doubling the scale quadruples the variance.</p></div><div id="tabset-1757255586-595-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-595-3-tab"><p>Let’s visualize how variance controls the spread of a distribution,
using exam scores as an example.</p><div id="906732e1" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the plot</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Common mean for all distributions</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> <span class="dv">75</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">40</span>, <span class="dv">110</span>, <span class="dv">1000</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Three different standard deviations</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>sigmas <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#2E86AB'</span>, <span class="st">'#A23B72'</span>, <span class="st">'#F18F01'</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'σ = 5 (Low variance)'</span>, <span class="st">'σ = 10 (Medium variance)'</span>, <span class="st">'σ = 20 (High variance)'</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each distribution</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sigma, color, label <span class="kw">in</span> <span class="bu">zip</span>(sigmas, colors, labels):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> stats.norm.pdf(x, mean, sigma)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span>label)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shade ±1σ region</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    x_fill <span class="op">=</span> x[(x <span class="op">&gt;=</span> mean <span class="op">-</span> sigma) <span class="op">&amp;</span> (x <span class="op">&lt;=</span> mean <span class="op">+</span> sigma)]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    y_fill <span class="op">=</span> stats.norm.pdf(x_fill, mean, sigma)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(x_fill, y_fill, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span>color)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Add vertical line at mean</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax.axvline(mean, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>ax.text(mean <span class="op">+</span> <span class="dv">1</span>, <span class="fl">0.085</span>, <span class="st">'Mean = 75'</span>, ha<span class="op">=</span><span class="st">'left'</span>, va<span class="op">=</span><span class="st">'bottom'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Styling</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Exam Score'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Same Mean, Different Variances: The Effect of Standard Deviation'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">40</span>, <span class="dv">110</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.09</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Add annotations for interpretation</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">'68</span><span class="sc">% o</span><span class="st">f scores</span><span class="ch">\n</span><span class="st">within ±σ'</span>, xy<span class="op">=</span>(mean <span class="op">+</span> <span class="dv">5</span>, <span class="fl">0.055</span>), xytext<span class="op">=</span>(mean <span class="op">+</span> <span class="dv">12</span>, <span class="fl">0.065</span>),</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'#2E86AB'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>),</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>, ha<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'#2E86AB'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Interpreting the visualization:"</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"• Small σ (blue): Scores cluster tightly around 75. Most students perform similarly."</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"• Medium σ (pink): Moderate spread. Typical variation in a well-designed exam."</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"• Large σ (orange): Wide spread. Large differences in student performance."</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">For any normal distribution, about 68% of values fall within ±1σ of the mean."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="02-expectation_files/figure-html/cell-6-output-1.png" width="661" height="372"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Interpreting the visualization:
• Small σ (blue): Scores cluster tightly around 75. Most students perform similarly.
• Medium σ (pink): Moderate spread. Typical variation in a well-designed exam.
• Large σ (orange): Wide spread. Large differences in student performance.

For any normal distribution, about 68% of values fall within ±1σ of the mean.</code></pre>
</div>
</div></div></div></div>
</section>
<section id="sec-properties-of-variance" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="sec-properties-of-variance"><span class="header-section-number">2.5.2</span> Properties of Variance</h3>
<p>The variance has a useful computational formula:</p>
<div class="theorem">
<p><span class="math display">\mathbb{V}(X) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Starting from the definition of variance: <span class="math display">\begin{align}
\mathbb{V}(X) &amp;= \mathbb{E}[(X - \mu)^2] \\
&amp;= \mathbb{E}[X^2 - 2X\mu + \mu^2] \\
&amp;= \mathbb{E}(X^2) - 2\mu\mathbb{E}(X) + \mu^2 \\
&amp;= \mathbb{E}(X^2) - 2\mu^2 + \mu^2 \\
&amp;= \mathbb{E}(X^2) - \mu^2
\end{align}</span> where we used linearity of expectation and the fact that <span class="math inline">\mathbb{E}(X) = \mu</span>.</p>
</div>
</div>
</div>
<p>This formula simplifies many calculations and can be used to prove multiple properties of the variance.</p>
<div class="theorem">
<p>Assuming the variance is well defined, it satisfies:</p>
<ol type="1">
<li><span class="math inline">\mathbb{V}(X) \geq 0</span>, with <span class="math inline">\mathbb{V}(X) = 0</span> if and only if <span class="math inline">X</span> is constant (a.s.)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>For constants <span class="math inline">a, b</span>: <span class="math display">\mathbb{V}(aX + b) = a^2\mathbb{V}(X)</span></li>
<li>If <span class="math inline">X</span> and <span class="math inline">Y</span> are <strong>independent</strong>: <span class="math display">\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y)</span></li>
<li>If <span class="math inline">X</span> and <span class="math inline">Y</span> are <strong>independent</strong>: <span class="math display">\mathbb{V}(X - Y) = \mathbb{V}(X) + \mathbb{V}(Y)</span> (not minus!)</li>
<li>If <span class="math inline">X_1, \ldots, X_n</span> are <strong>independent</strong>, for constants <span class="math inline">a_1, \ldots, a_n</span>: <span class="math display">\mathbb{V}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathbb{V}(X_i)</span></li>
</ol>
</div>
<p>Property 4 often surprises students. You can find the proof below.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Property 4
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If <span class="math inline">X</span> and <span class="math inline">Y</span> are independent with means <span class="math inline">\mu_X, \mu_Y</span>: <span class="math display">\begin{align}
\mathbb{V}(X - Y) &amp;= \mathbb{E}[(X - Y - (\mu_X - \mu_Y))^2] \\
&amp;= \mathbb{E}[((X - \mu_X) - (Y - \mu_Y))^2] \\
&amp;= \mathbb{E}[(X - \mu_X)^2 - 2(X - \mu_X)(Y - \mu_Y) + (Y - \mu_Y)^2] \\
&amp;= \mathbb{E}[(X - \mu_X)^2] + \mathbb{E}[(Y - \mu_Y)^2] - 2\mathbb{E}[(X - \mu_X)(Y - \mu_Y)] \\
&amp;= \mathbb{V}(X) + \mathbb{V}(Y) - 2 \cdot 0 \\
&amp;= \mathbb{V}(X) + \mathbb{V}(Y)
\end{align}</span></p>
<p>The key step uses independence: <span class="math inline">\mathbb{E}[(X - \mu_X)(Y - \mu_Y)] = \mathbb{E}[X - \mu_X]\mathbb{E}[Y - \mu_Y] = 0 \cdot 0 = 0</span>.</p>
<p>Let’s visualize how subtracting <strong>independent</strong> variables increases variance, while subtracting <strong>dependent</strong> variables can reduce it – to the point that substracting perfectly correlated variables completely eliminates any variance!</p>
<div id="49534382" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="6">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing Var(X-Y) = Var(X) + Var(Y) for independent variables</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent case</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>X_indep <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)  <span class="co"># Var = 1</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>Y_indep <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)  <span class="co"># Var = 1</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>diff_indep <span class="op">=</span> X_indep <span class="op">-</span> Y_indep      <span class="co"># Var should be 2</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perfectly correlated case (not independent)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>X_corr <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>Y_corr <span class="op">=</span> X_corr  <span class="co"># Perfect correlation</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>diff_corr <span class="op">=</span> X_corr <span class="op">-</span> Y_corr  <span class="co"># Should be 0</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent case</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>ax1.hist(diff_indep, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'blue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'X - Y'</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="ss">f'Independent: Var(X-Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(diff_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 2'</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlated case</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>ax2.hist(diff_corr, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'red'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'X - Y'</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="ss">f'Perfect Correlation: Var(X-Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(diff_corr, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0'</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>ax2.set_xlim(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"When X and Y are independent N(0,1):"</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X) = </span><span class="sc">{</span>np<span class="sc">.</span>var(X_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(Y_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X-Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(diff_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ Var(X) + Var(Y) = 2"</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">When Y = X (perfect dependence):"</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X-Y) = Var(0) = 0"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-expectation_files/figure-html/cell-7-output-1.png" width="693" height="468" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>When X and Y are independent N(0,1):
  Var(X) = 1.007
  Var(Y) = 1.002
  Var(X-Y) = 2.026 ≈ Var(X) + Var(Y) = 2

When Y = X (perfect dependence):
  Var(X-Y) = Var(0) = 0</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Variance of Binomial via Decomposition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X \sim \text{Binomial}(n, p)</span>. We already know <span class="math inline">\mathbb{E}(X) = np</span>. What’s the variance?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Write <span class="math inline">X = \sum_{i=1}^n X_i</span> where <span class="math inline">X_i \sim \text{Bernoulli}(p)</span> independently.</p>
<p>For a single Bernoulli:</p>
<ul>
<li><span class="math inline">\mathbb{E}(X_i) = p</span></li>
<li><span class="math inline">\mathbb{E}(X_i^2) = 0^2 \cdot (1-p) + 1^2 \cdot p = p</span></li>
<li><span class="math inline">\mathbb{V}(X_i) = \mathbb{E}(X_i^2) - (\mathbb{E}(X_i))^2 = p - p^2 = p(1-p)</span></li>
</ul>
<p>Since the <span class="math inline">X_i</span> are independent: <span class="math display">\mathbb{V}(X) = \mathbb{V}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathbb{V}(X_i) = np(1-p)</span></p>
<p>Note that variance is maximized when <span class="math inline">p = 1/2</span>, which makes intuitive sense – there’s most uncertainty when success and failure are equally likely.</p>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mean and Variance of Common Distributions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<colgroup>
<col style="width: 31%">
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th><span class="math inline">\mathbb{E}[X]</span></th>
<th><span class="math inline">\mathbb{V}(X)</span></th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bernoulli(<span class="math inline">p</span>)</td>
<td><span class="math inline">p</span></td>
<td><span class="math inline">p(1-p)</span></td>
<td>Single yes/no trial</td>
</tr>
<tr class="even">
<td>Binomial(<span class="math inline">n,p</span>)</td>
<td><span class="math inline">np</span></td>
<td><span class="math inline">np(1-p)</span></td>
<td>Count of successes</td>
</tr>
<tr class="odd">
<td>Poisson(<span class="math inline">\lambda</span>)</td>
<td><span class="math inline">\lambda</span></td>
<td><span class="math inline">\lambda</span></td>
<td>Count of rare events</td>
</tr>
<tr class="even">
<td>Geometric(<span class="math inline">p</span>)</td>
<td><span class="math inline">1/p</span></td>
<td><span class="math inline">(1-p)/p^2</span></td>
<td>Trials until success</td>
</tr>
<tr class="odd">
<td>Uniform(<span class="math inline">a,b</span>)</td>
<td><span class="math inline">(a+b)/2</span></td>
<td><span class="math inline">(b-a)^2/12</span></td>
<td>Equal likelihood</td>
</tr>
<tr class="even">
<td>Normal(<span class="math inline">\mu,\sigma^2</span>)</td>
<td><span class="math inline">\mu</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td>Sums of many effects</td>
</tr>
<tr class="odd">
<td>Exponential(<span class="math inline">\beta</span>)</td>
<td><span class="math inline">\beta</span></td>
<td><span class="math inline">\beta^2</span></td>
<td>Time between events</td>
</tr>
<tr class="even">
<td>Gamma(<span class="math inline">\alpha,\beta</span>)</td>
<td><span class="math inline">\alpha\beta</span></td>
<td><span class="math inline">\alpha\beta^2</span></td>
<td>Sum of exponentials</td>
</tr>
<tr class="odd">
<td>Beta(<span class="math inline">\alpha,\beta</span>)</td>
<td><span class="math inline">\alpha/(\alpha+\beta)</span></td>
<td><span class="math inline">\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}</span></td>
<td>Proportions</td>
</tr>
<tr class="even">
<td><span class="math inline">t_{\nu}</span></td>
<td>0 (if <span class="math inline">\nu &gt; 1</span>)</td>
<td><span class="math inline">\nu/(\nu-2)</span> (if <span class="math inline">\nu &gt; 2</span>)</td>
<td>Heavy-tailed data</td>
</tr>
<tr class="odd">
<td><span class="math inline">\chi^2_p</span></td>
<td><span class="math inline">p</span></td>
<td><span class="math inline">2p</span></td>
<td>Sum of squared normals</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="sample-mean-and-variance" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sample-mean-and-variance"><span class="header-section-number">2.6</span> Sample Mean and Variance</h2>
<p>When we observe data, we compute sample statistics to estimate population parameters.</p>
<p>Recall from our introduction: we have a <strong>sample</strong> <span class="math inline">X_1, \ldots, X_n</span> drawn from a <strong>population</strong> distribution <span class="math inline">F_X</span>. The population has true parameters (like <span class="math inline">\mu = \mathbb{E}(X)</span> and <span class="math inline">\sigma^2 = \mathbb{V}(X)</span>) that we want to know, but we can only compute statistics from our finite sample. This gap between what we can calculate and what we want to know is fundamental to statistics.</p>
<div class="definition">
<p>Given random variables <span class="math inline">X_1, \ldots, X_n</span>:</p>
<p>The <strong>sample mean</strong> is: <span class="math display">\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i</span></p>
<p>The <strong>sample variance</strong> is: <span class="math display">S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2</span></p>
</div>
<p>Note the <span class="math inline">n-1</span> in the denominator of the sample variance. This makes it an <em>unbiased</em> estimator of the population variance (see below).</p>
<div class="theorem">
<p>Let <span class="math inline">X_1, \ldots, X_n</span> be IID with <span class="math inline">\mu = \mathbb{E}(X_i)</span> and <span class="math inline">\sigma^2 = \mathbb{V}(X_i)</span>. Then: <span class="math display">\mathbb{E}(\bar{X}_n) = \mu, \quad \mathbb{V}(\bar{X}_n) = \frac{\sigma^2}{n}, \quad \mathbb{E}(S_n^2) = \sigma^2</span></p>
</div>
<p>This theorem tells us:</p>
<ul>
<li>The sample mean is unbiased (its expectation is equal to the population mean)</li>
<li>Its variance decreases as <span class="math inline">n</span> increases</li>
<li>The sample variance (with <span class="math inline">n-1</span>) is unbiased</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Wait, What Does “Unbiased” Mean?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An estimator or sample statistic is <strong>unbiased</strong> if its expected value equals the parameter it’s trying to estimate.</p>
<ul>
<li><strong>Unbiased</strong>: <span class="math inline">\mathbb{E}(\text{estimator}) = \text{true parameter}</span></li>
<li><strong>Biased</strong>: <span class="math inline">\mathbb{E}(\text{estimator}) \neq \text{true parameter}</span></li>
</ul>
<p>For example:</p>
<ul>
<li>As stated above, <span class="math inline">\bar{X}_n</span> is unbiased for <span class="math inline">\mu</span> because <span class="math inline">\mathbb{E}(\bar{X}_n) = \mu</span></li>
<li><span class="math inline">S_n^2</span> (with <span class="math inline">n-1</span>) is unbiased for <span class="math inline">\sigma^2</span> because <span class="math inline">\mathbb{E}(S_n^2) = \sigma^2</span></li>
<li>If we used <span class="math inline">n</span> instead of <span class="math inline">n-1</span> at the denominator, we’d get <span class="math inline">\mathbb{E}(S_n^2) = \frac{n-1}{n}\sigma^2 &lt; \sigma^2</span> (biased!)</li>
</ul>
<p>Being unbiased means that <em>on average</em> across many sets of samples, our sample statistic would match the true value – though any individual estimate may be too high or too low. This also doesn’t tell us anything about the <em>rate of convergence</em> – how fast the estimator converges to the true value.</p>
</div>
</div>
</div>
</section>
<section id="covariance-and-correlation" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="covariance-and-correlation"><span class="header-section-number">2.7</span> Covariance and Correlation</h2>
<section id="linear-relationships" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="linear-relationships"><span class="header-section-number">2.7.1</span> Linear Relationships</h3>
<p>When we have two random variables, we often want to measure how they vary together and quantify the strength of their <em>linear</em> relation.</p>
<div class="definition">
<p>Let <span class="math inline">X</span> and <span class="math inline">Y</span> be random variables with means <span class="math inline">\mu_X</span> and <span class="math inline">\mu_Y</span> and standard deviations <span class="math inline">\sigma_X</span> and <span class="math inline">\sigma_Y</span>. The <strong>covariance</strong> between <span class="math inline">X</span> and <span class="math inline">Y</span> is: <span class="math display">\mathrm{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)]</span></p>
<p>The <strong>correlation</strong> is: <span class="math display">\rho = \rho_{X,Y} = \rho(X, Y) = \frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y}</span></p>
</div>
</section>
<section id="properties-of-covariance-and-correlation" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="properties-of-covariance-and-correlation"><span class="header-section-number">2.7.2</span> Properties of Covariance and Correlation</h3>
<div class="theorem">
<p>The covariance can be rewritten as: <span class="math display">\mathrm{Cov}(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)</span></p>
<p>The correlation satisfies: <span class="math display">-1 \leq \rho(X, Y) \leq 1</span></p>
</div>
<p>The correlation is a sort of “normalized covariance”. By dividing the covariance by <span class="math inline">\sigma_X</span> and <span class="math inline">\sigma_Y</span>, we remove the <em>magnitude</em> (and units/scale) of the two random variables, and what remains is a pure number that measures of how much they change together on average, in a range from -1 to 1.</p>
<div class="theorem">
<p>Covariance and correlation further satisfy the following properties:</p>
<ul>
<li>If <span class="math inline">Y = aX + b</span> for constants <span class="math inline">a, b</span>: <span class="math display">\rho(X, Y) = \begin{cases}
1 &amp; \text{if } a &gt; 0 \\
-1 &amp; \text{if } a &lt; 0
\end{cases}</span></li>
<li>If <span class="math inline">X</span> and <span class="math inline">Y</span> are <strong>independent</strong>: <span class="math inline">\mathrm{Cov}(X, Y) = \rho = 0</span></li>
<li>The converse is NOT true in general!</li>
</ul>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Common Misconception</strong>: Uncorrelated ≠ Independent!</p>
<p>Independence implies zero correlation, but zero correlation does NOT imply independence.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Uncorrelated but Dependent
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X \sim \text{Uniform}(-1, 1)</span> and <span class="math inline">Y = X^2</span>.</p>
<p>These two random variables are clearly dependent (knowing <span class="math inline">X</span> determines <span class="math inline">Y</span> exactly!), but:</p>
<p><span class="math display">\mathbb{E}(X) = 0</span> <span class="math display">\mathbb{E}(Y) = \mathbb{E}(X^2) = \int_{-1}^1 x^2 \cdot \frac{1}{2} \, dx = \frac{1}{3}</span> <span class="math display">\mathbb{E}(XY) = \mathbb{E}(X^3) = \int_{-1}^1 x^3 \cdot \frac{1}{2} \, dx = 0</span></p>
<p>Therefore: <span class="math display">\mathrm{Cov}(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = 0 - 0 \cdot \frac{1}{3} = 0</span></p>
<p>So <span class="math inline">X</span> and <span class="math inline">Y</span> are <strong>uncorrelated</strong> despite being perfectly dependent!</p>
<p>The plot below shows <span class="math inline">X</span> and <span class="math inline">Y</span>. See also <a href="https://www.scientificamerican.com/article/what-this-graph-of-a-dinosaur-can-teach-us-about-doing-better-science/">this article</a> on Scientific American for more examples.</p>
<div id="8ef39d11" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="7">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing uncorrelated but dependent variables</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># X ~ Uniform(-1, 1), Y = X²</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, n)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> X<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, Y, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Y = X²'</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Uncorrelated but Dependent: ρ(X,Y) = 0'</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the parabola</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>x_line <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>y_line <span class="op">=</span> x_line<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.plot(x_line, y_line, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Y = X²'</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Y is completely determined by X, yet they are uncorrelated!"</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This is because the linear association is zero due to symmetry."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-expectation_files/figure-html/cell-8-output-1.png" width="661" height="372" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Y is completely determined by X, yet they are uncorrelated!
This is because the linear association is zero due to symmetry.</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="variance-of-sums-general-case" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="variance-of-sums-general-case"><span class="header-section-number">2.7.3</span> Variance of Sums (General Case)</h3>
<p>We saw in <a href="#sec-properties-of-variance" class="quarto-xref"><span>Section 2.5.2</span></a> that for <strong>independent</strong> variables,</p>
<p><span class="math display">\mathbb{V}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathbb{V}(X_i) \qquad \text{(independent)}</span></p>
<p>We now generalize this result to any random variables, whether dependent or independent:</p>
<div class="theorem">
<p><span class="math display">\mathbb{V}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathbb{V}(X_i) + 2\sum_{i=1}^n \sum_{j=i+1}^n a_i a_j \mathrm{Cov}(X_i, X_j)</span></p>
</div>
<p><strong>Special cases:</strong></p>
<ul>
<li><span class="math inline">\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y) + 2\mathrm{Cov}(X, Y)</span></li>
<li><span class="math inline">\mathbb{V}(X - Y) = \mathbb{V}(X) + \mathbb{V}(Y) - 2\mathrm{Cov}(X, Y)</span></li>
<li>When the variables are independent, check that this indeed reduces to the simpler formula for independent variables</li>
</ul>
</section>
</section>
<section id="expectation-with-matrices" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="expectation-with-matrices"><span class="header-section-number">2.8</span> Expectation with Matrices</h2>
<section id="random-vectors" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="random-vectors"><span class="header-section-number">2.8.1</span> Random Vectors</h3>
<p>In <em>multivariate</em> settings – that is, involving multiple random variables –, we work with random <em>vectors</em> and their expectations.</p>
<p><strong>Notation:</strong> The mathematical convention is to work with column vectors. For example, we write</p>
<p><span class="math display">\mathbf{X} = \begin{pmatrix} X_1 \\ X_2 \\ X_3 \end{pmatrix}</span></p>
<p>rather than <span class="math inline">(X_1, X_2, X_3)</span>. The column vector can also be written as the transpose of a row vector: <span class="math inline">\mathbf{X} = (X_1, X_2, X_3)^T</span>.</p>
<div class="definition">
<p>For a random vector <span class="math inline">\mathbf{X} = (X_1, \ldots, X_k)^T</span>:</p>
<p>The <strong>mean vector</strong> is: <span class="math display">\boldsymbol{\mu} = \mathbb{E}(\mathbf{X}) = \begin{pmatrix} \mathbb{E}(X_1) \\ \vdots \\ \mathbb{E}(X_k) \end{pmatrix}</span></p>
<p>The <strong>covariance matrix</strong> <span class="math inline">\boldsymbol{\Sigma}</span> (also written <span class="math inline">\mathbb{V}(\mathbf{X})</span>) is: <span class="math display">\boldsymbol{\Sigma} = \begin{bmatrix}
\mathbb{V}(X_1) &amp; \mathrm{Cov}(X_1, X_2) &amp; \cdots &amp; \mathrm{Cov}(X_1, X_k) \\
\mathrm{Cov}(X_2, X_1) &amp; \mathbb{V}(X_2) &amp; \cdots &amp; \mathrm{Cov}(X_2, X_k) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathrm{Cov}(X_k, X_1) &amp; \mathrm{Cov}(X_k, X_2) &amp; \cdots &amp; \mathbb{V}(X_k)
\end{bmatrix}</span></p>
</div>
<p>The inverse <span class="math inline">\boldsymbol{\Sigma}^{-1}</span> is called the <strong>precision matrix</strong>.</p>
<p><strong>Notation:</strong> The <em>uppercase</em> Greek letter <span class="math inline">\Sigma</span> (<a href="https://en.wikipedia.org/wiki/Sigma">sigma</a>) is almost invariably used to denote a covariance matrix. Note that the <em>lowercase</em> <span class="math inline">\sigma</span> denotes the standard deviation.</p>
</section>
<section id="covariance-matrix-properties" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="covariance-matrix-properties"><span class="header-section-number">2.8.2</span> Covariance Matrix Properties</h3>
<p>The covariance matrix can be written compactly as: <span class="math display">\boldsymbol{\Sigma} = \mathbb{E}[(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T]</span></p>
<p>An alternative formula for the covariance matrix is: <span class="math display">\boldsymbol{\Sigma} = \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \boldsymbol{\mu}\boldsymbol{\mu}^T</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Starting from the definition: <span class="math display">\begin{align}
\boldsymbol{\Sigma} &amp;= \mathbb{E}[(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T] \\
&amp;= \mathbb{E}[\mathbf{X}\mathbf{X}^T - \mathbf{X}\boldsymbol{\mu}^T - \boldsymbol{\mu}\mathbf{X}^T + \boldsymbol{\mu}\boldsymbol{\mu}^T] \\
&amp;= \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \mathbb{E}(\mathbf{X})\boldsymbol{\mu}^T - \boldsymbol{\mu}\mathbb{E}(\mathbf{X}^T) + \boldsymbol{\mu}\boldsymbol{\mu}^T \\
&amp;= \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \boldsymbol{\mu}\boldsymbol{\mu}^T - \boldsymbol{\mu}\boldsymbol{\mu}^T + \boldsymbol{\mu}\boldsymbol{\mu}^T \\
&amp;= \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \boldsymbol{\mu}\boldsymbol{\mu}^T
\end{align}</span> where we used the fact that <span class="math inline">\mathbb{E}(\mathbf{X}) = \boldsymbol{\mu}</span> and the linearity of expectation.</p>
</div>
</div>
</div>
<p>Properties:</p>
<ul>
<li>Symmetric: <span class="math inline">\boldsymbol{\Sigma} = \boldsymbol{\Sigma}^T</span></li>
<li>Positive semi-definite: <span class="math inline">\mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a} \geq 0</span> for all <span class="math inline">\mathbf{a}</span></li>
<li>Diagonal elements are variances (non-negative)</li>
<li>Off-diagonal elements are covariances</li>
</ul>
</section>
<section id="linear-transformations" class="level3" data-number="2.8.3">
<h3 data-number="2.8.3" class="anchored" data-anchor-id="linear-transformations"><span class="header-section-number">2.8.3</span> Linear Transformations</h3>
<div class="theorem">
<p>If <span class="math inline">\mathbf{X}</span> has mean <span class="math inline">\boldsymbol{\mu}</span> and covariance <span class="math inline">\boldsymbol{\Sigma}</span>, and <span class="math inline">\mathbf{A}</span> is a matrix: <span class="math display">\mathbb{E}(\mathbf{A}\mathbf{X}) = \mathbf{A}\boldsymbol{\mu}</span> <span class="math display">\mathbb{V}(\mathbf{A}\mathbf{X}) = \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^T</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of the Variance Formula
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using the definition of variance for vectors and the fact that <span class="math inline">\mathbb{E}(\mathbf{A}\mathbf{X}) = \mathbf{A}\boldsymbol{\mu}</span>: <span class="math display">\begin{align}
\mathbb{V}(\mathbf{A}\mathbf{X}) &amp;= \mathbb{E}[(\mathbf{A}\mathbf{X} - \mathbf{A}\boldsymbol{\mu})(\mathbf{A}\mathbf{X} - \mathbf{A}\boldsymbol{\mu})^T] \\
&amp;= \mathbb{E}[\mathbf{A}(\mathbf{X} - \boldsymbol{\mu})(\mathbf{A}(\mathbf{X} - \boldsymbol{\mu}))^T] \\
&amp;= \mathbb{E}[\mathbf{A}(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T\mathbf{A}^T] \\
&amp;= \mathbf{A}\mathbb{E}[(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T]\mathbf{A}^T \\
&amp;= \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^T
\end{align}</span> where we used the fact that <span class="math inline">\mathbf{A}</span> is a constant matrix that can be taken outside the expectation.</p>
</div>
</div>
</div>
<p>Similarly, for a vector <span class="math inline">\mathbf{a}</span> (this is just a special case of the equations above – why?): <span class="math display">\mathbb{E}(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T\boldsymbol{\mu}</span> <span class="math display">\mathbb{V}(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a}</span></p>
</div>
</section>
<section id="interpreting-the-covariance-matrix" class="level3" data-number="2.8.4">
<h3 data-number="2.8.4" class="anchored" data-anchor-id="interpreting-the-covariance-matrix"><span class="header-section-number">2.8.4</span> Interpreting the Covariance Matrix</h3>
<p>The covariance matrix encodes the second-order structure of a random vector—that is, how the variables vary and co-vary together. To understand this structure, we can examine its <strong>spectral decomposition</strong> (<a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigendecomposition</a>).</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255586-854-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-854-1" role="tab" aria-controls="tabset-1757255586-854-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-854-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-854-2" role="tab" aria-controls="tabset-1757255586-854-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-854-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-854-3" role="tab" aria-controls="tabset-1757255586-854-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255586-854-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255586-854-1-tab"><p>Imagine your data as a cloud of points in space. This cloud rarely
forms a perfect sphere—it’s usually stretched more in some directions
than others, like an ellipse or ellipsoid.</p><p>The covariance matrix captures this shape:</p><ul>
<li><strong>Eigenvectors</strong> are the “natural axes” of your data
cloud—the directions along which it stretches</li>
<li><strong>Eigenvalues</strong> tell you how much the cloud stretches
in each direction</li>
<li>The largest eigenvalue corresponds to the direction of greatest
spread</li>
</ul><p>This is like finding the best way to orient a box around your
data:</p><ul>
<li>The box edges align with the eigenvectors</li>
<li>The box dimensions are proportional to the square roots of
eigenvalues</li>
</ul><p><strong>Principal Component Analysis (PCA)</strong> uses this
insight: keep the directions with large spread (high variance), discard
those with little spread. This reduces dimensions while preserving most
of the data’s structure.</p></div><div id="tabset-1757255586-854-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-854-2-tab"><p>Since <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is
symmetric and positive semi-definite, recall from earlier linear algebra
classes that it has spectral decomposition:
<span class="math display">\[\boldsymbol{\Sigma} = \sum_{i=1}^k \lambda_i \mathbf{v}_i \mathbf{v}_i^T\]</span></p><p>where:</p><ul>
<li><span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_k \geq 0\)</span>
are the <em>eigenvalues</em> (which we can order from larger to
smaller)</li>
<li><span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\)</span>
are the corresponding orthonormal <em>eigenvectors</em></li>
</ul><p>This decomposition reveals the geometric structure of the data:</p><ul>
<li><strong>Eigenvalues</strong>
<span class="math inline">\(\lambda_i\)</span>: represent the variance
along each principal axis</li>
<li><strong>Eigenvectors</strong>
<span class="math inline">\(\mathbf{v}_i\)</span>: define the directions
of these principal axes</li>
<li><strong>Largest eigenvalue/vector</strong>: indicates the direction
of maximum variance in the data</li>
</ul></div><div id="tabset-1757255586-854-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-854-3-tab"><p>Let’s visualize how eigendecomposition reveals the structure of
data:</p><div id="16f56321" class="cell" data-fig-height="6" data-fig-width="7" data-execution_count="8">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate correlated 2D data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="fl">2.5</span>, <span class="fl">1.5</span>], </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>       [<span class="fl">1.5</span>, <span class="fl">1.5</span>]]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.multivariate_normal(mean, cov, <span class="dv">300</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute eigendecomposition</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eigh(cov)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by eigenvalue (largest first)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> eigenvalues.argsort()[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>eigenvalues <span class="op">=</span> eigenvalues[idx]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>eigenvectors <span class="op">=</span> eigenvectors[:, idx]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data and principal axes</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot eigenvectors from the mean</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale eigenvector by sqrt(eigenvalue) for visualization</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> eigenvectors[:, i] <span class="op">*</span> np.sqrt(eigenvalues[i]) <span class="op">*</span> <span class="fl">1.96</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    plt.arrow(mean[<span class="dv">0</span>], mean[<span class="dv">1</span>], v[<span class="dv">0</span>], v[<span class="dv">1</span>], </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>              head_width<span class="op">=</span><span class="fl">0.1</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>              fc<span class="op">=</span>colors[i], ec<span class="op">=</span>colors[i], linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>              label<span class="op">=</span><span class="ss">f'PC</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: λ=</span><span class="sc">{</span>eigenvalues[i]<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Also draw the negative direction</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    plt.arrow(mean[<span class="dv">0</span>], mean[<span class="dv">1</span>], <span class="op">-</span>v[<span class="dv">0</span>], <span class="op">-</span>v[<span class="dv">1</span>], </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>              head_width<span class="op">=</span><span class="fl">0.1</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>              fc<span class="op">=</span>colors[i], ec<span class="op">=</span>colors[i], linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X₁'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'X₂'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Principal Axes of the Covariance Matrix'</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Covariance matrix:</span><span class="ch">\n</span><span class="sc">{</span>np<span class="sc">.</span>array(cov)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Eigenvalues: </span><span class="sc">{</span>eigenvalues<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Eigenvectors (as columns):</span><span class="ch">\n</span><span class="sc">{</span>eigenvectors<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">The first principal component explains </span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>eigenvalues[<span class="dv">0</span>]<span class="op">/</span><span class="bu">sum</span>(eigenvalues)<span class="sc">:.1f}</span><span class="ss">% of the variance"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="02-expectation_files/figure-html/cell-9-output-1.png" width="650" height="564"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Covariance matrix:
[[2.5 1.5]
 [1.5 1.5]]

Eigenvalues: [3.58113883 0.41886117]
Eigenvectors (as columns):
[[-0.81124219  0.58471028]
 [-0.58471028 -0.81124219]]

The first principal component explains 89.5% of the variance</code></pre>
</div>
</div><p>The red arrow shows the first principal component (direction of
maximum variance), while the blue arrow shows the second. In the plot,
each eigenvector <span class="math inline">\(\mathbf{v}_i\)</span> is
rescaled by <span class="math inline">\(1.96 \sqrt{\lambda_i}\)</span>
which covers about <span class="math inline">\(95 \%\)</span> of the
normal distribution.</p><p>In Principal Component Analysis (PCA), we might keep only the first
component to reduce from 2D to 1D while preserving most of the
structure.</p></div></div></div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Multinomial Covariance Structure
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a> provides a concrete example of covariance matrix structure. If</p>
<p><span class="math display">\mathbf{X} = (X_1, \ldots, X_k)^T \sim \text{Multinomial}(n, \mathbf{p})</span></p>
<p>where <span class="math inline">\mathbf{p} = (p_1, \ldots, p_k)^T</span> with <span class="math inline">\sum p_i = 1</span>, then:</p>
<p><strong>Mean vector</strong>: <span class="math inline">\mathbb{E}(\mathbf{X}) = n\mathbf{p} = (np_1, \ldots, np_k)^T</span></p>
<p><strong>Covariance matrix</strong>: <span class="math display">\boldsymbol{\Sigma} = \begin{pmatrix}
np_1(1-p_1) &amp; -np_1p_2 &amp; \cdots &amp; -np_1p_k \\
-np_2p_1 &amp; np_2(1-p_2) &amp; \cdots &amp; -np_2p_k \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-np_kp_1 &amp; -np_kp_2 &amp; \cdots &amp; np_k(1-p_k)
\end{pmatrix}</span></p>
<p>Key observations:</p>
<ul>
<li>Diagonal: <span class="math inline">\mathbb{V}(X_i) = np_i(1-p_i)</span> (same as binomial)</li>
<li>Off-diagonal: <span class="math inline">\mathrm{Cov}(X_i, X_j) = -np_ip_j</span> for <span class="math inline">i \neq j</span> (always negative!)</li>
<li>Intuition: If more outcomes fall in category <span class="math inline">i</span>, fewer can fall in category <span class="math inline">j</span></li>
</ul>
<p><strong>Special case</strong>: For a die roll with equal probabilities (<span class="math inline">p_i = 1/6</span> for all <span class="math inline">i</span>):</p>
<ul>
<li><span class="math inline">\mathbb{V}(X_i) = n \cdot \frac{1}{6} \cdot \frac{5}{6} = \frac{5n}{36}</span></li>
<li><span class="math inline">\mathrm{Cov}(X_i, X_j) = -n \cdot \frac{1}{6} \cdot \frac{1}{6} = -\frac{n}{36}</span> for <span class="math inline">i \neq j</span></li>
</ul>
</div>
</div>
</section>
</section>
<section id="conditional-expectation" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="conditional-expectation"><span class="header-section-number">2.9</span> Conditional Expectation</h2>
<section id="expectation-given-information" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="expectation-given-information"><span class="header-section-number">2.9.1</span> Expectation Given Information</h3>
<p>Conditional expectation captures how the mean changes when we have additional information. It is computed similarly to a regular expectation, just replacing the pdf (or PMF) with a <em>conditional</em> pdf (or PMF).</p>
<div class="definition">
<p>The <strong>conditional expectation</strong> of <span class="math inline">X</span> given <span class="math inline">Y = y</span> is: <span class="math display">\mathbb{E}(X | Y = y) = \begin{cases}
\sum_x x \mathbb{P}_{X|Y}(x|y) &amp; \text{discrete case} \\
\int x f_{X|Y}(x|y) \, dx &amp; \text{continuous case}
\end{cases}</span></p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Subtle but Important</strong>:</p>
<ul>
<li><span class="math inline">\mathbb{E}(X)</span> is a number</li>
<li><span class="math inline">\mathbb{E}(X | Y = y)</span> is a number (for fixed <span class="math inline">y</span>)</li>
<li><span class="math inline">\mathbb{E}(X | Y)</span> is a random variable (because it’s a function of <span class="math inline">Y</span>!)</li>
</ul>
</div>
</div>
</section>
<section id="properties-of-conditional-expectation" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="properties-of-conditional-expectation"><span class="header-section-number">2.9.2</span> Properties of Conditional Expectation</h3>
<div class="theorem" name="Law of Iterated Expectations">
<p><span class="math display">\mathbb{E}[\mathbb{E}(Y | X)] = \mathbb{E}(Y)</span></p>
<p>More generally, for any function <span class="math inline">r(x, y)</span>: <span class="math display">\mathbb{E}[\mathbb{E}(r(X, Y) | X)] = \mathbb{E}(r(X, Y))</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of the Law of Iterated Expectations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We’ll prove the first equation. Using the definition of conditional expectation and the fact that the joint pdf can be written as <span class="math inline">f(x, y) = f_X(x) f_{Y|X}(y|x)</span>:</p>
<p><span class="math display">\begin{align}
\mathbb{E}[\mathbb{E}(Y | X)] &amp;= \mathbb{E}\left[\int y f_{Y|X}(y|X) \, dy\right] \\
&amp;= \int \left[\int y f_{Y|X}(y|x) \, dy\right] f_X(x) \, dx \\
&amp;= \int \int y f_{Y|X}(y|x) f_X(x) \, dy \, dx \\
&amp;= \int \int y f(x, y) \, dy \, dx \\
&amp;= \int y \left[\int f(x, y) \, dx\right] dy \\
&amp;= \int y f_Y(y) \, dy \\
&amp;= \mathbb{E}(Y)
\end{align}</span></p>
<p>The key steps are:</p>
<ol type="1">
<li><span class="math inline">\mathbb{E}(Y|X)</span> is a function of <span class="math inline">X</span>, so we take its expectation with respect to <span class="math inline">X</span></li>
<li>We can interchange the order of integration</li>
<li><span class="math inline">f_{Y|X}(y|x) f_X(x) = f(x,y)</span> by the definition of conditional probability</li>
<li>Integrating the joint pdf over <span class="math inline">x</span> gives the marginal pdf <span class="math inline">f_Y(y)</span></li>
</ol>
</div>
</div>
</div>
<p>This powerful result lets us compute expectations by conditioning on useful information.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Breaking Stick Revisited
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine placing two random points on a unit stick. First, we place point <span class="math inline">X</span> uniformly at random. Then, we place point <span class="math inline">Y</span> uniformly at random between <span class="math inline">X</span> and the end of the stick.</p>
<p>Formally: Draw <span class="math inline">X \sim \text{Uniform}(0, 1)</span>. After observing <span class="math inline">X = x</span>, draw <span class="math inline">Y | X = x \sim \text{Uniform}(x, 1)</span>.</p>
<p><strong>Question</strong>: What is the expected position of the second point <span class="math inline">Y</span>?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We could find the marginal distribution of <span class="math inline">Y</span> (which is complex), but it’s easier to use conditional expectation:</p>
<p>First, find <span class="math inline">\mathbb{E}(Y | X = x)</span>: <span class="math display">\mathbb{E}(Y | X = x) = \frac{x + 1}{2}</span> This makes sense: given <span class="math inline">X = x</span>, point <span class="math inline">Y</span> is uniform on <span class="math inline">(x, 1)</span>, so its expected position is the midpoint.</p>
<p>So <span class="math inline">\mathbb{E}(Y | X) = \frac{X + 1}{2}</span> (a random variable).</p>
<p>Now use iterated expectations: <span class="math display">\mathbb{E}(Y) = \mathbb{E}[\mathbb{E}(Y | X)] = \mathbb{E}\left[\frac{X + 1}{2}\right] = \frac{\mathbb{E}(X) + 1}{2} = \frac{1/2 + 1}{2} = \frac{3}{4}</span></p>
<p>The second point lands, on average, at position 3/4 along the stick.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="conditional-variance" class="level3" data-number="2.9.3">
<h3 data-number="2.9.3" class="anchored" data-anchor-id="conditional-variance"><span class="header-section-number">2.9.3</span> Conditional Variance</h3>
<div class="definition">
<p>The <strong>conditional variance</strong> is: <span class="math display">\mathbb{V}(Y | X = x) = \mathbb{E}[(Y - \mathbb{E}(Y | X = x))^2 | X = x]</span></p>
</div>
<div class="theorem" name="Law of Total Variance">
<p><span class="math display">\mathbb{V}(Y) = \mathbb{E}[\mathbb{V}(Y | X)] + \mathbb{V}[\mathbb{E}(Y | X)]</span></p>
</div>
<p>This decomposition says: Total variance = Average within-group variance + Between-group variance.</p>
</section>
</section>
<section id="more-about-the-normal-distribution" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="more-about-the-normal-distribution"><span class="header-section-number">2.10</span> More About the Normal Distribution</h2>
<section id="quick-recap" class="level3" data-number="2.10.1">
<h3 data-number="2.10.1" class="anchored" data-anchor-id="quick-recap"><span class="header-section-number">2.10.1</span> Quick Recap</h3>
<p>Recall that if <span class="math inline">X \sim \mathcal{N}(\mu, \sigma^2)</span>, then:</p>
<ul>
<li>PDF: <span class="math inline">f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)</span></li>
<li>Mean: <span class="math inline">\mathbb{E}(X) = \mu</span></li>
<li>Variance: <span class="math inline">\mathbb{V}(X) = \sigma^2</span></li>
</ul>
<p>The normal distribution plays a central role in statistics due to the Central Limit Theorem (Chapter 3) and its many convenient mathematical properties.</p>
</section>
<section id="entropy-of-the-normal-distribution" class="level3" data-number="2.10.2">
<h3 data-number="2.10.2" class="anchored" data-anchor-id="entropy-of-the-normal-distribution"><span class="header-section-number">2.10.2</span> Entropy of the Normal Distribution</h3>
<p>We can use the expectation to compute the entropy of the normal distribution.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Normal Distribution Entropy
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>differential entropy</strong> of a continuous random variable measures the average uncertainty in the distribution: <span class="math display">H(X) = \mathbb{E}[-\ln f_X(X)] = -\int f_X(x) \ln f_X(x) \, dx</span></p>
<p>Let’s calculate this for <span class="math inline">X \sim \mathcal{N}(\mu, \sigma^2)</span>. First, find <span class="math inline">-\ln f_X(x)</span>: <span class="math display">-\ln f_X(x) = \ln(\sqrt{2\pi\sigma^2}) + \frac{(x-\mu)^2}{2\sigma^2} = \frac{1}{2}\ln(2\pi\sigma^2) + \frac{(x-\mu)^2}{2\sigma^2}</span></p>
<p>Now compute the expectation: <span class="math display">\begin{align}
H(X) &amp;= \mathbb{E}[-\ln f_X(X)] \\
&amp;= \mathbb{E}\left[\frac{1}{2}\ln(2\pi\sigma^2) + \frac{(X-\mu)^2}{2\sigma^2}\right] \\
&amp;= \frac{1}{2}\ln(2\pi\sigma^2) + \frac{1}{2\sigma^2}\mathbb{E}[(X-\mu)^2] \\
&amp;= \frac{1}{2}\ln(2\pi\sigma^2) + \frac{1}{2\sigma^2} \cdot \sigma^2 \\
&amp;= \frac{1}{2}\ln(2\pi\sigma^2) + \frac{1}{2} \\
&amp;= \frac{1}{2}\ln(2\pi e\sigma^2) \\
&amp;= \ln(\sqrt{2\pi e\sigma^2})
\end{align}</span></p>
<p><strong>Key insights</strong>:</p>
<ul>
<li>The entropy increases with <span class="math inline">\sigma</span> (more spread = more uncertainty)</li>
<li>Among all distributions with fixed variance <span class="math inline">\sigma^2</span>, the normal has maximum entropy</li>
</ul>
</div>
</div>
</section>
<section id="multivariate-normal-properties" class="level3" data-number="2.10.3">
<h3 data-number="2.10.3" class="anchored" data-anchor-id="multivariate-normal-properties"><span class="header-section-number">2.10.3</span> Multivariate Normal Properties</h3>
<p>The <span class="math inline">d</span>-dimensional multivariate normal is parametrized by mean vector <span class="math inline">\boldsymbol{\mu} \in \mathbb{R}^d</span> and covariance matrix <span class="math inline">\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}</span> (symmetric and positive definite).</p>
<p>For <span class="math inline">\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span>:</p>
<ol type="1">
<li><p><strong>Independence and diagonal covariance</strong>: Components <span class="math inline">X_i</span> and <span class="math inline">X_j</span> are independent if and only if <span class="math inline">\Sigma_{ij} = 0</span>. Thus, the components are mutually independent if and only if <span class="math inline">\boldsymbol{\Sigma}</span> is diagonal.</p></li>
<li><p><strong>Standard multivariate normal</strong>: If <span class="math inline">Z_1, \ldots, Z_d \sim \mathcal{N}(0, 1)</span> are independent, then <span class="math inline">\mathbf{Z} = (Z_1, \ldots, Z_d)^T \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_d)</span>, where <span class="math inline">\mathbf{I}_d</span> is the <span class="math inline">d \times d</span> identity matrix.</p></li>
<li><p><strong>Marginals are normal</strong>: Each <span class="math inline">X_i \sim \mathcal{N}(\mu_i, \Sigma_{ii})</span></p></li>
<li><p><strong>Linear combinations are normal</strong>: For any vector <span class="math inline">\mathbf{a}</span>, <span class="math inline">\mathbf{a}^T\mathbf{X} \sim \mathcal{N}(\mathbf{a}^T\boldsymbol{\mu}, \mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a})</span></p></li>
<li><p><strong>Conditionals are normal</strong>: Suppose we partition: <span class="math display">\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{pmatrix}, \quad
\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix}, \quad
\boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{21} &amp; \boldsymbol{\Sigma}_{22} \end{pmatrix}</span></p>
<p>Then the conditional distribution of <span class="math inline">\mathbf{X}_2</span> given <span class="math inline">\mathbf{X}_1 = \mathbf{x}_1</span> is: <span class="math display">\mathbf{X}_2 | \mathbf{X}_1 = \mathbf{x}_1 \sim \mathcal{N}(\boldsymbol{\mu}_{2|1}, \boldsymbol{\Sigma}_{2|1})</span> where:</p>
<ul>
<li><strong>Conditional mean</strong>: <span class="math inline">\boldsymbol{\mu}_{2|1} = \boldsymbol{\mu}_2 + \boldsymbol{\Sigma}_{21}\boldsymbol{\Sigma}_{11}^{-1}(\mathbf{x}_1 - \boldsymbol{\mu}_1)</span></li>
<li><strong>Conditional covariance</strong>: <span class="math inline">\boldsymbol{\Sigma}_{2|1} = \boldsymbol{\Sigma}_{22} - \boldsymbol{\Sigma}_{21}\boldsymbol{\Sigma}_{11}^{-1}\boldsymbol{\Sigma}_{12}</span></li>
</ul>
<p>Note that the conditional covariance doesn’t depend on the observed value <span class="math inline">\mathbf{x}_1</span>!</p></li>
</ol>
<p>These properties are used in multiple algorithms and methods in statistics, including for example:</p>
<ul>
<li>Gaussian processes</li>
<li>Kalman filtering</li>
<li>Linear regression theory</li>
<li>Multivariate statistical methods</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cholesky Decomposition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Every symmetric positive definite covariance matrix <span class="math inline">\boldsymbol{\Sigma}</span> can be decomposed as: <span class="math display">\boldsymbol{\Sigma} = \mathbf{L}\mathbf{L}^T</span> where <span class="math inline">\mathbf{L}</span> is a lower-triangular matrix called the <strong>Cholesky decomposition</strong> (or Cholesky factor).</p>
<p>This decomposition is crucial for:</p>
<ul>
<li><strong>Simulating multivariate normals</strong>: If <span class="math inline">\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})</span> and <span class="math inline">\mathbf{X} = \boldsymbol{\mu} + \mathbf{L}\mathbf{Z}</span>, then <span class="math inline">\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span></li>
<li><strong>Transforming to standard form</strong>: If <span class="math inline">\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span>, then <span class="math inline">\mathbf{L}^{-1}(\mathbf{X} - \boldsymbol{\mu}) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})</span></li>
<li><strong>Efficient computation</strong>: Solving linear systems and computing determinants</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Generating Multivariate Normal Random Vectors via Cholesky
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s see how the Cholesky decomposition can be used to transform independent standard normals into correlated multivariate normals.</p>
<div id="99270f59" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="9">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Ellipse</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define mean and covariance</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="fl">1.2</span>], </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">1.2</span>, <span class="dv">1</span>]])</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Cholesky decomposition</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(Sigma)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Covariance matrix Σ:"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Sigma)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cholesky factor L (lower triangular):"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(L)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Verification: L @ L.T ="</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(L <span class="op">@</span> L.T)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples step by step</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Generate independent standard normals</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.random.standard_normal((n_samples, <span class="dv">2</span>))  <span class="co"># N(0, I)</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Transform using Cholesky</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> mu <span class="op">+</span> Z <span class="op">@</span> L.T  <span class="co"># Transform to N(mu, Sigma)</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformation</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot independent standard normals</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>ax1.scatter(Z[:, <span class="dv">0</span>], Z[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Z₁'</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Z₂'</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Independent N(0,1)'</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>ax1.axis(<span class="st">'equal'</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot transformed correlated normals</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>ax2.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Add confidence ellipse</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eigh(Sigma)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>angle <span class="op">=</span> np.degrees(np.arctan2(eigenvectors[<span class="dv">1</span>, <span class="dv">1</span>], eigenvectors[<span class="dv">0</span>, <span class="dv">1</span>]))</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>ellipse <span class="op">=</span> Ellipse(mu, <span class="dv">2</span><span class="op">*</span>np.sqrt(eigenvalues[<span class="dv">1</span>]), <span class="dv">2</span><span class="op">*</span>np.sqrt(eigenvalues[<span class="dv">0</span>]),</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>                  angle<span class="op">=</span>angle, facecolor<span class="op">=</span><span class="st">'none'</span>, edgecolor<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>ax2.add_patch(ellipse)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'X₁'</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'X₂'</span>)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Correlated N(μ, Σ)'</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>ax2.axis(<span class="st">'equal'</span>)</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">The Cholesky decomposition transforms:"</span>)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"• Independent standard normals Z ~ N(0, I)"</span>)</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"• Into correlated normals X = μ + LZ ~ N(μ, Σ)"</span>)</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample covariance:</span><span class="ch">\n</span><span class="sc">{</span>np<span class="sc">.</span>cov(X.T)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Covariance matrix Σ:
[[2.  1.2]
 [1.2 1. ]]

Cholesky factor L (lower triangular):
[[1.41421356 0.        ]
 [0.84852814 0.52915026]]

Verification: L @ L.T =
[[2.  1.2]
 [1.2 1. ]]

The Cholesky decomposition transforms:
• Independent standard normals Z ~ N(0, I)
• Into correlated normals X = μ + LZ ~ N(μ, Σ)

Sample covariance:
[[1.87031161 1.10325785]
 [1.10325785 0.92611656]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-expectation_files/figure-html/cell-10-output-2.png" width="661" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">2.11</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="2.11.1">
<h3 data-number="2.11.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">2.11.1</span> Key Concepts Review</h3>
<p>We’ve explored the fundamental tools for summarizing and understanding random variables:</p>
<ol type="1">
<li><strong>Expectation as weighted average</strong>: The fundamental summary of a distribution</li>
<li><strong>Linearity—the master property</strong>: <span class="math inline">\mathbb{E}(\sum a_i X_i) = \sum a_i \mathbb{E}(X_i)</span> always works!</li>
<li><strong>Variance measures spread</strong>: How far from the mean should we expect outcomes?</li>
<li><strong>Covariance measures linear relationships</strong>: Do variables move together?</li>
<li><strong>Conditional expectation as best prediction</strong>: What’s our best guess given information?</li>
<li><strong>Matrix operations extend naturally</strong>: Same concepts work for random vectors</li>
</ol>
</section>
<section id="why-these-concepts-matter" class="level3" data-number="2.11.2">
<h3 data-number="2.11.2" class="anchored" data-anchor-id="why-these-concepts-matter"><span class="header-section-number">2.11.2</span> Why These Concepts Matter</h3>
<p><strong>For Statistical Inference</strong>:</p>
<ul>
<li>Sample means estimate population expectations</li>
<li>Variance quantifies uncertainty in estimates</li>
<li>Covariance reveals relationships between variables</li>
<li>Conditional expectation enables regression analysis</li>
</ul>
<p><strong>For Machine Learning</strong>:</p>
<ul>
<li>Loss functions are expectations over data distributions</li>
<li>Gradient descent minimizes expected loss</li>
<li>Feature correlations affect model performance</li>
<li>Conditional expectations define optimal predictors</li>
</ul>
<p><strong>For Data Science Practice</strong>:</p>
<ul>
<li>Summary statistics (mean, variance) describe data concisely</li>
<li>Correlation analysis reveals variable relationships</li>
<li>Variance decomposition explains data structure</li>
<li>Linear algebra connects to dimensionality reduction (PCA)</li>
</ul>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="2.11.3">
<h3 data-number="2.11.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">2.11.3</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><strong>Assuming <span class="math inline">\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)</span> without independence</strong>
<ul>
<li>This only works when <span class="math inline">X</span> and <span class="math inline">Y</span> are independent!</li>
</ul></li>
<li><strong>Confusing the <span class="math inline">\mathbb{V}(X - Y)</span> formula</strong>
<ul>
<li>Remember: <span class="math inline">\mathbb{V}(X - Y) = \mathbb{V}(X) + \mathbb{V}(Y)</span> when independent (plus, not minus!)</li>
</ul></li>
<li><strong>Treating <span class="math inline">\mathbb{E}(X|Y)</span> as a number instead of a random variable</strong>
<ul>
<li><span class="math inline">\mathbb{E}(X|Y = y)</span> is a number, but <span class="math inline">\mathbb{E}(X|Y)</span> is a function of <span class="math inline">Y</span></li>
</ul></li>
<li><strong>Assuming uncorrelated means independent</strong>
<ul>
<li>Zero correlation does NOT imply independence (remember <span class="math inline">X</span> and <span class="math inline">X^2</span>)</li>
</ul></li>
<li><strong>Forgetting existence conditions</strong>
<ul>
<li>Not all distributions have finite expectation (Cauchy!)</li>
</ul></li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="2.11.4">
<h3 data-number="2.11.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">2.11.4</span> Chapter Connections</h3>
<p>This chapter builds on Chapter 1’s probability foundations and provides essential tools for all statistical inference:</p>
<ul>
<li><strong>From Chapter 1</strong>: We’ve formalized the expectation concept briefly introduced with random variables, showing how it connects to supervised learning through risk minimization and cross-entropy loss</li>
<li><strong>Next - Chapter 3 (Convergence &amp; Inference)</strong>: The sample mean and variance we studied will be shown to converge to population values (Law of Large Numbers) and have predictable distributions (Central Limit Theorem), justifying their use as estimators</li>
<li><strong>Chapter 4 (Bootstrap)</strong>: We’ll use the plug-in principle with empirical distributions to estimate variances and other functionals when theoretical calculations become intractable</li>
<li><strong>Future Applications</strong>: Conditional expectation forms the foundation for regression (Chapter 5+), while variance decomposition and covariance matrices are central to multivariate methods throughout the course</li>
</ul>
</section>
<section id="self-test-problems" class="level3" data-number="2.11.5">
<h3 data-number="2.11.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">2.11.5</span> Self-Test Problems</h3>
<p>Try these problems to test your understanding:</p>
<ol type="1">
<li><p><strong>Linearity puzzle</strong>: In a class of 30 students, each has probability 1/365 of having a birthday today. What’s the expected number of birthdays today? (Ignore leap years) <em>Hint: Define indicator variables <span class="math inline">X_i</span> for each student. What is <span class="math inline">\mathbb{E}(X_i)</span>? Then use linearity.</em></p></li>
<li><p><strong>Variance with correlation</strong>: If <span class="math inline">\mathbb{V}(X) = 4</span>, <span class="math inline">\mathbb{V}(Y) = 9</span>, and <span class="math inline">\rho(X,Y) = 0.5</span>, find <span class="math inline">\mathbb{V}(2X - Y)</span>.</p></li>
<li><p><strong>Conditional expectation</strong>: Toss a fair coin. If heads, draw <span class="math inline">X \sim \mathcal{N}(0, 1)</span>. If tails, draw <span class="math inline">X \sim \mathcal{N}(2, 1)</span>. Find <span class="math inline">\mathbb{E}(X)</span> and <span class="math inline">\mathbb{V}(X)</span>.</p></li>
<li><p><strong>Matrix expectation</strong>: If <span class="math inline">\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_2)</span> and <span class="math inline">\mathbf{A} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}</span>, find the distribution of <span class="math inline">\mathbf{AX}</span>.</p></li>
</ol>
</section>
<section id="python-and-r-reference" class="level3" data-number="2.11.6">
<h3 data-number="2.11.6" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">2.11.6</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255586-758-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-758-1" role="tab" aria-controls="tabset-1757255586-758-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255586-758-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255586-758-2" role="tab" aria-controls="tabset-1757255586-758-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255586-758-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255586-758-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic expectations</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete expectation</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.<span class="bu">sum</span>(x <span class="op">*</span> probabilities)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>variance <span class="op">=</span> np.<span class="bu">sum</span>((x <span class="op">-</span> mean)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> probabilities)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">15</span>, <span class="dv">1000</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>sample_var <span class="op">=</span> np.var(data, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># ddof=1 for unbiased</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sample_std <span class="op">=</span> np.std(data, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Covariance and correlation</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> np.random.multivariate_normal([<span class="dv">0</span>, <span class="dv">0</span>], [[<span class="dv">1</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">1</span>]], <span class="dv">1000</span>).T</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>covariance <span class="op">=</span> np.cov(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># or np.cov(x, y, ddof=1)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>correlation <span class="op">=</span> np.corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix operations</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>mean_vector <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>cov_matrix <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">1</span>]])</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.multivariate_normal(mean_vector, cov_matrix, <span class="dv">1000</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear transformation</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>transformed_mean <span class="op">=</span> A <span class="op">@</span> mean_vector</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>transformed_cov <span class="op">=</span> A <span class="op">@</span> cov_matrix <span class="op">@</span> A.T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255586-758-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255586-758-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic expectations</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>probabilities <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete expectation</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>mean_val <span class="ot">&lt;-</span> <span class="fu">sum</span>(x <span class="sc">*</span> probabilities)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>variance_val <span class="ot">&lt;-</span> <span class="fu">sum</span>((x <span class="sc">-</span> mean_val)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> probabilities)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(data)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>sample_var <span class="ot">&lt;-</span> <span class="fu">var</span>(data)  <span class="co"># Automatically uses n-1</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>sample_sd <span class="ot">&lt;-</span> <span class="fu">sd</span>(data)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Covariance and correlation</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">1000</span>, <span class="at">mu =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>                   <span class="at">Sigma =</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>covariance <span class="ot">&lt;-</span> <span class="fu">cov</span>(samples[,<span class="dv">1</span>], samples[,<span class="dv">2</span>])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>correlation <span class="ot">&lt;-</span> <span class="fu">cor</span>(samples[,<span class="dv">1</span>], samples[,<span class="dv">2</span>])</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix operations</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>mean_vector <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>cov_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear transformation</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>transformed_mean <span class="ot">&lt;-</span> A <span class="sc">%*%</span> mean_vector</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>transformed_cov <span class="ot">&lt;-</span> A <span class="sc">%*%</span> cov_matrix <span class="sc">%*%</span> <span class="fu">t</span>(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="2.11.7">
<h3 data-number="2.11.7" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">2.11.7</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-35-contents" aria-controls="callout-35" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-35" class="callout-35-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This table maps sections in these lecture notes to the corresponding sections in <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> (“All of Statistics” or AoS).</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding AoS Section(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction and Motivation</strong></td>
<td style="text-align: left;">Expanded material from the slides, contextualizing expectation for machine learning.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Foundations of Expectation</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Definition and Basic Properties</td>
<td style="text-align: left;">AoS §3.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Existence of Expectation</td>
<td style="text-align: left;">AoS §3.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Expectation of Functions</td>
<td style="text-align: left;">AoS §3.1 (Rule of the Lazy Statistician)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Properties of Expectation</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Linearity Property</td>
<td style="text-align: left;">AoS §3.2 (Theorem 3.11)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Independence and Products</td>
<td style="text-align: left;">AoS §3.2 (Theorem 3.13)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Variance and Its Properties</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Measuring Spread</td>
<td style="text-align: left;">AoS §3.3 (Definition 3.14)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Properties of Variance</td>
<td style="text-align: left;">AoS §3.3 (Theorem 3.15)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Sample Mean and Variance</strong></td>
<td style="text-align: left;">AoS §3.3 (Definitions and Theorem 3.17)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Covariance and Correlation</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Linear Relationships</td>
<td style="text-align: left;">AoS §3.3 (Definition 3.18)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Properties of Covariance and Correlation</td>
<td style="text-align: left;">AoS §3.3 (Theorem 3.19)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Variance of Sums (General Case)</td>
<td style="text-align: left;">AoS §3.3 (Theorem 3.20)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Expectation with Matrices</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Random Vectors &amp; Covariance Matrix</td>
<td style="text-align: left;">AoS §3.4, §14.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Linear Transformations</td>
<td style="text-align: left;">AoS §3.4 (Lemma 3.21)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Interpreting the Covariance Matrix</td>
<td style="text-align: left;">New material (connects to PCA).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Example: Multinomial Covariance</td>
<td style="text-align: left;">AoS §3.4</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Conditional Expectation</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Expectation Given Information</td>
<td style="text-align: left;">AoS §3.5 (Definition 3.22)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Properties (Iterated Expectations)</td>
<td style="text-align: left;">AoS §3.5 (Theorem 3.24)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Conditional Variance (Total Variance)</td>
<td style="text-align: left;">AoS §3.5 (Definition 3.26, Theorem 3.27)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>More About the Normal Distribution</strong></td>
<td style="text-align: left;">New material, applying expectation concepts. Some properties from AoS §2.10 are revisited.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Chapter Summary and Connections</strong></td>
<td style="text-align: left;">New summary material.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-reading" class="level3" data-number="2.11.8">
<h3 data-number="2.11.8" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">2.11.8</span> Further Reading</h3>
<ul>
<li><strong>Statistical perspective</strong>: Casella &amp; Berger, “Statistical Inference”</li>
<li><strong>Machine learning view</strong>: Bishop, “Pattern Recognition and Machine Learning” Chapters 1 and 2</li>
<li><strong>Matrix cookbook</strong>: Petersen &amp; Pedersen, “The Matrix Cookbook” (for multivariate formulas) – <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">link</a></li>
</ul>
<hr>
<p><em>Next time you compute a sample mean, remember: you’re estimating an expectation. When you minimize a loss function, you’re approximating an expected loss. The gap between what we can compute (sample statistics) and what we want to know (population parameters) drives all of statistical inference. Expectation is the bridge!</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Almost surely (a.s.) means “with probability 1”. A random variable is almost surely constant if <span class="math inline">\mathbb{P}(X = c) = 1</span> for some constant <span class="math inline">c</span>. The “almost” acknowledges that technically there could be probability-0 events where <span class="math inline">X \neq c</span>, but these never occur in practice.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/01-probability-foundations.html" class="pagination-link" aria-label="Probability Foundations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/03-convergence-inference.html" class="pagination-link" aria-label="Convergence and The Basics of Inference">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Expectation</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain the concept of expectation and its role in summarizing distributions and machine learning.</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply key properties of expectation, especially its linearity, to simplify complex calculations.</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate and interpret variance, covariance, and correlation as measures of spread and linear dependence.</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extend expectation concepts to random vectors, including mean vectors and covariance matrices.</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define and apply conditional expectation and understand its key properties.</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>This chapter covers expectation, variance, and related concepts essential for statistical inference. The material is adapted and expanded from Chapter 3 of @wasserman2013all, which interested readers are encouraged to consult directly.</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction and Motivation</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Essence of Supervised Machine Learning</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>The fundamental goal of supervised machine learning is seemingly simple: train a model that makes successful predictions on new, unseen data.</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>However, this goal hides a deeper challenge that lies at the heart of statistics.</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>When we train a model, we work with a finite training set:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>$$X_1, \ldots, X_n \sim F_X$$</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>where $F_X$ is the data generating distribution. Our true objective is to find a model that minimizes the **expected loss**:</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">L(X)</span><span class="co">]</span>$$</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>over the entire distribution $F_X$, where $L(\cdot)$ is some suitable loss function. But we can only compute the **empirical loss**:</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n} \sum_{i=1}^n L(X_i),$$</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>which is the loss function summed over the training data.</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>This gap between what we want (*expected* loss) and what we can calculate (*empirical* loss) is the central challenge of machine learning.</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>The concept of expectation provides the mathematical framework to understand and bridge this gap.</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>**Goal**: Imagine training a neural network to classify cat and dog images.</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>You have 10,000 training images, but your model needs to work on millions of future images it's never seen. When your model achieves 98% accuracy on training data, that's just the average over your specific 10,000 images. What you really care about is the accuracy over *all possible* cat and dog images that exist or could exist.</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>This gap—between what we can measure (training performance) and what we want (real-world performance)—is why expectation is central to machine learning. Every loss function is secretly an expectation!</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>The **cross-entropy loss** used for classification tasks measures how "surprised" your model is by the true labels. Lower surprise = better predictions. The key insight: we minimize the *average surprise* over our training data, hoping it approximates the *expected surprise* over all possible data.</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical </span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>**Setup**: We want to classify images as cats ($y=1$) or dogs ($y=0$).</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>Our model outputs:</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>$$ \hat{p}(x) = \text{predicted probability that image } x \text{ is a cat.} $$</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>**Step 1: Define the loss for one example**</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>For a single image-label pair $(x, y)$, the cross-entropy loss is:</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>$$L(x, y) = -<span class="co">[</span><span class="ot">y \log(\hat{p}(x)) + (1-y) \log(1-\hat{p}(x))</span><span class="co">]</span>$$</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>This penalizes wrong predictions:</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $y = 1$ (cat) but $\hat{p}(x) \approx 0$: large loss</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $y = 0$ (dog) but $\hat{p}(x) \approx 1$: large loss</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Correct predictions → small loss</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>**Step 2: The fundamental problem**</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>What we want to minimize (expected loss over all possible images):</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>$$R_{\text{true}} = \mathbb{E}_{(X,Y)}<span class="co">[</span><span class="ot">L(X, Y)</span><span class="co">]</span>$$</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>What we can compute (average loss over training data):</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>$$R_{\text{train}} = \frac{1}{n} \sum_{i=1}^n L(x_i, y_i)$$</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>**Step 3: The connection to expectation**</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>Notice that $R_{\text{train}}$ is just the *sample mean* of the losses, while $R_{\text{true}}$ is the *expectation* of the loss. By the Law of Large Numbers:</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>$$R_{\text{train}} \xrightarrow{n \to \infty} R_{\text{true}}$$</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>**This is why machine learning is fundamentally about expectation!**</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>Let's see cross-entropy loss in action with a simple cat/dog classifier.</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>We'll simulate predictions and compute both the loss on a small training set and the true expected loss over the entire population.</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>Note that in this example we are not *training* a model. We are given a model, and we want to compute its loss.</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>What we see is how close the empirical loss is to the expected loss as we change the dataset size over which we compute the loss.</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a simple "classifier" that predicts cat probability based on </span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a><span class="co"># a single feature (e.g., "ear pointiness" from 0 to 1)</span></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a><span class="co"># True probabilities: cats have pointier ears</span></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> true_cat_probability(ear_pointiness):</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Logistic function: more pointy → more likely cat</span></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">5</span> <span class="op">*</span> (ear_pointiness <span class="op">-</span> <span class="fl">0.5</span>)))</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate population data</span></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>n_population <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>ear_pointiness <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, n_population)</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>true_probs <span class="op">=</span> true_cat_probability(ear_pointiness)</span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample actual labels based on true probabilities</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> (np.random.random(n_population) <span class="op">&lt;</span> true_probs).astype(<span class="bu">int</span>)</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a><span class="co"># Our (imperfect) model's predictions</span></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_prediction(x):</span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Slightly wrong sigmoid (shifted and less steep)</span></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">3</span> <span class="op">*</span> (x <span class="op">-</span> <span class="fl">0.45</span>)))</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute cross-entropy loss</span></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_loss(y_true, y_pred):</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Avoid log(0) with small epsilon</span></span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.clip(y_pred, eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(y_true <span class="op">*</span> np.log(y_pred) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y_true) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> y_pred))</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a><span class="co"># Show what training sees vs reality</span></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a><span class="co"># Left: Model predictions vs truth</span></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>x_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_plot, true_cat_probability(x_plot), <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, </span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'True P(cat|x)'</span>)</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_plot, model_prediction(x_plot), <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'Model P̂(cat|x)'</span>)</span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>ax1.scatter(ear_pointiness[:<span class="dv">100</span>], labels[:<span class="dv">100</span>], alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>[<span class="st">'red'</span> <span class="cf">if</span> l <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'blue'</span> <span class="cf">for</span> l <span class="kw">in</span> labels[:<span class="dv">100</span>]])</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Ear Pointiness'</span>)</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Probability of Cat'</span>)</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Model vs Reality'</span>)</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: Empirical vs Expected Loss</span></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>training_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>]</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>empirical_losses <span class="op">=</span> []</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> training_sizes:</span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample n training examples</span></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.random.choice(n_population, n, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> ear_pointiness[idx]</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>    train_y <span class="op">=</span> labels[idx]</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>    train_pred <span class="op">=</span> model_prediction(train_x)</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Empirical loss on training set</span></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>    emp_loss <span class="op">=</span> np.mean(cross_entropy_loss(train_y, train_pred))</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>    empirical_losses.append(emp_loss)</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a><span class="co"># True expected loss over entire population</span></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a>all_predictions <span class="op">=</span> model_prediction(ear_pointiness)</span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a>expected_loss <span class="op">=</span> np.mean(cross_entropy_loss(labels, all_predictions))</span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a>ax2.semilogx(training_sizes, empirical_losses, <span class="st">'bo-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>             markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'Empirical Loss (Training)'</span>)</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span>expected_loss, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Expected Loss = </span><span class="sc">{</span>expected_loss<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Training Set Size'</span>)</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Cross-Entropy Loss'</span>)</span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Convergence to Expected Loss'</span>)</span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"With just 10 samples: empirical loss = </span><span class="sc">{</span>empirical_losses[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"With 5000 samples: empirical loss = </span><span class="sc">{</span>empirical_losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"True expected loss: </span><span class="sc">{</span>expected_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">As we get more training data to calculate the loss, our empirical"</span>)</span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"loss estimate gets closer to the true expected loss we care about!"</span>)</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Expectation Matters in ML and Beyond</span></span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a>The concept of expectation appears throughout data science and statistics:</span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Statistical Inference**: Estimating population parameters from samples</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Decision Theory**: Maximizing expected utility or minimizing expected risk</span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**A/B Testing**: Measuring expected treatment effects</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Financial Modeling**: Expected returns and risk assessment</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Loss Functions in Deep Learning**: Cross-entropy loss and ELBO in VAEs</span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a>In each case, we're trying to understand average behavior over some distribution, which is precisely what expectation captures.</span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a><span class="fu">## Foundations of Expectation</span></span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here's a reference table of key terms in this chapter:</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>| Expected value/Expectation | Odotusarvo | The mean of a distribution |</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a>| Mean | Keskiarvo | Same as expectation |</span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a>| Variance | Varianssi | Measure of spread |</span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a>| Standard deviation | Keskihajonta | Square root of variance |</span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>| Covariance | Kovarianssi | Linear relationship between variables |</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>| Correlation | Korrelaatio | Standardized covariance |</span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a>| Sample mean | Otoskeskiarvo | Average of data points |</span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a>| Sample variance | Otosvarianssi | Empirical measure of spread |</span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>| Conditional expectation | Ehdollinen odotusarvo | Mean given information |</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a>| Moment | Momentti | Powers of random variable |</span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a>| Random vector | Satunnaisvektori | Vector of random variables |</span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a>| Covariance matrix | Kovarianssimatriisi | Matrix of covariances |</span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>| Precision matrix | Tarkkuusmatriisi | Inverse of covariance matrix |</span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a>| Moment generating function | Momenttigeneroiva funktio | Transform for finding moments |</span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a>| Central moment | Keskusmomentti | Moment about the mean |</span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a><span class="fu">### Definition and Basic Properties</span></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>The **expected value**, or **mean**, or **first moment** of a random variable $X$ is defined to be:</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \begin{cases}</span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a>\sum_x x \mathbb{P}(X = x) &amp; \text{if } X \text{ is discrete} <span class="sc">\\</span></span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}} x f_X(x) \, dx &amp; \text{if } X \text{ is continuous}</span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a>assuming the sum (or integral) is well defined. We use the following notation interchangeably:</span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \mathbb{E} X = \mu = \mu_X$$</span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a>The expectation represents the average value of the distribution -- the balance point where the distribution would balance if it were a physical object.</span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a>**Notation:** The lowercase Greek letter $\mu$ (<span class="co">[</span><span class="ot">mu</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Mu_(letter)); pronounced *mju*) is universally used to denote the mean.</span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simplified Notation</span></span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a>In this course, we will write the expectation using the simplified notation:</span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \int x f_X(x) dx$$</span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a>when the type of random variable is unspecified and could be either continuous or discrete.</span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a>For a discrete random variable, you would substitute the integral with a sum, and the PDF $f_X(x)$ (probability density function) with the PMF $\mathbb{P}_X(x)$ (probability mass function), as seen in Chapter 1 of the lecture notes.</span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a>Note that this is an abuse of notation and is not mathematically correct, but we found it to be more intuitive in previous iterations of the course.</span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Simple Expectations</span></span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a>Let's calculate expectations for some basic distributions:</span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bernoulli(0.3)**: </span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a>   $$\mathbb{E}(X) = 0 \times 0.7 + 1 \times 0.3 = 0.3$$</span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Fair six-sided die**:</span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a>   $$\mathbb{E}(X) = \frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = \frac{21}{6} = 3.5$$</span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Two coin flips** (X = number of heads):</span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a>   $$\mathbb{E}(X) = 0 \times \frac{1}{4} + 1 \times \frac{1}{2} + 2 \times \frac{1}{4} = 1$$</span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a>Expectation is the "center of mass" of a distribution. Imagine:</span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Physical analogy**: If you made a <span class="co">[</span><span class="ot">histogram out of metal</span><span class="co">](https://johncanning.net/wp/?p=1863)</span>, the expected value is where you'd place a fulcrum to balance it perfectly.</span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Long-run average**: If you repeat an experiment millions of times and average the results, you'll get very close to the expectation. This isn't just intuition—it's a theorem (the Law of Large Numbers) we'll prove in Chapter 3.</span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fair price**: In gambling, the expectation tells you the fair price to pay for a game. If a lottery ticket has expected winnings of €2, then €2 is the break-even price.</span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a>Think of expectation as answering: "If I had to summarize this entire distribution with a single number pointing at its *center*, what would it be?"</span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a>The expectation or mean is not the *only* number we could use to represent the center of a distribution, but it is a very common choice suitable for most situations.</span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a>The expectation is a linear functional on the space of random variables. </span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a>For a random variable $X$ with distribution function $F$, the correct mathematical notation would be:</span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \int x \, dF(x)$$</span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a>This notation correctly unifies the discrete and continuous cases:</span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For discrete $X$: the integral becomes a sum over the jump points of $F$</span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For continuous $X$: we have $dF(x) = f_X(x)dx$</span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a>This notation is particularly useful when dealing with mixed distributions or when stating results that apply to both discrete and continuous random variables without writing separate formulas.</span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a>We won't be using this notation in the course, but you may find it in mathematical or statistical textbooks, including Wasserman (2013).</span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a>Let's demonstrate expectation through simulation, showing how sample averages converge to the true expectation. We'll also show a case where expectation doesn't exist (see next section).</span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> cauchy</span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up figure with subplots</span></span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convergence for Bernoulli(0.3)</span></span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a>n_flips <span class="op">=</span> <span class="dv">40000</span></span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> np.random.choice([<span class="dv">0</span>, <span class="dv">1</span>], size<span class="op">=</span>n_flips, p<span class="op">=</span>[<span class="dv">1</span><span class="op">-</span>p, p])</span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a>running_mean <span class="op">=</span> np.cumsum(flips) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_flips <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a>ax1.plot(running_mean, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span>p, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'True E[X] = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Number of trials'</span>)</span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb10-331"><a href="#cb10-331" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Sample Mean Converges to Expectation: Bernoulli(0.3)'</span>)</span>
<span id="cb10-332"><a href="#cb10-332" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb10-333"><a href="#cb10-333" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-334"><a href="#cb10-334" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="fl">0.2</span>, <span class="fl">0.4</span>)</span>
<span id="cb10-335"><a href="#cb10-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-336"><a href="#cb10-336" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Cauchy distribution - no expectation exists</span></span>
<span id="cb10-337"><a href="#cb10-337" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">40000</span></span>
<span id="cb10-338"><a href="#cb10-338" aria-hidden="true" tabindex="-1"></a>cauchy_samples <span class="op">=</span> cauchy.rvs(size<span class="op">=</span>n_samples, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-339"><a href="#cb10-339" aria-hidden="true" tabindex="-1"></a>cauchy_running_mean <span class="op">=</span> np.cumsum(cauchy_samples) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_samples <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb10-340"><a href="#cb10-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-341"><a href="#cb10-341" aria-hidden="true" tabindex="-1"></a>ax2.plot(cauchy_running_mean, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb10-342"><a href="#cb10-342" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-343"><a href="#cb10-343" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Number of samples'</span>)</span>
<span id="cb10-344"><a href="#cb10-344" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb10-345"><a href="#cb10-345" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Cauchy Distribution: Sample Mean Does Not Converge (No Expectation)'</span>)</span>
<span id="cb10-346"><a href="#cb10-346" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-347"><a href="#cb10-347" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb10-348"><a href="#cb10-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-349"><a href="#cb10-349" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-350"><a href="#cb10-350" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-351"><a href="#cb10-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-352"><a href="#cb10-352" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bernoulli: After </span><span class="sc">{</span>n_flips<span class="sc">}</span><span class="ss"> flips, sample mean = </span><span class="sc">{</span>running_mean[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-353"><a href="#cb10-353" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cauchy: After </span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss"> samples, sample mean = </span><span class="sc">{</span>cauchy_running_mean[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-354"><a href="#cb10-354" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Notice how Cauchy's sample mean keeps eventually jumping around,"</span>)</span>
<span id="cb10-355"><a href="#cb10-355" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"even when you think it's converging to zero!"</span>)</span>
<span id="cb10-356"><a href="#cb10-356" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-357"><a href="#cb10-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-358"><a href="#cb10-358" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-359"><a href="#cb10-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-360"><a href="#cb10-360" aria-hidden="true" tabindex="-1"></a><span class="fu">### Existence of Expectation</span></span>
<span id="cb10-361"><a href="#cb10-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-362"><a href="#cb10-362" aria-hidden="true" tabindex="-1"></a>Not all random variables have well-defined expectations. </span>
<span id="cb10-363"><a href="#cb10-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-364"><a href="#cb10-364" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-365"><a href="#cb10-365" aria-hidden="true" tabindex="-1"></a>The expectation $\mathbb{E}(X)$ exists if and only if:</span>
<span id="cb10-366"><a href="#cb10-366" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(|X|) = \int |x| f_X(x) \, dx &lt; \infty$$</span>
<span id="cb10-367"><a href="#cb10-367" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-368"><a href="#cb10-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-369"><a href="#cb10-369" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb10-370"><a href="#cb10-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-371"><a href="#cb10-371" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Cauchy Distribution</span></span>
<span id="cb10-372"><a href="#cb10-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-373"><a href="#cb10-373" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">Cauchy distribution</span><span class="co">](https://en.wikipedia.org/wiki/Cauchy_distribution)</span> is a classic example of probability density with no expectation:</span>
<span id="cb10-374"><a href="#cb10-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-375"><a href="#cb10-375" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \frac{1}{\pi(1 + x^2)}$$</span>
<span id="cb10-376"><a href="#cb10-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-377"><a href="#cb10-377" aria-hidden="true" tabindex="-1"></a>To check if expectation exists:</span>
<span id="cb10-378"><a href="#cb10-378" aria-hidden="true" tabindex="-1"></a>$$\int_{-\infty}^{\infty} |x| \cdot \frac{1}{\pi(1 + x^2)} \, dx = \frac{2}{\pi} \int_0^{\infty} \frac{x}{1 + x^2} \, dx = \infty$$</span>
<span id="cb10-379"><a href="#cb10-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-380"><a href="#cb10-380" aria-hidden="true" tabindex="-1"></a>The integral diverges! This means:</span>
<span id="cb10-381"><a href="#cb10-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-382"><a href="#cb10-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample averages don't converge to any value</span>
<span id="cb10-383"><a href="#cb10-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The Law of Large Numbers doesn't apply</span>
<span id="cb10-384"><a href="#cb10-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extreme observations are common due to heavy tails</span>
<span id="cb10-385"><a href="#cb10-385" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-386"><a href="#cb10-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-387"><a href="#cb10-387" aria-hidden="true" tabindex="-1"></a><span class="fu">### Expectation of Functions</span></span>
<span id="cb10-388"><a href="#cb10-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-389"><a href="#cb10-389" aria-hidden="true" tabindex="-1"></a>Often we need the expectation of a function of a random variable. The "Rule of the Lazy Statistician" saves us from finding the distribution of the transformed variable.</span>
<span id="cb10-390"><a href="#cb10-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-391"><a href="#cb10-391" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Rule of the Lazy Statistician"}</span>
<span id="cb10-392"><a href="#cb10-392" aria-hidden="true" tabindex="-1"></a>Let $Y = r(X)$. Then:</span>
<span id="cb10-393"><a href="#cb10-393" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(Y) = \mathbb{E}(r(X)) = \int r(x) f_X(x) \, dx$$</span>
<span id="cb10-394"><a href="#cb10-394" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-395"><a href="#cb10-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-396"><a href="#cb10-396" aria-hidden="true" tabindex="-1"></a>This result is incredibly useful—we can find $\mathbb{E}(Y)$ without determining $f_Y(y)$!</span>
<span id="cb10-397"><a href="#cb10-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-398"><a href="#cb10-398" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-399"><a href="#cb10-399" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Breaking a Stick</span></span>
<span id="cb10-400"><a href="#cb10-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-401"><a href="#cb10-401" aria-hidden="true" tabindex="-1"></a>A stick of unit length is broken at a random point. What's the expected length of the longer piece?</span>
<span id="cb10-402"><a href="#cb10-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-403"><a href="#cb10-403" aria-hidden="true" tabindex="-1"></a>Let $X \sim \text{Uniform}(0,1)$ be the break point. The longer piece has length:</span>
<span id="cb10-404"><a href="#cb10-404" aria-hidden="true" tabindex="-1"></a>$$Y = r(X) = \max<span class="sc">\{</span>X, 1-X<span class="sc">\}</span>$$</span>
<span id="cb10-405"><a href="#cb10-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-406"><a href="#cb10-406" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-407"><a href="#cb10-407" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb10-408"><a href="#cb10-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-409"><a href="#cb10-409" aria-hidden="true" tabindex="-1"></a>We can identify that:</span>
<span id="cb10-410"><a href="#cb10-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-411"><a href="#cb10-411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $X &lt; 1/2$: longer piece = $1-X$</span>
<span id="cb10-412"><a href="#cb10-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $X \geq 1/2$: longer piece = $X$</span>
<span id="cb10-413"><a href="#cb10-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-414"><a href="#cb10-414" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb10-415"><a href="#cb10-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-416"><a href="#cb10-416" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(Y) = \int_0^{1/2} (1-x) \cdot 1 \, dx + \int_{1/2}^1 x \cdot 1 \, dx$$</span>
<span id="cb10-417"><a href="#cb10-417" aria-hidden="true" tabindex="-1"></a>$$= \left<span class="co">[</span><span class="ot">x - \frac{x^2}{2}\right</span><span class="co">]</span>_0^{1/2} + \left[\frac{x^2}{2}\right]_{1/2}^1$$</span>
<span id="cb10-418"><a href="#cb10-418" aria-hidden="true" tabindex="-1"></a>$$= \left(\frac{1}{2} - \frac{1}{8}\right) + \left(\frac{1}{2} - \frac{1}{8}\right) = \frac{3}{4}$$</span>
<span id="cb10-419"><a href="#cb10-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-420"><a href="#cb10-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-423"><a href="#cb10-423" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-424"><a href="#cb10-424" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-425"><a href="#cb10-425" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb10-426"><a href="#cb10-426" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the breaking stick problem</span></span>
<span id="cb10-427"><a href="#cb10-427" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-428"><a href="#cb10-428" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-429"><a href="#cb10-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-430"><a href="#cb10-430" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb10-431"><a href="#cb10-431" aria-hidden="true" tabindex="-1"></a>longer_piece <span class="op">=</span> np.maximum(x, <span class="dv">1</span><span class="op">-</span>x)</span>
<span id="cb10-432"><a href="#cb10-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-433"><a href="#cb10-433" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb10-434"><a href="#cb10-434" aria-hidden="true" tabindex="-1"></a>plt.plot(x, longer_piece, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Length of longer piece'</span>)</span>
<span id="cb10-435"><a href="#cb10-435" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, <span class="dv">0</span>, longer_piece, alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'lightblue'</span>)</span>
<span id="cb10-436"><a href="#cb10-436" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.75</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb10-437"><a href="#cb10-437" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="st">'E[longer piece] = 3/4'</span>)</span>
<span id="cb10-438"><a href="#cb10-438" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-439"><a href="#cb10-439" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Break point (X)'</span>)</span>
<span id="cb10-440"><a href="#cb10-440" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Length of longer piece'</span>)</span>
<span id="cb10-441"><a href="#cb10-441" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Breaking a Unit Stick: Expected Length of Longer Piece'</span>)</span>
<span id="cb10-442"><a href="#cb10-442" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-443"><a href="#cb10-443" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-444"><a href="#cb10-444" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb10-445"><a href="#cb10-445" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb10-446"><a href="#cb10-446" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-447"><a href="#cb10-447" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-448"><a href="#cb10-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-449"><a href="#cb10-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-450"><a href="#cb10-450" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-451"><a href="#cb10-451" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-452"><a href="#cb10-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-453"><a href="#cb10-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-454"><a href="#cb10-454" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-455"><a href="#cb10-455" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Exponential Prize Game</span></span>
<span id="cb10-456"><a href="#cb10-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-457"><a href="#cb10-457" aria-hidden="true" tabindex="-1"></a>A game show offers a prize based on rolling a die: if you roll $X$, you win $2^X$ euros. What's your expected winnings?</span>
<span id="cb10-458"><a href="#cb10-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-459"><a href="#cb10-459" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-460"><a href="#cb10-460" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb10-461"><a href="#cb10-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-462"><a href="#cb10-462" aria-hidden="true" tabindex="-1"></a>Using the lazy statistician's rule with $X \sim \text{DiscreteUniform}(1,6)$:</span>
<span id="cb10-463"><a href="#cb10-463" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(2^X) = \sum_{x=1}^6 2^x \cdot \frac{1}{6} = \frac{1}{6}(2^1 + 2^2 + \cdots + 2^6)$$</span>
<span id="cb10-464"><a href="#cb10-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-465"><a href="#cb10-465" aria-hidden="true" tabindex="-1"></a>Direct calculation:</span>
<span id="cb10-466"><a href="#cb10-466" aria-hidden="true" tabindex="-1"></a>$$= \frac{1}{6}(2 + 4 + 8 + 16 + 32 + 64) = \frac{126}{6} = 21$$</span>
<span id="cb10-467"><a href="#cb10-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-468"><a href="#cb10-468" aria-hidden="true" tabindex="-1"></a>So you expect to win €21 on average.</span>
<span id="cb10-469"><a href="#cb10-469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-470"><a href="#cb10-470" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-471"><a href="#cb10-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-472"><a href="#cb10-472" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb10-473"><a href="#cb10-473" aria-hidden="true" tabindex="-1"></a>**Special case**: Probability as expectation of indicator functions.</span>
<span id="cb10-474"><a href="#cb10-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-475"><a href="#cb10-475" aria-hidden="true" tabindex="-1"></a>If $A$ is an event and the indicator function is defined as:</span>
<span id="cb10-476"><a href="#cb10-476" aria-hidden="true" tabindex="-1"></a>$$I_A(x) = \begin{cases}</span>
<span id="cb10-477"><a href="#cb10-477" aria-hidden="true" tabindex="-1"></a>1 &amp; \text{if } x \in A <span class="sc">\\</span></span>
<span id="cb10-478"><a href="#cb10-478" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{if } x \notin A</span>
<span id="cb10-479"><a href="#cb10-479" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb10-480"><a href="#cb10-480" aria-hidden="true" tabindex="-1"></a>then:</span>
<span id="cb10-481"><a href="#cb10-481" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(I_A(X)) = \mathbb{P}(X \in A)$$</span>
<span id="cb10-482"><a href="#cb10-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-483"><a href="#cb10-483" aria-hidden="true" tabindex="-1"></a>This shows that probability is just a special case of expectation!</span>
<span id="cb10-484"><a href="#cb10-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-485"><a href="#cb10-485" aria-hidden="true" tabindex="-1"></a>This trick of using the indicator function with expectations is used commonly in probability, statistics and machine learning.</span>
<span id="cb10-486"><a href="#cb10-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-487"><a href="#cb10-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-488"><a href="#cb10-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-489"><a href="#cb10-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-490"><a href="#cb10-490" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Expectation</span></span>
<span id="cb10-491"><a href="#cb10-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-492"><a href="#cb10-492" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Linearity Property</span></span>
<span id="cb10-493"><a href="#cb10-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-494"><a href="#cb10-494" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-495"><a href="#cb10-495" aria-hidden="true" tabindex="-1"></a>If $X_1, \ldots, X_n$ are random variables and $a_1, \ldots, a_n$ are constants, then:</span>
<span id="cb10-496"><a href="#cb10-496" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i \mathbb{E}(X_i)$$</span>
<span id="cb10-497"><a href="#cb10-497" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-498"><a href="#cb10-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-499"><a href="#cb10-499" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb10-500"><a href="#cb10-500" aria-hidden="true" tabindex="-1"></a><span class="fu">## Possibly The Most Important Result in This Course</span></span>
<span id="cb10-501"><a href="#cb10-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-502"><a href="#cb10-502" aria-hidden="true" tabindex="-1"></a>**Expectation is LINEAR!**</span>
<span id="cb10-503"><a href="#cb10-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-504"><a href="#cb10-504" aria-hidden="true" tabindex="-1"></a>This property:</span>
<span id="cb10-505"><a href="#cb10-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-506"><a href="#cb10-506" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works WITHOUT independence (unlike the product rule)</span>
<span id="cb10-507"><a href="#cb10-507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simplifies hard calculations</span>
<span id="cb10-508"><a href="#cb10-508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Is the key to understanding sampling distributions</span>
<span id="cb10-509"><a href="#cb10-509" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Will be used in almost every proof and application</span>
<span id="cb10-510"><a href="#cb10-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-511"><a href="#cb10-511" aria-hidden="true" tabindex="-1"></a>If you remember only one thing from this chapter, remember that expectation is linear. You'll use it constantly throughout statistics and machine learning!</span>
<span id="cb10-512"><a href="#cb10-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-513"><a href="#cb10-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-514"><a href="#cb10-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applications of Linearity</span></span>
<span id="cb10-515"><a href="#cb10-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-516"><a href="#cb10-516" aria-hidden="true" tabindex="-1"></a>The power of linearity becomes clear when we use it to solve problems that would be difficult otherwise.</span>
<span id="cb10-517"><a href="#cb10-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-518"><a href="#cb10-518" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-519"><a href="#cb10-519" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Binomial Mean via Indicator Decomposition</span></span>
<span id="cb10-520"><a href="#cb10-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-521"><a href="#cb10-521" aria-hidden="true" tabindex="-1"></a>Let $X \sim \text{Binomial}(n, p)$. Finding $\mathbb{E}(X)$ directly requires evaluating:</span>
<span id="cb10-522"><a href="#cb10-522" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \sum_{x=0}^n x \binom{n}{x} p^x (1-p)^{n-x}$$</span>
<span id="cb10-523"><a href="#cb10-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-524"><a href="#cb10-524" aria-hidden="true" tabindex="-1"></a>Have fun calculating this! But with linearity, it's trivial.</span>
<span id="cb10-525"><a href="#cb10-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-526"><a href="#cb10-526" aria-hidden="true" tabindex="-1"></a>Remember that $\text{Binomial}(n, p)$ is the distribution of the sum of $n$ $\text{Bernoulli}(p)$ random variables.</span>
<span id="cb10-527"><a href="#cb10-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-528"><a href="#cb10-528" aria-hidden="true" tabindex="-1"></a>Thus, we can write $X = \sum_{i=1}^n X_i$, where $X_i$ are independent Bernoulli($p$) indicators. </span>
<span id="cb10-529"><a href="#cb10-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-530"><a href="#cb10-530" aria-hidden="true" tabindex="-1"></a>With this, we have:</span>
<span id="cb10-531"><a href="#cb10-531" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \mathbb{E}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathbb{E}(X_i) = \sum_{i=1}^n p = np$$</span>
<span id="cb10-532"><a href="#cb10-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-533"><a href="#cb10-533" aria-hidden="true" tabindex="-1"></a>Done!</span>
<span id="cb10-534"><a href="#cb10-534" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-535"><a href="#cb10-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-536"><a href="#cb10-536" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-537"><a href="#cb10-537" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Linearity Works Even with Dependent Variables</span></span>
<span id="cb10-538"><a href="#cb10-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-539"><a href="#cb10-539" aria-hidden="true" tabindex="-1"></a>A common misconception is that linearity of expectation requires independence. It doesn't! Let's demonstrate this crucial fact by computing $\mathbb{E}<span class="co">[</span><span class="ot">2X + 3Y</span><span class="co">]</span>$ where $X$ and $Y$ are strongly correlated.</span>
<span id="cb10-540"><a href="#cb10-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-541"><a href="#cb10-541" aria-hidden="true" tabindex="-1"></a>We'll generate $X$ and $Y$ with correlation 0.8 (highly dependent!) and verify that linearity still holds:</span>
<span id="cb10-542"><a href="#cb10-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-545"><a href="#cb10-545" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-546"><a href="#cb10-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-547"><a href="#cb10-547" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb10-548"><a href="#cb10-548" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrating linearity even with dependence</span></span>
<span id="cb10-549"><a href="#cb10-549" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-550"><a href="#cb10-550" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-551"><a href="#cb10-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-552"><a href="#cb10-552" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-553"><a href="#cb10-553" aria-hidden="true" tabindex="-1"></a>n_sims <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb10-554"><a href="#cb10-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-555"><a href="#cb10-555" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate correlated X and Y</span></span>
<span id="cb10-556"><a href="#cb10-556" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb10-557"><a href="#cb10-557" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">0.8</span>], [<span class="fl">0.8</span>, <span class="dv">1</span>]]  <span class="co"># Correlation = 0.8</span></span>
<span id="cb10-558"><a href="#cb10-558" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.multivariate_normal(mean, cov, n_sims)</span>
<span id="cb10-559"><a href="#cb10-559" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> samples[:, <span class="dv">0</span>]</span>
<span id="cb10-560"><a href="#cb10-560" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> samples[:, <span class="dv">1</span>]</span>
<span id="cb10-561"><a href="#cb10-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-562"><a href="#cb10-562" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute E[2X + 3Y] empirically</span></span>
<span id="cb10-563"><a href="#cb10-563" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>X <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>Y</span>
<span id="cb10-564"><a href="#cb10-564" aria-hidden="true" tabindex="-1"></a>empirical_mean <span class="op">=</span> np.mean(Z)</span>
<span id="cb10-565"><a href="#cb10-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-566"><a href="#cb10-566" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical value using linearity</span></span>
<span id="cb10-567"><a href="#cb10-567" aria-hidden="true" tabindex="-1"></a>theoretical_mean <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>mean[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>mean[<span class="dv">1</span>]</span>
<span id="cb10-568"><a href="#cb10-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-569"><a href="#cb10-569" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb10-570"><a href="#cb10-570" aria-hidden="true" tabindex="-1"></a>plt.hist(Z, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'green'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb10-571"><a href="#cb10-571" aria-hidden="true" tabindex="-1"></a>plt.axvline(empirical_mean, color<span class="op">=</span><span class="st">'blue'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb10-572"><a href="#cb10-572" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Empirical: </span><span class="sc">{</span>empirical_mean<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb10-573"><a href="#cb10-573" aria-hidden="true" tabindex="-1"></a>plt.axvline(theoretical_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb10-574"><a href="#cb10-574" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Theoretical: </span><span class="sc">{</span>theoretical_mean<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb10-575"><a href="#cb10-575" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'2X + 3Y'</span>)</span>
<span id="cb10-576"><a href="#cb10-576" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb10-577"><a href="#cb10-577" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Linearity of Expectation Works Even with Dependent Variables!'</span>)</span>
<span id="cb10-578"><a href="#cb10-578" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-579"><a href="#cb10-579" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-580"><a href="#cb10-580" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-581"><a href="#cb10-581" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-582"><a href="#cb10-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-583"><a href="#cb10-583" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X and Y are dependent (correlation = 0.8)"</span>)</span>
<span id="cb10-584"><a href="#cb10-584" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"But E[2X + 3Y] = 2E[X] + 3E[Y] still holds!"</span>)</span>
<span id="cb10-585"><a href="#cb10-585" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical: 2×</span><span class="sc">{</span>mean[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> + 3×</span><span class="sc">{</span>mean[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>theoretical_mean<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-586"><a href="#cb10-586" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical: </span><span class="sc">{</span>empirical_mean<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-587"><a href="#cb10-587" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-588"><a href="#cb10-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-589"><a href="#cb10-589" aria-hidden="true" tabindex="-1"></a>**Key takeaway**: Despite the strong correlation between $X$ and $Y$, the empirical mean of $2X + 3Y$ matches the theoretical value $2\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> + 3\mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$ perfectly. This is why linearity of expectation is so powerful—it works unconditionally!</span>
<span id="cb10-590"><a href="#cb10-590" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-591"><a href="#cb10-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-592"><a href="#cb10-592" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-593"><a href="#cb10-593" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Examples: More Applications of Linearity</span></span>
<span id="cb10-594"><a href="#cb10-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-595"><a href="#cb10-595" aria-hidden="true" tabindex="-1"></a>**Expected Number of Fixed Points in Random Permutation:**</span>
<span id="cb10-596"><a href="#cb10-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-597"><a href="#cb10-597" aria-hidden="true" tabindex="-1"></a>In a random permutation of {1, 2, ..., n}, what's the expected number of elements that stay in their original position?</span>
<span id="cb10-598"><a href="#cb10-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-599"><a href="#cb10-599" aria-hidden="true" tabindex="-1"></a>Let $X_i = 1$ if element $i$ stays in position $i$, and 0 otherwise. The total number of fixed points is $X = \sum_{i=1}^n X_i$.</span>
<span id="cb10-600"><a href="#cb10-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-601"><a href="#cb10-601" aria-hidden="true" tabindex="-1"></a>For any position $i$: $\mathbb{P}(X_i = 1) = \frac{1}{n}$ (element $i$ has probability $1/n$ of being in position $i$).</span>
<span id="cb10-602"><a href="#cb10-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-603"><a href="#cb10-603" aria-hidden="true" tabindex="-1"></a>Therefore: $\mathbb{E}(X_i) = \frac{1}{n}$</span>
<span id="cb10-604"><a href="#cb10-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-605"><a href="#cb10-605" aria-hidden="true" tabindex="-1"></a>By linearity:</span>
<span id="cb10-606"><a href="#cb10-606" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = \sum_{i=1}^n \mathbb{E}(X_i) = \sum_{i=1}^n \frac{1}{n} = 1$$</span>
<span id="cb10-607"><a href="#cb10-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-608"><a href="#cb10-608" aria-hidden="true" tabindex="-1"></a>Amazing! No matter how large $n$ is, we expect exactly 1 fixed point on average.</span>
<span id="cb10-609"><a href="#cb10-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-610"><a href="#cb10-610" aria-hidden="true" tabindex="-1"></a>**Fortune Doubling Game (from Wasserman Exercise 3.1):**</span>
<span id="cb10-611"><a href="#cb10-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-612"><a href="#cb10-612" aria-hidden="true" tabindex="-1"></a>You start with $c$ dollars. On each play, you either double your money or halve it, each with probability 1/2. What's your expected fortune after $n$ plays?</span>
<span id="cb10-613"><a href="#cb10-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-614"><a href="#cb10-614" aria-hidden="true" tabindex="-1"></a>Let $X_i$ be your fortune after $i$ plays. Then:</span>
<span id="cb10-615"><a href="#cb10-615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_0 = c$</span>
<span id="cb10-616"><a href="#cb10-616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i+1} = 2X_i$ with probability 1/2</span>
<span id="cb10-617"><a href="#cb10-617" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i+1} = X_i/2$ with probability 1/2</span>
<span id="cb10-618"><a href="#cb10-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-619"><a href="#cb10-619" aria-hidden="true" tabindex="-1"></a>Using conditional expectation:</span>
<span id="cb10-620"><a href="#cb10-620" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X_{i+1} | X_i) = \frac{1}{2}(2X_i) + \frac{1}{2}\left(\frac{X_i}{2}\right) = X_i + \frac{X_i}{4} = X_i$$</span>
<span id="cb10-621"><a href="#cb10-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-622"><a href="#cb10-622" aria-hidden="true" tabindex="-1"></a>By the law of iterated expectations:</span>
<span id="cb10-623"><a href="#cb10-623" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X_{i+1}) = \mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}(X_{i+1} | X_i)</span><span class="co">]</span> = \mathbb{E}(X_i)$$</span>
<span id="cb10-624"><a href="#cb10-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-625"><a href="#cb10-625" aria-hidden="true" tabindex="-1"></a>Therefore, by induction: $\mathbb{E}(X_n) = \mathbb{E}(X_0) = c$</span>
<span id="cb10-626"><a href="#cb10-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-627"><a href="#cb10-627" aria-hidden="true" tabindex="-1"></a>Your expected fortune never changes! This is an example of a *martingale*—a fair game where the expected future value equals the current value.</span>
<span id="cb10-628"><a href="#cb10-628" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-629"><a href="#cb10-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-630"><a href="#cb10-630" aria-hidden="true" tabindex="-1"></a><span class="fu">### Independence and Products</span></span>
<span id="cb10-631"><a href="#cb10-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-632"><a href="#cb10-632" aria-hidden="true" tabindex="-1"></a>While expectation is linear for all random variables, products require independence.</span>
<span id="cb10-633"><a href="#cb10-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-634"><a href="#cb10-634" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-635"><a href="#cb10-635" aria-hidden="true" tabindex="-1"></a>If $X_1, \ldots, X_n$ are **independent** random variables, then:</span>
<span id="cb10-636"><a href="#cb10-636" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}\left(\prod_{i=1}^n X_i\right) = \prod_{i=1}^n \mathbb{E}(X_i)$$</span>
<span id="cb10-637"><a href="#cb10-637" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-638"><a href="#cb10-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-639"><a href="#cb10-639" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb10-640"><a href="#cb10-640" aria-hidden="true" tabindex="-1"></a>This ONLY works with **independent** random variables! As a clear counterexample, $\mathbb{E}(X^2) \neq (\mathbb{E}(X))^2$ in general, since $X$ and $X$ are clearly not independent.</span>
<span id="cb10-641"><a href="#cb10-641" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-642"><a href="#cb10-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-643"><a href="#cb10-643" aria-hidden="true" tabindex="-1"></a><span class="fu">## Variance and Its Properties</span></span>
<span id="cb10-644"><a href="#cb10-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-645"><a href="#cb10-645" aria-hidden="true" tabindex="-1"></a><span class="fu">### Measuring Spread</span></span>
<span id="cb10-646"><a href="#cb10-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-647"><a href="#cb10-647" aria-hidden="true" tabindex="-1"></a>While expectation tells us the center of a distribution, variance measures how "spread out" it is.</span>
<span id="cb10-648"><a href="#cb10-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-649"><a href="#cb10-649" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-650"><a href="#cb10-650" aria-hidden="true" tabindex="-1"></a>Let $X$ be a random variable with mean $\mu$. The **variance** of $X$ -- denoted by $\sigma^2$, $\sigma_X^2$, $\mathbb{V}(X)$, $\mathbb{V}X$ or $\text{Var}(X)$ -- is defined as:</span>
<span id="cb10-651"><a href="#cb10-651" aria-hidden="true" tabindex="-1"></a>$$\sigma^2 = \mathbb{V}(X) = \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu)^2</span><span class="co">]</span>$$</span>
<span id="cb10-652"><a href="#cb10-652" aria-hidden="true" tabindex="-1"></a>assuming this expectation exists.</span>
<span id="cb10-653"><a href="#cb10-653" aria-hidden="true" tabindex="-1"></a>The **standard deviation** is </span>
<span id="cb10-654"><a href="#cb10-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-655"><a href="#cb10-655" aria-hidden="true" tabindex="-1"></a>$$\mathrm{sd}(X) = \sqrt{\mathbb{V}(X)}$$ </span>
<span id="cb10-656"><a href="#cb10-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-657"><a href="#cb10-657" aria-hidden="true" tabindex="-1"></a>and is also denoted by $\sigma$ and $\sigma_X$.</span>
<span id="cb10-658"><a href="#cb10-658" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-659"><a href="#cb10-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-660"><a href="#cb10-660" aria-hidden="true" tabindex="-1"></a>**Notation:** The lowercase Greek letter $\sigma$ (<span class="co">[</span><span class="ot">sigma</span><span class="co">](https://en.wikipedia.org/wiki/Sigma)</span>) is almost universally used to denote the standard deviation (and more generally the "scale" of a distribution, related to its spread).</span>
<span id="cb10-661"><a href="#cb10-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-662"><a href="#cb10-662" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-663"><a href="#cb10-663" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Both Variance and Standard Deviation?</span></span>
<span id="cb10-664"><a href="#cb10-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-665"><a href="#cb10-665" aria-hidden="true" tabindex="-1"></a>**Variance** ($\sigma^2$) is in squared units—if $X$ measures height in cm, then $\mathbb{V}(X)$ is in cm². This makes it hard to interpret directly.</span>
<span id="cb10-666"><a href="#cb10-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-667"><a href="#cb10-667" aria-hidden="true" tabindex="-1"></a>**Standard deviation** ($\sigma$) is in the same units as $X$, making it more interpretable: "typical deviation from the mean."</span>
<span id="cb10-668"><a href="#cb10-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-669"><a href="#cb10-669" aria-hidden="true" tabindex="-1"></a>So why use variance at all? Variance works better for doing math because:</span>
<span id="cb10-670"><a href="#cb10-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-671"><a href="#cb10-671" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It has nicer properties (like additivity for independent variables, as we will see later)</span>
<span id="cb10-672"><a href="#cb10-672" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It appears naturally in formulas and proofs</span>
<span id="cb10-673"><a href="#cb10-673" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It's easier to manipulate algebraically</span>
<span id="cb10-674"><a href="#cb10-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-675"><a href="#cb10-675" aria-hidden="true" tabindex="-1"></a>In short: we do calculations with variance, then take the square root for interpretation.</span>
<span id="cb10-676"><a href="#cb10-676" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-677"><a href="#cb10-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-678"><a href="#cb10-678" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb10-679"><a href="#cb10-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-680"><a href="#cb10-680" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb10-681"><a href="#cb10-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-682"><a href="#cb10-682" aria-hidden="true" tabindex="-1"></a>Think of variance as measuring **how wrong your guess will typically be** if you always guess the mean.</span>
<span id="cb10-683"><a href="#cb10-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-684"><a href="#cb10-684" aria-hidden="true" tabindex="-1"></a>Imagine predicting tomorrow's temperature. If you live in Nice or Lisbon (low variance), guessing the average temperature works well year-round. If you live in Helsinki or Berlin (high variance), that same strategy leads to large errors -- you'll be way off in both summer and winter.</span>
<span id="cb10-685"><a href="#cb10-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-686"><a href="#cb10-686" aria-hidden="true" tabindex="-1"></a>**Standard deviation** puts this in interpretable units:</span>
<span id="cb10-687"><a href="#cb10-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-688"><a href="#cb10-688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Low $\sigma$: Your guesses are usually close (precise manufacturing, stable processes)</span>
<span id="cb10-689"><a href="#cb10-689" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High $\sigma$: Your guesses are often far off (volatile stocks, unpredictable weather)</span>
<span id="cb10-690"><a href="#cb10-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-691"><a href="#cb10-691" aria-hidden="true" tabindex="-1"></a>The famous **68-95-99.7 rule** tells us that for bell-shaped data:</span>
<span id="cb10-692"><a href="#cb10-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-693"><a href="#cb10-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>68% of observations fall within 1$\sigma$ of the mean</span>
<span id="cb10-694"><a href="#cb10-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>95% fall within 2$\sigma$  </span>
<span id="cb10-695"><a href="#cb10-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>99.7% fall within 3$\sigma$</span>
<span id="cb10-696"><a href="#cb10-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-697"><a href="#cb10-697" aria-hidden="true" tabindex="-1"></a>This is why "3-sigma events" are considered rare outliers in quality control.</span>
<span id="cb10-698"><a href="#cb10-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-699"><a href="#cb10-699" aria-hidden="true" tabindex="-1"></a>(This rule is *exactly* true for normally-distributed data.)</span>
<span id="cb10-700"><a href="#cb10-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-701"><a href="#cb10-701" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb10-702"><a href="#cb10-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-703"><a href="#cb10-703" aria-hidden="true" tabindex="-1"></a>Variance has an elegant mathematical interpretation as the **expected squared distance from the mean**:</span>
<span id="cb10-704"><a href="#cb10-704" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(X) = \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu)^2</span><span class="co">]</span>$$</span>
<span id="cb10-705"><a href="#cb10-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-706"><a href="#cb10-706" aria-hidden="true" tabindex="-1"></a>This squared distance has deep connections:</span>
<span id="cb10-707"><a href="#cb10-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-708"><a href="#cb10-708" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Minimization property**: The mean $\mu$ minimizes $\mathbb{E}<span class="co">[</span><span class="ot">(X - c)^2</span><span class="co">]</span>$ over all constants $c$</span>
<span id="cb10-709"><a href="#cb10-709" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-710"><a href="#cb10-710" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Pythagorean theorem**: For independent $X, Y$:</span>
<span id="cb10-711"><a href="#cb10-711" aria-hidden="true" tabindex="-1"></a>   $$\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y)$$</span>
<span id="cb10-712"><a href="#cb10-712" aria-hidden="true" tabindex="-1"></a>   Just like $|a + b|^2 = |a|^2 + |b|^2$ for perpendicular vectors!</span>
<span id="cb10-713"><a href="#cb10-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-714"><a href="#cb10-714" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Information theory**: Variance of a Gaussian determines its entropy (uncertainty)</span>
<span id="cb10-715"><a href="#cb10-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-716"><a href="#cb10-716" aria-hidden="true" tabindex="-1"></a>The quadratic nature ($a^2$ scaling) reflects that variance measures *squared deviations*: doubling the scale quadruples the variance.</span>
<span id="cb10-717"><a href="#cb10-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-718"><a href="#cb10-718" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb10-719"><a href="#cb10-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-720"><a href="#cb10-720" aria-hidden="true" tabindex="-1"></a>Let's visualize how variance controls the spread of a distribution, using exam scores as an example.</span>
<span id="cb10-721"><a href="#cb10-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-724"><a href="#cb10-724" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-725"><a href="#cb10-725" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-726"><a href="#cb10-726" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb10-727"><a href="#cb10-727" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-728"><a href="#cb10-728" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-729"><a href="#cb10-729" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb10-730"><a href="#cb10-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-731"><a href="#cb10-731" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the plot</span></span>
<span id="cb10-732"><a href="#cb10-732" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb10-733"><a href="#cb10-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-734"><a href="#cb10-734" aria-hidden="true" tabindex="-1"></a><span class="co"># Common mean for all distributions</span></span>
<span id="cb10-735"><a href="#cb10-735" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> <span class="dv">75</span></span>
<span id="cb10-736"><a href="#cb10-736" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">40</span>, <span class="dv">110</span>, <span class="dv">1000</span>)</span>
<span id="cb10-737"><a href="#cb10-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-738"><a href="#cb10-738" aria-hidden="true" tabindex="-1"></a><span class="co"># Three different standard deviations</span></span>
<span id="cb10-739"><a href="#cb10-739" aria-hidden="true" tabindex="-1"></a>sigmas <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>]</span>
<span id="cb10-740"><a href="#cb10-740" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#2E86AB'</span>, <span class="st">'#A23B72'</span>, <span class="st">'#F18F01'</span>]</span>
<span id="cb10-741"><a href="#cb10-741" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'σ = 5 (Low variance)'</span>, <span class="st">'σ = 10 (Medium variance)'</span>, <span class="st">'σ = 20 (High variance)'</span>]</span>
<span id="cb10-742"><a href="#cb10-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-743"><a href="#cb10-743" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each distribution</span></span>
<span id="cb10-744"><a href="#cb10-744" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sigma, color, label <span class="kw">in</span> <span class="bu">zip</span>(sigmas, colors, labels):</span>
<span id="cb10-745"><a href="#cb10-745" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> stats.norm.pdf(x, mean, sigma)</span>
<span id="cb10-746"><a href="#cb10-746" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span>label)</span>
<span id="cb10-747"><a href="#cb10-747" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-748"><a href="#cb10-748" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shade ±1σ region</span></span>
<span id="cb10-749"><a href="#cb10-749" aria-hidden="true" tabindex="-1"></a>    x_fill <span class="op">=</span> x[(x <span class="op">&gt;=</span> mean <span class="op">-</span> sigma) <span class="op">&amp;</span> (x <span class="op">&lt;=</span> mean <span class="op">+</span> sigma)]</span>
<span id="cb10-750"><a href="#cb10-750" aria-hidden="true" tabindex="-1"></a>    y_fill <span class="op">=</span> stats.norm.pdf(x_fill, mean, sigma)</span>
<span id="cb10-751"><a href="#cb10-751" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(x_fill, y_fill, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span>color)</span>
<span id="cb10-752"><a href="#cb10-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-753"><a href="#cb10-753" aria-hidden="true" tabindex="-1"></a><span class="co"># Add vertical line at mean</span></span>
<span id="cb10-754"><a href="#cb10-754" aria-hidden="true" tabindex="-1"></a>ax.axvline(mean, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb10-755"><a href="#cb10-755" aria-hidden="true" tabindex="-1"></a>ax.text(mean <span class="op">+</span> <span class="dv">1</span>, <span class="fl">0.085</span>, <span class="st">'Mean = 75'</span>, ha<span class="op">=</span><span class="st">'left'</span>, va<span class="op">=</span><span class="st">'bottom'</span>)</span>
<span id="cb10-756"><a href="#cb10-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-757"><a href="#cb10-757" aria-hidden="true" tabindex="-1"></a><span class="co"># Styling</span></span>
<span id="cb10-758"><a href="#cb10-758" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Exam Score'</span>)</span>
<span id="cb10-759"><a href="#cb10-759" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb10-760"><a href="#cb10-760" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Same Mean, Different Variances: The Effect of Standard Deviation'</span>)</span>
<span id="cb10-761"><a href="#cb10-761" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb10-762"><a href="#cb10-762" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">40</span>, <span class="dv">110</span>)</span>
<span id="cb10-763"><a href="#cb10-763" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.09</span>)</span>
<span id="cb10-764"><a href="#cb10-764" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-765"><a href="#cb10-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-766"><a href="#cb10-766" aria-hidden="true" tabindex="-1"></a><span class="co"># Add annotations for interpretation</span></span>
<span id="cb10-767"><a href="#cb10-767" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">'68</span><span class="sc">% o</span><span class="st">f scores</span><span class="ch">\n</span><span class="st">within ±σ'</span>, xy<span class="op">=</span>(mean <span class="op">+</span> <span class="dv">5</span>, <span class="fl">0.055</span>), xytext<span class="op">=</span>(mean <span class="op">+</span> <span class="dv">12</span>, <span class="fl">0.065</span>),</span>
<span id="cb10-768"><a href="#cb10-768" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'#2E86AB'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>),</span>
<span id="cb10-769"><a href="#cb10-769" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>, ha<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'#2E86AB'</span>)</span>
<span id="cb10-770"><a href="#cb10-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-771"><a href="#cb10-771" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-772"><a href="#cb10-772" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-773"><a href="#cb10-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-774"><a href="#cb10-774" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Interpreting the visualization:"</span>)</span>
<span id="cb10-775"><a href="#cb10-775" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"• Small σ (blue): Scores cluster tightly around 75. Most students perform similarly."</span>)</span>
<span id="cb10-776"><a href="#cb10-776" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"• Medium σ (pink): Moderate spread. Typical variation in a well-designed exam."</span>)</span>
<span id="cb10-777"><a href="#cb10-777" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"• Large σ (orange): Wide spread. Large differences in student performance."</span>)</span>
<span id="cb10-778"><a href="#cb10-778" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">For any normal distribution, about 68% of values fall within ±1σ of the mean."</span>)</span>
<span id="cb10-779"><a href="#cb10-779" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-780"><a href="#cb10-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-781"><a href="#cb10-781" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-782"><a href="#cb10-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-783"><a href="#cb10-783" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of Variance {#sec-properties-of-variance}</span></span>
<span id="cb10-784"><a href="#cb10-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-785"><a href="#cb10-785" aria-hidden="true" tabindex="-1"></a>The variance has a useful computational formula:</span>
<span id="cb10-786"><a href="#cb10-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-787"><a href="#cb10-787" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-788"><a href="#cb10-788" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(X) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2$$</span>
<span id="cb10-789"><a href="#cb10-789" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-790"><a href="#cb10-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-791"><a href="#cb10-791" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-792"><a href="#cb10-792" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb10-793"><a href="#cb10-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-794"><a href="#cb10-794" aria-hidden="true" tabindex="-1"></a>Starting from the definition of variance:</span>
<span id="cb10-795"><a href="#cb10-795" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb10-796"><a href="#cb10-796" aria-hidden="true" tabindex="-1"></a>\mathbb{V}(X) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu)^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-797"><a href="#cb10-797" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">X^2 - 2X\mu + \mu^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-798"><a href="#cb10-798" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(X^2) - 2\mu\mathbb{E}(X) + \mu^2 <span class="sc">\\</span></span>
<span id="cb10-799"><a href="#cb10-799" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(X^2) - 2\mu^2 + \mu^2 <span class="sc">\\</span></span>
<span id="cb10-800"><a href="#cb10-800" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(X^2) - \mu^2</span>
<span id="cb10-801"><a href="#cb10-801" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb10-802"><a href="#cb10-802" aria-hidden="true" tabindex="-1"></a>where we used linearity of expectation and the fact that $\mathbb{E}(X) = \mu$.</span>
<span id="cb10-803"><a href="#cb10-803" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-804"><a href="#cb10-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-805"><a href="#cb10-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-806"><a href="#cb10-806" aria-hidden="true" tabindex="-1"></a>This formula simplifies many calculations and can be used to prove multiple properties of the variance.</span>
<span id="cb10-807"><a href="#cb10-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-808"><a href="#cb10-808" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-809"><a href="#cb10-809" aria-hidden="true" tabindex="-1"></a>Assuming the variance is well defined, it satisfies:</span>
<span id="cb10-810"><a href="#cb10-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-811"><a href="#cb10-811" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbb{V}(X) \geq 0$, with $\mathbb{V}(X) = 0$ if and only if $X$ is constant (a.s.)^<span class="co">[</span><span class="ot">Almost surely (a.s.) means "with probability 1". A random variable is almost surely constant if $\mathbb{P}(X = c) = 1$ for some constant $c$. The "almost" acknowledges that technically there could be probability-0 events where $X \neq c$, but these never occur in practice.</span><span class="co">]</span></span>
<span id="cb10-812"><a href="#cb10-812" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For constants $a, b$:</span>
<span id="cb10-813"><a href="#cb10-813" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(aX + b) = a^2\mathbb{V}(X)$$</span>
<span id="cb10-814"><a href="#cb10-814" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If $X$ and $Y$ are **independent**: </span>
<span id="cb10-815"><a href="#cb10-815" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y)$$</span>
<span id="cb10-816"><a href="#cb10-816" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>If $X$ and $Y$ are **independent**: </span>
<span id="cb10-817"><a href="#cb10-817" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(X - Y) = \mathbb{V}(X) + \mathbb{V}(Y)$$ (not minus!)</span>
<span id="cb10-818"><a href="#cb10-818" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>If $X_1, \ldots, X_n$ are **independent**, for constants $a_1, \ldots, a_n$: </span>
<span id="cb10-819"><a href="#cb10-819" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathbb{V}(X_i)$$</span>
<span id="cb10-820"><a href="#cb10-820" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-821"><a href="#cb10-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-822"><a href="#cb10-822" aria-hidden="true" tabindex="-1"></a>Property 4 often surprises students. You can find the proof below.</span>
<span id="cb10-823"><a href="#cb10-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-824"><a href="#cb10-824" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-825"><a href="#cb10-825" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Property 4</span></span>
<span id="cb10-826"><a href="#cb10-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-827"><a href="#cb10-827" aria-hidden="true" tabindex="-1"></a>If $X$ and $Y$ are independent with means $\mu_X, \mu_Y$:</span>
<span id="cb10-828"><a href="#cb10-828" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb10-829"><a href="#cb10-829" aria-hidden="true" tabindex="-1"></a>\mathbb{V}(X - Y) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X - Y - (\mu_X - \mu_Y))^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-830"><a href="#cb10-830" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">((X - \mu_X) - (Y - \mu_Y))^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-831"><a href="#cb10-831" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_X)^2 - 2(X - \mu_X)(Y - \mu_Y) + (Y - \mu_Y)^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-832"><a href="#cb10-832" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_X)^2</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">(Y - \mu_Y)^2</span><span class="co">]</span> - 2\mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_X)(Y - \mu_Y)</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-833"><a href="#cb10-833" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{V}(X) + \mathbb{V}(Y) - 2 \cdot 0 <span class="sc">\\</span></span>
<span id="cb10-834"><a href="#cb10-834" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{V}(X) + \mathbb{V}(Y)</span>
<span id="cb10-835"><a href="#cb10-835" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb10-836"><a href="#cb10-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-837"><a href="#cb10-837" aria-hidden="true" tabindex="-1"></a>The key step uses independence: $\mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_X)(Y - \mu_Y)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">X - \mu_X</span><span class="co">]</span>\mathbb{E}<span class="co">[</span><span class="ot">Y - \mu_Y</span><span class="co">]</span> = 0 \cdot 0 = 0$.</span>
<span id="cb10-838"><a href="#cb10-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-839"><a href="#cb10-839" aria-hidden="true" tabindex="-1"></a>Let's visualize how subtracting **independent** variables increases variance, while subtracting **dependent** variables can reduce it -- to the point that substracting perfectly correlated variables completely eliminates any variance!</span>
<span id="cb10-840"><a href="#cb10-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-843"><a href="#cb10-843" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-844"><a href="#cb10-844" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-845"><a href="#cb10-845" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb10-846"><a href="#cb10-846" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing Var(X-Y) = Var(X) + Var(Y) for independent variables</span></span>
<span id="cb10-847"><a href="#cb10-847" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-848"><a href="#cb10-848" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-849"><a href="#cb10-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-850"><a href="#cb10-850" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-851"><a href="#cb10-851" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb10-852"><a href="#cb10-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-853"><a href="#cb10-853" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent case</span></span>
<span id="cb10-854"><a href="#cb10-854" aria-hidden="true" tabindex="-1"></a>X_indep <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)  <span class="co"># Var = 1</span></span>
<span id="cb10-855"><a href="#cb10-855" aria-hidden="true" tabindex="-1"></a>Y_indep <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)  <span class="co"># Var = 1</span></span>
<span id="cb10-856"><a href="#cb10-856" aria-hidden="true" tabindex="-1"></a>diff_indep <span class="op">=</span> X_indep <span class="op">-</span> Y_indep      <span class="co"># Var should be 2</span></span>
<span id="cb10-857"><a href="#cb10-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-858"><a href="#cb10-858" aria-hidden="true" tabindex="-1"></a><span class="co"># Perfectly correlated case (not independent)</span></span>
<span id="cb10-859"><a href="#cb10-859" aria-hidden="true" tabindex="-1"></a>X_corr <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb10-860"><a href="#cb10-860" aria-hidden="true" tabindex="-1"></a>Y_corr <span class="op">=</span> X_corr  <span class="co"># Perfect correlation</span></span>
<span id="cb10-861"><a href="#cb10-861" aria-hidden="true" tabindex="-1"></a>diff_corr <span class="op">=</span> X_corr <span class="op">-</span> Y_corr  <span class="co"># Should be 0</span></span>
<span id="cb10-862"><a href="#cb10-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-863"><a href="#cb10-863" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb10-864"><a href="#cb10-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-865"><a href="#cb10-865" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent case</span></span>
<span id="cb10-866"><a href="#cb10-866" aria-hidden="true" tabindex="-1"></a>ax1.hist(diff_indep, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'blue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb10-867"><a href="#cb10-867" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'X - Y'</span>)</span>
<span id="cb10-868"><a href="#cb10-868" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb10-869"><a href="#cb10-869" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="ss">f'Independent: Var(X-Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(diff_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 2'</span>)</span>
<span id="cb10-870"><a href="#cb10-870" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb10-871"><a href="#cb10-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-872"><a href="#cb10-872" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlated case</span></span>
<span id="cb10-873"><a href="#cb10-873" aria-hidden="true" tabindex="-1"></a>ax2.hist(diff_corr, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'red'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb10-874"><a href="#cb10-874" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'X - Y'</span>)</span>
<span id="cb10-875"><a href="#cb10-875" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb10-876"><a href="#cb10-876" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="ss">f'Perfect Correlation: Var(X-Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(diff_corr, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0'</span>)</span>
<span id="cb10-877"><a href="#cb10-877" aria-hidden="true" tabindex="-1"></a>ax2.set_xlim(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb10-878"><a href="#cb10-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-879"><a href="#cb10-879" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-880"><a href="#cb10-880" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-881"><a href="#cb10-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-882"><a href="#cb10-882" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"When X and Y are independent N(0,1):"</span>)</span>
<span id="cb10-883"><a href="#cb10-883" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X) = </span><span class="sc">{</span>np<span class="sc">.</span>var(X_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-884"><a href="#cb10-884" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(Y_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-885"><a href="#cb10-885" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X-Y) = </span><span class="sc">{</span>np<span class="sc">.</span>var(diff_indep, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ Var(X) + Var(Y) = 2"</span>)</span>
<span id="cb10-886"><a href="#cb10-886" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">When Y = X (perfect dependence):"</span>)</span>
<span id="cb10-887"><a href="#cb10-887" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X-Y) = Var(0) = 0"</span>)</span>
<span id="cb10-888"><a href="#cb10-888" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-889"><a href="#cb10-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-890"><a href="#cb10-890" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-891"><a href="#cb10-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-892"><a href="#cb10-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-893"><a href="#cb10-893" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-894"><a href="#cb10-894" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Variance of Binomial via Decomposition</span></span>
<span id="cb10-895"><a href="#cb10-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-896"><a href="#cb10-896" aria-hidden="true" tabindex="-1"></a>Let $X \sim \text{Binomial}(n, p)$. We already know $\mathbb{E}(X) = np$. What's the variance?</span>
<span id="cb10-897"><a href="#cb10-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-898"><a href="#cb10-898" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-899"><a href="#cb10-899" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb10-900"><a href="#cb10-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-901"><a href="#cb10-901" aria-hidden="true" tabindex="-1"></a>Write $X = \sum_{i=1}^n X_i$ where $X_i \sim \text{Bernoulli}(p)$ independently.</span>
<span id="cb10-902"><a href="#cb10-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-903"><a href="#cb10-903" aria-hidden="true" tabindex="-1"></a>For a single Bernoulli:</span>
<span id="cb10-904"><a href="#cb10-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-905"><a href="#cb10-905" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(X_i) = p$</span>
<span id="cb10-906"><a href="#cb10-906" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(X_i^2) = 0^2 \cdot (1-p) + 1^2 \cdot p = p$</span>
<span id="cb10-907"><a href="#cb10-907" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{V}(X_i) = \mathbb{E}(X_i^2) - (\mathbb{E}(X_i))^2 = p - p^2 = p(1-p)$</span>
<span id="cb10-908"><a href="#cb10-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-909"><a href="#cb10-909" aria-hidden="true" tabindex="-1"></a>Since the $X_i$ are independent:</span>
<span id="cb10-910"><a href="#cb10-910" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(X) = \mathbb{V}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathbb{V}(X_i) = np(1-p)$$</span>
<span id="cb10-911"><a href="#cb10-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-912"><a href="#cb10-912" aria-hidden="true" tabindex="-1"></a>Note that variance is maximized when $p = 1/2$, which makes intuitive sense -- there's most uncertainty when success and failure are equally likely.</span>
<span id="cb10-913"><a href="#cb10-913" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-914"><a href="#cb10-914" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-915"><a href="#cb10-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-916"><a href="#cb10-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-917"><a href="#cb10-917" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-918"><a href="#cb10-918" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mean and Variance of Common Distributions</span></span>
<span id="cb10-919"><a href="#cb10-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-920"><a href="#cb10-920" aria-hidden="true" tabindex="-1"></a>| Distribution | $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>$ | $\mathbb{V}(X)$ | When to Use |</span>
<span id="cb10-921"><a href="#cb10-921" aria-hidden="true" tabindex="-1"></a>|-------------|------|---------|-------------|</span>
<span id="cb10-922"><a href="#cb10-922" aria-hidden="true" tabindex="-1"></a>| Bernoulli($p$) | $p$ | $p(1-p)$ | Single yes/no trial |</span>
<span id="cb10-923"><a href="#cb10-923" aria-hidden="true" tabindex="-1"></a>| Binomial($n,p$) | $np$ | $np(1-p)$ | Count of successes |</span>
<span id="cb10-924"><a href="#cb10-924" aria-hidden="true" tabindex="-1"></a>| Poisson($\lambda$) | $\lambda$ | $\lambda$ | Count of rare events |</span>
<span id="cb10-925"><a href="#cb10-925" aria-hidden="true" tabindex="-1"></a>| Geometric($p$) | $1/p$ | $(1-p)/p^2$ | Trials until success |</span>
<span id="cb10-926"><a href="#cb10-926" aria-hidden="true" tabindex="-1"></a>| Uniform($a,b$) | $(a+b)/2$ | $(b-a)^2/12$ | Equal likelihood |</span>
<span id="cb10-927"><a href="#cb10-927" aria-hidden="true" tabindex="-1"></a>| Normal($\mu,\sigma^2$) | $\mu$ | $\sigma^2$ | Sums of many effects |</span>
<span id="cb10-928"><a href="#cb10-928" aria-hidden="true" tabindex="-1"></a>| Exponential($\beta$) | $\beta$ | $\beta^2$ | Time between events |</span>
<span id="cb10-929"><a href="#cb10-929" aria-hidden="true" tabindex="-1"></a>| Gamma($\alpha,\beta$) | $\alpha\beta$ | $\alpha\beta^2$ | Sum of exponentials |</span>
<span id="cb10-930"><a href="#cb10-930" aria-hidden="true" tabindex="-1"></a>| Beta($\alpha,\beta$) | $\alpha/(\alpha+\beta)$ | $\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ | Proportions |</span>
<span id="cb10-931"><a href="#cb10-931" aria-hidden="true" tabindex="-1"></a>| $t_{\nu}$ | 0 (if $\nu &gt; 1$) | $\nu/(\nu-2)$ (if $\nu &gt; 2$) | Heavy-tailed data |</span>
<span id="cb10-932"><a href="#cb10-932" aria-hidden="true" tabindex="-1"></a>| $\chi^2_p$ | $p$ | $2p$ | Sum of squared normals |</span>
<span id="cb10-933"><a href="#cb10-933" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-934"><a href="#cb10-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-935"><a href="#cb10-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-936"><a href="#cb10-936" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sample Mean and Variance</span></span>
<span id="cb10-937"><a href="#cb10-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-938"><a href="#cb10-938" aria-hidden="true" tabindex="-1"></a>When we observe data, we compute sample statistics to estimate population parameters.</span>
<span id="cb10-939"><a href="#cb10-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-940"><a href="#cb10-940" aria-hidden="true" tabindex="-1"></a>Recall from our introduction: we have a **sample** $X_1, \ldots, X_n$ drawn from a **population** distribution $F_X$. The population has true parameters (like $\mu = \mathbb{E}(X)$ and $\sigma^2 = \mathbb{V}(X)$) that we want to know, but we can only compute statistics from our finite sample. This gap between what we can calculate and what we want to know is fundamental to statistics.</span>
<span id="cb10-941"><a href="#cb10-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-942"><a href="#cb10-942" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-943"><a href="#cb10-943" aria-hidden="true" tabindex="-1"></a>Given random variables $X_1, \ldots, X_n$:</span>
<span id="cb10-944"><a href="#cb10-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-945"><a href="#cb10-945" aria-hidden="true" tabindex="-1"></a>The **sample mean** is:</span>
<span id="cb10-946"><a href="#cb10-946" aria-hidden="true" tabindex="-1"></a>$$\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$$</span>
<span id="cb10-947"><a href="#cb10-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-948"><a href="#cb10-948" aria-hidden="true" tabindex="-1"></a>The **sample variance** is:</span>
<span id="cb10-949"><a href="#cb10-949" aria-hidden="true" tabindex="-1"></a>$$S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2$$</span>
<span id="cb10-950"><a href="#cb10-950" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-951"><a href="#cb10-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-952"><a href="#cb10-952" aria-hidden="true" tabindex="-1"></a>Note the $n-1$ in the denominator of the sample variance. This makes it an *unbiased* estimator of the population variance (see below).</span>
<span id="cb10-953"><a href="#cb10-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-954"><a href="#cb10-954" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-955"><a href="#cb10-955" aria-hidden="true" tabindex="-1"></a>Let $X_1, \ldots, X_n$ be IID with $\mu = \mathbb{E}(X_i)$ and $\sigma^2 = \mathbb{V}(X_i)$. Then:</span>
<span id="cb10-956"><a href="#cb10-956" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(\bar{X}_n) = \mu, \quad \mathbb{V}(\bar{X}_n) = \frac{\sigma^2}{n}, \quad \mathbb{E}(S_n^2) = \sigma^2$$</span>
<span id="cb10-957"><a href="#cb10-957" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-958"><a href="#cb10-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-959"><a href="#cb10-959" aria-hidden="true" tabindex="-1"></a>This theorem tells us:</span>
<span id="cb10-960"><a href="#cb10-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-961"><a href="#cb10-961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The sample mean is unbiased (its expectation is equal to the population mean)</span>
<span id="cb10-962"><a href="#cb10-962" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Its variance decreases as $n$ increases</span>
<span id="cb10-963"><a href="#cb10-963" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The sample variance (with $n-1$) is unbiased</span>
<span id="cb10-964"><a href="#cb10-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-965"><a href="#cb10-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-966"><a href="#cb10-966" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-967"><a href="#cb10-967" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wait, What Does "Unbiased" Mean?</span></span>
<span id="cb10-968"><a href="#cb10-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-969"><a href="#cb10-969" aria-hidden="true" tabindex="-1"></a>An estimator or sample statistic is **unbiased** if its expected value equals the parameter it's trying to estimate.</span>
<span id="cb10-970"><a href="#cb10-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-971"><a href="#cb10-971" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Unbiased**: $\mathbb{E}(\text{estimator}) = \text{true parameter}$</span>
<span id="cb10-972"><a href="#cb10-972" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Biased**: $\mathbb{E}(\text{estimator}) \neq \text{true parameter}$</span>
<span id="cb10-973"><a href="#cb10-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-974"><a href="#cb10-974" aria-hidden="true" tabindex="-1"></a>For example:</span>
<span id="cb10-975"><a href="#cb10-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-976"><a href="#cb10-976" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>As stated above, $\bar{X}_n$ is unbiased for $\mu$ because $\mathbb{E}(\bar{X}_n) = \mu$</span>
<span id="cb10-977"><a href="#cb10-977" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$S_n^2$ (with $n-1$) is unbiased for $\sigma^2$ because $\mathbb{E}(S_n^2) = \sigma^2$</span>
<span id="cb10-978"><a href="#cb10-978" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If we used $n$ instead of $n-1$ at the denominator, we'd get $\mathbb{E}(S_n^2) = \frac{n-1}{n}\sigma^2 &lt; \sigma^2$ (biased!)</span>
<span id="cb10-979"><a href="#cb10-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-980"><a href="#cb10-980" aria-hidden="true" tabindex="-1"></a>Being unbiased means that *on average* across many sets of samples, our sample statistic would match the true value -- though any individual estimate may be too high or too low.</span>
<span id="cb10-981"><a href="#cb10-981" aria-hidden="true" tabindex="-1"></a>This also doesn't tell us anything about the *rate of convergence* -- how fast the estimator converges to the true value.</span>
<span id="cb10-982"><a href="#cb10-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-983"><a href="#cb10-983" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-984"><a href="#cb10-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-985"><a href="#cb10-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-986"><a href="#cb10-986" aria-hidden="true" tabindex="-1"></a><span class="fu">## Covariance and Correlation</span></span>
<span id="cb10-987"><a href="#cb10-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-988"><a href="#cb10-988" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear Relationships</span></span>
<span id="cb10-989"><a href="#cb10-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-990"><a href="#cb10-990" aria-hidden="true" tabindex="-1"></a>When we have two random variables, we often want to measure how they vary together and quantify the strength of their *linear* relation.</span>
<span id="cb10-991"><a href="#cb10-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-992"><a href="#cb10-992" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-993"><a href="#cb10-993" aria-hidden="true" tabindex="-1"></a>Let $X$ and $Y$ be random variables with means $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$. The **covariance** between $X$ and $Y$ is:</span>
<span id="cb10-994"><a href="#cb10-994" aria-hidden="true" tabindex="-1"></a>$$\mathrm{Cov}(X, Y) = \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_X)(Y - \mu_Y)</span><span class="co">]</span>$$</span>
<span id="cb10-995"><a href="#cb10-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-996"><a href="#cb10-996" aria-hidden="true" tabindex="-1"></a>The **correlation** is:</span>
<span id="cb10-997"><a href="#cb10-997" aria-hidden="true" tabindex="-1"></a>$$\rho = \rho_{X,Y} = \rho(X, Y) = \frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y}$$</span>
<span id="cb10-998"><a href="#cb10-998" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-999"><a href="#cb10-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1000"><a href="#cb10-1000" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of Covariance and Correlation</span></span>
<span id="cb10-1001"><a href="#cb10-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1002"><a href="#cb10-1002" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-1003"><a href="#cb10-1003" aria-hidden="true" tabindex="-1"></a>The covariance can be rewritten as:</span>
<span id="cb10-1004"><a href="#cb10-1004" aria-hidden="true" tabindex="-1"></a>$$\mathrm{Cov}(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)$$</span>
<span id="cb10-1005"><a href="#cb10-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1006"><a href="#cb10-1006" aria-hidden="true" tabindex="-1"></a>The correlation satisfies:</span>
<span id="cb10-1007"><a href="#cb10-1007" aria-hidden="true" tabindex="-1"></a>$$-1 \leq \rho(X, Y) \leq 1$$</span>
<span id="cb10-1008"><a href="#cb10-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1009"><a href="#cb10-1009" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1010"><a href="#cb10-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1011"><a href="#cb10-1011" aria-hidden="true" tabindex="-1"></a>The correlation is a sort of "normalized covariance". By dividing the covariance by $\sigma_X$ and $\sigma_Y$, we remove the *magnitude* (and units/scale) of the two random variables, and what remains is a pure number that measures of how much they change together on average, in a range from -1 to 1.</span>
<span id="cb10-1012"><a href="#cb10-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1013"><a href="#cb10-1013" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-1014"><a href="#cb10-1014" aria-hidden="true" tabindex="-1"></a>Covariance and correlation further satisfy the following properties:</span>
<span id="cb10-1015"><a href="#cb10-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1016"><a href="#cb10-1016" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $Y = aX + b$ for constants $a, b$:</span>
<span id="cb10-1017"><a href="#cb10-1017" aria-hidden="true" tabindex="-1"></a>  $$\rho(X, Y) = \begin{cases}</span>
<span id="cb10-1018"><a href="#cb10-1018" aria-hidden="true" tabindex="-1"></a>  1 &amp; \text{if } a &gt; 0 <span class="sc">\\</span></span>
<span id="cb10-1019"><a href="#cb10-1019" aria-hidden="true" tabindex="-1"></a>  -1 &amp; \text{if } a &lt; 0</span>
<span id="cb10-1020"><a href="#cb10-1020" aria-hidden="true" tabindex="-1"></a>  \end{cases}$$</span>
<span id="cb10-1021"><a href="#cb10-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $X$ and $Y$ are **independent**: $\mathrm{Cov}(X, Y) = \rho = 0$</span>
<span id="cb10-1022"><a href="#cb10-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The converse is NOT true in general!</span>
<span id="cb10-1023"><a href="#cb10-1023" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1024"><a href="#cb10-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1025"><a href="#cb10-1025" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb10-1026"><a href="#cb10-1026" aria-hidden="true" tabindex="-1"></a>**Common Misconception**: Uncorrelated ≠ Independent!</span>
<span id="cb10-1027"><a href="#cb10-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1028"><a href="#cb10-1028" aria-hidden="true" tabindex="-1"></a>Independence implies zero correlation, but zero correlation does NOT imply independence.</span>
<span id="cb10-1029"><a href="#cb10-1029" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1030"><a href="#cb10-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1031"><a href="#cb10-1031" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false collapse="true"}</span>
<span id="cb10-1032"><a href="#cb10-1032" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Uncorrelated but Dependent</span></span>
<span id="cb10-1033"><a href="#cb10-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1034"><a href="#cb10-1034" aria-hidden="true" tabindex="-1"></a>Let $X \sim \text{Uniform}(-1, 1)$ and $Y = X^2$. </span>
<span id="cb10-1035"><a href="#cb10-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1036"><a href="#cb10-1036" aria-hidden="true" tabindex="-1"></a>These two random variables are clearly dependent (knowing $X$ determines $Y$ exactly!), but:</span>
<span id="cb10-1037"><a href="#cb10-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1038"><a href="#cb10-1038" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X) = 0$$</span>
<span id="cb10-1039"><a href="#cb10-1039" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(Y) = \mathbb{E}(X^2) = \int_{-1}^1 x^2 \cdot \frac{1}{2} \, dx = \frac{1}{3}$$</span>
<span id="cb10-1040"><a href="#cb10-1040" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(XY) = \mathbb{E}(X^3) = \int_{-1}^1 x^3 \cdot \frac{1}{2} \, dx = 0$$</span>
<span id="cb10-1041"><a href="#cb10-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1042"><a href="#cb10-1042" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb10-1043"><a href="#cb10-1043" aria-hidden="true" tabindex="-1"></a>$$\mathrm{Cov}(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = 0 - 0 \cdot \frac{1}{3} = 0$$</span>
<span id="cb10-1044"><a href="#cb10-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1045"><a href="#cb10-1045" aria-hidden="true" tabindex="-1"></a>So $X$ and $Y$ are **uncorrelated** despite being perfectly dependent!</span>
<span id="cb10-1046"><a href="#cb10-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1047"><a href="#cb10-1047" aria-hidden="true" tabindex="-1"></a>The plot below shows $X$ and $Y$. See also <span class="co">[</span><span class="ot">this article</span><span class="co">](https://www.scientificamerican.com/article/what-this-graph-of-a-dinosaur-can-teach-us-about-doing-better-science/)</span> on Scientific American for more examples.</span>
<span id="cb10-1048"><a href="#cb10-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1049"><a href="#cb10-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1052"><a href="#cb10-1052" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-1053"><a href="#cb10-1053" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-1054"><a href="#cb10-1054" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb10-1055"><a href="#cb10-1055" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing uncorrelated but dependent variables</span></span>
<span id="cb10-1056"><a href="#cb10-1056" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-1057"><a href="#cb10-1057" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-1058"><a href="#cb10-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1059"><a href="#cb10-1059" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-1060"><a href="#cb10-1060" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb10-1061"><a href="#cb10-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1062"><a href="#cb10-1062" aria-hidden="true" tabindex="-1"></a><span class="co"># X ~ Uniform(-1, 1), Y = X²</span></span>
<span id="cb10-1063"><a href="#cb10-1063" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, n)</span>
<span id="cb10-1064"><a href="#cb10-1064" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> X<span class="op">**</span><span class="dv">2</span></span>
<span id="cb10-1065"><a href="#cb10-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1066"><a href="#cb10-1066" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb10-1067"><a href="#cb10-1067" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, Y, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-1068"><a href="#cb10-1068" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb10-1069"><a href="#cb10-1069" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Y = X²'</span>)</span>
<span id="cb10-1070"><a href="#cb10-1070" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Uncorrelated but Dependent: ρ(X,Y) = 0'</span>)</span>
<span id="cb10-1071"><a href="#cb10-1071" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-1072"><a href="#cb10-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1073"><a href="#cb10-1073" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the parabola</span></span>
<span id="cb10-1074"><a href="#cb10-1074" aria-hidden="true" tabindex="-1"></a>x_line <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb10-1075"><a href="#cb10-1075" aria-hidden="true" tabindex="-1"></a>y_line <span class="op">=</span> x_line<span class="op">**</span><span class="dv">2</span></span>
<span id="cb10-1076"><a href="#cb10-1076" aria-hidden="true" tabindex="-1"></a>plt.plot(x_line, y_line, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Y = X²'</span>)</span>
<span id="cb10-1077"><a href="#cb10-1077" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-1078"><a href="#cb10-1078" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-1079"><a href="#cb10-1079" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-1080"><a href="#cb10-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1081"><a href="#cb10-1081" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Y is completely determined by X, yet they are uncorrelated!"</span>)</span>
<span id="cb10-1082"><a href="#cb10-1082" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This is because the linear association is zero due to symmetry."</span>)</span>
<span id="cb10-1083"><a href="#cb10-1083" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-1084"><a href="#cb10-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1085"><a href="#cb10-1085" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1086"><a href="#cb10-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1087"><a href="#cb10-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1088"><a href="#cb10-1088" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variance of Sums (General Case)</span></span>
<span id="cb10-1089"><a href="#cb10-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1090"><a href="#cb10-1090" aria-hidden="true" tabindex="-1"></a>We saw in @sec-properties-of-variance that for **independent** variables, </span>
<span id="cb10-1091"><a href="#cb10-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1092"><a href="#cb10-1092" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathbb{V}(X_i) \qquad \text{(independent)}$$</span>
<span id="cb10-1093"><a href="#cb10-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1094"><a href="#cb10-1094" aria-hidden="true" tabindex="-1"></a>We now generalize this result to any random variables, whether dependent or independent:</span>
<span id="cb10-1095"><a href="#cb10-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1096"><a href="#cb10-1096" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-1097"><a href="#cb10-1097" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathbb{V}(X_i) + 2\sum_{i=1}^n \sum_{j=i+1}^n a_i a_j \mathrm{Cov}(X_i, X_j)$$</span>
<span id="cb10-1098"><a href="#cb10-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1099"><a href="#cb10-1099" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1100"><a href="#cb10-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1101"><a href="#cb10-1101" aria-hidden="true" tabindex="-1"></a>**Special cases:**</span>
<span id="cb10-1102"><a href="#cb10-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1103"><a href="#cb10-1103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y) + 2\mathrm{Cov}(X, Y)$</span>
<span id="cb10-1104"><a href="#cb10-1104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{V}(X - Y) = \mathbb{V}(X) + \mathbb{V}(Y) - 2\mathrm{Cov}(X, Y)$</span>
<span id="cb10-1105"><a href="#cb10-1105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When the variables are independent, check that this indeed reduces to the simpler formula for independent variables</span>
<span id="cb10-1106"><a href="#cb10-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1107"><a href="#cb10-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1108"><a href="#cb10-1108" aria-hidden="true" tabindex="-1"></a><span class="fu">## Expectation with Matrices</span></span>
<span id="cb10-1109"><a href="#cb10-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1110"><a href="#cb10-1110" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Vectors</span></span>
<span id="cb10-1111"><a href="#cb10-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1112"><a href="#cb10-1112" aria-hidden="true" tabindex="-1"></a>In *multivariate* settings -- that is, involving multiple random variables --, we work with random *vectors* and their expectations.</span>
<span id="cb10-1113"><a href="#cb10-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1114"><a href="#cb10-1114" aria-hidden="true" tabindex="-1"></a>**Notation:** The mathematical convention is to work with column vectors. For example, we write </span>
<span id="cb10-1115"><a href="#cb10-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1116"><a href="#cb10-1116" aria-hidden="true" tabindex="-1"></a>$$\mathbf{X} = \begin{pmatrix} X_1 <span class="sc">\\</span> X_2 <span class="sc">\\</span> X_3 \end{pmatrix}$$ </span>
<span id="cb10-1117"><a href="#cb10-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1118"><a href="#cb10-1118" aria-hidden="true" tabindex="-1"></a>rather than $(X_1, X_2, X_3)$. The column vector can also be written as the transpose of a row vector: $\mathbf{X} = (X_1, X_2, X_3)^T$.</span>
<span id="cb10-1119"><a href="#cb10-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1120"><a href="#cb10-1120" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-1121"><a href="#cb10-1121" aria-hidden="true" tabindex="-1"></a>For a random vector $\mathbf{X} = (X_1, \ldots, X_k)^T$:</span>
<span id="cb10-1122"><a href="#cb10-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1123"><a href="#cb10-1123" aria-hidden="true" tabindex="-1"></a>The **mean vector** is:</span>
<span id="cb10-1124"><a href="#cb10-1124" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\mu} = \mathbb{E}(\mathbf{X}) = \begin{pmatrix} \mathbb{E}(X_1) <span class="sc">\\</span> \vdots <span class="sc">\\</span> \mathbb{E}(X_k) \end{pmatrix}$$</span>
<span id="cb10-1125"><a href="#cb10-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1126"><a href="#cb10-1126" aria-hidden="true" tabindex="-1"></a>The **covariance matrix** $\boldsymbol{\Sigma}$ (also written $\mathbb{V}(\mathbf{X})$) is:</span>
<span id="cb10-1127"><a href="#cb10-1127" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \begin{bmatrix}</span>
<span id="cb10-1128"><a href="#cb10-1128" aria-hidden="true" tabindex="-1"></a>\mathbb{V}(X_1) &amp; \mathrm{Cov}(X_1, X_2) &amp; \cdots &amp; \mathrm{Cov}(X_1, X_k) <span class="sc">\\</span></span>
<span id="cb10-1129"><a href="#cb10-1129" aria-hidden="true" tabindex="-1"></a>\mathrm{Cov}(X_2, X_1) &amp; \mathbb{V}(X_2) &amp; \cdots &amp; \mathrm{Cov}(X_2, X_k) <span class="sc">\\</span></span>
<span id="cb10-1130"><a href="#cb10-1130" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb10-1131"><a href="#cb10-1131" aria-hidden="true" tabindex="-1"></a>\mathrm{Cov}(X_k, X_1) &amp; \mathrm{Cov}(X_k, X_2) &amp; \cdots &amp; \mathbb{V}(X_k)</span>
<span id="cb10-1132"><a href="#cb10-1132" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb10-1133"><a href="#cb10-1133" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1134"><a href="#cb10-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1135"><a href="#cb10-1135" aria-hidden="true" tabindex="-1"></a>The inverse $\boldsymbol{\Sigma}^{-1}$ is called the **precision matrix**.</span>
<span id="cb10-1136"><a href="#cb10-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1137"><a href="#cb10-1137" aria-hidden="true" tabindex="-1"></a>**Notation:** The *uppercase* Greek letter $\Sigma$ ([sigma](https://en.wikipedia.org/wiki/Sigma)) is almost invariably used to denote a covariance matrix. Note that the *lowercase* $\sigma$ denotes the standard deviation.</span>
<span id="cb10-1138"><a href="#cb10-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1139"><a href="#cb10-1139" aria-hidden="true" tabindex="-1"></a><span class="fu">### Covariance Matrix Properties</span></span>
<span id="cb10-1140"><a href="#cb10-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1141"><a href="#cb10-1141" aria-hidden="true" tabindex="-1"></a>The covariance matrix can be written compactly as:</span>
<span id="cb10-1142"><a href="#cb10-1142" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \mathbb{E}<span class="co">[</span><span class="ot">(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T</span><span class="co">]</span>$$</span>
<span id="cb10-1143"><a href="#cb10-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1144"><a href="#cb10-1144" aria-hidden="true" tabindex="-1"></a>An alternative formula for the covariance matrix is:</span>
<span id="cb10-1145"><a href="#cb10-1145" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \boldsymbol{\mu}\boldsymbol{\mu}^T$$</span>
<span id="cb10-1146"><a href="#cb10-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1147"><a href="#cb10-1147" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-1148"><a href="#cb10-1148" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb10-1149"><a href="#cb10-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1150"><a href="#cb10-1150" aria-hidden="true" tabindex="-1"></a>Starting from the definition:</span>
<span id="cb10-1151"><a href="#cb10-1151" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb10-1152"><a href="#cb10-1152" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\Sigma} &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1153"><a href="#cb10-1153" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">\mathbf{X}\mathbf{X}^T - \mathbf{X}\boldsymbol{\mu}^T - \boldsymbol{\mu}\mathbf{X}^T + \boldsymbol{\mu}\boldsymbol{\mu}^T</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1154"><a href="#cb10-1154" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \mathbb{E}(\mathbf{X})\boldsymbol{\mu}^T - \boldsymbol{\mu}\mathbb{E}(\mathbf{X}^T) + \boldsymbol{\mu}\boldsymbol{\mu}^T <span class="sc">\\</span></span>
<span id="cb10-1155"><a href="#cb10-1155" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \boldsymbol{\mu}\boldsymbol{\mu}^T - \boldsymbol{\mu}\boldsymbol{\mu}^T + \boldsymbol{\mu}\boldsymbol{\mu}^T <span class="sc">\\</span></span>
<span id="cb10-1156"><a href="#cb10-1156" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(\mathbf{X}\mathbf{X}^T) - \boldsymbol{\mu}\boldsymbol{\mu}^T</span>
<span id="cb10-1157"><a href="#cb10-1157" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb10-1158"><a href="#cb10-1158" aria-hidden="true" tabindex="-1"></a>where we used the fact that $\mathbb{E}(\mathbf{X}) = \boldsymbol{\mu}$ and the linearity of expectation.</span>
<span id="cb10-1159"><a href="#cb10-1159" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1160"><a href="#cb10-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1161"><a href="#cb10-1161" aria-hidden="true" tabindex="-1"></a>Properties:</span>
<span id="cb10-1162"><a href="#cb10-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1163"><a href="#cb10-1163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Symmetric: $\boldsymbol{\Sigma} = \boldsymbol{\Sigma}^T$</span>
<span id="cb10-1164"><a href="#cb10-1164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Positive semi-definite: $\mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a} \geq 0$ for all $\mathbf{a}$</span>
<span id="cb10-1165"><a href="#cb10-1165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Diagonal elements are variances (non-negative)</span>
<span id="cb10-1166"><a href="#cb10-1166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Off-diagonal elements are covariances</span>
<span id="cb10-1167"><a href="#cb10-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1168"><a href="#cb10-1168" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear Transformations</span></span>
<span id="cb10-1169"><a href="#cb10-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1170"><a href="#cb10-1170" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb10-1171"><a href="#cb10-1171" aria-hidden="true" tabindex="-1"></a>If $\mathbf{X}$ has mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma}$, and $\mathbf{A}$ is a matrix:</span>
<span id="cb10-1172"><a href="#cb10-1172" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(\mathbf{A}\mathbf{X}) = \mathbf{A}\boldsymbol{\mu}$$</span>
<span id="cb10-1173"><a href="#cb10-1173" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(\mathbf{A}\mathbf{X}) = \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^T$$</span>
<span id="cb10-1174"><a href="#cb10-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1175"><a href="#cb10-1175" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-1176"><a href="#cb10-1176" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of the Variance Formula</span></span>
<span id="cb10-1177"><a href="#cb10-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1178"><a href="#cb10-1178" aria-hidden="true" tabindex="-1"></a>Using the definition of variance for vectors and the fact that $\mathbb{E}(\mathbf{A}\mathbf{X}) = \mathbf{A}\boldsymbol{\mu}$:</span>
<span id="cb10-1179"><a href="#cb10-1179" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb10-1180"><a href="#cb10-1180" aria-hidden="true" tabindex="-1"></a>\mathbb{V}(\mathbf{A}\mathbf{X}) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(\mathbf{A}\mathbf{X} - \mathbf{A}\boldsymbol{\mu})(\mathbf{A}\mathbf{X} - \mathbf{A}\boldsymbol{\mu})^T</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1181"><a href="#cb10-1181" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">\mathbf{A}(\mathbf{X} - \boldsymbol{\mu})(\mathbf{A}(\mathbf{X} - \boldsymbol{\mu}))^T</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1182"><a href="#cb10-1182" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">\mathbf{A}(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T\mathbf{A}^T</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1183"><a href="#cb10-1183" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbf{A}\mathbb{E}<span class="co">[</span><span class="ot">(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T</span><span class="co">]</span>\mathbf{A}^T <span class="sc">\\</span></span>
<span id="cb10-1184"><a href="#cb10-1184" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^T</span>
<span id="cb10-1185"><a href="#cb10-1185" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb10-1186"><a href="#cb10-1186" aria-hidden="true" tabindex="-1"></a>where we used the fact that $\mathbf{A}$ is a constant matrix that can be taken outside the expectation.</span>
<span id="cb10-1187"><a href="#cb10-1187" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1188"><a href="#cb10-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1189"><a href="#cb10-1189" aria-hidden="true" tabindex="-1"></a>Similarly, for a vector $\mathbf{a}$ (this is just a special case of the equations above -- why?):</span>
<span id="cb10-1190"><a href="#cb10-1190" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T\boldsymbol{\mu}$$</span>
<span id="cb10-1191"><a href="#cb10-1191" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a}$$</span>
<span id="cb10-1192"><a href="#cb10-1192" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1193"><a href="#cb10-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1194"><a href="#cb10-1194" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting the Covariance Matrix</span></span>
<span id="cb10-1195"><a href="#cb10-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1196"><a href="#cb10-1196" aria-hidden="true" tabindex="-1"></a>The covariance matrix encodes the second-order structure of a random vector—that is, how the variables vary and co-vary together. To understand this structure, we can examine its **spectral decomposition** (<span class="co">[</span><span class="ot">eigendecomposition</span><span class="co">](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)</span>).</span>
<span id="cb10-1197"><a href="#cb10-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1198"><a href="#cb10-1198" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb10-1199"><a href="#cb10-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1200"><a href="#cb10-1200" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb10-1201"><a href="#cb10-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1202"><a href="#cb10-1202" aria-hidden="true" tabindex="-1"></a>Imagine your data as a cloud of points in space. This cloud rarely forms a perfect sphere—it's usually stretched more in some directions than others, like an ellipse or ellipsoid.</span>
<span id="cb10-1203"><a href="#cb10-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1204"><a href="#cb10-1204" aria-hidden="true" tabindex="-1"></a>The covariance matrix captures this shape:</span>
<span id="cb10-1205"><a href="#cb10-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1206"><a href="#cb10-1206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvectors** are the "natural axes" of your data cloud—the directions along which it stretches</span>
<span id="cb10-1207"><a href="#cb10-1207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvalues** tell you how much the cloud stretches in each direction</span>
<span id="cb10-1208"><a href="#cb10-1208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The largest eigenvalue corresponds to the direction of greatest spread</span>
<span id="cb10-1209"><a href="#cb10-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1210"><a href="#cb10-1210" aria-hidden="true" tabindex="-1"></a>This is like finding the best way to orient a box around your data:</span>
<span id="cb10-1211"><a href="#cb10-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1212"><a href="#cb10-1212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The box edges align with the eigenvectors</span>
<span id="cb10-1213"><a href="#cb10-1213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The box dimensions are proportional to the square roots of eigenvalues</span>
<span id="cb10-1214"><a href="#cb10-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1215"><a href="#cb10-1215" aria-hidden="true" tabindex="-1"></a>**Principal Component Analysis (PCA)** uses this insight: keep the directions with large spread (high variance), discard those with little spread. This reduces dimensions while preserving most of the data's structure.</span>
<span id="cb10-1216"><a href="#cb10-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1217"><a href="#cb10-1217" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb10-1218"><a href="#cb10-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1219"><a href="#cb10-1219" aria-hidden="true" tabindex="-1"></a>Since $\boldsymbol{\Sigma}$ is symmetric and positive semi-definite, recall from earlier linear algebra classes that it has spectral decomposition:</span>
<span id="cb10-1220"><a href="#cb10-1220" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \sum_{i=1}^k \lambda_i \mathbf{v}_i \mathbf{v}_i^T$$</span>
<span id="cb10-1221"><a href="#cb10-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1222"><a href="#cb10-1222" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb10-1223"><a href="#cb10-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1224"><a href="#cb10-1224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_k \geq 0$ are the *eigenvalues* (which we can order from larger to smaller)</span>
<span id="cb10-1225"><a href="#cb10-1225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ are the corresponding orthonormal *eigenvectors*</span>
<span id="cb10-1226"><a href="#cb10-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1227"><a href="#cb10-1227" aria-hidden="true" tabindex="-1"></a>This decomposition reveals the geometric structure of the data:</span>
<span id="cb10-1228"><a href="#cb10-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1229"><a href="#cb10-1229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvalues** $\lambda_i$: represent the variance along each principal axis</span>
<span id="cb10-1230"><a href="#cb10-1230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvectors** $\mathbf{v}_i$: define the directions of these principal axes</span>
<span id="cb10-1231"><a href="#cb10-1231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Largest eigenvalue/vector**: indicates the direction of maximum variance in the data</span>
<span id="cb10-1232"><a href="#cb10-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1233"><a href="#cb10-1233" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb10-1234"><a href="#cb10-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1235"><a href="#cb10-1235" aria-hidden="true" tabindex="-1"></a>Let's visualize how eigendecomposition reveals the structure of data:</span>
<span id="cb10-1236"><a href="#cb10-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1239"><a href="#cb10-1239" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-1240"><a href="#cb10-1240" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-1241"><a href="#cb10-1241" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb10-1242"><a href="#cb10-1242" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-1243"><a href="#cb10-1243" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-1244"><a href="#cb10-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1245"><a href="#cb10-1245" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate correlated 2D data</span></span>
<span id="cb10-1246"><a href="#cb10-1246" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-1247"><a href="#cb10-1247" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb10-1248"><a href="#cb10-1248" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="fl">2.5</span>, <span class="fl">1.5</span>], </span>
<span id="cb10-1249"><a href="#cb10-1249" aria-hidden="true" tabindex="-1"></a>       [<span class="fl">1.5</span>, <span class="fl">1.5</span>]]</span>
<span id="cb10-1250"><a href="#cb10-1250" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.multivariate_normal(mean, cov, <span class="dv">300</span>)</span>
<span id="cb10-1251"><a href="#cb10-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1252"><a href="#cb10-1252" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute eigendecomposition</span></span>
<span id="cb10-1253"><a href="#cb10-1253" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eigh(cov)</span>
<span id="cb10-1254"><a href="#cb10-1254" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by eigenvalue (largest first)</span></span>
<span id="cb10-1255"><a href="#cb10-1255" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> eigenvalues.argsort()[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-1256"><a href="#cb10-1256" aria-hidden="true" tabindex="-1"></a>eigenvalues <span class="op">=</span> eigenvalues[idx]</span>
<span id="cb10-1257"><a href="#cb10-1257" aria-hidden="true" tabindex="-1"></a>eigenvectors <span class="op">=</span> eigenvectors[:, idx]</span>
<span id="cb10-1258"><a href="#cb10-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1259"><a href="#cb10-1259" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data and principal axes</span></span>
<span id="cb10-1260"><a href="#cb10-1260" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb10-1261"><a href="#cb10-1261" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb10-1262"><a href="#cb10-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1263"><a href="#cb10-1263" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot eigenvectors from the mean</span></span>
<span id="cb10-1264"><a href="#cb10-1264" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>]</span>
<span id="cb10-1265"><a href="#cb10-1265" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb10-1266"><a href="#cb10-1266" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale eigenvector by sqrt(eigenvalue) for visualization</span></span>
<span id="cb10-1267"><a href="#cb10-1267" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> eigenvectors[:, i] <span class="op">*</span> np.sqrt(eigenvalues[i]) <span class="op">*</span> <span class="fl">1.96</span></span>
<span id="cb10-1268"><a href="#cb10-1268" aria-hidden="true" tabindex="-1"></a>    plt.arrow(mean[<span class="dv">0</span>], mean[<span class="dv">1</span>], v[<span class="dv">0</span>], v[<span class="dv">1</span>], </span>
<span id="cb10-1269"><a href="#cb10-1269" aria-hidden="true" tabindex="-1"></a>              head_width<span class="op">=</span><span class="fl">0.1</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb10-1270"><a href="#cb10-1270" aria-hidden="true" tabindex="-1"></a>              fc<span class="op">=</span>colors[i], ec<span class="op">=</span>colors[i], linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-1271"><a href="#cb10-1271" aria-hidden="true" tabindex="-1"></a>              label<span class="op">=</span><span class="ss">f'PC</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: λ=</span><span class="sc">{</span>eigenvalues[i]<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb10-1272"><a href="#cb10-1272" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-1273"><a href="#cb10-1273" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Also draw the negative direction</span></span>
<span id="cb10-1274"><a href="#cb10-1274" aria-hidden="true" tabindex="-1"></a>    plt.arrow(mean[<span class="dv">0</span>], mean[<span class="dv">1</span>], <span class="op">-</span>v[<span class="dv">0</span>], <span class="op">-</span>v[<span class="dv">1</span>], </span>
<span id="cb10-1275"><a href="#cb10-1275" aria-hidden="true" tabindex="-1"></a>              head_width<span class="op">=</span><span class="fl">0.1</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb10-1276"><a href="#cb10-1276" aria-hidden="true" tabindex="-1"></a>              fc<span class="op">=</span>colors[i], ec<span class="op">=</span>colors[i], linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-1277"><a href="#cb10-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1278"><a href="#cb10-1278" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X₁'</span>)</span>
<span id="cb10-1279"><a href="#cb10-1279" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'X₂'</span>)</span>
<span id="cb10-1280"><a href="#cb10-1280" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Principal Axes of the Covariance Matrix'</span>)</span>
<span id="cb10-1281"><a href="#cb10-1281" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-1282"><a href="#cb10-1282" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb10-1283"><a href="#cb10-1283" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-1284"><a href="#cb10-1284" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-1285"><a href="#cb10-1285" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-1286"><a href="#cb10-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1287"><a href="#cb10-1287" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Covariance matrix:</span><span class="ch">\n</span><span class="sc">{</span>np<span class="sc">.</span>array(cov)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-1288"><a href="#cb10-1288" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Eigenvalues: </span><span class="sc">{</span>eigenvalues<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-1289"><a href="#cb10-1289" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Eigenvectors (as columns):</span><span class="ch">\n</span><span class="sc">{</span>eigenvectors<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-1290"><a href="#cb10-1290" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">The first principal component explains </span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>eigenvalues[<span class="dv">0</span>]<span class="op">/</span><span class="bu">sum</span>(eigenvalues)<span class="sc">:.1f}</span><span class="ss">% of the variance"</span>)</span>
<span id="cb10-1291"><a href="#cb10-1291" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-1292"><a href="#cb10-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1293"><a href="#cb10-1293" aria-hidden="true" tabindex="-1"></a>The red arrow shows the first principal component (direction of maximum variance), while the blue arrow shows the second. In the plot, each eigenvector $\mathbf{v}_i$ is rescaled by $1.96 \sqrt{\lambda_i}$ which covers about $95 \%$ of the normal distribution.</span>
<span id="cb10-1294"><a href="#cb10-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1295"><a href="#cb10-1295" aria-hidden="true" tabindex="-1"></a>In Principal Component Analysis (PCA), we might keep only the first component to reduce from 2D to 1D while preserving most of the structure.</span>
<span id="cb10-1296"><a href="#cb10-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1297"><a href="#cb10-1297" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1298"><a href="#cb10-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1299"><a href="#cb10-1299" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-1300"><a href="#cb10-1300" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Multinomial Covariance Structure</span></span>
<span id="cb10-1301"><a href="#cb10-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1302"><a href="#cb10-1302" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">multinomial distribution</span><span class="co">](https://en.wikipedia.org/wiki/Multinomial_distribution)</span> provides a concrete example of covariance matrix structure. If </span>
<span id="cb10-1303"><a href="#cb10-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1304"><a href="#cb10-1304" aria-hidden="true" tabindex="-1"></a>$$\mathbf{X} = (X_1, \ldots, X_k)^T \sim \text{Multinomial}(n, \mathbf{p})$$ </span>
<span id="cb10-1305"><a href="#cb10-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1306"><a href="#cb10-1306" aria-hidden="true" tabindex="-1"></a>where $\mathbf{p} = (p_1, \ldots, p_k)^T$ with $\sum p_i = 1$, then:</span>
<span id="cb10-1307"><a href="#cb10-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1308"><a href="#cb10-1308" aria-hidden="true" tabindex="-1"></a>**Mean vector**: $\mathbb{E}(\mathbf{X}) = n\mathbf{p} = (np_1, \ldots, np_k)^T$</span>
<span id="cb10-1309"><a href="#cb10-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1310"><a href="#cb10-1310" aria-hidden="true" tabindex="-1"></a>**Covariance matrix**:</span>
<span id="cb10-1311"><a href="#cb10-1311" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \begin{pmatrix}</span>
<span id="cb10-1312"><a href="#cb10-1312" aria-hidden="true" tabindex="-1"></a>np_1(1-p_1) &amp; -np_1p_2 &amp; \cdots &amp; -np_1p_k <span class="sc">\\</span></span>
<span id="cb10-1313"><a href="#cb10-1313" aria-hidden="true" tabindex="-1"></a>-np_2p_1 &amp; np_2(1-p_2) &amp; \cdots &amp; -np_2p_k <span class="sc">\\</span></span>
<span id="cb10-1314"><a href="#cb10-1314" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb10-1315"><a href="#cb10-1315" aria-hidden="true" tabindex="-1"></a>-np_kp_1 &amp; -np_kp_2 &amp; \cdots &amp; np_k(1-p_k)</span>
<span id="cb10-1316"><a href="#cb10-1316" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}$$</span>
<span id="cb10-1317"><a href="#cb10-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1318"><a href="#cb10-1318" aria-hidden="true" tabindex="-1"></a>Key observations:</span>
<span id="cb10-1319"><a href="#cb10-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1320"><a href="#cb10-1320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Diagonal: $\mathbb{V}(X_i) = np_i(1-p_i)$ (same as binomial)</span>
<span id="cb10-1321"><a href="#cb10-1321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Off-diagonal: $\mathrm{Cov}(X_i, X_j) = -np_ip_j$ for $i \neq j$ (always negative!)</span>
<span id="cb10-1322"><a href="#cb10-1322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Intuition: If more outcomes fall in category $i$, fewer can fall in category $j$</span>
<span id="cb10-1323"><a href="#cb10-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1324"><a href="#cb10-1324" aria-hidden="true" tabindex="-1"></a>**Special case**: For a die roll with equal probabilities ($p_i = 1/6$ for all $i$):</span>
<span id="cb10-1325"><a href="#cb10-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1326"><a href="#cb10-1326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{V}(X_i) = n \cdot \frac{1}{6} \cdot \frac{5}{6} = \frac{5n}{36}$</span>
<span id="cb10-1327"><a href="#cb10-1327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathrm{Cov}(X_i, X_j) = -n \cdot \frac{1}{6} \cdot \frac{1}{6} = -\frac{n}{36}$ for $i \neq j$</span>
<span id="cb10-1328"><a href="#cb10-1328" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1329"><a href="#cb10-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1330"><a href="#cb10-1330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1331"><a href="#cb10-1331" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conditional Expectation</span></span>
<span id="cb10-1332"><a href="#cb10-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1333"><a href="#cb10-1333" aria-hidden="true" tabindex="-1"></a><span class="fu">### Expectation Given Information</span></span>
<span id="cb10-1334"><a href="#cb10-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1335"><a href="#cb10-1335" aria-hidden="true" tabindex="-1"></a>Conditional expectation captures how the mean changes when we have additional information.</span>
<span id="cb10-1336"><a href="#cb10-1336" aria-hidden="true" tabindex="-1"></a>It is computed similarly to a regular expectation, just replacing the pdf (or PMF) with a *conditional* pdf (or PMF).</span>
<span id="cb10-1337"><a href="#cb10-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1338"><a href="#cb10-1338" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-1339"><a href="#cb10-1339" aria-hidden="true" tabindex="-1"></a>The **conditional expectation** of $X$ given $Y = y$ is:</span>
<span id="cb10-1340"><a href="#cb10-1340" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(X | Y = y) = \begin{cases}</span>
<span id="cb10-1341"><a href="#cb10-1341" aria-hidden="true" tabindex="-1"></a>\sum_x x \mathbb{P}_{X|Y}(x|y) &amp; \text{discrete case} <span class="sc">\\</span></span>
<span id="cb10-1342"><a href="#cb10-1342" aria-hidden="true" tabindex="-1"></a>\int x f_{X|Y}(x|y) \, dx &amp; \text{continuous case}</span>
<span id="cb10-1343"><a href="#cb10-1343" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb10-1344"><a href="#cb10-1344" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1345"><a href="#cb10-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1346"><a href="#cb10-1346" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb10-1347"><a href="#cb10-1347" aria-hidden="true" tabindex="-1"></a>**Subtle but Important**: </span>
<span id="cb10-1348"><a href="#cb10-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1349"><a href="#cb10-1349" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(X)$ is a number</span>
<span id="cb10-1350"><a href="#cb10-1350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(X | Y = y)$ is a number (for fixed $y$)</span>
<span id="cb10-1351"><a href="#cb10-1351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(X | Y)$ is a random variable (because it's a function of $Y$!)</span>
<span id="cb10-1352"><a href="#cb10-1352" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1353"><a href="#cb10-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1354"><a href="#cb10-1354" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of Conditional Expectation</span></span>
<span id="cb10-1355"><a href="#cb10-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1356"><a href="#cb10-1356" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Law of Iterated Expectations"}</span>
<span id="cb10-1357"><a href="#cb10-1357" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}(Y | X)</span><span class="co">]</span> = \mathbb{E}(Y)$$</span>
<span id="cb10-1358"><a href="#cb10-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1359"><a href="#cb10-1359" aria-hidden="true" tabindex="-1"></a>More generally, for any function $r(x, y)$:</span>
<span id="cb10-1360"><a href="#cb10-1360" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}(r(X, Y) | X)</span><span class="co">]</span> = \mathbb{E}(r(X, Y))$$</span>
<span id="cb10-1361"><a href="#cb10-1361" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1362"><a href="#cb10-1362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1363"><a href="#cb10-1363" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-1364"><a href="#cb10-1364" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of the Law of Iterated Expectations</span></span>
<span id="cb10-1365"><a href="#cb10-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1366"><a href="#cb10-1366" aria-hidden="true" tabindex="-1"></a>We'll prove the first equation. Using the definition of conditional expectation and the fact that the joint pdf can be written as $f(x, y) = f_X(x) f_{Y|X}(y|x)$:</span>
<span id="cb10-1367"><a href="#cb10-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1368"><a href="#cb10-1368" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb10-1369"><a href="#cb10-1369" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}(Y | X)</span><span class="co">]</span> &amp;= \mathbb{E}\left<span class="co">[</span><span class="ot">\int y f_{Y|X}(y|X) \, dy\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1370"><a href="#cb10-1370" aria-hidden="true" tabindex="-1"></a>&amp;= \int \left<span class="co">[</span><span class="ot">\int y f_{Y|X}(y|x) \, dy\right</span><span class="co">]</span> f_X(x) \, dx <span class="sc">\\</span></span>
<span id="cb10-1371"><a href="#cb10-1371" aria-hidden="true" tabindex="-1"></a>&amp;= \int \int y f_{Y|X}(y|x) f_X(x) \, dy \, dx <span class="sc">\\</span></span>
<span id="cb10-1372"><a href="#cb10-1372" aria-hidden="true" tabindex="-1"></a>&amp;= \int \int y f(x, y) \, dy \, dx <span class="sc">\\</span></span>
<span id="cb10-1373"><a href="#cb10-1373" aria-hidden="true" tabindex="-1"></a>&amp;= \int y \left<span class="co">[</span><span class="ot">\int f(x, y) \, dx\right</span><span class="co">]</span> dy <span class="sc">\\</span></span>
<span id="cb10-1374"><a href="#cb10-1374" aria-hidden="true" tabindex="-1"></a>&amp;= \int y f_Y(y) \, dy <span class="sc">\\</span></span>
<span id="cb10-1375"><a href="#cb10-1375" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(Y)</span>
<span id="cb10-1376"><a href="#cb10-1376" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb10-1377"><a href="#cb10-1377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1378"><a href="#cb10-1378" aria-hidden="true" tabindex="-1"></a>The key steps are:</span>
<span id="cb10-1379"><a href="#cb10-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1380"><a href="#cb10-1380" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbb{E}(Y|X)$ is a function of $X$, so we take its expectation with respect to $X$</span>
<span id="cb10-1381"><a href="#cb10-1381" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We can interchange the order of integration</span>
<span id="cb10-1382"><a href="#cb10-1382" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$f_{Y|X}(y|x) f_X(x) = f(x,y)$ by the definition of conditional probability</span>
<span id="cb10-1383"><a href="#cb10-1383" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Integrating the joint pdf over $x$ gives the marginal pdf $f_Y(y)$</span>
<span id="cb10-1384"><a href="#cb10-1384" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1385"><a href="#cb10-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1386"><a href="#cb10-1386" aria-hidden="true" tabindex="-1"></a>This powerful result lets us compute expectations by conditioning on useful information.</span>
<span id="cb10-1387"><a href="#cb10-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1388"><a href="#cb10-1388" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-1389"><a href="#cb10-1389" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Breaking Stick Revisited</span></span>
<span id="cb10-1390"><a href="#cb10-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1391"><a href="#cb10-1391" aria-hidden="true" tabindex="-1"></a>Imagine placing two random points on a unit stick. First, we place point $X$ uniformly at random. Then, we place point $Y$ uniformly at random between $X$ and the end of the stick.</span>
<span id="cb10-1392"><a href="#cb10-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1393"><a href="#cb10-1393" aria-hidden="true" tabindex="-1"></a>Formally: Draw $X \sim \text{Uniform}(0, 1)$. After observing $X = x$, draw $Y | X = x \sim \text{Uniform}(x, 1)$.</span>
<span id="cb10-1394"><a href="#cb10-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1395"><a href="#cb10-1395" aria-hidden="true" tabindex="-1"></a>**Question**: What is the expected position of the second point $Y$?</span>
<span id="cb10-1396"><a href="#cb10-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1397"><a href="#cb10-1397" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-1398"><a href="#cb10-1398" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb10-1399"><a href="#cb10-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1400"><a href="#cb10-1400" aria-hidden="true" tabindex="-1"></a>We could find the marginal distribution of $Y$ (which is complex), but it's easier to use conditional expectation:</span>
<span id="cb10-1401"><a href="#cb10-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1402"><a href="#cb10-1402" aria-hidden="true" tabindex="-1"></a>First, find $\mathbb{E}(Y | X = x)$:</span>
<span id="cb10-1403"><a href="#cb10-1403" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(Y | X = x) = \frac{x + 1}{2}$$</span>
<span id="cb10-1404"><a href="#cb10-1404" aria-hidden="true" tabindex="-1"></a>This makes sense: given $X = x$, point $Y$ is uniform on $(x, 1)$, so its expected position is the midpoint.</span>
<span id="cb10-1405"><a href="#cb10-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1406"><a href="#cb10-1406" aria-hidden="true" tabindex="-1"></a>So $\mathbb{E}(Y | X) = \frac{X + 1}{2}$ (a random variable).</span>
<span id="cb10-1407"><a href="#cb10-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1408"><a href="#cb10-1408" aria-hidden="true" tabindex="-1"></a>Now use iterated expectations:</span>
<span id="cb10-1409"><a href="#cb10-1409" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(Y) = \mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}(Y | X)</span><span class="co">]</span> = \mathbb{E}\left<span class="co">[</span><span class="ot">\frac{X + 1}{2}\right</span><span class="co">]</span> = \frac{\mathbb{E}(X) + 1}{2} = \frac{1/2 + 1}{2} = \frac{3}{4}$$</span>
<span id="cb10-1410"><a href="#cb10-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1411"><a href="#cb10-1411" aria-hidden="true" tabindex="-1"></a>The second point lands, on average, at position 3/4 along the stick.</span>
<span id="cb10-1412"><a href="#cb10-1412" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1413"><a href="#cb10-1413" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1414"><a href="#cb10-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1415"><a href="#cb10-1415" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditional Variance</span></span>
<span id="cb10-1416"><a href="#cb10-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1417"><a href="#cb10-1417" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb10-1418"><a href="#cb10-1418" aria-hidden="true" tabindex="-1"></a>The **conditional variance** is:</span>
<span id="cb10-1419"><a href="#cb10-1419" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(Y | X = x) = \mathbb{E}<span class="co">[</span><span class="ot">(Y - \mathbb{E}(Y | X = x))^2 | X = x</span><span class="co">]</span>$$</span>
<span id="cb10-1420"><a href="#cb10-1420" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1421"><a href="#cb10-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1422"><a href="#cb10-1422" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Law of Total Variance"}</span>
<span id="cb10-1423"><a href="#cb10-1423" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(Y) = \mathbb{E}<span class="co">[</span><span class="ot">\mathbb{V}(Y | X)</span><span class="co">]</span> + \mathbb{V}<span class="co">[</span><span class="ot">\mathbb{E}(Y | X)</span><span class="co">]</span>$$</span>
<span id="cb10-1424"><a href="#cb10-1424" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1425"><a href="#cb10-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1426"><a href="#cb10-1426" aria-hidden="true" tabindex="-1"></a>This decomposition says: Total variance = Average within-group variance + Between-group variance.</span>
<span id="cb10-1427"><a href="#cb10-1427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1428"><a href="#cb10-1428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1429"><a href="#cb10-1429" aria-hidden="true" tabindex="-1"></a><span class="fu">## More About the Normal Distribution</span></span>
<span id="cb10-1430"><a href="#cb10-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1431"><a href="#cb10-1431" aria-hidden="true" tabindex="-1"></a><span class="fu">### Quick Recap</span></span>
<span id="cb10-1432"><a href="#cb10-1432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1433"><a href="#cb10-1433" aria-hidden="true" tabindex="-1"></a>Recall that if $X \sim \mathcal{N}(\mu, \sigma^2)$, then:</span>
<span id="cb10-1434"><a href="#cb10-1434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1435"><a href="#cb10-1435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PDF: $f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$</span>
<span id="cb10-1436"><a href="#cb10-1436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mean: $\mathbb{E}(X) = \mu$</span>
<span id="cb10-1437"><a href="#cb10-1437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variance: $\mathbb{V}(X) = \sigma^2$</span>
<span id="cb10-1438"><a href="#cb10-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1439"><a href="#cb10-1439" aria-hidden="true" tabindex="-1"></a>The normal distribution plays a central role in statistics due to the Central Limit Theorem (Chapter 3) and its many convenient mathematical properties.</span>
<span id="cb10-1440"><a href="#cb10-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1441"><a href="#cb10-1441" aria-hidden="true" tabindex="-1"></a><span class="fu">### Entropy of the Normal Distribution</span></span>
<span id="cb10-1442"><a href="#cb10-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1443"><a href="#cb10-1443" aria-hidden="true" tabindex="-1"></a>We can use the expectation to compute the entropy of the normal distribution.</span>
<span id="cb10-1444"><a href="#cb10-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1445"><a href="#cb10-1445" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-1446"><a href="#cb10-1446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Normal Distribution Entropy</span></span>
<span id="cb10-1447"><a href="#cb10-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1448"><a href="#cb10-1448" aria-hidden="true" tabindex="-1"></a>The **differential entropy** of a continuous random variable measures the average uncertainty in the distribution:</span>
<span id="cb10-1449"><a href="#cb10-1449" aria-hidden="true" tabindex="-1"></a>$$H(X) = \mathbb{E}<span class="co">[</span><span class="ot">-\ln f_X(X)</span><span class="co">]</span> = -\int f_X(x) \ln f_X(x) \, dx$$</span>
<span id="cb10-1450"><a href="#cb10-1450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1451"><a href="#cb10-1451" aria-hidden="true" tabindex="-1"></a>Let's calculate this for $X \sim \mathcal{N}(\mu, \sigma^2)$. First, find $-\ln f_X(x)$:</span>
<span id="cb10-1452"><a href="#cb10-1452" aria-hidden="true" tabindex="-1"></a>$$-\ln f_X(x) = \ln(\sqrt{2\pi\sigma^2}) + \frac{(x-\mu)^2}{2\sigma^2} = \frac{1}{2}\ln(2\pi\sigma^2) + \frac{(x-\mu)^2}{2\sigma^2}$$</span>
<span id="cb10-1453"><a href="#cb10-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1454"><a href="#cb10-1454" aria-hidden="true" tabindex="-1"></a>Now compute the expectation:</span>
<span id="cb10-1455"><a href="#cb10-1455" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb10-1456"><a href="#cb10-1456" aria-hidden="true" tabindex="-1"></a>H(X) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">-\ln f_X(X)</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1457"><a href="#cb10-1457" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}\left<span class="co">[</span><span class="ot">\frac{1}{2}\ln(2\pi\sigma^2) + \frac{(X-\mu)^2}{2\sigma^2}\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1458"><a href="#cb10-1458" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\ln(2\pi\sigma^2) + \frac{1}{2\sigma^2}\mathbb{E}<span class="co">[</span><span class="ot">(X-\mu)^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb10-1459"><a href="#cb10-1459" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\ln(2\pi\sigma^2) + \frac{1}{2\sigma^2} \cdot \sigma^2 <span class="sc">\\</span></span>
<span id="cb10-1460"><a href="#cb10-1460" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\ln(2\pi\sigma^2) + \frac{1}{2} <span class="sc">\\</span></span>
<span id="cb10-1461"><a href="#cb10-1461" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\ln(2\pi e\sigma^2) <span class="sc">\\</span></span>
<span id="cb10-1462"><a href="#cb10-1462" aria-hidden="true" tabindex="-1"></a>&amp;= \ln(\sqrt{2\pi e\sigma^2})</span>
<span id="cb10-1463"><a href="#cb10-1463" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb10-1464"><a href="#cb10-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1465"><a href="#cb10-1465" aria-hidden="true" tabindex="-1"></a>**Key insights**:</span>
<span id="cb10-1466"><a href="#cb10-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1467"><a href="#cb10-1467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The entropy increases with $\sigma$ (more spread = more uncertainty)</span>
<span id="cb10-1468"><a href="#cb10-1468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Among all distributions with fixed variance $\sigma^2$, the normal has maximum entropy</span>
<span id="cb10-1469"><a href="#cb10-1469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1470"><a href="#cb10-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1471"><a href="#cb10-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1472"><a href="#cb10-1472" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multivariate Normal Properties</span></span>
<span id="cb10-1473"><a href="#cb10-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1474"><a href="#cb10-1474" aria-hidden="true" tabindex="-1"></a>The $d$-dimensional multivariate normal is parametrized by mean vector $\boldsymbol{\mu} \in \mathbb{R}^d$ and covariance matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$ (symmetric and positive definite).</span>
<span id="cb10-1475"><a href="#cb10-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1476"><a href="#cb10-1476" aria-hidden="true" tabindex="-1"></a>For $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$:</span>
<span id="cb10-1477"><a href="#cb10-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1478"><a href="#cb10-1478" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Independence and diagonal covariance**: Components $X_i$ and $X_j$ are independent if and only if $\Sigma_{ij} = 0$. Thus, the components are mutually independent if and only if $\boldsymbol{\Sigma}$ is diagonal.</span>
<span id="cb10-1479"><a href="#cb10-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1480"><a href="#cb10-1480" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Standard multivariate normal**: If $Z_1, \ldots, Z_d \sim \mathcal{N}(0, 1)$ are independent, then $\mathbf{Z} = (Z_1, \ldots, Z_d)^T \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_d)$, where $\mathbf{I}_d$ is the $d \times d$ identity matrix.</span>
<span id="cb10-1481"><a href="#cb10-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1482"><a href="#cb10-1482" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Marginals are normal**: Each $X_i \sim \mathcal{N}(\mu_i, \Sigma_{ii})$</span>
<span id="cb10-1483"><a href="#cb10-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1484"><a href="#cb10-1484" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Linear combinations are normal**: For any vector $\mathbf{a}$, $\mathbf{a}^T\mathbf{X} \sim \mathcal{N}(\mathbf{a}^T\boldsymbol{\mu}, \mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a})$</span>
<span id="cb10-1485"><a href="#cb10-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1486"><a href="#cb10-1486" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Conditionals are normal**: Suppose we partition:</span>
<span id="cb10-1487"><a href="#cb10-1487" aria-hidden="true" tabindex="-1"></a>   $$\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 <span class="sc">\\</span> \mathbf{X}_2 \end{pmatrix}, \quad </span>
<span id="cb10-1488"><a href="#cb10-1488" aria-hidden="true" tabindex="-1"></a>   \boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 <span class="sc">\\</span> \boldsymbol{\mu}_2 \end{pmatrix}, \quad</span>
<span id="cb10-1489"><a href="#cb10-1489" aria-hidden="true" tabindex="-1"></a>   \boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{21} &amp; \boldsymbol{\Sigma}_{22} \end{pmatrix}$$</span>
<span id="cb10-1490"><a href="#cb10-1490" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-1491"><a href="#cb10-1491" aria-hidden="true" tabindex="-1"></a>   Then the conditional distribution of $\mathbf{X}_2$ given $\mathbf{X}_1 = \mathbf{x}_1$ is:</span>
<span id="cb10-1492"><a href="#cb10-1492" aria-hidden="true" tabindex="-1"></a>   $$\mathbf{X}_2 | \mathbf{X}_1 = \mathbf{x}_1 \sim \mathcal{N}(\boldsymbol{\mu}_{2|1}, \boldsymbol{\Sigma}_{2|1})$$</span>
<span id="cb10-1493"><a href="#cb10-1493" aria-hidden="true" tabindex="-1"></a>   where:</span>
<span id="cb10-1494"><a href="#cb10-1494" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Conditional mean**: $\boldsymbol{\mu}_{2|1} = \boldsymbol{\mu}_2 + \boldsymbol{\Sigma}_{21}\boldsymbol{\Sigma}_{11}^{-1}(\mathbf{x}_1 - \boldsymbol{\mu}_1)$</span>
<span id="cb10-1495"><a href="#cb10-1495" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Conditional covariance**: $\boldsymbol{\Sigma}_{2|1} = \boldsymbol{\Sigma}_{22} - \boldsymbol{\Sigma}_{21}\boldsymbol{\Sigma}_{11}^{-1}\boldsymbol{\Sigma}_{12}$</span>
<span id="cb10-1496"><a href="#cb10-1496" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-1497"><a href="#cb10-1497" aria-hidden="true" tabindex="-1"></a>   Note that the conditional covariance doesn't depend on the observed value $\mathbf{x}_1$!</span>
<span id="cb10-1498"><a href="#cb10-1498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1499"><a href="#cb10-1499" aria-hidden="true" tabindex="-1"></a>These properties are used in multiple algorithms and methods in statistics, including for example:</span>
<span id="cb10-1500"><a href="#cb10-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1501"><a href="#cb10-1501" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gaussian processes</span>
<span id="cb10-1502"><a href="#cb10-1502" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kalman filtering</span>
<span id="cb10-1503"><a href="#cb10-1503" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear regression theory</span>
<span id="cb10-1504"><a href="#cb10-1504" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multivariate statistical methods</span>
<span id="cb10-1505"><a href="#cb10-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1506"><a href="#cb10-1506" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-1507"><a href="#cb10-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1508"><a href="#cb10-1508" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cholesky Decomposition</span></span>
<span id="cb10-1509"><a href="#cb10-1509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1510"><a href="#cb10-1510" aria-hidden="true" tabindex="-1"></a>Every symmetric positive definite covariance matrix $\boldsymbol{\Sigma}$ can be decomposed as:</span>
<span id="cb10-1511"><a href="#cb10-1511" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \mathbf{L}\mathbf{L}^T$$</span>
<span id="cb10-1512"><a href="#cb10-1512" aria-hidden="true" tabindex="-1"></a>where $\mathbf{L}$ is a lower-triangular matrix called the **Cholesky decomposition** (or Cholesky factor).</span>
<span id="cb10-1513"><a href="#cb10-1513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1514"><a href="#cb10-1514" aria-hidden="true" tabindex="-1"></a>This decomposition is crucial for:</span>
<span id="cb10-1515"><a href="#cb10-1515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1516"><a href="#cb10-1516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Simulating multivariate normals**: If $\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and $\mathbf{X} = \boldsymbol{\mu} + \mathbf{L}\mathbf{Z}$, then $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$</span>
<span id="cb10-1517"><a href="#cb10-1517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Transforming to standard form**: If $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, then $\mathbf{L}^{-1}(\mathbf{X} - \boldsymbol{\mu}) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$</span>
<span id="cb10-1518"><a href="#cb10-1518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Efficient computation**: Solving linear systems and computing determinants</span>
<span id="cb10-1519"><a href="#cb10-1519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1520"><a href="#cb10-1520" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1521"><a href="#cb10-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1522"><a href="#cb10-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1523"><a href="#cb10-1523" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb10-1524"><a href="#cb10-1524" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Generating Multivariate Normal Random Vectors via Cholesky</span></span>
<span id="cb10-1525"><a href="#cb10-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1526"><a href="#cb10-1526" aria-hidden="true" tabindex="-1"></a>Let's see how the Cholesky decomposition can be used to transform independent standard normals into correlated multivariate normals.</span>
<span id="cb10-1527"><a href="#cb10-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1530"><a href="#cb10-1530" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-1531"><a href="#cb10-1531" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb10-1532"><a href="#cb10-1532" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb10-1533"><a href="#cb10-1533" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-1534"><a href="#cb10-1534" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-1535"><a href="#cb10-1535" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Ellipse</span>
<span id="cb10-1536"><a href="#cb10-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1537"><a href="#cb10-1537" aria-hidden="true" tabindex="-1"></a><span class="co"># Define mean and covariance</span></span>
<span id="cb10-1538"><a href="#cb10-1538" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb10-1539"><a href="#cb10-1539" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="fl">1.2</span>], </span>
<span id="cb10-1540"><a href="#cb10-1540" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">1.2</span>, <span class="dv">1</span>]])</span>
<span id="cb10-1541"><a href="#cb10-1541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1542"><a href="#cb10-1542" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Cholesky decomposition</span></span>
<span id="cb10-1543"><a href="#cb10-1543" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(Sigma)</span>
<span id="cb10-1544"><a href="#cb10-1544" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Covariance matrix Σ:"</span>)</span>
<span id="cb10-1545"><a href="#cb10-1545" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Sigma)</span>
<span id="cb10-1546"><a href="#cb10-1546" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cholesky factor L (lower triangular):"</span>)</span>
<span id="cb10-1547"><a href="#cb10-1547" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(L)</span>
<span id="cb10-1548"><a href="#cb10-1548" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Verification: L @ L.T ="</span>)</span>
<span id="cb10-1549"><a href="#cb10-1549" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(L <span class="op">@</span> L.T)</span>
<span id="cb10-1550"><a href="#cb10-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1551"><a href="#cb10-1551" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples step by step</span></span>
<span id="cb10-1552"><a href="#cb10-1552" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-1553"><a href="#cb10-1553" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb10-1554"><a href="#cb10-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1555"><a href="#cb10-1555" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Generate independent standard normals</span></span>
<span id="cb10-1556"><a href="#cb10-1556" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.random.standard_normal((n_samples, <span class="dv">2</span>))  <span class="co"># N(0, I)</span></span>
<span id="cb10-1557"><a href="#cb10-1557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1558"><a href="#cb10-1558" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Transform using Cholesky</span></span>
<span id="cb10-1559"><a href="#cb10-1559" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> mu <span class="op">+</span> Z <span class="op">@</span> L.T  <span class="co"># Transform to N(mu, Sigma)</span></span>
<span id="cb10-1560"><a href="#cb10-1560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1561"><a href="#cb10-1561" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformation</span></span>
<span id="cb10-1562"><a href="#cb10-1562" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb10-1563"><a href="#cb10-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1564"><a href="#cb10-1564" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot independent standard normals</span></span>
<span id="cb10-1565"><a href="#cb10-1565" aria-hidden="true" tabindex="-1"></a>ax1.scatter(Z[:, <span class="dv">0</span>], Z[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-1566"><a href="#cb10-1566" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Z₁'</span>)</span>
<span id="cb10-1567"><a href="#cb10-1567" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Z₂'</span>)</span>
<span id="cb10-1568"><a href="#cb10-1568" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Independent N(0,1)'</span>)</span>
<span id="cb10-1569"><a href="#cb10-1569" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb10-1570"><a href="#cb10-1570" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb10-1571"><a href="#cb10-1571" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-1572"><a href="#cb10-1572" aria-hidden="true" tabindex="-1"></a>ax1.axis(<span class="st">'equal'</span>)</span>
<span id="cb10-1573"><a href="#cb10-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1574"><a href="#cb10-1574" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot transformed correlated normals</span></span>
<span id="cb10-1575"><a href="#cb10-1575" aria-hidden="true" tabindex="-1"></a>ax2.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-1576"><a href="#cb10-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1577"><a href="#cb10-1577" aria-hidden="true" tabindex="-1"></a><span class="co"># Add confidence ellipse</span></span>
<span id="cb10-1578"><a href="#cb10-1578" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eigh(Sigma)</span>
<span id="cb10-1579"><a href="#cb10-1579" aria-hidden="true" tabindex="-1"></a>angle <span class="op">=</span> np.degrees(np.arctan2(eigenvectors[<span class="dv">1</span>, <span class="dv">1</span>], eigenvectors[<span class="dv">0</span>, <span class="dv">1</span>]))</span>
<span id="cb10-1580"><a href="#cb10-1580" aria-hidden="true" tabindex="-1"></a>ellipse <span class="op">=</span> Ellipse(mu, <span class="dv">2</span><span class="op">*</span>np.sqrt(eigenvalues[<span class="dv">1</span>]), <span class="dv">2</span><span class="op">*</span>np.sqrt(eigenvalues[<span class="dv">0</span>]),</span>
<span id="cb10-1581"><a href="#cb10-1581" aria-hidden="true" tabindex="-1"></a>                  angle<span class="op">=</span>angle, facecolor<span class="op">=</span><span class="st">'none'</span>, edgecolor<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-1582"><a href="#cb10-1582" aria-hidden="true" tabindex="-1"></a>ax2.add_patch(ellipse)</span>
<span id="cb10-1583"><a href="#cb10-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1584"><a href="#cb10-1584" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'X₁'</span>)</span>
<span id="cb10-1585"><a href="#cb10-1585" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'X₂'</span>)</span>
<span id="cb10-1586"><a href="#cb10-1586" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Correlated N(μ, Σ)'</span>)</span>
<span id="cb10-1587"><a href="#cb10-1587" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-1588"><a href="#cb10-1588" aria-hidden="true" tabindex="-1"></a>ax2.axis(<span class="st">'equal'</span>)</span>
<span id="cb10-1589"><a href="#cb10-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1590"><a href="#cb10-1590" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-1591"><a href="#cb10-1591" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-1592"><a href="#cb10-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1593"><a href="#cb10-1593" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">The Cholesky decomposition transforms:"</span>)</span>
<span id="cb10-1594"><a href="#cb10-1594" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"• Independent standard normals Z ~ N(0, I)"</span>)</span>
<span id="cb10-1595"><a href="#cb10-1595" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"• Into correlated normals X = μ + LZ ~ N(μ, Σ)"</span>)</span>
<span id="cb10-1596"><a href="#cb10-1596" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample covariance:</span><span class="ch">\n</span><span class="sc">{</span>np<span class="sc">.</span>cov(X.T)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-1597"><a href="#cb10-1597" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-1598"><a href="#cb10-1598" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1599"><a href="#cb10-1599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1600"><a href="#cb10-1600" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb10-1601"><a href="#cb10-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1602"><a href="#cb10-1602" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb10-1603"><a href="#cb10-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1604"><a href="#cb10-1604" aria-hidden="true" tabindex="-1"></a>We've explored the fundamental tools for summarizing and understanding random variables:</span>
<span id="cb10-1605"><a href="#cb10-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1606"><a href="#cb10-1606" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Expectation as weighted average**: The fundamental summary of a distribution</span>
<span id="cb10-1607"><a href="#cb10-1607" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Linearity—the master property**: $\mathbb{E}(\sum a_i X_i) = \sum a_i \mathbb{E}(X_i)$ always works!</span>
<span id="cb10-1608"><a href="#cb10-1608" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Variance measures spread**: How far from the mean should we expect outcomes?</span>
<span id="cb10-1609"><a href="#cb10-1609" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Covariance measures linear relationships**: Do variables move together?</span>
<span id="cb10-1610"><a href="#cb10-1610" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Conditional expectation as best prediction**: What's our best guess given information?</span>
<span id="cb10-1611"><a href="#cb10-1611" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Matrix operations extend naturally**: Same concepts work for random vectors</span>
<span id="cb10-1612"><a href="#cb10-1612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1613"><a href="#cb10-1613" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why These Concepts Matter</span></span>
<span id="cb10-1614"><a href="#cb10-1614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1615"><a href="#cb10-1615" aria-hidden="true" tabindex="-1"></a>**For Statistical Inference**:</span>
<span id="cb10-1616"><a href="#cb10-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1617"><a href="#cb10-1617" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample means estimate population expectations</span>
<span id="cb10-1618"><a href="#cb10-1618" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variance quantifies uncertainty in estimates</span>
<span id="cb10-1619"><a href="#cb10-1619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Covariance reveals relationships between variables</span>
<span id="cb10-1620"><a href="#cb10-1620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conditional expectation enables regression analysis</span>
<span id="cb10-1621"><a href="#cb10-1621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1622"><a href="#cb10-1622" aria-hidden="true" tabindex="-1"></a>**For Machine Learning**:</span>
<span id="cb10-1623"><a href="#cb10-1623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1624"><a href="#cb10-1624" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss functions are expectations over data distributions</span>
<span id="cb10-1625"><a href="#cb10-1625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gradient descent minimizes expected loss</span>
<span id="cb10-1626"><a href="#cb10-1626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature correlations affect model performance</span>
<span id="cb10-1627"><a href="#cb10-1627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conditional expectations define optimal predictors</span>
<span id="cb10-1628"><a href="#cb10-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1629"><a href="#cb10-1629" aria-hidden="true" tabindex="-1"></a>**For Data Science Practice**:</span>
<span id="cb10-1630"><a href="#cb10-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1631"><a href="#cb10-1631" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Summary statistics (mean, variance) describe data concisely</span>
<span id="cb10-1632"><a href="#cb10-1632" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Correlation analysis reveals variable relationships</span>
<span id="cb10-1633"><a href="#cb10-1633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variance decomposition explains data structure</span>
<span id="cb10-1634"><a href="#cb10-1634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear algebra connects to dimensionality reduction (PCA)</span>
<span id="cb10-1635"><a href="#cb10-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1636"><a href="#cb10-1636" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb10-1637"><a href="#cb10-1637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1638"><a href="#cb10-1638" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Assuming $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$ without independence**</span>
<span id="cb10-1639"><a href="#cb10-1639" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>This only works when $X$ and $Y$ are independent!</span>
<span id="cb10-1640"><a href="#cb10-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1641"><a href="#cb10-1641" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Confusing the $\mathbb{V}(X - Y)$ formula**</span>
<span id="cb10-1642"><a href="#cb10-1642" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Remember: $\mathbb{V}(X - Y) = \mathbb{V}(X) + \mathbb{V}(Y)$ when independent (plus, not minus!)</span>
<span id="cb10-1643"><a href="#cb10-1643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1644"><a href="#cb10-1644" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Treating $\mathbb{E}(X|Y)$ as a number instead of a random variable**</span>
<span id="cb10-1645"><a href="#cb10-1645" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\mathbb{E}(X|Y = y)$ is a number, but $\mathbb{E}(X|Y)$ is a function of $Y$</span>
<span id="cb10-1646"><a href="#cb10-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1647"><a href="#cb10-1647" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Assuming uncorrelated means independent**</span>
<span id="cb10-1648"><a href="#cb10-1648" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Zero correlation does NOT imply independence (remember $X$ and $X^2$)</span>
<span id="cb10-1649"><a href="#cb10-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1650"><a href="#cb10-1650" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Forgetting existence conditions**</span>
<span id="cb10-1651"><a href="#cb10-1651" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Not all distributions have finite expectation (Cauchy!)</span>
<span id="cb10-1652"><a href="#cb10-1652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1653"><a href="#cb10-1653" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb10-1654"><a href="#cb10-1654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1655"><a href="#cb10-1655" aria-hidden="true" tabindex="-1"></a>This chapter builds on Chapter 1's probability foundations and provides essential tools for all statistical inference:</span>
<span id="cb10-1656"><a href="#cb10-1656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1657"><a href="#cb10-1657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**From Chapter 1**: We've formalized the expectation concept briefly introduced with random variables, showing how it connects to supervised learning through risk minimization and cross-entropy loss</span>
<span id="cb10-1658"><a href="#cb10-1658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next - Chapter 3 (Convergence &amp; Inference)**: The sample mean and variance we studied will be shown to converge to population values (Law of Large Numbers) and have predictable distributions (Central Limit Theorem), justifying their use as estimators</span>
<span id="cb10-1659"><a href="#cb10-1659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chapter 4 (Bootstrap)**: We'll use the plug-in principle with empirical distributions to estimate variances and other functionals when theoretical calculations become intractable</span>
<span id="cb10-1660"><a href="#cb10-1660" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Future Applications**: Conditional expectation forms the foundation for regression (Chapter 5+), while variance decomposition and covariance matrices are central to multivariate methods throughout the course</span>
<span id="cb10-1661"><a href="#cb10-1661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1662"><a href="#cb10-1662" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb10-1663"><a href="#cb10-1663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1664"><a href="#cb10-1664" aria-hidden="true" tabindex="-1"></a>Try these problems to test your understanding:</span>
<span id="cb10-1665"><a href="#cb10-1665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1666"><a href="#cb10-1666" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Linearity puzzle**: In a class of 30 students, each has probability 1/365 of having a birthday today. What's the expected number of birthdays today? (Ignore leap years) *Hint: Define indicator variables $X_i$ for each student. What is $\mathbb{E}(X_i)$? Then use linearity.*</span>
<span id="cb10-1667"><a href="#cb10-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1668"><a href="#cb10-1668" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Variance with correlation**: If $\mathbb{V}(X) = 4$, $\mathbb{V}(Y) = 9$, and $\rho(X,Y) = 0.5$, find $\mathbb{V}(2X - Y)$.</span>
<span id="cb10-1669"><a href="#cb10-1669" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-1670"><a href="#cb10-1670" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Conditional expectation**: Toss a fair coin. If heads, draw $X \sim \mathcal{N}(0, 1)$. If tails, draw $X \sim \mathcal{N}(2, 1)$. Find $\mathbb{E}(X)$ and $\mathbb{V}(X)$.</span>
<span id="cb10-1671"><a href="#cb10-1671" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-1672"><a href="#cb10-1672" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Matrix expectation**: If $\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_2)$ and $\mathbf{A} = \begin{pmatrix} 1 &amp; 1 <span class="sc">\\</span> 1 &amp; -1 \end{pmatrix}$, find the distribution of $\mathbf{AX}$.</span>
<span id="cb10-1673"><a href="#cb10-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1674"><a href="#cb10-1674" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb10-1675"><a href="#cb10-1675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1676"><a href="#cb10-1676" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb10-1677"><a href="#cb10-1677" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb10-1678"><a href="#cb10-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1679"><a href="#cb10-1679" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb10-1680"><a href="#cb10-1680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1681"><a href="#cb10-1681" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb10-1682"><a href="#cb10-1682" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-1683"><a href="#cb10-1683" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb10-1684"><a href="#cb10-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1685"><a href="#cb10-1685" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic expectations</span></span>
<span id="cb10-1686"><a href="#cb10-1686" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb10-1687"><a href="#cb10-1687" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>])</span>
<span id="cb10-1688"><a href="#cb10-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1689"><a href="#cb10-1689" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete expectation</span></span>
<span id="cb10-1690"><a href="#cb10-1690" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.<span class="bu">sum</span>(x <span class="op">*</span> probabilities)</span>
<span id="cb10-1691"><a href="#cb10-1691" aria-hidden="true" tabindex="-1"></a>variance <span class="op">=</span> np.<span class="bu">sum</span>((x <span class="op">-</span> mean)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> probabilities)</span>
<span id="cb10-1692"><a href="#cb10-1692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1693"><a href="#cb10-1693" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb10-1694"><a href="#cb10-1694" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">15</span>, <span class="dv">1000</span>)</span>
<span id="cb10-1695"><a href="#cb10-1695" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb10-1696"><a href="#cb10-1696" aria-hidden="true" tabindex="-1"></a>sample_var <span class="op">=</span> np.var(data, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># ddof=1 for unbiased</span></span>
<span id="cb10-1697"><a href="#cb10-1697" aria-hidden="true" tabindex="-1"></a>sample_std <span class="op">=</span> np.std(data, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-1698"><a href="#cb10-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1699"><a href="#cb10-1699" aria-hidden="true" tabindex="-1"></a><span class="co"># Covariance and correlation</span></span>
<span id="cb10-1700"><a href="#cb10-1700" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> np.random.multivariate_normal([<span class="dv">0</span>, <span class="dv">0</span>], [[<span class="dv">1</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">1</span>]], <span class="dv">1000</span>).T</span>
<span id="cb10-1701"><a href="#cb10-1701" aria-hidden="true" tabindex="-1"></a>covariance <span class="op">=</span> np.cov(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># or np.cov(x, y, ddof=1)</span></span>
<span id="cb10-1702"><a href="#cb10-1702" aria-hidden="true" tabindex="-1"></a>correlation <span class="op">=</span> np.corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb10-1703"><a href="#cb10-1703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1704"><a href="#cb10-1704" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix operations</span></span>
<span id="cb10-1705"><a href="#cb10-1705" aria-hidden="true" tabindex="-1"></a>mean_vector <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb10-1706"><a href="#cb10-1706" aria-hidden="true" tabindex="-1"></a>cov_matrix <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">1</span>]])</span>
<span id="cb10-1707"><a href="#cb10-1707" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.multivariate_normal(mean_vector, cov_matrix, <span class="dv">1000</span>)</span>
<span id="cb10-1708"><a href="#cb10-1708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1709"><a href="#cb10-1709" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear transformation</span></span>
<span id="cb10-1710"><a href="#cb10-1710" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb10-1711"><a href="#cb10-1711" aria-hidden="true" tabindex="-1"></a>transformed_mean <span class="op">=</span> A <span class="op">@</span> mean_vector</span>
<span id="cb10-1712"><a href="#cb10-1712" aria-hidden="true" tabindex="-1"></a>transformed_cov <span class="op">=</span> A <span class="op">@</span> cov_matrix <span class="op">@</span> A.T</span>
<span id="cb10-1713"><a href="#cb10-1713" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-1714"><a href="#cb10-1714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1715"><a href="#cb10-1715" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb10-1716"><a href="#cb10-1716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1717"><a href="#cb10-1717" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb10-1718"><a href="#cb10-1718" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic expectations</span></span>
<span id="cb10-1719"><a href="#cb10-1719" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span></span>
<span id="cb10-1720"><a href="#cb10-1720" aria-hidden="true" tabindex="-1"></a>probabilities <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>)</span>
<span id="cb10-1721"><a href="#cb10-1721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1722"><a href="#cb10-1722" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete expectation</span></span>
<span id="cb10-1723"><a href="#cb10-1723" aria-hidden="true" tabindex="-1"></a>mean_val <span class="ot">&lt;-</span> <span class="fu">sum</span>(x <span class="sc">*</span> probabilities)</span>
<span id="cb10-1724"><a href="#cb10-1724" aria-hidden="true" tabindex="-1"></a>variance_val <span class="ot">&lt;-</span> <span class="fu">sum</span>((x <span class="sc">-</span> mean_val)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> probabilities)</span>
<span id="cb10-1725"><a href="#cb10-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1726"><a href="#cb10-1726" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb10-1727"><a href="#cb10-1727" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>)</span>
<span id="cb10-1728"><a href="#cb10-1728" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(data)</span>
<span id="cb10-1729"><a href="#cb10-1729" aria-hidden="true" tabindex="-1"></a>sample_var <span class="ot">&lt;-</span> <span class="fu">var</span>(data)  <span class="co"># Automatically uses n-1</span></span>
<span id="cb10-1730"><a href="#cb10-1730" aria-hidden="true" tabindex="-1"></a>sample_sd <span class="ot">&lt;-</span> <span class="fu">sd</span>(data)</span>
<span id="cb10-1731"><a href="#cb10-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1732"><a href="#cb10-1732" aria-hidden="true" tabindex="-1"></a><span class="co"># Covariance and correlation</span></span>
<span id="cb10-1733"><a href="#cb10-1733" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb10-1734"><a href="#cb10-1734" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">1000</span>, <span class="at">mu =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb10-1735"><a href="#cb10-1735" aria-hidden="true" tabindex="-1"></a>                   <span class="at">Sigma =</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb10-1736"><a href="#cb10-1736" aria-hidden="true" tabindex="-1"></a>covariance <span class="ot">&lt;-</span> <span class="fu">cov</span>(samples[,<span class="dv">1</span>], samples[,<span class="dv">2</span>])</span>
<span id="cb10-1737"><a href="#cb10-1737" aria-hidden="true" tabindex="-1"></a>correlation <span class="ot">&lt;-</span> <span class="fu">cor</span>(samples[,<span class="dv">1</span>], samples[,<span class="dv">2</span>])</span>
<span id="cb10-1738"><a href="#cb10-1738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1739"><a href="#cb10-1739" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix operations</span></span>
<span id="cb10-1740"><a href="#cb10-1740" aria-hidden="true" tabindex="-1"></a>mean_vector <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb10-1741"><a href="#cb10-1741" aria-hidden="true" tabindex="-1"></a>cov_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb10-1742"><a href="#cb10-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1743"><a href="#cb10-1743" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear transformation</span></span>
<span id="cb10-1744"><a href="#cb10-1744" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb10-1745"><a href="#cb10-1745" aria-hidden="true" tabindex="-1"></a>transformed_mean <span class="ot">&lt;-</span> A <span class="sc">%*%</span> mean_vector</span>
<span id="cb10-1746"><a href="#cb10-1746" aria-hidden="true" tabindex="-1"></a>transformed_cov <span class="ot">&lt;-</span> A <span class="sc">%*%</span> cov_matrix <span class="sc">%*%</span> <span class="fu">t</span>(A)</span>
<span id="cb10-1747"><a href="#cb10-1747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-1748"><a href="#cb10-1748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1749"><a href="#cb10-1749" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1750"><a href="#cb10-1750" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1751"><a href="#cb10-1751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1752"><a href="#cb10-1752" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb10-1753"><a href="#cb10-1753" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-1754"><a href="#cb10-1754" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb10-1755"><a href="#cb10-1755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1756"><a href="#cb10-1756" aria-hidden="true" tabindex="-1"></a>Python and R code examples for this chapter can be found in the HTML version of these notes.</span>
<span id="cb10-1757"><a href="#cb10-1757" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1758"><a href="#cb10-1758" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1759"><a href="#cb10-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1760"><a href="#cb10-1760" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb10-1761"><a href="#cb10-1761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1762"><a href="#cb10-1762" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb10-1763"><a href="#cb10-1763" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb10-1764"><a href="#cb10-1764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1765"><a href="#cb10-1765" aria-hidden="true" tabindex="-1"></a>This table maps sections in these lecture notes to the corresponding sections in @wasserman2013all ("All of Statistics" or AoS).</span>
<span id="cb10-1766"><a href="#cb10-1766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1767"><a href="#cb10-1767" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding AoS Section(s) |</span>
<span id="cb10-1768"><a href="#cb10-1768" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb10-1769"><a href="#cb10-1769" aria-hidden="true" tabindex="-1"></a>| **Introduction and Motivation** | Expanded material from the slides, contextualizing expectation for machine learning. |</span>
<span id="cb10-1770"><a href="#cb10-1770" aria-hidden="true" tabindex="-1"></a>| **Foundations of Expectation** | |</span>
<span id="cb10-1771"><a href="#cb10-1771" aria-hidden="true" tabindex="-1"></a>| ↳ Definition and Basic Properties | AoS §3.1 |</span>
<span id="cb10-1772"><a href="#cb10-1772" aria-hidden="true" tabindex="-1"></a>| ↳ Existence of Expectation | AoS §3.1 |</span>
<span id="cb10-1773"><a href="#cb10-1773" aria-hidden="true" tabindex="-1"></a>| ↳ Expectation of Functions | AoS §3.1 (Rule of the Lazy Statistician) |</span>
<span id="cb10-1774"><a href="#cb10-1774" aria-hidden="true" tabindex="-1"></a>| **Properties of Expectation** | |</span>
<span id="cb10-1775"><a href="#cb10-1775" aria-hidden="true" tabindex="-1"></a>| ↳ The Linearity Property | AoS §3.2 (Theorem 3.11) |</span>
<span id="cb10-1776"><a href="#cb10-1776" aria-hidden="true" tabindex="-1"></a>| ↳ Independence and Products | AoS §3.2 (Theorem 3.13) |</span>
<span id="cb10-1777"><a href="#cb10-1777" aria-hidden="true" tabindex="-1"></a>| **Variance and Its Properties** | |</span>
<span id="cb10-1778"><a href="#cb10-1778" aria-hidden="true" tabindex="-1"></a>| ↳ Measuring Spread | AoS §3.3 (Definition 3.14) |</span>
<span id="cb10-1779"><a href="#cb10-1779" aria-hidden="true" tabindex="-1"></a>| ↳ Properties of Variance | AoS §3.3 (Theorem 3.15) |</span>
<span id="cb10-1780"><a href="#cb10-1780" aria-hidden="true" tabindex="-1"></a>| **Sample Mean and Variance** | AoS §3.3 (Definitions and Theorem 3.17) |</span>
<span id="cb10-1781"><a href="#cb10-1781" aria-hidden="true" tabindex="-1"></a>| **Covariance and Correlation** | |</span>
<span id="cb10-1782"><a href="#cb10-1782" aria-hidden="true" tabindex="-1"></a>| ↳ Linear Relationships | AoS §3.3 (Definition 3.18) |</span>
<span id="cb10-1783"><a href="#cb10-1783" aria-hidden="true" tabindex="-1"></a>| ↳ Properties of Covariance and Correlation | AoS §3.3 (Theorem 3.19) |</span>
<span id="cb10-1784"><a href="#cb10-1784" aria-hidden="true" tabindex="-1"></a>| ↳ Variance of Sums (General Case) | AoS §3.3 (Theorem 3.20) |</span>
<span id="cb10-1785"><a href="#cb10-1785" aria-hidden="true" tabindex="-1"></a>| **Expectation with Matrices** | |</span>
<span id="cb10-1786"><a href="#cb10-1786" aria-hidden="true" tabindex="-1"></a>| ↳ Random Vectors &amp; Covariance Matrix | AoS §3.4, §14.1 |</span>
<span id="cb10-1787"><a href="#cb10-1787" aria-hidden="true" tabindex="-1"></a>| ↳ Linear Transformations | AoS §3.4 (Lemma 3.21) |</span>
<span id="cb10-1788"><a href="#cb10-1788" aria-hidden="true" tabindex="-1"></a>| ↳ Interpreting the Covariance Matrix | New material (connects to PCA). |</span>
<span id="cb10-1789"><a href="#cb10-1789" aria-hidden="true" tabindex="-1"></a>| ↳ Example: Multinomial Covariance | AoS §3.4 |</span>
<span id="cb10-1790"><a href="#cb10-1790" aria-hidden="true" tabindex="-1"></a>| **Conditional Expectation** | |</span>
<span id="cb10-1791"><a href="#cb10-1791" aria-hidden="true" tabindex="-1"></a>| ↳ Expectation Given Information | AoS §3.5 (Definition 3.22) |</span>
<span id="cb10-1792"><a href="#cb10-1792" aria-hidden="true" tabindex="-1"></a>| ↳ Properties (Iterated Expectations) | AoS §3.5 (Theorem 3.24) |</span>
<span id="cb10-1793"><a href="#cb10-1793" aria-hidden="true" tabindex="-1"></a>| ↳ Conditional Variance (Total Variance) | AoS §3.5 (Definition 3.26, Theorem 3.27) |</span>
<span id="cb10-1794"><a href="#cb10-1794" aria-hidden="true" tabindex="-1"></a>| **More About the Normal Distribution** | New material, applying expectation concepts. Some properties from AoS §2.10 are revisited. |</span>
<span id="cb10-1795"><a href="#cb10-1795" aria-hidden="true" tabindex="-1"></a>| **Chapter Summary and Connections** | New summary material. |</span>
<span id="cb10-1796"><a href="#cb10-1796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1797"><a href="#cb10-1797" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-1798"><a href="#cb10-1798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1799"><a href="#cb10-1799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1800"><a href="#cb10-1800" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Reading</span></span>
<span id="cb10-1801"><a href="#cb10-1801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1802"><a href="#cb10-1802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistical perspective**: Casella &amp; Berger, "Statistical Inference"</span>
<span id="cb10-1803"><a href="#cb10-1803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Machine learning view**: Bishop, "Pattern Recognition and Machine Learning" Chapters 1 and 2</span>
<span id="cb10-1804"><a href="#cb10-1804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Matrix cookbook**: Petersen &amp; Pedersen, "The Matrix Cookbook" (for multivariate formulas) -- <span class="co">[</span><span class="ot">link</span><span class="co">](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)</span></span>
<span id="cb10-1805"><a href="#cb10-1805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1806"><a href="#cb10-1806" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-1807"><a href="#cb10-1807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1808"><a href="#cb10-1808" aria-hidden="true" tabindex="-1"></a>*Next time you compute a sample mean, remember: you're estimating an expectation. When you minimize a loss function, you're approximating an expected loss. The gap between what we can compute (sample statistics) and what we want to know (population parameters) drives all of statistical inference. Expectation is the bridge!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>