<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 6&nbsp; Parametric Inference II: Properties of Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/07-hypothesis-testing.html" rel="next">
<link href="../chapters/05-parametric-inference-I.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/06-parametric-inference-II.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">6.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-how-good-are-our-estimators" id="toc-introduction-how-good-are-our-estimators" class="nav-link" data-scroll-target="#introduction-how-good-are-our-estimators"><span class="header-section-number">6.2</span> Introduction: How Good Are Our Estimators?</a></li>
  <li><a href="#warm-up-a-complete-mle-example-with-numerical-optimization" id="toc-warm-up-a-complete-mle-example-with-numerical-optimization" class="nav-link" data-scroll-target="#warm-up-a-complete-mle-example-with-numerical-optimization"><span class="header-section-number">6.3</span> Warm-up: A Complete MLE Example with Numerical Optimization</a></li>
  <li><a href="#core-properties-of-the-maximum-likelihood-estimator" id="toc-core-properties-of-the-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#core-properties-of-the-maximum-likelihood-estimator"><span class="header-section-number">6.4</span> Core Properties of the Maximum Likelihood Estimator</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><span class="header-section-number">6.4.1</span> Overview</a></li>
  <li><a href="#consistency-getting-it-right-eventually" id="toc-consistency-getting-it-right-eventually" class="nav-link" data-scroll-target="#consistency-getting-it-right-eventually"><span class="header-section-number">6.4.2</span> Consistency: Getting It Right Eventually</a></li>
  <li><a href="#equivariance-reparameterization-invariance" id="toc-equivariance-reparameterization-invariance" class="nav-link" data-scroll-target="#equivariance-reparameterization-invariance"><span class="header-section-number">6.4.3</span> Equivariance: Reparameterization Invariance</a></li>
  <li><a href="#asymptotic-normality-optimality" id="toc-asymptotic-normality-optimality" class="nav-link" data-scroll-target="#asymptotic-normality-optimality"><span class="header-section-number">6.4.4</span> Asymptotic Normality &amp; Optimality</a></li>
  </ul></li>
  <li><a href="#fisher-information-and-confidence-intervals" id="toc-fisher-information-and-confidence-intervals" class="nav-link" data-scroll-target="#fisher-information-and-confidence-intervals"><span class="header-section-number">6.5</span> Fisher Information and Confidence Intervals</a>
  <ul class="collapse">
  <li><a href="#fisher-information-quantifying-what-data-can-tell-us" id="toc-fisher-information-quantifying-what-data-can-tell-us" class="nav-link" data-scroll-target="#fisher-information-quantifying-what-data-can-tell-us"><span class="header-section-number">6.5.1</span> Fisher Information: Quantifying What Data Can Tell Us</a></li>
  <li><a href="#asymptotic-normality-of-the-mle" id="toc-asymptotic-normality-of-the-mle" class="nav-link" data-scroll-target="#asymptotic-normality-of-the-mle"><span class="header-section-number">6.5.2</span> Asymptotic Normality of the MLE</a></li>
  <li><a href="#constructing-confidence-intervals-for-the-mle" id="toc-constructing-confidence-intervals-for-the-mle" class="nav-link" data-scroll-target="#constructing-confidence-intervals-for-the-mle"><span class="header-section-number">6.5.3</span> Constructing Confidence Intervals for the MLE</a></li>
  </ul></li>
  <li><a href="#additional-topics" id="toc-additional-topics" class="nav-link" data-scroll-target="#additional-topics"><span class="header-section-number">6.6</span> Additional Topics</a>
  <ul class="collapse">
  <li><a href="#the-delta-method-confidence-intervals-for-transformations" id="toc-the-delta-method-confidence-intervals-for-transformations" class="nav-link" data-scroll-target="#the-delta-method-confidence-intervals-for-transformations"><span class="header-section-number">6.6.1</span> The Delta Method: Confidence Intervals for Transformations</a></li>
  <li><a href="#multiparameter-models" id="toc-multiparameter-models" class="nav-link" data-scroll-target="#multiparameter-models"><span class="header-section-number">6.6.2</span> Multiparameter Models</a></li>
  <li><a href="#sufficient-statistics" id="toc-sufficient-statistics" class="nav-link" data-scroll-target="#sufficient-statistics"><span class="header-section-number">6.6.3</span> Sufficient Statistics</a></li>
  </ul></li>
  <li><a href="#connection-to-machine-learning-cross-entropy-as-mle" id="toc-connection-to-machine-learning-cross-entropy-as-mle" class="nav-link" data-scroll-target="#connection-to-machine-learning-cross-entropy-as-mle"><span class="header-section-number">6.7</span> Connection to Machine Learning: Cross-Entropy as MLE</a></li>
  <li><a href="#mle-for-latent-variable-models-the-em-algorithm" id="toc-mle-for-latent-variable-models-the-em-algorithm" class="nav-link" data-scroll-target="#mle-for-latent-variable-models-the-em-algorithm"><span class="header-section-number">6.8</span> MLE for Latent Variable Models: The EM Algorithm</a>
  <ul class="collapse">
  <li><a href="#the-challenge-of-latent-variables" id="toc-the-challenge-of-latent-variables" class="nav-link" data-scroll-target="#the-challenge-of-latent-variables"><span class="header-section-number">6.8.1</span> The Challenge of Latent Variables</a></li>
  <li><a href="#the-expectation-maximization-em-algorithm" id="toc-the-expectation-maximization-em-algorithm" class="nav-link" data-scroll-target="#the-expectation-maximization-em-algorithm"><span class="header-section-number">6.8.2</span> The Expectation-Maximization (EM) Algorithm</a></li>
  <li><a href="#properties-and-practical-considerations" id="toc-properties-and-practical-considerations" class="nav-link" data-scroll-target="#properties-and-practical-considerations"><span class="header-section-number">6.8.3</span> Properties and Practical Considerations</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">6.9</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">6.9.1</span> Key Concepts Review</a></li>
  <li><a href="#the-big-picture" id="toc-the-big-picture" class="nav-link" data-scroll-target="#the-big-picture"><span class="header-section-number">6.9.2</span> The Big Picture</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">6.9.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">6.9.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">6.9.5</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">6.9.6</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">6.9.7</span> Connections to Source Material</a></li>
  <li><a href="#further-materials" id="toc-further-materials" class="nav-link" data-scroll-target="#further-materials"><span class="header-section-number">6.9.8</span> Further Materials</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">6.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li><strong>Explain the key properties of the MLE</strong> (consistency, equivariance, asymptotic normality, and efficiency) and their practical implications.</li>
<li><strong>Define Fisher Information</strong> and use it to quantify the precision of parameter estimates.</li>
<li><strong>Construct and interpret confidence intervals</strong> for parameters using the MLE and its standard error.</li>
<li><strong>Apply the Delta Method</strong> to find confidence intervals for transformed parameters.</li>
<li><strong>Implement the EM algorithm</strong> for finding MLEs in models with latent variables.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter explores the theoretical properties of Maximum Likelihood Estimators and provides practical tools for statistical inference. The material is adapted from Chapter 9 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span>, with additional computational examples and modern perspectives on optimization for latent variable models.</p>
</div>
</div>
</section>
<section id="introduction-how-good-are-our-estimators" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="introduction-how-good-are-our-estimators"><span class="header-section-number">6.2</span> Introduction: How Good Are Our Estimators?</h2>
<p>In Chapter 5, we learned how to <em>find</em> estimators for parametric models using the Method of Moments (MoM) and Maximum Likelihood Estimation (MLE). We saw that finding the MLE often requires numerical optimization, and we explored practical algorithms for this task.</p>
<p>But finding an estimator is only half the story. The next natural question is: <strong>how good are these estimators?</strong></p>
<p>This chapter addresses this fundamental question by exploring the <em>properties</em> of estimators, with a special focus on the MLE. We’ll discover why the MLE is considered the “gold standard” of parametric estimation – it turns out to have a remarkable collection of desirable properties that make it optimal in many senses.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Questions We’ll Answer
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Does our estimator converge to the true value</strong> as we get more data? (Consistency)</li>
<li><strong>How much uncertainty is associated with our estimate?</strong> Can we quantify it? (Confidence Intervals)</li>
<li><strong>Is our estimator the best possible one</strong>, or could we do better? (Efficiency)</li>
<li><strong>How do these properties extend</strong> to complex models with multiple parameters or latent variables?</li>
</ul>
</div>
</div>
<p>Understanding these properties is crucial for several reasons:</p>
<ol type="1">
<li><p><strong>Practical inference</strong>: Knowing that <span class="math inline">\hat{\theta}_n = 2.3</span> is not very useful without understanding the uncertainty. Is the true value likely between 2.2 and 2.4, or between 1 and 4?</p></li>
<li><p><strong>Method selection</strong>: When multiple estimation methods exist, these properties help us choose the best one.</p></li>
<li><p><strong>Foundation for advanced methods</strong>: The concepts in this chapter – especially Fisher Information and the EM algorithm – are fundamental to modern statistical methods and machine learning.</p></li>
<li><p><strong>Connection to optimization</strong>: We’ll see that the same mathematical quantities that determine statistical uncertainty also appear in optimization algorithms.</p></li>
</ol>
<p>Let’s begin with a concrete example to bridge our previous work on finding MLEs with the new focus on analyzing their properties.</p>
</section>
<section id="warm-up-a-complete-mle-example-with-numerical-optimization" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="warm-up-a-complete-mle-example-with-numerical-optimization"><span class="header-section-number">6.3</span> Warm-up: A Complete MLE Example with Numerical Optimization</h2>
<p>Before diving into the theoretical properties, let’s work through a complete example that bridges Chapter 5 (finding MLEs) and Chapter 6 (analyzing them). We’ll find the MLE for a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>, which requires numerical optimization.</p>
<p>The <strong>likelihood</strong> for the Beta distribution with parameters <span class="math inline">\alpha, \beta &gt; 0</span> is: <span class="math display">f(x; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}</span></p>
<p>The <strong>log-likelihood</strong> for <span class="math inline">n</span> observations is: <span class="math display">\begin{aligned}
\ell_n(\alpha, \beta) = \sum_{i=1}^n \Big[&amp;\log \Gamma(\alpha + \beta) - \log \Gamma(\alpha) - \log \Gamma(\beta) \\
&amp;+ (\alpha - 1) \log(X_i) + (\beta - 1) \log(1-X_i)\Big]
\end{aligned}</span></p>
<p>We will implement it below as the negative log-likelihood (<code>beta_nll</code>).</p>
<div id="1e807dc6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.scipy.special <span class="im">as</span> jsps</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from a Beta distribution</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">43</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.beta(<span class="fl">1.5</span>, <span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the negative log-likelihood for Beta(α, β)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beta_nll(theta, x):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Negative log-likelihood for Beta distribution using JAX"""</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> theta</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use log-gamma function for numerical stability</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>jnp.<span class="bu">sum</span>(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        jsps.gammaln(alpha <span class="op">+</span> beta) </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span> jsps.gammaln(alpha) </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span> jsps.gammaln(beta)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> (alpha <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> jnp.log(x)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> (beta <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> jnp.log(<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the gradient function automatically</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>beta_grad <span class="op">=</span> jax.grad(beta_nll)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># We specify bounds: α, β &gt; 0 </span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># - Bounds are *inclusive* so we set a small positive number as lower bound</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># - None here denotes infinity</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> [(<span class="fl">0.0001</span>, <span class="va">None</span>), (<span class="fl">0.0001</span>, <span class="va">None</span>)]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize using L-BFGS-B with bounds to ensure positivity</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> scipy.optimize.minimize(</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    beta_nll, </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">1.0</span>]),  <span class="co"># Initial guess</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>(x,), <span class="co"># Tuple of additional arguments to pass to jac (x is data)</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    jac<span class="op">=</span>beta_grad,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    bounds<span class="op">=</span>bounds</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Raw optimization result:</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Extracted results:</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  MLE: α̂ = </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, β̂ = </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Negative log-likelihood: </span><span class="sc">{</span>result<span class="sc">.</span>fun<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Converged: </span><span class="sc">{</span>result<span class="sc">.</span>success<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Iterations: </span><span class="sc">{</span>result<span class="sc">.</span>nit<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Raw optimization result:

  message: CONVERGENCE: RELATIVE REDUCTION OF F &lt;= FACTR*EPSMCH
  success: True
   status: 0
      fun: -55.24228286743164
        x: [ 1.501e+00  4.985e-01]
      nit: 10
      jac: [-1.030e-04 -9.155e-05]
     nfev: 41
     njev: 41
 hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;

Extracted results:

  MLE: α̂ = 1.501, β̂ = 0.499
  Negative log-likelihood: -55.242
  Converged: True
  Iterations: 10</code></pre>
</div>
</div>
<p>This example showcases several important points:</p>
<ol type="1">
<li><strong>Automatic differentiation</strong>: We use JAX to automatically compute gradients, avoiding error-prone manual derivations</li>
<li><strong>Constrained optimization</strong>: The L-BFGS-B method handles the constraint that both parameters must be positive</li>
<li><strong>Numerical stability</strong>: We work with log-gamma functions rather than raw factorials</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The MLE as a Starting Point
</div>
</div>
<div class="callout-body-container callout-body">
<p>Now that we have <span class="math inline">\hat{\alpha}</span> and <span class="math inline">\hat{\beta}</span>, we can ask the crucial questions:</p>
<ul>
<li>How accurate are these estimates?</li>
<li>What’s their sampling distribution?</li>
<li>Are they optimal?</li>
</ul>
<p>These are exactly the questions this chapter will answer!</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish-English Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here are the key terms we’ll use in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Equivariant</td>
<td>Ekvivariantti</td>
<td>Property of MLE under reparameterization</td>
</tr>
<tr class="even">
<td>Efficient</td>
<td>Tehokas</td>
<td>Optimal variance property</td>
</tr>
<tr class="odd">
<td>Score function</td>
<td>Rinnefunktio</td>
<td>Gradient of log-likelihood</td>
</tr>
<tr class="even">
<td>Fisher information</td>
<td>Fisherin informaatio</td>
<td>Variance of score function</td>
</tr>
<tr class="odd">
<td>Delta method</td>
<td>Delta-menetelmä</td>
<td>Method for transformed parameters</td>
</tr>
<tr class="even">
<td>Sufficient statistic</td>
<td>Tyhjentävä tunnusluku</td>
<td>Contains all information about parameter</td>
</tr>
<tr class="odd">
<td>Latent variable</td>
<td>Piilomuuttuja</td>
<td>Unobserved variable in model</td>
</tr>
<tr class="even">
<td>EM Algorithm</td>
<td>EM-algoritmi</td>
<td>Expectation-Maximization algorithm</td>
</tr>
<tr class="odd">
<td>Asymptotic normality</td>
<td>Asymptoottinen normaalisuus</td>
<td>Large-sample distribution property</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>Konsistenssi/Tarkentuva</td>
<td>Convergence to true value</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="core-properties-of-the-maximum-likelihood-estimator" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="core-properties-of-the-maximum-likelihood-estimator"><span class="header-section-number">6.4</span> Core Properties of the Maximum Likelihood Estimator</h2>
<p>Before exploring its properties, let’s recall the definition of the <strong>maximum likelihood estimator</strong> (MLE) from Chapter 5:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="definition">
<p>Let <span class="math inline">X_1, \ldots, X_n</span> be i.i.d. with PDF (or PMF) <span class="math inline">f(x; \theta)</span>.</p>
<p>The <strong>likelihood function</strong> is: <span class="math display">\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta)</span></p>
<p>The <strong>log-likelihood function</strong> is: <span class="math display">\ell_n(\theta) = \log \mathcal{L}_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)</span></p>
<p>The <strong>maximum likelihood estimator</strong> (MLE), denoted by <span class="math inline">\hat{\theta}_n</span>, is the value of <span class="math inline">\theta</span> that maximizes <span class="math inline">\mathcal{L}_n(\theta)</span> (or equivalently, <span class="math inline">\ell_n(\theta)</span>).</p>
</div>
<section id="overview" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">6.4.1</span> Overview</h3>
<p>In Chapter 5, we saw that the MLE is found by maximizing the likelihood function – a principle that seems intuitively reasonable. But intuition alone doesn’t make for good statistics. We need to understand <em>why</em> the MLE works so well.</p>
<p>It turns out that the MLE has several remarkable properties that make it the “gold standard” of parametric estimation:</p>
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 43%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Mathematical Statement</th>
<th>Intuitive Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Consistency</strong></td>
<td><span class="math inline">\hat{\theta}_n \xrightarrow{P} \theta_*</span></td>
<td>The MLE converges to the true parameter value as <span class="math inline">n \to \infty</span></td>
</tr>
<tr class="even">
<td><strong>Equivariance</strong></td>
<td>If <span class="math inline">\hat{\theta}_n</span> is the MLE of <span class="math inline">\theta</span>, then <span class="math inline">g(\hat{\theta}_n)</span> is the MLE of <span class="math inline">g(\theta)</span></td>
<td>The MLE behaves sensibly under parameter transformations</td>
</tr>
<tr class="odd">
<td><strong>Asymptotic Normality</strong></td>
<td><span class="math inline">\frac{(\hat{\theta}_n - \theta_*)}{\hat{\text{se}}} \rightsquigarrow \mathcal{N}(0, 1)</span></td>
<td>The MLE has an approximately normal distribution for large samples</td>
</tr>
<tr class="even">
<td><strong>Asymptotic Efficiency</strong></td>
<td><span class="math inline">\text{Var}(\hat{\theta}_n)</span> achieves the Cramér-Rao lower bound</td>
<td>The MLE has the smallest possible variance among consistent estimators</td>
</tr>
<tr class="odd">
<td><strong>Approximate Bayes</strong></td>
<td><span class="math inline">\hat{\theta}_n \approx \arg\max_\theta \pi(\theta \mid X^n)</span> with flat prior</td>
<td>The MLE approximates the Bayesian posterior mode</td>
</tr>
</tbody>
</table>
<p>Let’s explore each of these properties in detail.</p>
</section>
<section id="consistency-getting-it-right-eventually" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="consistency-getting-it-right-eventually"><span class="header-section-number">6.4.2</span> Consistency: Getting It Right Eventually</h3>
<p>The most fundamental property we could ask of any estimator is that it gets closer to the truth as we collect more data. This is the property of <strong>consistency</strong>.</p>
<div class="definition">
<p>An estimator <span class="math inline">\hat{\theta}_n</span> is <strong>consistent</strong> for <span class="math inline">\theta_*</span> if: <span class="math display">\hat{\theta}_n \xrightarrow{P} \theta_*</span> That is, for any <span class="math inline">\epsilon &gt; 0</span>: <span class="math display">\lim_{n \to \infty} P(|\hat{\theta}_n - \theta_*| &gt; \epsilon) = 0</span></p>
</div>
<p>In words: as we collect more data, the probability that our estimate is far from the truth goes to zero. The MLE has this property under mild conditions.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Theory Behind Consistency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The consistency of the MLE is deeply connected to the Kullback-Leibler (KL) divergence. For two densities <span class="math inline">f</span> and <span class="math inline">g</span>, the KL divergence is:</p>
<p><span class="math display">D(f, g) = \int f(x) \log \frac{f(x)}{g(x)} dx</span></p>
<p>Key properties:</p>
<ul>
<li><span class="math inline">D(f, g) \geq 0</span> always</li>
<li><span class="math inline">D(f, g) = 0</span> if and only if <span class="math inline">f = g</span> (almost everywhere)</li>
</ul>
<p>The crucial insight is that maximizing the log-likelihood is equivalent to minimizing the KL divergence between the true distribution and our model. As <span class="math inline">n \to \infty</span>, the empirical distribution converges to the true distribution, so the MLE converges to the parameter that makes the model distribution equal to the true distribution.</p>
<p><strong>Identifiability</strong>: For this to work, the model must be <strong>identifiable</strong>: different parameter values must correspond to different distributions. That is, <span class="math inline">\theta \neq \psi</span> implies <span class="math inline">D(f(\cdot; \theta), f(\cdot; \psi)) &gt; 0</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Consistency in Action
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s visualize how the MLE becomes more accurate with increasing sample size <span class="math inline">n</span>. We’ll use the Poisson distribution where the MLE for the rate parameter <span class="math inline">\lambda</span> is simply the sample mean:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div id="d41b014a" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="2">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>true_lambda <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample sizes to consider</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>num_simulations <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Store results</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>mle_estimates <span class="op">=</span> {n: [] <span class="cf">for</span> n <span class="kw">in</span> sample_sizes}</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Run simulations</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> sample_sizes:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_simulations):</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate Poisson data</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.poisson(true_lambda, size<span class="op">=</span>n)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MLE for Poisson is just the sample mean</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        mle <span class="op">=</span> np.mean(data)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        mle_estimates[n].append(mle)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plot</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: Box plots showing distribution at each sample size</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>positions <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>ax1.boxplot([mle_estimates[n] <span class="cf">for</span> n <span class="kw">in</span> sample_sizes], </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            positions<span class="op">=</span>positions,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>[<span class="bu">str</span>(n) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes])</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span>true_lambda, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'True λ = </span><span class="sc">{</span>true_lambda<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Sample size n'</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'MLE estimate'</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Distribution of MLE'</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: Standard deviation vs sample size</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>std_devs <span class="op">=</span> [np.std(mle_estimates[n]) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes]</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>ax2.loglog(sample_sizes, std_devs, <span class="st">'bo-'</span>, label<span class="op">=</span><span class="st">'Empirical SD'</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical standard deviation: sqrt(λ/n)</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>theoretical_sd <span class="op">=</span> [np.sqrt(true_lambda<span class="op">/</span>n) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes]</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>ax2.loglog(sample_sizes, theoretical_sd, <span class="st">'r--'</span>, label<span class="op">=</span><span class="st">'Theoretical SD'</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Sample size n'</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Standard deviation of MLE'</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Convergence Rate'</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-parametric-inference-II_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The plots demonstrate consistency: as <span class="math inline">n</span> increases, the MLE concentrates more tightly around the true value, with standard deviation decreasing at rate <span class="math inline">1/\sqrt{n}</span>.</p>
</div>
</div>
</section>
<section id="equivariance-reparameterization-invariance" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="equivariance-reparameterization-invariance"><span class="header-section-number">6.4.3</span> Equivariance: Reparameterization Invariance</h3>
<p>A subtle but important property of the MLE is <strong>equivariance</strong> (or “functional invariance”). This means that if we reparameterize our model, the MLE transforms in the natural way.</p>
<div class="theorem" name="Equivariance of the MLE">
<p>If <span class="math inline">\hat{\theta}_n</span> is the MLE of <span class="math inline">\theta</span>, then for any function <span class="math inline">g</span>: <span class="math display">\widehat{g(\theta)} = g(\hat{\theta}_n)</span> That is, the MLE of <span class="math inline">\tau = g(\theta)</span> is <span class="math inline">\hat{\tau}_n = g(\hat{\theta}_n)</span>.</p>
</div>
<p><strong>Proof</strong>: Let <span class="math inline">\tau = g(\theta)</span> where <span class="math inline">g</span> has inverse <span class="math inline">h</span>, so <span class="math inline">\theta = h(\tau)</span>. For any <span class="math inline">\tau</span>:</p>
<p><span class="math display">\mathcal{L}_n(\tau) = \prod_{i=1}^n f(X_i; h(\tau)) = \prod_{i=1}^n f(X_i; \theta) = \mathcal{L}_n(\theta)</span></p>
<p>where <span class="math inline">\theta = h(\tau)</span>. Therefore:</p>
<p><span class="math display">\mathcal{L}_n(\tau) = \mathcal{L}_n(\theta) \leq \mathcal{L}_n(\hat{\theta}_n) = \mathcal{L}_n(\hat{\tau}_n)</span></p>
<p>Since this holds for any <span class="math inline">\tau</span>, we have <span class="math inline">\hat{\tau}_n = g(\hat{\theta}_n)</span>. □</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Equivariance in Practice
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\theta, 1)</span> where we’re interested in both:</p>
<ul>
<li>The mean <span class="math inline">\theta</span></li>
<li>The parameter <span class="math inline">\tau = e^\theta</span> (perhaps <span class="math inline">\theta</span> is log-income and <span class="math inline">\tau</span> is income)</li>
</ul>
<p>The MLE for <span class="math inline">\theta</span> is <span class="math inline">\hat{\theta}_n = \bar{X}_n</span>. By equivariance: <span class="math display">\hat{\tau}_n = e^{\hat{\theta}_n} = e^{\bar{X}_n}</span></p>
<p>No need to rederive from scratch! This is particularly convenient when dealing with complex transformations.</p>
</div>
</div>
<p><strong>Why Equivariance Matters</strong>:</p>
<ol type="1">
<li><p><strong>Convenience</strong>: We can work in whatever parameterization is most natural for finding the MLE, then transform to the parameterization of interest.</p></li>
<li><p><strong>Consistency across parameterizations</strong>: Different researchers might parameterize the same model differently (e.g., variance vs.&nbsp;precision in a normal distribution). Equivariance ensures they’ll get equivalent results.</p></li>
<li><p><strong>Not universal</strong>: This property is special to MLEs! Other estimators, like the Bayesian posterior mode (also known as MAP or <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"><em>maximum a posteriori</em></a> estimate), generally lack this property. For instance, if <span class="math inline">\theta</span> has a uniform prior, <span class="math inline">\tau = \theta^2</span> does not have a uniform prior, leading to different posterior modes.</p></li>
</ol>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Common Misconception
</div>
</div>
<div class="callout-body-container callout-body">
<p>Equivariance does NOT mean that: <span class="math display">\mathbb{E}[g(\hat{\theta}_n)] = g(\mathbb{E}[\hat{\theta}_n])</span></p>
<p>In general, <span class="math inline">g(\hat{\theta}_n)</span> is a biased estimator of <span class="math inline">g(\theta)</span> even if <span class="math inline">\hat{\theta}_n</span> is unbiased for <span class="math inline">\theta</span> (unless <span class="math inline">g</span> is linear). Equivariance is about what parameter value maximizes the likelihood, not about expected values.</p>
</div>
</div>
</section>
<section id="asymptotic-normality-optimality" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="asymptotic-normality-optimality"><span class="header-section-number">6.4.4</span> Asymptotic Normality &amp; Optimality</h3>
<p>The consistency and equivariance properties are nice, but they don’t tell us about the <em>distribution</em> of the MLE. How much uncertainty is there in our estimate? How efficient is it compared to other estimators?</p>
<p>The remarkable answer is that the MLE is approximately normally distributed with the smallest possible variance. We’ll explore these twin properties – asymptotic normality and efficiency – in detail in the next section, as they require us to first understand a fundamental concept: the Fisher Information.</p>
</section>
</section>
<section id="fisher-information-and-confidence-intervals" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="fisher-information-and-confidence-intervals"><span class="header-section-number">6.5</span> Fisher Information and Confidence Intervals</h2>
<section id="fisher-information-quantifying-what-data-can-tell-us" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="fisher-information-quantifying-what-data-can-tell-us"><span class="header-section-number">6.5.1</span> Fisher Information: Quantifying What Data Can Tell Us</h3>
<p>To understand the precision of the MLE, we need to introduce one of the most important concepts in statistical theory: the <strong>Fisher Information</strong>. Named after statistician and polymath <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R.A. Fisher</a>, this quantity measures how much “information” about a parameter is contained in the data.</p>
<p>The Fisher Information is formally defined through the <strong>score function</strong> and its variance.</p>
<div class="definition">
<p>The <strong>score function</strong> is the gradient of the log-likelihood: <span class="math display">s(X; \theta) = \frac{\partial \log f(X; \theta)}{\partial \theta}</span></p>
<p>The <strong>Fisher Information</strong> is: <span class="math display">I_n(\theta) = \mathbb{V}_\theta\left(\sum_{i=1}^n s(X_i; \theta)\right) = \sum_{i=1}^n \mathbb{V}_\theta(s(X_i; \theta))</span></p>
</div>
<p>For a single observation (<span class="math inline">n=1</span>), we often write <span class="math inline">I(\theta) = I_1(\theta)</span>.</p>
<p>The computation of the Fisher information can be simplified using the following results:</p>
<div class="theorem">
<p>For an IID sample of size <span class="math inline">n</span>:</p>
<ol type="1">
<li><span class="math inline">I_n(\theta) = n \cdot I(\theta)</span> (information accumulates linearly)</li>
<li><span class="math inline">\mathbb{E}_\theta[s(X; \theta)] = 0</span> (expected score is zero at the true parameter)</li>
<li><span class="math inline">I(\theta) = -\mathbb{E}_\theta\left[\frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}\right]</span> (expected negative curvature)</li>
</ol>
</div>
<p>The last property shows that Fisher Information is literally the expected curvature of the log-likelihood – confirming our intuition about “sharpness”!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Properties of Fisher Information and Score
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Property 1: Linear accumulation for IID samples</strong></p>
<p>For IID samples <span class="math inline">X_1, \ldots, X_n</span>: <span class="math display">I_n(\theta) = \mathbb{V}_\theta\left(\sum_{i=1}^n s(X_i; \theta)\right)</span></p>
<p>Since the <span class="math inline">X_i</span> are independent and <span class="math inline">\mathbb{V}(\sum Y_i) = \sum \mathbb{V}(Y_i)</span> for independent random variables: <span class="math display">I_n(\theta) = \sum_{i=1}^n \mathbb{V}_\theta(s(X_i; \theta)) = n \cdot \mathbb{V}_\theta(s(X; \theta)) = n \cdot I(\theta)</span></p>
<p><strong>Property 2: Expected score is zero</strong></p>
<p>Under regularity conditions that allow interchange of derivative and integral: <span class="math display">\mathbb{E}_\theta[s(X; \theta)] = \mathbb{E}_\theta\left[\frac{\partial \log f(X; \theta)}{\partial \theta}\right] = \int \frac{\partial \log f(x; \theta)}{\partial \theta} f(x; \theta) dx</span></p>
<p>Using <span class="math inline">\frac{\partial \log f}{\partial \theta} = \frac{1}{f} \frac{\partial f}{\partial \theta}</span>: <span class="math display">= \int \frac{\partial f(x; \theta)}{\partial \theta} dx = \frac{\partial}{\partial \theta} \int f(x; \theta) dx = \frac{\partial}{\partial \theta} (1) = 0</span></p>
<p><strong>Property 3: Alternative formula using second derivative</strong></p>
<p>We start with Property 2: <span class="math inline">\mathbb{E}_\theta[s(X; \theta)] = 0</span>, which we can write explicitly as: <span class="math display">\int s(x; \theta) f(x; \theta) dx = \int \frac{\partial \log f(x; \theta)}{\partial \theta} f(x; \theta) dx = 0</span></p>
<p>Since this holds for all <span class="math inline">\theta</span>, we can differentiate both sides with respect to <span class="math inline">\theta</span>: <span class="math display">\frac{\partial}{\partial \theta} \int s(x; \theta) f(x; \theta) dx = 0</span></p>
<p>Under regularity conditions (allowing interchange of derivative and integral): <span class="math display">\int \frac{\partial}{\partial \theta}[s(x; \theta) f(x; \theta)] dx = 0</span></p>
<p>Using the product rule: <span class="math display">\int \left[\frac{\partial s(x; \theta)}{\partial \theta} f(x; \theta) + s(x; \theta) \frac{\partial f(x; \theta)}{\partial \theta}\right] dx = 0</span></p>
<p>Now, note that <span class="math inline">\frac{\partial f(x; \theta)}{\partial \theta} = f(x; \theta) \cdot \frac{\partial \log f(x; \theta)}{\partial \theta} = f(x; \theta) \cdot s(x; \theta)</span></p>
<p>Substituting: <span class="math display">\int \left[\frac{\partial s(x; \theta)}{\partial \theta} f(x; \theta) + s(x; \theta)^2 f(x; \theta)\right] dx = 0</span></p>
<p>This can be rewritten as: <span class="math display">\mathbb{E}_\theta\left[\frac{\partial s(X; \theta)}{\partial \theta}\right] + \mathbb{E}_\theta[s(X; \theta)^2] = 0</span></p>
<p>Since <span class="math inline">s(X; \theta) = \frac{\partial \log f(X; \theta)}{\partial \theta}</span>, we have:</p>
<ul>
<li><span class="math inline">\frac{\partial s(X; \theta)}{\partial \theta} = \frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}</span></li>
<li><span class="math inline">\mathbb{E}_\theta[s(X; \theta)^2] = \mathbb{V}_\theta[s(X; \theta)] = I(\theta)</span> (since <span class="math inline">\mathbb{E}_\theta[s(X; \theta)] = 0</span>)</li>
</ul>
<p>Therefore: <span class="math display">\mathbb{E}_\theta\left[\frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}\right] + I(\theta) = 0</span></p>
<p>Which gives us: <span class="math inline">I(\theta) = -\mathbb{E}_\theta\left[\frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}\right]</span></p>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255601-300-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-300-1" role="tab" aria-controls="tabset-1757255601-300-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255601-300-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-300-2" role="tab" aria-controls="tabset-1757255601-300-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255601-300-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-300-3" role="tab" aria-controls="tabset-1757255601-300-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255601-300-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255601-300-1-tab"><p>Imagine you’re trying to estimate a parameter by maximizing the
log-likelihood function. Picture this function as a hill that you’re
climbing to find the peak (the MLE).</p><p>Now think about two scenarios:</p><ol type="1">
<li><p><strong>Sharp, pointy peak</strong>: The log-likelihood drops off
steeply as you move away from the maximum. Even a small change in the
parameter makes the data much less likely. This means the data is very
“informative” – it strongly prefers one specific parameter
value.</p></li>
<li><p><strong>Flat, broad peak</strong>: The log-likelihood changes
slowly near the maximum. Many different parameter values give similar
likelihoods. The data isn’t very informative about the exact parameter
value.</p></li>
</ol><p>The <strong>Fisher Information</strong> measures the “sharpness” or
curvature of the log-likelihood at its peak:</p><ul>
<li><strong>Sharp peak</strong> → High Fisher Information → Low variance
for <span class="math inline">\(\hat{\theta}\)</span> → Narrow
confidence interval</li>
<li><strong>Flat peak</strong> → Low Fisher Information → High variance
for <span class="math inline">\(\hat{\theta}\)</span> → Wide confidence
interval</li>
</ul><p>This is the key insight: the same curvature that makes optimization
easy (sharp peak = clear maximum) also makes estimation precise (sharp
peak = low uncertainty)!</p></div><div id="tabset-1757255601-300-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255601-300-2-tab"><p>The meaning of
<span class="math inline">\(\mathbb{E}_\theta[s(X; \theta)] = 0\)</span>
is subtle and often misunderstood. It does NOT mean:</p><ul>
<li>The derivative is zero (that’s what happens at the MLE for a
specific dataset)</li>
<li>The log-likelihood is maximized at the true parameter</li>
</ul><p>What it DOES mean:</p><ul>
<li>When data is generated from
<span class="math inline">\(f(x; \theta_*)\)</span>, the score evaluated
at <span class="math inline">\(\theta_*\)</span> (true parameter)
averages to zero across all possible datasets</li>
<li>If you’re at the true parameter value and observe a random data
point, it’s equally likely to suggest increasing or decreasing
<span class="math inline">\(\theta\)</span></li>
<li>This is why the true parameter is “stable” – random samples don’t
systematically pull the estimate away from the truth</li>
</ul><p>Think of it as a balance point: at the true parameter, the data
provides no systematic evidence to move in either direction, even though
any individual sample might suggest moving up or down.</p></div><div id="tabset-1757255601-300-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255601-300-3-tab"><p>Let’s calculate and visualize the Fisher Information for a concrete
example. We’ll examine the <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distribution</a>, where the Fisher Information has a particularly
interesting form, which we will derive later:</p><p><span class="math display">\[I(p) = 1/(p(1-p))\]</span></p><p>This shows how the precision of estimating a probability depends on
the true probability value:</p><div id="16584342" class="cell" data-fig-height="8" data-fig-width="7" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Fisher Information for Bernoulli(p)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For Bernoulli: I(p) = 1/(p(1-p))</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">200</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>fisher_info <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (p_values <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_values))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Top plot: Fisher Information</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>ax1.plot(p_values, fisher_info, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'I(p)'</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Fisher Information for Bernoulli(p)'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Middle plot: Standard error (1/sqrt(nI(p))) for different n</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'orange'</span>, <span class="st">'green'</span>, <span class="st">'blue'</span>]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(n_values, colors):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(n <span class="op">*</span> fisher_info)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    ax2.plot(p_values, se, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Standard Error'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Standard Error of MLE'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="dv">0</span>, <span class="fl">0.2</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom plot: Log-likelihood curves for different p values</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>n_obs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>theta_range <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">200</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Show log-likelihood for different true p values</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p_true, color <span class="kw">in</span> [(<span class="fl">0.5</span>, <span class="st">'red'</span>), (<span class="fl">0.6</span>, <span class="st">'green'</span>), (<span class="fl">0.8</span>, <span class="st">'blue'</span>)]:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    log_lik <span class="op">=</span> successes <span class="op">*</span> np.log(theta_range) <span class="op">+</span> (n_obs <span class="op">-</span> successes) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> theta_range)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    ax3.plot(theta_range, log_lik, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'Data from p = </span><span class="sc">{</span>p_true<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark the MLE</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    mle <span class="op">=</span> successes <span class="op">/</span> n_obs</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    mle_ll <span class="op">=</span> successes <span class="op">*</span> np.log(mle) <span class="op">+</span> (n_obs <span class="op">-</span> successes) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> mle)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    ax3.plot(mle, mle_ll, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'θ'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'Log-likelihood'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>ax3.set_title(<span class="ss">f'Log-likelihood curves (n=</span><span class="sc">{</span>n_obs<span class="sc">}</span><span class="ss">, observed </span><span class="sc">{</span>successes<span class="sc">}</span><span class="ss"> successes)'</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>ax3.legend()</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>ax3.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="06-parametric-inference-II_files/figure-html/cell-4-output-1.png"></p>
</div>
</div><p><strong>Key insights from the plots:</strong></p><ol type="1">
<li>Fisher Information is lowest at
<span class="math inline">\(p=0.5\)</span> (hardest to estimate a fair
coin)</li>
<li>Fisher Information
<span class="math inline">\(\rightarrow \infty\)</span> as
<span class="math inline">\(p \rightarrow 0\)</span> or
<span class="math inline">\(p \rightarrow 1\)</span> (extreme
probabilities are ‘easier’ to pin down)</li>
<li>Standard error decreases with both
<span class="math inline">\(n\)</span> and
<span class="math inline">\(I(p)\)</span></li>
<li>The curvature of the log-likelihood reflects the Fisher
Information</li>
</ol></div></div></div>
</section>
<section id="asymptotic-normality-of-the-mle" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="asymptotic-normality-of-the-mle"><span class="header-section-number">6.5.2</span> Asymptotic Normality of the MLE</h3>
<p>Now we can state the key theorem that connects Fisher Information to the distribution of the MLE:</p>
<div class="theorem" name="Asymptotic Normality of the MLE">
<p>Let <span class="math inline">\text{se} = \sqrt{\mathbb{V}(\hat{\theta}_n)}</span>. Under appropriate regularity conditions we have the following:</p>
<ol type="1">
<li><p><span class="math inline">\text{se} \approx \sqrt{1 / I_n(\theta)}</span> and <span class="math display">\frac{(\hat{\theta}_n - \theta)}{\text{se}} \rightsquigarrow \mathcal{N}(0,1)</span></p></li>
<li><p>Let <span class="math inline">\widehat{\text{se}} = \sqrt{1 / I_n(\hat{\theta}_n)}</span>. Then <span class="math display">\frac{(\hat{\theta}_n - \theta)}{\widehat{\text{se}}} \rightsquigarrow \mathcal{N}(0,1)</span></p></li>
</ol>
</div>
<p>According to this theorem, the distribution of the MLE is approximately <span class="math inline">\mathcal{N}(\theta, \widehat{\text{se}}^2)</span>. This is one of the most important results in statistics: it tells us not just that the MLE converges to the true value (consistency), but also gives us its approximate distribution for finite samples.</p>
<p>The key insight is that the same Fisher Information that measures how “sharp” the likelihood is also determines the precision of our estimate. Sharp likelihood → High Fisher Information → Small standard error → Precise estimate.</p>
</section>
<section id="constructing-confidence-intervals-for-the-mle" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="constructing-confidence-intervals-for-the-mle"><span class="header-section-number">6.5.3</span> Constructing Confidence Intervals for the MLE</h3>
<p>With the asymptotic normality result in hand, we can now construct confidence intervals for our parameter estimates.</p>
<div class="theorem">
<p>The interval <span class="math display">C_n = \left( \hat{\theta}_n - z_{\alpha/2}\widehat{\text{se}}, \hat{\theta}_n + z_{\alpha/2}\widehat{\text{se}} \right)</span> is an asymptotically valid <span class="math inline">(1-\alpha)</span> confidence interval for <span class="math inline">\theta</span>.</p>
</div>
<p>For a 95% confidence interval, <span class="math inline">z_{0.025} \approx 1.96 \approx 2</span>, giving the simple rule:</p>
<p><span class="math display">\text{95\% CI} \approx \hat{\theta}_n \pm 2 \cdot \widehat{\text{se}}</span></p>
<p>This type of confidence interval is used as the basis for statements about expected error in opinion polls and many other applications.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Confidence Interval for a Proportion
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s work through the complete derivation for the Bernoulli case, which gives us confidence intervals for proportions – one of the most common applications in practice.</p>
<p>For <span class="math inline">X_1, \ldots, X_n \sim \text{Bernoulli}(p)</span>, we have <span class="math inline">f(x; p) = p^x (1-p)^{1-x}</span> for <span class="math inline">x \in \{0, 1\}</span>.</p>
<p><strong>Step 1: Find the MLE</strong> The likelihood for <span class="math inline">n</span> observations is: <span class="math display">\mathcal{L}_n(p) = \prod_{i=1}^n p^{X_i} (1-p)^{1-X_i} = p^{S} (1-p)^{n-S}</span> where <span class="math inline">S = \sum_{i=1}^n X_i</span> is the total number of successes.</p>
<p>Taking the log: <span class="math inline">\ell_n(p) = S \log p + (n-S) \log(1-p)</span></p>
<p>Setting <span class="math inline">\frac{d\ell_n}{dp} = \frac{S}{p} - \frac{n-S}{1-p} = 0</span> gives <span class="math inline">\hat{p}_n = \frac{S}{n} = \bar{X}_n</span></p>
<p><strong>Step 2: Compute the score function</strong> For a single observation: <span class="math display">\log f(X; p) = X \log p + (1-X) \log(1-p)</span></p>
<p>The score function is: <span class="math display">s(X; p) = \frac{\partial \log f(X; p)}{\partial p} = \frac{X}{p} - \frac{1-X}{1-p}</span></p>
<p>Let’s verify <span class="math inline">\mathbb{E}_p[s(X; p)] = 0</span>: <span class="math display">\mathbb{E}_p[s(X; p)] = \mathbb{E}_p\left[\frac{X}{p} - \frac{1-X}{1-p}\right] = \frac{p}{p} - \frac{1-p}{1-p} = 1 - 1 = 0 \checkmark</span></p>
<p><strong>Step 3: Compute the second derivative</strong> <span class="math display">\frac{\partial^2 \log f(X; p)}{\partial p^2} = \frac{\partial}{\partial p}\left[\frac{X}{p} - \frac{1-X}{1-p}\right] = -\frac{X}{p^2} - \frac{1-X}{(1-p)^2}</span></p>
<p><strong>Step 4: Find the Fisher Information</strong> Using the formula <span class="math inline">I(p) = -\mathbb{E}_p\left[\frac{\partial^2 \log f(X; p)}{\partial p^2}\right]</span>:</p>
<p><span class="math display">I(p) = -\mathbb{E}_p\left[-\frac{X}{p^2} - \frac{1-X}{(1-p)^2}\right] = \mathbb{E}_p\left[\frac{X}{p^2}\right] + \mathbb{E}_p\left[\frac{1-X}{(1-p)^2}\right]</span></p>
<p>Since <span class="math inline">\mathbb{E}_p[X] = p</span> and <span class="math inline">\mathbb{E}_p[1-X] = 1-p</span>: <span class="math display">I(p) = \frac{p}{p^2} + \frac{1-p}{(1-p)^2} = \frac{1}{p} + \frac{1}{1-p} = \frac{1-p+p}{p(1-p)} = \frac{1}{p(1-p)}</span></p>
<p><strong>Step 5: Derive the standard error</strong> For <span class="math inline">n</span> observations, <span class="math inline">I_n(p) = n \cdot I(p) = \frac{n}{p(1-p)}</span></p>
<p>The standard error of <span class="math inline">\hat{p}_n</span> is: <span class="math display">\widehat{\text{se}} = \frac{1}{\sqrt{I_n(\hat{p}_n)}} = \frac{1}{\sqrt{\frac{n}{\hat{p}_n(1-\hat{p}_n)}}} = \sqrt{\frac{\hat{p}_n(1-\hat{p}_n)}{n}}</span></p>
<p><strong>Step 6: Construct the confidence interval</strong> From the asymptotic normality theorem: <span class="math inline">\frac{\hat{p}_n - p}{\widehat{\text{se}}} \rightsquigarrow \mathcal{N}(0,1)</span></p>
<p>For a <span class="math inline">(1-\alpha)</span> confidence interval: <span class="math display">\mathbb{P}\left(-z_{\alpha/2} \leq \frac{\hat{p}_n - p}{\widehat{\text{se}}} \leq z_{\alpha/2}\right) \approx 1-\alpha</span></p>
<p>Rearranging: <span class="math display">\mathbb{P}\left(\hat{p}_n - z_{\alpha/2} \cdot \widehat{\text{se}} \leq p \leq \hat{p}_n + z_{\alpha/2} \cdot \widehat{\text{se}}\right) \approx 1-\alpha</span></p>
<p>Therefore, the <span class="math inline">(1-\alpha)</span> confidence interval is: <span class="math display">\hat{p}_n \pm z_{\alpha/2} \sqrt{\frac{\hat{p}_n(1-\hat{p}_n)}{n}}</span></p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Political Opinion Polling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s apply the confidence interval for a proportion from the previous worked example to understand how political polls work and what their “margin of error” really means.</p>
<div id="88b23b4f" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Political polling example</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose we poll n=1000 randomly selected likely voters about a referendum</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Sample size</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">520</span>  <span class="co"># Number who support the referendum</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> successes <span class="op">/</span> n  <span class="co"># Sample proportion = 0.52 (52%)</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard error using the formula we derived</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p_hat <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_hat) <span class="op">/</span> n)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% confidence interval (using z_{0.025} = 1.96)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>z_critical <span class="op">=</span> <span class="fl">1.96</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>ci_lower <span class="op">=</span> p_hat <span class="op">-</span> z_critical <span class="op">*</span> se</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>ci_upper <span class="op">=</span> p_hat <span class="op">+</span> z_critical <span class="op">*</span> se</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"POLL RESULTS:"</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> voters"</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Support for referendum: </span><span class="sc">{</span>p_hat<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% (</span><span class="sc">{</span>successes<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard error: </span><span class="sc">{</span>se<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss"> percentage points"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"95% confidence interval: (</span><span class="sc">{</span>ci_lower<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%, </span><span class="sc">{</span>ci_upper<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%)"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Margin of error: ±</span><span class="sc">{</span>z_critical<span class="op">*</span>se<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss"> percentage points"</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpretation</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">INTERPRETATION:"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> ci_lower <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The referendum has statistically significant majority support."</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> ci_upper <span class="op">&lt;</span> <span class="fl">0.5</span>:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The referendum has statistically significant minority support."</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The poll cannot determine if there's majority support (CI includes 50%)."</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare different scenarios to understand margin of error</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"HOW MARGIN OF ERROR VARIES:"</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>scenarios <span class="op">=</span> [</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.50</span>, <span class="dv">500</span>, <span class="st">"50</span><span class="sc">% s</span><span class="st">upport, n=500"</span>),</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.50</span>, <span class="dv">1000</span>, <span class="st">"50</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>), </span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.50</span>, <span class="dv">2000</span>, <span class="st">"50</span><span class="sc">% s</span><span class="st">upport, n=2000"</span>),</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.20</span>, <span class="dv">1000</span>, <span class="st">"20</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>),</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.10</span>, <span class="dv">1000</span>, <span class="st">"10</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>),</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.01</span>, <span class="dv">1000</span>, <span class="st">"1</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Scenario'</span><span class="sc">:&lt;25}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Margin of Error'</span><span class="sc">:&gt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'95% CI'</span><span class="sc">:&gt;20}</span><span class="ss">"</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">65</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p, n, desc <span class="kw">in</span> scenarios:</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    margin <span class="op">=</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">/</span> n) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    ci_low <span class="op">=</span> (p <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">/</span> n)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    ci_high <span class="op">=</span> (p <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">/</span> n)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>desc<span class="sc">:&lt;25}</span><span class="ss"> ±</span><span class="sc">{</span>margin<span class="sc">:&gt;6.1f}</span><span class="ss"> percentage pts  (</span><span class="sc">{</span>ci_low<span class="sc">:&gt;5.1f}</span><span class="ss">%, </span><span class="sc">{</span>ci_high<span class="sc">:&gt;5.1f}</span><span class="ss">%)"</span>)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Key assumptions and limitations</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">KEY ASSUMPTIONS:"</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"1. Random sampling from the population of interest"</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"2. Respondents answer truthfully"</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"3. The sample size is large enough for normal approximation (np ≥ 10 and n(1-p) ≥ 10)"</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"4. No systematic bias in who responds to the poll"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>POLL RESULTS:
Sample size: 1000 voters
Support for referendum: 52.0% (520 out of 1000)
Standard error: 1.58 percentage points
95% confidence interval: (48.9%, 55.1%)
Margin of error: ±3.1 percentage points

INTERPRETATION:
The poll cannot determine if there's majority support (CI includes 50%).

============================================================
HOW MARGIN OF ERROR VARIES:
============================================================
Scenario                       Margin of Error               95% CI
-----------------------------------------------------------------
50% support, n=500        ±   4.4 percentage pts  ( 45.6%,  54.4%)
50% support, n=1000       ±   3.1 percentage pts  ( 46.9%,  53.1%)
50% support, n=2000       ±   2.2 percentage pts  ( 47.8%,  52.2%)
20% support, n=1000       ±   2.5 percentage pts  ( 17.5%,  22.5%)
10% support, n=1000       ±   1.9 percentage pts  (  8.1%,  11.9%)
1% support, n=1000        ±   0.6 percentage pts  (  0.4%,   1.6%)

KEY ASSUMPTIONS:
1. Random sampling from the population of interest
2. Respondents answer truthfully
3. The sample size is large enough for normal approximation (np ≥ 10 and n(1-p) ≥ 10)
4. No systematic bias in who responds to the poll</code></pre>
</div>
</div>
<p><strong>Important insights about polling:</strong></p>
<ol type="1">
<li><p><strong>The margin of error is largest when p = 0.5</strong>: This is why close races are hardest to call. When support is near 50%, we have maximum uncertainty about which side has the majority.</p></li>
<li><p><strong>Sample size matters, but with diminishing returns</strong>: To halve the margin of error, you need to quadruple the sample size (since margin ∝ 1/√n).</p></li>
<li><p><strong>Extreme proportions have smaller margins</strong>: If only 1% support something, we can estimate that quite precisely even with modest sample sizes.</p></li>
<li><p><strong>“Statistical ties”</strong>: When the confidence interval includes 50%, we cannot conclude which side has majority support. This is often called a “statistical tie” in media reports.</p></li>
<li><p><strong>The stated margin usually assumes 95% confidence</strong>: When polls report “±3 percentage points,” they typically mean the 95% confidence interval extends 3 points in each direction.</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Limitations of the Wald Interval
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The confidence interval we derived above, known as the <strong>Wald interval</strong>, has poor coverage when <span class="math inline">p</span> is near 0 or 1, or when <span class="math inline">n</span> is small. In practice, consider using:</p>
<ul>
<li><strong>Agresti-Coull interval</strong>: Add 2 successes and 2 failures before computing: <span class="math inline">\tilde{p} = (S+2)/(n+4)</span>.</li>
<li><strong>Wilson score interval</strong>: More complex but widely recommended as the default choice.</li>
</ul>
<p>The Wald interval remains important for understanding the theory, but these alternatives perform better in practice.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: The Cramér-Rao Lower Bound
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We mentioned that the MLE achieves the smallest possible variance. This is formalized by the <strong>Cramér-Rao Lower Bound</strong>:</p>
<p>For any unbiased estimator <span class="math inline">\tilde{\theta}</span>: <span class="math display">\mathbb{V}(\tilde{\theta}) \geq \frac{1}{I_n(\theta)}</span></p>
<p>Since the MLE asymptotically achieves variance <span class="math inline">1/I_n(\theta)</span>, it is <strong>asymptotically efficient</strong> – no consistent estimator can do better!</p>
<p>This is why we call the MLE “optimal”: it extracts all possible information from the data about the parameter.</p>
</div>
</div>
</div>
</section>
</section>
<section id="additional-topics" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="additional-topics"><span class="header-section-number">6.6</span> Additional Topics</h2>
<section id="the-delta-method-confidence-intervals-for-transformations" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="the-delta-method-confidence-intervals-for-transformations"><span class="header-section-number">6.6.1</span> The Delta Method: Confidence Intervals for Transformations</h3>
<p>Often we’re not interested in the parameter <span class="math inline">\theta</span> itself, but in some transformation <span class="math inline">\tau = g(\theta)</span>. For example:</p>
<ul>
<li>If <span class="math inline">\theta</span> is log-odds, we might want odds <span class="math inline">\tau = e^\theta</span></li>
<li>If <span class="math inline">\theta</span> is standard deviation, we might want variance <span class="math inline">\tau = \theta^2</span></li>
<li>If <span class="math inline">\theta</span> is a rate parameter, we might want mean lifetime <span class="math inline">\tau = 1/\theta</span></li>
</ul>
<p>Due to the <strong>equivariance property</strong> seen earlier, if we know the MLE <span class="math inline">\hat{\theta}</span>, we know also that the MLE of <span class="math inline">\tau</span> is <span class="math inline">\hat{\tau} = g(\hat{\theta})</span>. However, what about the confidence intervals of <span class="math inline">\hat{\tau}</span>?</p>
<p>The <strong>Delta Method</strong> provides a way to find confidence intervals for transformed parameters.</p>
<div class="theorem" name="The Delta Method">
<p>If <span class="math inline">\tau = g(\theta)</span> where <span class="math inline">g</span> is differentiable and <span class="math inline">g'(\theta) \neq 0</span>, then: <span class="math display">\widehat{\text{se}}(\hat{\tau}_n) \approx |g'(\hat{\theta}_n)| \cdot \widehat{\text{se}}(\hat{\theta}_n)</span></p>
<p>Therefore: <span class="math display">\frac{(\hat{\tau}_n - \tau)}{\widehat{\text{se}}(\hat{\tau}_n)} \rightsquigarrow \mathcal{N}(0,1)</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuition Behind the Delta Method
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we zoom in enough, any smooth function looks linear. The Delta Method says that when we transform our estimate through a function <span class="math inline">g</span>, the standard error gets multiplied by <span class="math inline">|g'(\hat{\theta}_n)|</span> – the absolute slope of the function at our estimate.</p>
<p>This approximation becomes exact as <span class="math inline">n \to \infty</span> because the variability of <span class="math inline">\hat{\theta}_n</span> shrinks, effectively “zooming in” on the region where <span class="math inline">g</span> is effectively linear.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Full Delta Method Workflow
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s work through estimating <span class="math inline">\psi = \log \sigma</span> for a Normal distribution with known mean. This example demonstrates every step of the Delta Method process.</p>
<p><strong>Setup</strong>: <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)</span> with <span class="math inline">\mu</span> known.</p>
<p><strong>Step 1: Find MLE for <span class="math inline">\sigma</span></strong></p>
<p>The log-likelihood is: <span class="math display">\ell(\sigma) = -n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2</span></p>
<p>Taking the derivative and setting to zero: <span class="math display">\frac{d\ell}{d\sigma} = -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^n (X_i - \mu)^2 = 0</span></p>
<p>Solving: <span class="math inline">\hat{\sigma}_n = \sqrt{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2}</span></p>
<p><strong>Step 2: Find Fisher Information for <span class="math inline">\sigma</span></strong></p>
<p>The log density for a single observation is: <span class="math display">\log f(X; \sigma) = -\log \sigma - \frac{(X-\mu)^2}{2\sigma^2}</span></p>
<p>First derivative: <span class="math display">\frac{\partial \log f(X; \sigma)}{\partial \sigma} = -\frac{1}{\sigma} + \frac{(X-\mu)^2}{\sigma^3}</span></p>
<p>Second derivative: <span class="math display">\frac{\partial^2 \log f(X; \sigma)}{\partial \sigma^2} = \frac{1}{\sigma^2} - \frac{3(X-\mu)^2}{\sigma^4}</span></p>
<p>Fisher Information: <span class="math display">I(\sigma) = -\mathbb{E}\left[\frac{\partial^2 \log f(X; \sigma)}{\partial \sigma^2}\right] = -\frac{1}{\sigma^2} + \frac{3\sigma^2}{\sigma^4} = \frac{2}{\sigma^2}</span></p>
<p><strong>Step 3: Standard Error for <span class="math inline">\hat{\sigma}</span></strong> <span class="math display">\widehat{\text{se}}(\hat{\sigma}_n) = \frac{1}{\sqrt{nI(\hat{\sigma}_n)}} = \frac{\hat{\sigma}_n}{\sqrt{2n}}</span></p>
<p><strong>Step 4: Apply the Delta Method</strong></p>
<p>For <span class="math inline">\psi = g(\sigma) = \log \sigma</span>, we have <span class="math inline">g'(\sigma) = 1/\sigma</span>. Therefore:</p>
<p><span class="math display">\widehat{\text{se}}(\hat{\psi}_n) = |g'(\hat{\sigma}_n)| \cdot \widehat{\text{se}}(\hat{\sigma}_n) = \frac{1}{\hat{\sigma}_n} \cdot \frac{\hat{\sigma}_n}{\sqrt{2n}} = \frac{1}{\sqrt{2n}}</span></p>
<p><strong>Step 5: Confidence Interval</strong></p>
<p>A 95% confidence interval for <span class="math inline">\psi = \log \sigma</span> is: <span class="math display">\hat{\psi}_n \pm \frac{2}{\sqrt{2n}} = \log \hat{\sigma}_n \pm \frac{2}{\sqrt{2n}}</span></p>
<p>Notice something remarkable: the standard error for <span class="math inline">\log \sigma</span> doesn’t depend on <span class="math inline">\sigma</span> itself! This is one reason why log-transformations are often used for scale parameters.</p>
<p><strong>Verification Via Simulation</strong></p>
<p>Let’s verify the Delta Method through simulation. We’ll generate many samples, compute both <span class="math inline">\hat{\sigma}</span> and <span class="math inline">\log \hat{\sigma}</span> for each, and check if their empirical standard errors match the theoretical predictions:</p>
<div id="1e1b6d90" class="cell" data-fig-height="6" data-fig-width="7" data-execution_count="5">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate the Delta Method with simulations</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameters</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data and compute MLEs</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>n_sims <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>sigma_mles <span class="op">=</span> []</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>log_sigma_mles <span class="op">=</span> []</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_sims):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.random.normal(mu_true, sigma_true, n)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    sigma_hat <span class="op">=</span> np.sqrt(np.mean((data <span class="op">-</span> mu_true)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    sigma_mles.append(sigma_hat)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    log_sigma_mles.append(np.log(sigma_hat))</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>sigma_mles <span class="op">=</span> np.array(sigma_mles)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>log_sigma_mles <span class="op">=</span> np.array(log_sigma_mles)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical values</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>theoretical_se_sigma <span class="op">=</span> sigma_true <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>n)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>theoretical_se_log_sigma <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>n)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plots</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Left: Distribution of sigma MLE</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>ax1.hist(sigma_mles, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(sigma_mles.<span class="bu">min</span>(), sigma_mles.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>ax1.plot(x, stats.norm.pdf(x, sigma_true, theoretical_se_sigma), </span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Theoretical'</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>ax1.axvline(sigma_true, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True value'</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'$\hat{\sigma}$'</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Distribution of $\hat{\sigma}$'</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: Distribution of log(sigma) MLE</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>ax2.hist(log_sigma_mles, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(log_sigma_mles.<span class="bu">min</span>(), log_sigma_mles.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>ax2.plot(x, stats.norm.pdf(x, np.log(sigma_true), theoretical_se_log_sigma), </span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Theoretical'</span>)</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>ax2.axvline(np.log(sigma_true), color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True value'</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'$\hat{\psi} = \log \hat{\sigma}$'</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Distribution of $\log \hat{\sigma}$'</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Standard Error Comparison:"</span>)</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical SE($\hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>np<span class="sc">.</span>std(sigma_mles)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical SE($\hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>theoretical_se_sigma<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical SE($\log \hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>np<span class="sc">.</span>std(log_sigma_mles)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical SE($\log \hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>theoretical_se_log_sigma<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">SE ratio (empirical): </span><span class="sc">{</span>np<span class="sc">.</span>std(log_sigma_mles)<span class="op">/</span>np<span class="sc">.</span>std(sigma_mles)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SE ratio (Delta method): </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>sigma_true<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-parametric-inference-II_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Standard Error Comparison:
Empirical SE($\hat{\sigma}$): 0.1403
Theoretical SE($\hat{\sigma}$): 0.1414
Empirical SE($\log \hat{\sigma}$): 0.0705
Theoretical SE($\log \hat{\sigma}$): 0.0707

SE ratio (empirical): 0.5027
SE ratio (Delta method): 0.5000</code></pre>
</div>
</div>
<p><strong>Key takeaways</strong>: The simulation confirms that the Delta Method works perfectly! The empirical standard error ratio between <span class="math inline">\log \hat{\sigma}</span> and <span class="math inline">\hat{\sigma}</span> matches exactly what the Delta Method predicts: <span class="math inline">1/\sigma</span>. Both distributions are well-approximated by normal distributions, validating the asymptotic normality of MLEs.</p>
</div>
</div>
</section>
<section id="multiparameter-models" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="multiparameter-models"><span class="header-section-number">6.6.2</span> Multiparameter Models</h3>
<p>Real-world problems often involve multiple parameters. The theory we saw in this chapter extends naturally, with matrices replacing scalars.</p>
<p>For <span class="math inline">\theta = (\theta_1, \ldots, \theta_k)</span>, let <span class="math inline">\hat{\theta} = (\hat{\theta}_1, \ldots, \hat{\theta}_k)</span> be the MLE. The key concepts become:</p>
<div class="definition">
<p>Let <span class="math inline">\ell_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)</span> and define: <span class="math display">H_{jk} = \frac{\partial^2 \ell_n}{\partial \theta_j \partial \theta_k}</span></p>
<p>The <strong>Fisher Information Matrix</strong> is: <span class="math display">I_n(\theta) = -
\begin{pmatrix}
  \mathbb{E}_\theta(H_{11}) &amp; \mathbb{E}_\theta(H_{12}) &amp; \cdots &amp; \mathbb{E}_\theta(H_{1k}) \\
  \mathbb{E}_\theta(H_{21}) &amp; \mathbb{E}_\theta(H_{22}) &amp; \cdots &amp; \mathbb{E}_\theta(H_{2k}) \\
    \vdots           &amp;    \vdots           &amp; \ddots &amp; \vdots \\
  \mathbb{E}_\theta(H_{k1}) &amp; \mathbb{E}_\theta(H_{k2}) &amp; \cdots &amp; \mathbb{E}_\theta(H_{kk})
\end{pmatrix}</span></p>
</div>
<div class="theorem">
<p>Under regularity conditions:</p>
<ol type="1">
<li><p>The MLE is approximately multivariate normal: <span class="math display">(\hat{\theta} - \theta) \approx \mathcal{N}(0, J_n)</span> where <span class="math inline">J_n = I_n^{-1}(\theta)</span> is the inverse Fisher Information matrix.</p></li>
<li><p>For parameter <span class="math inline">\theta_j</span>, the standard error is: <span class="math display">\widehat{\text{se}}_j = \sqrt{J_n(j,j)}</span> (the square root of the <span class="math inline">j</span>-th diagonal element of <span class="math inline">J_n</span>)</p></li>
<li><p>The covariance between <span class="math inline">\hat{\theta}_j</span> and <span class="math inline">\hat{\theta}_k</span> is <span class="math inline">J_n(j,k)</span>.</p></li>
</ol>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Multiparameter Normal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)</span> with both parameters unknown:</p>
<p>The Fisher Information matrix is: <span class="math display">I_n(\mu, \sigma) = n \begin{pmatrix}
\frac{1}{\sigma^2} &amp; 0 \\
0 &amp; \frac{2}{\sigma^2}
\end{pmatrix}</span></p>
<p>Therefore: <span class="math display">J_n = \frac{1}{n} \begin{pmatrix}
\sigma^2 &amp; 0 \\
0 &amp; \frac{\sigma^2}{2}
\end{pmatrix}</span></p>
<p>This tells us:</p>
<ul>
<li><span class="math inline">\widehat{\text{se}}(\hat{\mu}) = \sigma/\sqrt{n}</span> (familiar formula!)</li>
<li><span class="math inline">\widehat{\text{se}}(\hat{\sigma}) = \sigma/\sqrt{2n}</span></li>
<li><span class="math inline">\text{Cov}(\hat{\mu}, \hat{\sigma}) = 0</span> (they’re independent!)</li>
</ul>
<p>The zero off-diagonal terms mean that uncertainty about <span class="math inline">\mu</span> doesn’t affect our estimate of <span class="math inline">\sigma</span>, and vice versa. This orthogonality is special to the normal distribution.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Multiparameter Delta Method
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For a function <span class="math inline">\tau = g(\theta_1, \ldots, \theta_k)</span>, the Delta Method generalizes to:</p>
<p><span class="math display">\widehat{\text{se}}(\hat{\tau}) = \sqrt{(\widehat{\nabla} g)^T \widehat{J}_n (\widehat{\nabla} g)}</span></p>
<p>where <span class="math inline">\widehat{\nabla} g</span> is the gradient of <span class="math inline">g</span> evaluated at <span class="math inline">\hat{\theta}</span>.</p>
<p><strong>Example</strong>: For the Normal distribution, if we’re interested in the coefficient of variation <span class="math inline">\tau = \sigma/\mu</span>:</p>
<p><span class="math display">\nabla g = \begin{pmatrix} -\sigma/\mu^2 \\ 1/\mu \end{pmatrix}</span></p>
<p>The standard error involves both parameter uncertainties and their covariance (though the covariance is zero for the normal case).</p>
</div>
</div>
</div>
</section>
<section id="sufficient-statistics" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="sufficient-statistics"><span class="header-section-number">6.6.3</span> Sufficient Statistics</h3>
<p>A <strong>statistic</strong> is a function <span class="math inline">T(X^n)</span> of the data. A <strong>sufficient statistic</strong> is a statistic that contains all the information in the data about the parameter.</p>
<div class="definition">
<p>A statistic <span class="math inline">T(X^n)</span> is <strong>sufficient</strong> for parameter <span class="math inline">\theta</span> if the conditional distribution of the data given <span class="math inline">T</span> doesn’t depend on <span class="math inline">\theta</span>.</p>
</div>
<div class="example">
<p>Let <span class="math inline">X_1, \ldots, X_n \sim \text{Bernoulli}(p)</span>. Then <span class="math inline">\mathcal{L}(p) = p^S(1-p)^{n-S}</span> where <span class="math inline">S = \sum_{i=1}^n X_i</span>.</p>
<p>The likelihood depends on the data only through <span class="math inline">S</span>, so <span class="math inline">S</span> is sufficient. This means that once we know the total number of successes, the individual outcomes provide no additional information about <span class="math inline">p</span>.</p>
</div>
<p>A sufficient statistic is <strong>minimal</strong> if it provides the most compressed summary possible while still retaining all information about the parameter. More precisely: any other sufficient statistic can be “reduced” to the minimal one, but not vice versa.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Connection Between MLE and Sufficiency
</div>
</div>
<div class="callout-body-container callout-body">
<p>When a non-trivial sufficient statistic exists, the MLE depends on the data only through that statistic. Examples:</p>
<ul>
<li><strong>Bernoulli</strong>: MLE <span class="math inline">\hat{p} = S/n</span> depends only on sufficient statistic <span class="math inline">S = \sum X_i</span></li>
<li><strong>Normal</strong>: MLE <span class="math inline">(\hat{\mu}, \hat{\sigma}^2)</span> depends only on sufficient statistics <span class="math inline">(\bar{X}, S^2)</span></li>
<li><strong>Uniform(0,<span class="math inline">\theta</span>)</strong>: MLE is exactly the sufficient statistic <span class="math inline">\max\{X_i\}</span></li>
</ul>
<p>This provides theoretical justification for data reduction: when sufficient statistics exist, we can compress our data without losing any information about the parameter. However, not all models have nice sufficient statistics beyond the trivial one (the entire dataset).</p>
</div>
</div>
</section>
</section>
<section id="connection-to-machine-learning-cross-entropy-as-mle" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="connection-to-machine-learning-cross-entropy-as-mle"><span class="header-section-number">6.7</span> Connection to Machine Learning: Cross-Entropy as MLE</h2>
<p>One of the most profound connections between statistics and modern machine learning is that many “loss functions” used in ML are actually negative log-likelihoods in disguise. This isn’t a coincidence – it reflects the deep statistical foundations of machine learning. Let’s consider the case of the cross-entropy loss used in classification tasks.</p>
<p>Consider a classification problem where we predict probabilities for <span class="math inline">K</span> classes using a machine learning model <span class="math inline">f(X; \theta)</span> parameterized by <span class="math inline">\theta</span>.</p>
<p>The <strong>cross-entropy loss</strong> is:</p>
<p><span class="math display">H(p, q) = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^K p(Y_i = j) \log q_\theta(Y_i = j)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">p(Y_i = j)</span> is the observed distribution (1 if observation <span class="math inline">i</span> is class <span class="math inline">j</span>, 0 otherwise)</li>
<li><span class="math inline">q_\theta(Y_i = j) = f(X_i; \theta)_j</span> is the predicted probability from our model with parameters <span class="math inline">\theta</span></li>
</ul>
<p>Let’s show this is exactly the negative log-likelihood for a categorical distribution.</p>
<p>The likelihood for categorical data is: <span class="math display">\mathcal{L}_n(\theta) = \prod_{i=1}^n \text{Categorical}(Y_i; f(X_i; \theta))</span></p>
<p>The categorical probability for observation <span class="math inline">i</span> is: <span class="math display">\text{Categorical}(Y_i; f(X_i; \theta)) = \prod_{j=1}^K f(X_i; \theta)_j^{p(Y_i = j)}</span></p>
<p>Taking the log: <span class="math display">\ell_n(\theta) = \sum_{i=1}^n \sum_{j=1}^K p(Y_i = j) \log f(X_i; \theta)_j</span></p>
<p>This is exactly <span class="math inline">-n \cdot H(p, q)</span>. Thus: <strong>Minimizing cross-entropy = Maximizing likelihood!</strong></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Other Common ML Loss Functions as MLEs
</div>
</div>
<div class="callout-body-container callout-body">
<p>This pattern appears throughout machine learning:</p>
<table class="table">
<colgroup>
<col style="width: 30%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>ML Loss Function</th>
<th>Statistical Model</th>
<th>MLE Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean Squared Error (MSE)</td>
<td><span class="math inline">Y \sim \mathcal{N}(\mu(x), \sigma^2)</span></td>
<td>Gaussian likelihood</td>
</tr>
<tr class="even">
<td>Mean Absolute Error (MAE)</td>
<td><span class="math inline">Y \sim \text{Laplace}(\mu(x), b)</span></td>
<td>Laplace likelihood</td>
</tr>
<tr class="odd">
<td>Huber Loss</td>
<td>Hybrid of <span class="math inline">L_1</span> and <span class="math inline">L_2</span></td>
<td>Robust to outliers</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Understanding that common ML losses are negative log-likelihoods provides:</p>
<ol type="1">
<li><strong>Principled loss design</strong>: When facing a new problem, you can derive an appropriate loss function by specifying a probabilistic model for your data.</li>
<li><strong>Historical context</strong>: It explains why these particular loss functions became standard – they weren’t arbitrary choices but emerged from statistical principles.</li>
<li><strong>Intuition</strong>: Knowing the probabilistic interpretation helps understand when each loss is appropriate (e.g., MAE for heavy-tailed errors, MSE for Gaussian noise).</li>
</ol>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Deriving a Custom Loss
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose you believe errors in your regression problem approximately follow a <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t distribution</a> (heavy tails for robustness). The negative log-likelihood gives you the loss function:</p>
<p><span class="math display">\text{Loss}(y, \hat{y}) = \frac{\nu + 1}{2} \log\left(1 + \frac{(y - \hat{y})^2}{\nu s^2}\right)</span></p>
<p>where <span class="math inline">\nu</span> is degrees of freedom and <span class="math inline">s</span> is scale. This naturally down-weights outliers!</p>
</div>
</div>
</section>
<section id="mle-for-latent-variable-models-the-em-algorithm" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="mle-for-latent-variable-models-the-em-algorithm"><span class="header-section-number">6.8</span> MLE for Latent Variable Models: The EM Algorithm</h2>
<section id="the-challenge-of-latent-variables" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="the-challenge-of-latent-variables"><span class="header-section-number">6.8.1</span> The Challenge of Latent Variables</h3>
<p>So far, we’ve assumed that maximizing the likelihood is straightforward – we take derivatives, set them to zero, and solve (possibly numerically). But what if the likelihood itself is intractable?</p>
<p>This often happens when our model involves <strong>latent (unobserved) variables</strong> – hidden quantities that would make the problem easy if only we could observe them.</p>
<p><strong>Common examples with latent variables:</strong></p>
<ul>
<li><strong>Mixture models</strong>: Which component generated each observation?</li>
<li><strong>Hidden Markov models</strong>: What’s the hidden state sequence?</li>
<li><strong>Factor analysis</strong>: What are the values of the latent factors?</li>
<li><strong>Missing data</strong>: What would the missing values have been?</li>
</ul>
<p>The canonical example is the <strong>Gaussian Mixture Model (GMM)</strong>:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="math display">f(y; \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(y; \mu_k, \sigma_k^2)</span></p>
<p>where <span class="math inline">\theta = (\pi_1, \ldots, \pi_K, \mu_1, \ldots, \mu_K, \sigma_1, \ldots, \sigma_K)</span>.</p>
<p>The likelihood involves a sum inside the logarithm: <span class="math display">\ell_n(\theta) = \sum_{i=1}^n \log\left[\sum_{k=1}^K \pi_k \mathcal{N}(y_i; \mu_k, \sigma_k^2)\right]</span></p>
<p>This is a nightmare to differentiate! The derivatives involve ratios of sums – messy and hard to work with.</p>
<p><strong>The key insight</strong>: If we knew which component <span class="math inline">Z_i \in \{1, \ldots, K\}</span> generated each observation <span class="math inline">Y_i</span>, the problem would become trivial – we’d just fit separate Gaussians to each group.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Two-Component Gaussian Mixture (<span class="math inline">K=2</span>)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For the special case of <span class="math inline">K=2</span> (two components), the mixture model simplifies to:</p>
<p><span class="math display">f(y; \theta) = \pi \mathcal{N}(y; \mu_1, \sigma_1^2) + (1-\pi) \mathcal{N}(y; \mu_2, \sigma_2^2)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\pi \in [0,1]</span> is the mixing weight (probability of component 1)</li>
<li><span class="math inline">(1-\pi)</span> is the probability of component 2</li>
<li><span class="math inline">\mu_1, \mu_2</span> are the means of the two Gaussian components</li>
<li><span class="math inline">\sigma_1^2, \sigma_2^2</span> are the variances of the two components</li>
<li>The parameter vector is <span class="math inline">\theta = (\pi, \mu_1, \mu_2, \sigma_1, \sigma_2)</span></li>
</ul>
<p><strong>Interpretation</strong>: Each observation <span class="math inline">Y_i</span> is generated by:</p>
<ol type="1">
<li>First, flip a biased coin with probability <span class="math inline">\pi</span> of heads</li>
<li>If heads: draw <span class="math inline">Y_i \sim \mathcal{N}(\mu_1, \sigma_1^2)</span></li>
<li>If tails: draw <span class="math inline">Y_i \sim \mathcal{N}(\mu_2, \sigma_2^2)</span></li>
</ol>
<p>The <strong>latent variables</strong> <span class="math inline">Z_1, \ldots, Z_n \in \{1, 2\}</span> indicate which component generated each observation. If we knew these <span class="math inline">Z_i</span> values, estimation would be trivial – we’d simply:</p>
<ul>
<li>Estimate <span class="math inline">\pi</span> as the proportion of observations from component 1</li>
<li>Estimate <span class="math inline">\mu_1, \sigma_1</span> using only observations where <span class="math inline">Z_i = 1</span></li>
<li>Estimate <span class="math inline">\mu_2, \sigma_2</span> using only observations where <span class="math inline">Z_i = 2</span></li>
</ul>
<p>But since the <span class="math inline">Z_i</span> are unobserved, we need a method to estimate both the component assignments and the parameters.</p>
</div>
</div>
</div>
</section>
<section id="the-expectation-maximization-em-algorithm" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="the-expectation-maximization-em-algorithm"><span class="header-section-number">6.8.2</span> The Expectation-Maximization (EM) Algorithm</h3>
<p>The EM algorithm is a clever iterative procedure that alternates between:</p>
<ol type="1">
<li>Guessing the values of the latent variables (E-step)</li>
<li>Updating parameters as if those guesses were correct (M-step)</li>
</ol>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255601-237-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-237-1" role="tab" aria-controls="tabset-1757255601-237-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255601-237-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-237-2" role="tab" aria-controls="tabset-1757255601-237-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255601-237-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-237-3" role="tab" aria-controls="tabset-1757255601-237-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255601-237-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255601-237-1-tab"><p>Think of the EM algorithm as a “chicken and egg” problem solver for
mixture models.</p><p><strong>The dilemma</strong>:</p><ul>
<li>If we knew which cluster each point belonged to, we could easily
estimate the cluster parameters (just compute means and variances for
each group)</li>
<li>If we knew the cluster parameters, we could easily assign points to
clusters (pick the most likely cluster for each point)</li>
</ul><p><strong>The EM solution</strong>: Start with a guess and
alternate!</p><ol type="1">
<li><strong>E-step (Expectation)</strong>: Given current cluster
parameters, compute “soft assignments” – the probability that each point
belongs to each cluster. These are called “responsibilities.”
<ul>
<li>Point near cluster 1 center: Maybe 90% probability for cluster 1,
10% for cluster 2</li>
<li>Point between clusters: Maybe 50-50 split</li>
</ul></li>
<li><strong>M-step (Maximization)</strong>: Update cluster parameters
using weighted calculations, where weights are the responsibilities.
<ul>
<li>New mean for cluster 1: Weighted average of all points, using their
cluster 1 responsibilities as weights</li>
<li>Points with high responsibility contribute more to the cluster’s
parameters</li>
</ul></li>
<li><strong>Iterate</strong>: Keep alternating until convergence.</li>
</ol><p>It’s like a dance where clusters and assignments gradually find their
proper configuration, each step making the other more accurate.</p></div><div id="tabset-1757255601-237-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255601-237-2-tab"><p>The EM algorithm maximizes the expected complete-data log-likelihood.
Let:</p><ul>
<li><span class="math inline">\(Y\)</span> = observed data</li>
<li><span class="math inline">\(Z\)</span> = latent/missing data</li>
<li><span class="math inline">\((Y, Z)\)</span> = complete data</li>
</ul><p>The algorithm iterates between:</p><p><strong>E-step</strong>: Compute the expected complete-data
log-likelihood:
<span class="math display">\[Q(\theta | \theta^{(t)}) = \mathbb{E}_{Z|Y,\theta^{(t)}}[\log P(Y, Z | \theta)]\]</span></p><p>This expectation is over the distribution of
<span class="math inline">\(Z\)</span> given the observed data
<span class="math inline">\(Y\)</span> and current parameters
<span class="math inline">\(\theta^{(t)}\)</span>.</p><p><strong>M-step</strong>: Maximize with respect to
<span class="math inline">\(\theta\)</span>:
<span class="math display">\[\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})\]</span></p><p><strong>Key property</strong>: The likelihood is guaranteed to
increase (or stay the same) at each iteration:
<span class="math display">\[\mathcal{L}(\theta^{(t+1)}) \geq \mathcal{L}(\theta^{(t)})\]</span></p><p>This monotonic improvement ensures convergence to a local
maximum.</p></div><div id="tabset-1757255601-237-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255601-237-3-tab"><p>Let’s implement and visualize the EM algorithm for a 2D Gaussian
mixture:</p><div id="95f9c9af" class="cell" data-fig-height="10" data-fig-width="7" data-execution_count="6">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Ellipse</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data from a mixture of 3 Gaussians</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameters</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>true_means <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>], [<span class="dv">0</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>true_covs <span class="op">=</span> [np.array([[<span class="fl">0.5</span>, <span class="fl">0.2</span>], [<span class="fl">0.2</span>, <span class="fl">0.5</span>]]),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>             np.array([[<span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.3</span>], [<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.8</span>]]),</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>             np.array([[<span class="fl">0.6</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="fl">0.3</span>]])]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>true_weights <span class="op">=</span> np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>])</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Choose component</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> np.random.choice(n_components, p<span class="op">=</span>true_weights)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    true_labels.append(k)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate point from that component</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    data.append(np.random.multivariate_normal(true_means[k], true_covs[k]))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(data)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> np.array(true_labels)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize EM with random parameters</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> np.random.randn(n_components, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> [np.eye(<span class="dv">2</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_components)]</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.ones(n_components) <span class="op">/</span> n_components</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to plot current state</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_em_state(data, means, covs, weights, responsibilities, iteration, log_likelihood_history):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Left plot: data colored by responsibilities</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> responsibilities <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    ax1.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span>colors, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot component centers and ellipses</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        ax1.plot(means[k, <span class="dv">0</span>], means[k, <span class="dv">1</span>], <span class="st">'k*'</span>, markersize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw ellipse representing covariance</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eigh(covs[k])</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> np.degrees(np.arctan2(eigenvectors[<span class="dv">1</span>, <span class="dv">0</span>], eigenvectors[<span class="dv">0</span>, <span class="dv">0</span>]))</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        width, height <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(eigenvalues)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        ellipse <span class="op">=</span> Ellipse(means[k], width, height, angle<span class="op">=</span>angle,</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>                         facecolor<span class="op">=</span><span class="st">'none'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        ax1.add_patch(ellipse)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="ss">f'Iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">: Soft Assignments'</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Right plot: log-likelihood history</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    ax2.plot(<span class="bu">range</span>(<span class="bu">len</span>(log_likelihood_history)), log_likelihood_history, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    ax2.plot(iteration, log_likelihood_history[iteration], <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Mark current iteration</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Log-likelihood'</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'Convergence'</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">9.5</span>)  <span class="co"># Always show full range</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    ax2.set_xticks(<span class="bu">range</span>(<span class="dv">10</span>))  <span class="co"># Show only integer ticks 0-9</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># EM iterations</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>log_likelihood_history <span class="op">=</span> []</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E-step: compute responsibilities</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    responsibilities <span class="op">=</span> np.zeros((n_samples, n_components))</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        responsibilities[:, k] <span class="op">=</span> weights[k] <span class="op">*</span> multivariate_normal.pdf(</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>            data, means[k], covs[k])</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    responsibilities <span class="op">/=</span> responsibilities.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute log-likelihood</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> np.<span class="bu">sum</span>(np.log(np.<span class="bu">sum</span>([</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        weights[k] <span class="op">*</span> multivariate_normal.pdf(data, means[k], covs[k])</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components)], axis<span class="op">=</span><span class="dv">0</span>)))</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    log_likelihood_history.append(log_likelihood)</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot current state (only show a few iterations)</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>]:</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plot_em_state(data, means, covs, weights, responsibilities, iteration, log_likelihood_history)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    <span class="co"># M-step: update parameters</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>        resp_k <span class="op">=</span> responsibilities[:, k]</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weight</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>        weights[k] <span class="op">=</span> resp_k.<span class="bu">sum</span>() <span class="op">/</span> n_samples</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update mean</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>        means[k] <span class="op">=</span> (resp_k[:, np.newaxis] <span class="op">*</span> data).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update covariance</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>        diff <span class="op">=</span> data <span class="op">-</span> means[k]</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>        covs[k] <span class="op">=</span> (resp_k[:, np.newaxis, np.newaxis] <span class="op">*</span> </span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>                   diff[:, :, np.newaxis] <span class="op">@</span> diff[:, np.newaxis, :]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final parameters:"</span>)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weights: </span><span class="sc">{</span>weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Means:</span><span class="ch">\n</span><span class="sc">{</span>means<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="06-parametric-inference-II_files/figure-html/cell-7-output-1.png"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="06-parametric-inference-II_files/figure-html/cell-7-output-2.png"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="06-parametric-inference-II_files/figure-html/cell-7-output-3.png"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="06-parametric-inference-II_files/figure-html/cell-7-output-4.png"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Final parameters:
Weights: [0.45669616 0.211968   0.33133584]
Means:
[[ 0.19903644  1.95778334]
 [ 3.02378239 -0.94719336]
 [-2.06041818 -1.92642337]]</code></pre>
</div>
</div><p>The visualizations show:</p><ul>
<li><strong>Left plots</strong>: Data points colored by their soft
assignments (RGB = probabilities for 3 clusters)</li>
<li><strong>Black stars</strong>: Cluster centers</li>
<li><strong>Ellipses</strong>: Covariance structure of each
component</li>
<li><strong>Right plot</strong>: Log-likelihood increasing
monotonically</li>
</ul><p>Notice how the algorithm gradually discovers the true cluster
structure!</p></div></div></div>
</section>
<section id="properties-and-practical-considerations" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="properties-and-practical-considerations"><span class="header-section-number">6.8.3</span> Properties and Practical Considerations</h3>
<p><strong>Strengths of EM:</strong></p>
<ol type="1">
<li><strong>Guaranteed improvement</strong>: The likelihood never decreases</li>
<li><strong>No step size tuning</strong>: Unlike gradient descent, no learning rate to choose</li>
<li><strong>Naturally handles constraints</strong>: Probabilities automatically sum to 1</li>
<li><strong>Interpretable intermediate results</strong>: Soft assignments have meaning</li>
</ol>
<p><strong>Limitations and solutions:</strong></p>
<ol type="1">
<li><strong>Local optima</strong>: EM only finds a local maximum
<ul>
<li><strong>Solution</strong>: Run from multiple random initializations</li>
<li><strong>Smart initialization</strong>: Use <a href="https://en.wikipedia.org/wiki/K-means%2B%2B">k-means++</a> or similar</li>
</ul></li>
<li><strong>Slow convergence</strong>: Can take many iterations near the optimum
<ul>
<li><strong>Solution</strong>: Switch to Newton’s method near convergence</li>
<li><strong>Early stopping</strong>: Monitor log-likelihood changes</li>
</ul></li>
<li><strong>Choosing number of components</strong>: How many clusters?
<ul>
<li><strong>Solution</strong>: Use information criteria (AIC, BIC)</li>
<li><strong>Cross-validation</strong>: Evaluate on held-out data</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Initialization Trap
</div>
</div>
<div class="callout-body-container callout-body">
<p>The EM algorithm is extremely sensitive to initialization. Poor starting values can lead to:</p>
<ul>
<li>Convergence to inferior local optima</li>
<li>One component “eating” all the data</li>
<li>Empty components (singularities)</li>
</ul>
<p>Always run EM multiple times with different initializations and choose the result with the highest likelihood!</p>
</div>
</div>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">6.9</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">6.9.1</span> Key Concepts Review</h3>
<p>We’ve explored the remarkable properties that make the Maximum Likelihood Estimator the “gold standard” of parametric estimation:</p>
<p><strong>Core Properties of the MLE</strong>:</p>
<ul>
<li><strong>Consistency</strong>: <span class="math inline">\hat{\theta}_n \xrightarrow{P} \theta_*</span> – the MLE converges to the truth as <span class="math inline">n \to \infty</span>.</li>
<li><strong>Equivariance</strong>: <span class="math inline">\widehat{g(\theta)} = g(\hat{\theta}_n)</span> – reparameterization doesn’t affect the MLE.</li>
<li><strong>Asymptotic Normality</strong>: <span class="math inline">\hat{\theta}_n \approx \mathcal{N}(\theta, 1/I_n(\theta))</span> – enabling confidence intervals.</li>
<li><strong>Asymptotic Efficiency</strong>: Achieves the Cramér-Rao lower bound – optimal variance.</li>
</ul>
<p><strong>Fisher Information</strong>:</p>
<ul>
<li>Measures the “information” about a parameter contained in data.</li>
<li>Equals the expected curvature of the log-likelihood: <span class="math inline">I(\theta) = -\mathbb{E}[\frac{\partial^2 \log f(X;\theta)}{\partial \theta^2}]</span>.</li>
<li>Determines the precision of the MLE: <span class="math inline">\text{Var}(\hat{\theta}_n) \approx 1/(nI(\theta))</span>.</li>
<li>Sharp likelihood peak → High information → Precise estimates.</li>
</ul>
<p><strong>Deep Connections</strong>:</p>
<ul>
<li>Cross-entropy loss in ML = Negative log-likelihood.</li>
<li>Many ML algorithms are secretly performing MLE.</li>
</ul>
<p><strong>Practical Tools</strong>:</p>
<ul>
<li><strong>Confidence Intervals</strong>: <span class="math inline">\hat{\theta}_n \pm z_{\alpha/2} \cdot \widehat{\text{se}}</span> where <span class="math inline">\widehat{\text{se}} = 1/\sqrt{I_n(\hat{\theta}_n)}</span>.</li>
<li><strong>Delta Method</strong>: For transformed parameters <span class="math inline">\tau = g(\theta)</span>: <span class="math inline">\widehat{\text{se}}(\hat{\tau}) \approx |g'(\hat{\theta})| \cdot \widehat{\text{se}}(\hat{\theta})</span>.</li>
<li><strong>EM Algorithm</strong>: Iterative method for MLEs with latent variables – alternates between E-step (soft assignments) and M-step (parameter updates).</li>
</ul>
</section>
<section id="the-big-picture" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="the-big-picture"><span class="header-section-number">6.9.2</span> The Big Picture</h3>
<p>We have moved from simply <em>finding</em> estimators (Chapter 5) to <em>evaluating their quality</em>. This chapter revealed why the MLE is so widely used:</p>
<ol type="1">
<li><strong>It works</strong>: Consistency ensures we get the right answer with enough data.</li>
<li><strong>It’s optimal</strong>: No other estimator has smaller asymptotic variance.</li>
<li><strong>It’s practical</strong>: To a degree, we can quantify uncertainty through Fisher Information.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
<li><strong>It’s flexible</strong>: Equivariance means we can work in convenient parameterizations.</li>
<li><strong>It extends</strong>: The EM algorithm handles complex models with latent structure.</li>
</ol>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">6.9.3</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><strong>Misinterpreting Confidence Intervals</strong>: A 95% CI does NOT mean:
<ul>
<li>“95% probability the parameter is in this interval” (that’s Bayesian!).</li>
<li>“95% of the data falls in this interval” (that’s a prediction interval).</li>
<li>Correct: “95% of such intervals constructed from repeated samples would contain the true parameter”.</li>
</ul></li>
<li><strong>Assuming Asymptotic Results for Small Samples</strong>:
<ul>
<li>Asymptotic normality may not hold for small <span class="math inline">n</span>.</li>
<li>Fisher Information formulas are approximate for finite samples.</li>
<li>Bootstrap or exact methods may be preferable when <span class="math inline">n &lt; 30</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
</ul></li>
<li><strong>Getting Stuck in Local Optima with EM</strong>:
<ul>
<li>EM only guarantees local, not global, maximum.</li>
<li>Always run from multiple starting points.</li>
<li>Monitor log-likelihood to ensure proper convergence.</li>
</ul></li>
<li><strong>Forgetting About Model Assumptions</strong>:
<ul>
<li>MLE theory assumes the model is correctly specified (i.e., the true distribution belongs to our model family).</li>
<li>We briefly mentioned that parameters must be identifiable (Section on Consistency) – different parameter values must give different distributions.</li>
<li>The asymptotic results (normality, efficiency) require “regularity conditions” that we haven’t detailed but include smoothness of the likelihood.</li>
</ul></li>
<li><strong>Over-interpreting Asymptotic Results</strong>:
<ul>
<li>All our results (asymptotic normality, efficiency) are for large samples.</li>
<li>With small samples, the MLE might not be normally distributed and confidence intervals may be inaccurate.</li>
<li>The MLE’s optimality is theoretical – in practice, we might prefer more robust methods when data contains outliers or model assumptions are questionable.</li>
</ul></li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">6.9.4</span> Chapter Connections</h3>
<p><strong>Building on Previous Chapters</strong>:</p>
<ul>
<li><strong>Chapter 3 (Statistical Inference)</strong>: Provided the framework for evaluating estimators. Now we have seen that MLEs have optimal properties within this framework.</li>
<li><strong>Chapter 4 (Bootstrap)</strong>: Offers a computational alternative to the analytical standard errors we derived here. When Fisher Information is hard to compute, bootstrap it!</li>
<li><strong>Chapter 5 (Finding Estimators)</strong>: Showed how to find MLEs. This chapter justifies why we bother – they have provably good properties.</li>
</ul>
<p><strong>Looking Ahead</strong>:</p>
<ul>
<li><strong>Bayesian Inference</strong>: While MLE obtains point estimates and confidence intervals, Bayesian methods provide full probability distributions over parameters. The MLE appears as the mode of the posterior with uniform priors.</li>
<li><strong>Regression Models</strong>: The theory in this chapter extends directly – least squares is MLE for normal errors, logistic regression is MLE for binary outcomes.</li>
<li><strong>Model Selection</strong>: Information criteria (AIC, BIC) build on the likelihood framework we’ve developed.</li>
<li><strong>Robust Statistics</strong>: When MLE assumptions fail, we need methods that sacrifice some efficiency for robustness.</li>
</ul>
<p>The properties we’ve studied – consistency, asymptotic normality, efficiency – will reappear throughout statistics. Whether you’re fitting neural networks or analyzing clinical trials, you’re likely using some form of MLE, and the theory in this chapter explains why it works.</p>
</section>
<section id="self-test-problems" class="level3" data-number="6.9.5">
<h3 data-number="6.9.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">6.9.5</span> Self-Test Problems</h3>
<ol type="1">
<li><p><strong>Fisher Information</strong>: For <span class="math inline">X_1, \ldots, X_n \sim \text{Exponential}(\lambda)</span> with PDF <span class="math inline">f(x; \lambda) = \lambda e^{-\lambda x}</span> for <span class="math inline">x &gt; 0</span>:</p>
<ul>
<li>Find the Fisher Information <span class="math inline">I(\lambda)</span> for a single observation.</li>
<li>What is the approximate standard error of the MLE <span class="math inline">\hat{\lambda}_n</span>?</li>
</ul></li>
<li><p><strong>Confidence Intervals</strong>: You flip a coin 100 times and observe 58 heads. Use Fisher Information to construct an approximate 95% confidence interval for the probability of heads.</p></li>
<li><p><strong>Delta Method</strong>: If <span class="math inline">\hat{p} = 0.4</span> with standard error 0.05, find the standard error of <span class="math inline">\hat{\tau} = \log(p/(1-p))</span> using the Delta Method.</p></li>
<li><p><strong>EM Intuition</strong>: In a two-component Gaussian mixture, the E-step computes “responsibilities” for each data point. What do these represent? Why can’t we just assign each point to its most likely cluster?</p></li>
<li><p><strong>MLE Properties</strong>: True or False (with brief explanation):</p>
<ul>
<li>If <span class="math inline">\hat{\theta}_n</span> is the MLE of <span class="math inline">\theta</span>, then <span class="math inline">\hat{\theta}_n^2</span> is the MLE of <span class="math inline">\theta^2</span>.</li>
<li>The MLE is always unbiased.</li>
<li>Fisher Information measures how “peaked” the likelihood function is.</li>
</ul></li>
</ol>
</section>
<section id="python-and-r-reference" class="level3" data-number="6.9.6">
<h3 data-number="6.9.6" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">6.9.6</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255601-661-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-661-1" role="tab" aria-controls="tabset-1757255601-661-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255601-661-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255601-661-2" role="tab" aria-controls="tabset-1757255601-661-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255601-661-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255601-661-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize <span class="im">as</span> optimize</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.scipy.stats <span class="im">as</span> jstats</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information calculation (symbolic)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fisher_info_symbolic():</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example: Fisher Information for Exponential distribution"""</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define symbols</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    x, lam <span class="op">=</span> sp.symbols(<span class="st">'x lambda'</span>, positive<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define log pdf</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    log_pdf <span class="op">=</span> sp.log(lam) <span class="op">-</span> lam <span class="op">*</span> x</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Score function (first derivative)</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> sp.diff(log_pdf, lam)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second derivative</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    second_deriv <span class="op">=</span> sp.diff(score, lam)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fisher Information (negative expectation)</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For exponential: E[X] = 1/lambda</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    fisher_info <span class="op">=</span> <span class="op">-</span>second_deriv.subs(x, <span class="dv">1</span><span class="op">/</span>lam)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sp.simplify(fisher_info)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information using JAX (automatic differentiation)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fisher_info_jax(log_pdf, theta, n_samples<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Estimate Fisher Information using automatic differentiation"""</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the Hessian (second derivatives)</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    hessian <span class="op">=</span> jax.hessian(log_pdf)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate samples and compute expectation</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is problem-specific - would need appropriate sampler</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example continues with general structure</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>jnp.mean(hessian(theta))</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Confidence intervals with Fisher Information</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mle_confidence_interval(mle, fisher_info_n, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Construct CI using Fisher Information"""</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(fisher_info_n)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    z_crit <span class="op">=</span> stats.norm.ppf(<span class="dv">1</span> <span class="op">-</span> alpha<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    ci_lower <span class="op">=</span> mle <span class="op">-</span> z_crit <span class="op">*</span> se</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    ci_upper <span class="op">=</span> mle <span class="op">+</span> z_crit <span class="op">*</span> se</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ci_lower, ci_upper</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta Method implementation</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> delta_method_se(mle, se_mle, g_derivative):</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co">    Apply Delta Method to find SE of transformed parameter</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co">    - mle: MLE of original parameter</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="co">    - se_mle: Standard error of MLE</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="co">    - g_derivative: Derivative of transformation g evaluated at MLE</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">abs</span>(g_derivative) <span class="op">*</span> se_mle</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Poisson rate to mean waiting time</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>lam_hat <span class="op">=</span> <span class="fl">3.0</span>  <span class="co"># MLE for rate</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>se_lam <span class="op">=</span> <span class="fl">0.3</span>   <span class="co"># Standard error</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co"># For tau = 1/lambda, g'(lambda) = -1/lambda^2</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>g_prime <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">/</span> lam_hat<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>se_tau <span class="op">=</span> delta_method_se(lam_hat, se_lam, g_prime)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="co"># EM Algorithm using sklearn</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_gaussian_mixture(data, n_components<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Fit Gaussian Mixture Model using EM"""</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    gmm <span class="op">=</span> GaussianMixture(</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        n_components<span class="op">=</span>n_components,</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        n_init<span class="op">=</span>n_init,  <span class="co"># Number of initializations</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>        init_params<span class="op">=</span><span class="st">'k-means++'</span>,  <span class="co"># Smart initialization</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>        tol<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>    gmm.fit(data)</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="st">'means'</span>: gmm.means_,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>        <span class="st">'covariances'</span>: gmm.covariances_,</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>        <span class="st">'weights'</span>: gmm.weights_,</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>        <span class="st">'log_likelihood'</span>: gmm.score(data) <span class="op">*</span> <span class="bu">len</span>(data),</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        <span class="st">'converged'</span>: gmm.converged_</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Advanced models with confidence intervals</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_model_with_cis(model, data):</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example using statsmodels for automatic CIs"""</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: Poisson regression</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.fit()</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract MLEs and standard errors</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> results.params</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> results.bse</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>    conf_int <span class="op">=</span> results.conf_int(alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fisher Information matrix</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>    fisher_info <span class="op">=</span> results.cov_params()</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mle'</span>: params,</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>        <span class="st">'se'</span>: se,</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>        <span class="st">'confidence_intervals'</span>: conf_int,</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>        <span class="st">'fisher_info_inverse'</span>: fisher_info</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual EM implementation for mixture of normals</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianMixtureEM:</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_components<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_components <span class="op">=</span> n_components</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> e_step(<span class="va">self</span>, X, means, covs, weights):</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute responsibilities (soft assignments)"""</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>        resp <span class="op">=</span> np.zeros((n_samples, <span class="va">self</span>.n_components))</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_components):</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>            resp[:, k] <span class="op">=</span> weights[k] <span class="op">*</span> multivariate_normal.pdf(</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>                X, means[k], covs[k]</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>        resp <span class="op">/=</span> resp.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> resp</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> m_step(<span class="va">self</span>, X, resp):</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Update parameters given responsibilities"""</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> resp.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> n_samples</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update means</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>        means <span class="op">=</span> []</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        covs <span class="op">=</span> []</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_components):</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>            resp_k <span class="op">=</span> resp[:, k]</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>            mean_k <span class="op">=</span> (resp_k[:, np.newaxis] <span class="op">*</span> X).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>            means.append(mean_k)</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update covariances</span></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>            diff <span class="op">=</span> X <span class="op">-</span> mean_k</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>            cov_k <span class="op">=</span> (resp_k[:, np.newaxis, np.newaxis] <span class="op">*</span> </span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>                    diff[:, :, np.newaxis] <span class="op">@</span> diff[:, np.newaxis, :]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>            cov_k <span class="op">/=</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>            covs.append(cov_k)</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(means), covs, weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255601-661-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255601-661-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)      <span class="co"># For fitdistr</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stats4)    <span class="co"># For mle function</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(numDeriv)  <span class="co"># For numerical derivatives</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mclust)    <span class="co"># For Gaussian mixture models</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(msm)       <span class="co"># For deltamethod function</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information calculation</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>fisher_info_exponential <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda) {</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># For Exponential(lambda): I(lambda) = 1/lambda^2</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> lambda<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information via numerical methods</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fisher_info_numerical <span class="ot">&lt;-</span> <span class="cf">function</span>(loglik_fn, theta, ...) {</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute negative expected Hessian</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  hess <span class="ot">&lt;-</span> <span class="fu">hessian</span>(loglik_fn, theta, ...)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>hess)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># MLE with standard errors using fitdistr</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>fit_with_se <span class="ot">&lt;-</span> <span class="cf">function</span>(data, distribution) {</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(data, distribution)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract estimates and standard errors</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  mle <span class="ot">&lt;-</span> fit<span class="sc">$</span>estimate</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  se <span class="ot">&lt;-</span> fit<span class="sc">$</span>sd</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Construct confidence intervals</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  ci_lower <span class="ot">&lt;-</span> mle <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>  ci_upper <span class="ot">&lt;-</span> mle <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">mle =</span> mle,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">se =</span> se,</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">ci =</span> <span class="fu">cbind</span>(<span class="at">lower =</span> ci_lower, <span class="at">upper =</span> ci_upper),</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">loglik =</span> fit<span class="sc">$</span>loglik</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta Method implementation</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>apply_delta_method <span class="ot">&lt;-</span> <span class="cf">function</span>(mle, var_mle, g_expr, param_name) {</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Using msm::deltamethod</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># g_expr: expression for g(theta) as a string</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># param_name: name of parameter in expression</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>  se_transformed <span class="ot">&lt;-</span> <span class="fu">deltamethod</span>(</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, g_expr)), </span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    mle, </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    var_mle</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(se_transformed)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Delta method for Poisson</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>lambda_hat <span class="ot">&lt;-</span> <span class="fl">3.0</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>se_lambda <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>var_lambda <span class="ot">&lt;-</span> se_lambda<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># For tau = 1/lambda</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>se_tau <span class="ot">&lt;-</span> <span class="fu">deltamethod</span>(<span class="sc">~</span> <span class="dv">1</span><span class="sc">/</span>x1, lambda_hat, var_lambda)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual Delta Method</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>delta_method_manual <span class="ot">&lt;-</span> <span class="cf">function</span>(mle, se_mle, g_deriv) {</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>  <span class="co"># g_deriv: derivative of g evaluated at MLE</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>  se_transformed <span class="ot">&lt;-</span> <span class="fu">abs</span>(g_deriv) <span class="sc">*</span> se_mle</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(se_transformed)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="co"># EM Algorithm using mclust</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>fit_gaussian_mixture <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">G =</span> <span class="dv">2</span>) {</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit Gaussian mixture model</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(data, <span class="at">G =</span> G)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="at">means =</span> fit<span class="sc">$</span>parameters<span class="sc">$</span>mean,</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    <span class="at">covariances =</span> fit<span class="sc">$</span>parameters<span class="sc">$</span>variance<span class="sc">$</span>sigma,</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    <span class="at">weights =</span> fit<span class="sc">$</span>parameters<span class="sc">$</span>pro,</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_likelihood =</span> fit<span class="sc">$</span>loglik,</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    <span class="at">classification =</span> fit<span class="sc">$</span>classification,</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    <span class="at">uncertainty =</span> fit<span class="sc">$</span>uncertainty</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual implementation of MLE with custom likelihood</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>mle_custom <span class="ot">&lt;-</span> <span class="cf">function</span>(data, start, nll_function) {</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Negative log-likelihood function</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>  nll <span class="ot">&lt;-</span> <span class="cf">function</span>(params) {</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nll_function</span>(params, data)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Optimize</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> start,</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> nll,</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>  <span class="co"># Get Hessian for Fisher Information</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fisher Information is the Hessian at MLE</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>  fisher_info <span class="ot">&lt;-</span> fit<span class="sc">$</span>hessian</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Standard errors from inverse Fisher Information</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>  se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">solve</span>(fisher_info)))</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>    <span class="at">mle =</span> fit<span class="sc">$</span>par,</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>    <span class="at">se =</span> se,</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>    <span class="at">fisher_info =</span> fisher_info,</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>    <span class="at">convergence =</span> fit<span class="sc">$</span>convergence,</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_likelihood =</span> <span class="sc">-</span>fit<span class="sc">$</span>value</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Beta distribution MLE</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>beta_nll <span class="ot">&lt;-</span> <span class="cf">function</span>(params, data) {</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> params[<span class="dv">1</span>]</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> params[<span class="dv">2</span>]</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(alpha <span class="sc">&lt;=</span> <span class="dv">0</span> <span class="sc">||</span> beta <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="cn">Inf</span>)</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dbeta</span>(data, alpha, beta, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Beta distribution</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="co"># data &lt;- rbeta(100, 1.5, 0.5)</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="co"># fit &lt;- mle_custom(data, c(1, 1), beta_nll)</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing profile likelihood confidence intervals</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>profile_ci <span class="ot">&lt;-</span> <span class="cf">function</span>(data, param_index, mle_full, nll_function, <span class="at">level =</span> <span class="fl">0.95</span>) {</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Critical value for likelihood ratio test</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>  crit <span class="ot">&lt;-</span> <span class="fu">qchisq</span>(level, <span class="at">df =</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Profile negative log-likelihood</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>  profile_nll <span class="ot">&lt;-</span> <span class="cf">function</span>(param_value, other_params) {</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>    params <span class="ot">&lt;-</span> mle_full</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>    params[param_index] <span class="ot">&lt;-</span> param_value</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optimize over other parameters...</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (simplified here)</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nll_function</span>(params, data)</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find confidence interval boundaries</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (simplified - would need root finding in practice)</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="co"># EM Algorithm manual implementation</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>em_gaussian_mixture <span class="ot">&lt;-</span> <span class="cf">function</span>(X, <span class="at">n_components =</span> <span class="dv">2</span>, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">tol =</span> <span class="fl">1e-4</span>) {</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize parameters</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>  km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(X, n_components)</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>  means <span class="ot">&lt;-</span> km<span class="sc">$</span>centers</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> <span class="fu">table</span>(km<span class="sc">$</span>cluster) <span class="sc">/</span> n</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>  covs <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_components) {</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>    covs[[k]] <span class="ot">&lt;-</span> <span class="fu">cov</span>(X[km<span class="sc">$</span>cluster <span class="sc">==</span> k, ])</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>  log_lik_old <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_iter) {</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E-step: compute responsibilities</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>    resp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n, n_components)</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_components) {</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>      resp[, k] <span class="ot">&lt;-</span> weights[k] <span class="sc">*</span> <span class="fu">dmvnorm</span>(X, means[k, ], covs[[k]])</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>    resp <span class="ot">&lt;-</span> resp <span class="sc">/</span> <span class="fu">rowSums</span>(resp)</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log-likelihood</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>    log_lik <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">rowSums</span>(resp)))</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">abs</span>(log_lik <span class="sc">-</span> log_lik_old) <span class="sc">&lt;</span> tol) <span class="cf">break</span></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>    log_lik_old <span class="ot">&lt;-</span> log_lik</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>    <span class="co"># M-step: update parameters</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_components) {</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>      nk <span class="ot">&lt;-</span> <span class="fu">sum</span>(resp[, k])</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>      weights[k] <span class="ot">&lt;-</span> nk <span class="sc">/</span> n</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>      means[k, ] <span class="ot">&lt;-</span> <span class="fu">colSums</span>(resp[, k] <span class="sc">*</span> X) <span class="sc">/</span> nk</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>      X_centered <span class="ot">&lt;-</span> <span class="fu">sweep</span>(X, <span class="dv">2</span>, means[k, ])</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>      covs[[k]] <span class="ot">&lt;-</span> <span class="fu">t</span>(X_centered) <span class="sc">%*%</span> (resp[, k] <span class="sc">*</span> X_centered) <span class="sc">/</span> nk</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>    <span class="at">means =</span> means,</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>    <span class="at">covariances =</span> covs,</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>    <span class="at">weights =</span> weights,</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>    <span class="at">responsibilities =</span> resp,</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_likelihood =</span> log_lik,</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>    <span class="at">iterations =</span> iter</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="6.9.7">
<h3 data-number="6.9.7" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">6.9.7</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This table maps sections in these lecture notes to the corresponding sections in <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> (“All of Statistics” or AoS).</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding AoS Section(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction: How Good Are Our Estimators?</strong></td>
<td style="text-align: left;">From slides and general context from AoS §9.4 introduction on properties of estimators.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Warm-up: Beta Distribution MLE</strong></td>
<td style="text-align: left;">Example from slides; similar numerical optimization examples in AoS §9.13.4.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Core Properties of the MLE</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Overview</td>
<td style="text-align: left;">AoS §9.4 (list of properties).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Consistency</td>
<td style="text-align: left;">AoS §9.5 (Theorem 9.13 and related discussion).</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Equivariance</td>
<td style="text-align: left;">AoS §9.6 (Theorem and Example).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Asymptotic Normality &amp; Optimality</td>
<td style="text-align: left;">AoS §9.7 introduction, §9.8.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Fisher Information and Confidence Intervals</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Fisher Information</td>
<td style="text-align: left;">AoS §9.7 (Definitions and Theorem 9.17).</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Constructing Confidence Intervals</td>
<td style="text-align: left;">AoS §9.7 (Theorem 9.19 and Examples).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Cramér-Rao Lower Bound</td>
<td style="text-align: left;">Mentioned in AoS §9.8.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Additional Topics</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Delta Method</td>
<td style="text-align: left;">AoS §9.9 (Theorem 9.24 and Examples).</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Multiparameter Models</td>
<td style="text-align: left;">AoS §9.10 (Fisher Information Matrix and related theorems).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Sufficient Statistics</td>
<td style="text-align: left;">AoS §9.13.2 (brief treatment from slides).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Connection to Machine Learning: Cross-Entropy as MLE</strong></td>
<td style="text-align: left;">Expanded from slides.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>MLE for Latent Variable Models: The EM Algorithm</strong></td>
<td style="text-align: left;">AoS §9.13.4 (Appendix on computing MLEs).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Self-Test Problems</strong></td>
<td style="text-align: left;">Inspired by AoS §9.14 exercises.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-materials" class="level3" data-number="6.9.8">
<h3 data-number="6.9.8" class="anchored" data-anchor-id="further-materials"><span class="header-section-number">6.9.8</span> Further Materials</h3>
<ul>
<li><strong>Connection to machine learning</strong>: Murphy (2022), “Probabilistic Machine Learning: An Introduction”, Chapter 4</li>
</ul>
<hr>
<p><em>Remember: The MLE isn’t just a computational procedure – it’s a principled approach to estimation with deep theoretical foundations. The properties we’ve studied explain why maximum likelihood appears everywhere from simple coin flips to complex neural networks. These concepts represent the statistical foundations of modern machine learning!</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note that the term MLE is used interchangeably to denote maximum-likelihood estimation (the technique), maximum-likelihood estimator (the rule to compute the estimate) and maximum-likelihood estimate (the result).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In this example, we don’t need numerical optimization since we can compute the MLE analytically. We could still do numerical optimization and we would get the same results – it just would be slower and not very smart.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Also called <strong>Mixture of Gaussians (MoG)</strong>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>As we will see in future lectures, a Bayesian approach is often more suitable for proper uncertainty quantitication.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This is just a rule of thumb.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/05-parametric-inference-I.html" class="pagination-link" aria-label="Parametric Inference I: Finding Estimators">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/07-hypothesis-testing.html" class="pagination-link" aria-label="Hypothesis Testing and p-values">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Parametric Inference II: Properties of Estimators</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain the key properties of the MLE** (consistency, equivariance, asymptotic normality, and efficiency) and their practical implications.</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define Fisher Information** and use it to quantify the precision of parameter estimates.</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Construct and interpret confidence intervals** for parameters using the MLE and its standard error.</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply the Delta Method** to find confidence intervals for transformed parameters.</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Implement the EM algorithm** for finding MLEs in models with latent variables.</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>This chapter explores the theoretical properties of Maximum Likelihood Estimators and provides practical tools for statistical inference. The material is adapted from Chapter 9 of @wasserman2013all, with additional computational examples and modern perspectives on optimization for latent variable models.</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction: How Good Are Our Estimators?</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>In Chapter 5, we learned how to *find* estimators for parametric models using the Method of Moments (MoM) and Maximum Likelihood Estimation (MLE). We saw that finding the MLE often requires numerical optimization, and we explored practical algorithms for this task.</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>But finding an estimator is only half the story. The next natural question is: **how good are these estimators?**</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>This chapter addresses this fundamental question by exploring the *properties* of estimators, with a special focus on the MLE. We'll discover why the MLE is considered the "gold standard" of parametric estimation -- it turns out to have a remarkable collection of desirable properties that make it optimal in many senses.</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Questions We'll Answer</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Does our estimator converge to the true value** as we get more data? (Consistency)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**How much uncertainty is associated with our estimate?** Can we quantify it? (Confidence Intervals)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Is our estimator the best possible one**, or could we do better? (Efficiency)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**How do these properties extend** to complex models with multiple parameters or latent variables?</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>Understanding these properties is crucial for several reasons:</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Practical inference**: Knowing that $\hat{\theta}_n = 2.3$ is not very useful without understanding the uncertainty. Is the true value likely between 2.2 and 2.4, or between 1 and 4?</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Method selection**: When multiple estimation methods exist, these properties help us choose the best one.</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Foundation for advanced methods**: The concepts in this chapter -- especially Fisher Information and the EM algorithm -- are fundamental to modern statistical methods and machine learning.</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Connection to optimization**: We'll see that the same mathematical quantities that determine statistical uncertainty also appear in optimization algorithms.</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>Let's begin with a concrete example to bridge our previous work on finding MLEs with the new focus on analyzing their properties.</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## Warm-up: A Complete MLE Example with Numerical Optimization</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>Before diving into the theoretical properties, let's work through a complete example that bridges Chapter 5 (finding MLEs) and Chapter 6 (analyzing them). We'll find the MLE for a <span class="co">[</span><span class="ot">Beta distribution</span><span class="co">](https://en.wikipedia.org/wiki/Beta_distribution)</span>, which requires numerical optimization.</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>The **likelihood** for the Beta distribution with parameters $\alpha, \beta &gt; 0$ is:</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>$$f(x; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}$$</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>The **log-likelihood** for $n$ observations is:</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>\ell_n(\alpha, \beta) = \sum_{i=1}^n \Big[&amp;\log \Gamma(\alpha + \beta) - \log \Gamma(\alpha) - \log \Gamma(\beta) <span class="sc">\\</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>&amp;+ (\alpha - 1) \log(X_i) + (\beta - 1) \log(1-X_i)\Big]</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>We will implement it below as the negative log-likelihood (<span class="in">`beta_nll`</span>).</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.scipy.special <span class="im">as</span> jsps</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from a Beta distribution</span></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">43</span>)</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.beta(<span class="fl">1.5</span>, <span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the negative log-likelihood for Beta(α, β)</span></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beta_nll(theta, x):</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Negative log-likelihood for Beta distribution using JAX"""</span></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> theta</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use log-gamma function for numerical stability</span></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>jnp.<span class="bu">sum</span>(</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>        jsps.gammaln(alpha <span class="op">+</span> beta) </span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span> jsps.gammaln(alpha) </span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span> jsps.gammaln(beta)</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> (alpha <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> jnp.log(x)</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> (beta <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> jnp.log(<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the gradient function automatically</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>beta_grad <span class="op">=</span> jax.grad(beta_nll)</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a><span class="co"># We specify bounds: α, β &gt; 0 </span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a><span class="co"># - Bounds are *inclusive* so we set a small positive number as lower bound</span></span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a><span class="co"># - None here denotes infinity</span></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> [(<span class="fl">0.0001</span>, <span class="va">None</span>), (<span class="fl">0.0001</span>, <span class="va">None</span>)]</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize using L-BFGS-B with bounds to ensure positivity</span></span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> scipy.optimize.minimize(</span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>    beta_nll, </span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">1.0</span>]),  <span class="co"># Initial guess</span></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>(x,), <span class="co"># Tuple of additional arguments to pass to jac (x is data)</span></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>    jac<span class="op">=</span>beta_grad,</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'L-BFGS-B'</span>,</span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>    bounds<span class="op">=</span>bounds</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Raw optimization result:</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Extracted results:</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  MLE: α̂ = </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, β̂ = </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Negative log-likelihood: </span><span class="sc">{</span>result<span class="sc">.</span>fun<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Converged: </span><span class="sc">{</span>result<span class="sc">.</span>success<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Iterations: </span><span class="sc">{</span>result<span class="sc">.</span>nit<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>This example showcases several important points:</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Automatic differentiation**: We use JAX to automatically compute gradients, avoiding error-prone manual derivations</span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Constrained optimization**: The L-BFGS-B method handles the constraint that both parameters must be positive</span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Numerical stability**: We work with log-gamma functions rather than raw factorials</span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a><span class="fu">## The MLE as a Starting Point</span></span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a>Now that we have $\hat{\alpha}$ and $\hat{\beta}$, we can ask the crucial questions:</span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How accurate are these estimates?</span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's their sampling distribution?</span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Are they optimal?</span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>These are exactly the questions this chapter will answer!</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish-English Terminology Reference</span></span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here are the key terms we'll use in this chapter:</span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a>| Equivariant | Ekvivariantti | Property of MLE under reparameterization |</span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a>| Efficient | Tehokas | Optimal variance property |</span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a>| Score function | Rinnefunktio | Gradient of log-likelihood |</span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a>| Fisher information | Fisherin informaatio | Variance of score function |</span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a>| Delta method | Delta-menetelmä | Method for transformed parameters |</span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a>| Sufficient statistic | Tyhjentävä tunnusluku | Contains all information about parameter |</span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>| Latent variable | Piilomuuttuja | Unobserved variable in model |</span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a>| EM Algorithm | EM-algoritmi | Expectation-Maximization algorithm |</span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a>| Asymptotic normality | Asymptoottinen normaalisuus | Large-sample distribution property |</span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a>| Consistency | Konsistenssi/Tarkentuva | Convergence to true value |</span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a><span class="fu">## Core Properties of the Maximum Likelihood Estimator</span></span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a>Before exploring its properties, let's recall the definition of the **maximum likelihood estimator** (MLE) from Chapter 5:^<span class="co">[</span><span class="ot">Note that the term MLE is used interchangeably to denote maximum-likelihood estimation (the technique), maximum-likelihood estimator (the rule to compute the estimate) and maximum-likelihood estimate (the result).</span><span class="co">]</span></span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a>Let $X_1, \ldots, X_n$ be i.i.d. with PDF (or PMF) $f(x; \theta)$.</span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a>The **likelihood function** is:</span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta)$$</span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a>The **log-likelihood function** is:</span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a>$$\ell_n(\theta) = \log \mathcal{L}_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)$$</span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a>The **maximum likelihood estimator** (MLE), denoted by $\hat{\theta}_n$, is the value of $\theta$ that maximizes $\mathcal{L}_n(\theta)$ (or equivalently, $\ell_n(\theta)$).</span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overview</span></span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a>In Chapter 5, we saw that the MLE is found by maximizing the likelihood function -- a principle that seems intuitively reasonable. But intuition alone doesn't make for good statistics. We need to understand *why* the MLE works so well.</span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a>It turns out that the MLE has several remarkable properties that make it the "gold standard" of parametric estimation:</span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a>| Property | Mathematical Statement | Intuitive Meaning |</span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a>|----------|----------------------|-------------------|</span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a>| **Consistency** | $\hat{\theta}_n \xrightarrow{P} \theta_*$ | The MLE converges to the true parameter value as $n \to \infty$ |</span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a>| **Equivariance** | If $\hat{\theta}_n$ is the MLE of $\theta$, then $g(\hat{\theta}_n)$ is the MLE of $g(\theta)$ | The MLE behaves sensibly under parameter transformations |</span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a>| **Asymptotic Normality** | $\frac{(\hat{\theta}_n - \theta_*)}{\hat{\text{se}}} \rightsquigarrow \mathcal{N}(0, 1)$ | The MLE has an approximately normal distribution for large samples |</span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a>| **Asymptotic Efficiency** | $\text{Var}(\hat{\theta}_n)$ achieves the Cramér-Rao lower bound | The MLE has the smallest possible variance among consistent estimators |</span>
<span id="cb8-187"><a href="#cb8-187" aria-hidden="true" tabindex="-1"></a>| **Approximate Bayes** | $\hat{\theta}_n \approx \arg\max_\theta \pi(\theta \mid X^n)$ with flat prior | The MLE approximates the Bayesian posterior mode |</span>
<span id="cb8-188"><a href="#cb8-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a>Let's explore each of these properties in detail.</span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a><span class="fu">### Consistency: Getting It Right Eventually</span></span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a>The most fundamental property we could ask of any estimator is that it gets closer to the truth as we collect more data. This is the property of **consistency**.</span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a>An estimator $\hat{\theta}_n$ is **consistent** for $\theta_*$ if:</span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_n \xrightarrow{P} \theta_*$$</span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a>That is, for any $\epsilon &gt; 0$:</span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a>$$\lim_{n \to \infty} P(|\hat{\theta}_n - \theta_*| &gt; \epsilon) = 0$$</span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a>In words: as we collect more data, the probability that our estimate is far from the truth goes to zero. The MLE has this property under mild conditions.</span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Theory Behind Consistency</span></span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a>The consistency of the MLE is deeply connected to the Kullback-Leibler (KL) divergence. For two densities $f$ and $g$, the KL divergence is:</span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a>$$D(f, g) = \int f(x) \log \frac{f(x)}{g(x)} dx$$</span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a>Key properties:</span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$D(f, g) \geq 0$ always</span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$D(f, g) = 0$ if and only if $f = g$ (almost everywhere)</span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a>The crucial insight is that maximizing the log-likelihood is equivalent to minimizing the KL divergence between the true distribution and our model. As $n \to \infty$, the empirical distribution converges to the true distribution, so the MLE converges to the parameter that makes the model distribution equal to the true distribution.</span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a>**Identifiability**: For this to work, the model must be **identifiable**: different parameter values must correspond to different distributions. That is, $\theta \neq \psi$ implies $D(f(\cdot; \theta), f(\cdot; \psi)) &gt; 0$.</span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Consistency in Action</span></span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a>Let's visualize how the MLE becomes more accurate with increasing sample size $n$. We'll use the Poisson distribution where the MLE for the rate parameter $\lambda$ is simply the sample mean:^<span class="co">[</span><span class="ot">In this example, we don't need numerical optimization since we can compute the MLE analytically. We could still do numerical optimization and we would get the same results -- it just would be slower and not very smart.</span><span class="co">]</span></span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter</span></span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a>true_lambda <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample sizes to consider</span></span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>]</span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a>num_simulations <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a><span class="co"># Store results</span></span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a>mle_estimates <span class="op">=</span> {n: [] <span class="cf">for</span> n <span class="kw">in</span> sample_sizes}</span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a><span class="co"># Run simulations</span></span>
<span id="cb8-244"><a href="#cb8-244" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-245"><a href="#cb8-245" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> sample_sizes:</span>
<span id="cb8-246"><a href="#cb8-246" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_simulations):</span>
<span id="cb8-247"><a href="#cb8-247" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate Poisson data</span></span>
<span id="cb8-248"><a href="#cb8-248" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.poisson(true_lambda, size<span class="op">=</span>n)</span>
<span id="cb8-249"><a href="#cb8-249" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MLE for Poisson is just the sample mean</span></span>
<span id="cb8-250"><a href="#cb8-250" aria-hidden="true" tabindex="-1"></a>        mle <span class="op">=</span> np.mean(data)</span>
<span id="cb8-251"><a href="#cb8-251" aria-hidden="true" tabindex="-1"></a>        mle_estimates[n].append(mle)</span>
<span id="cb8-252"><a href="#cb8-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-253"><a href="#cb8-253" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plot</span></span>
<span id="cb8-254"><a href="#cb8-254" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb8-255"><a href="#cb8-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-256"><a href="#cb8-256" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: Box plots showing distribution at each sample size</span></span>
<span id="cb8-257"><a href="#cb8-257" aria-hidden="true" tabindex="-1"></a>positions <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb8-258"><a href="#cb8-258" aria-hidden="true" tabindex="-1"></a>ax1.boxplot([mle_estimates[n] <span class="cf">for</span> n <span class="kw">in</span> sample_sizes], </span>
<span id="cb8-259"><a href="#cb8-259" aria-hidden="true" tabindex="-1"></a>            positions<span class="op">=</span>positions,</span>
<span id="cb8-260"><a href="#cb8-260" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>[<span class="bu">str</span>(n) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes])</span>
<span id="cb8-261"><a href="#cb8-261" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span>true_lambda, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'True λ = </span><span class="sc">{</span>true_lambda<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-262"><a href="#cb8-262" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Sample size n'</span>)</span>
<span id="cb8-263"><a href="#cb8-263" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'MLE estimate'</span>)</span>
<span id="cb8-264"><a href="#cb8-264" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Distribution of MLE'</span>)</span>
<span id="cb8-265"><a href="#cb8-265" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb8-266"><a href="#cb8-266" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-267"><a href="#cb8-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-268"><a href="#cb8-268" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: Standard deviation vs sample size</span></span>
<span id="cb8-269"><a href="#cb8-269" aria-hidden="true" tabindex="-1"></a>std_devs <span class="op">=</span> [np.std(mle_estimates[n]) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes]</span>
<span id="cb8-270"><a href="#cb8-270" aria-hidden="true" tabindex="-1"></a>ax2.loglog(sample_sizes, std_devs, <span class="st">'bo-'</span>, label<span class="op">=</span><span class="st">'Empirical SD'</span>)</span>
<span id="cb8-271"><a href="#cb8-271" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical standard deviation: sqrt(λ/n)</span></span>
<span id="cb8-272"><a href="#cb8-272" aria-hidden="true" tabindex="-1"></a>theoretical_sd <span class="op">=</span> [np.sqrt(true_lambda<span class="op">/</span>n) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes]</span>
<span id="cb8-273"><a href="#cb8-273" aria-hidden="true" tabindex="-1"></a>ax2.loglog(sample_sizes, theoretical_sd, <span class="st">'r--'</span>, label<span class="op">=</span><span class="st">'Theoretical SD'</span>)</span>
<span id="cb8-274"><a href="#cb8-274" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Sample size n'</span>)</span>
<span id="cb8-275"><a href="#cb8-275" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Standard deviation of MLE'</span>)</span>
<span id="cb8-276"><a href="#cb8-276" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Convergence Rate'</span>)</span>
<span id="cb8-277"><a href="#cb8-277" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb8-278"><a href="#cb8-278" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-279"><a href="#cb8-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-280"><a href="#cb8-280" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-281"><a href="#cb8-281" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-282"><a href="#cb8-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-283"><a href="#cb8-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-284"><a href="#cb8-284" aria-hidden="true" tabindex="-1"></a>The plots demonstrate consistency: as $n$ increases, the MLE concentrates more tightly around the true value, with standard deviation decreasing at rate $1/\sqrt{n}$.</span>
<span id="cb8-285"><a href="#cb8-285" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-286"><a href="#cb8-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-287"><a href="#cb8-287" aria-hidden="true" tabindex="-1"></a><span class="fu">### Equivariance: Reparameterization Invariance</span></span>
<span id="cb8-288"><a href="#cb8-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-289"><a href="#cb8-289" aria-hidden="true" tabindex="-1"></a>A subtle but important property of the MLE is **equivariance** (or "functional invariance"). This means that if we reparameterize our model, the MLE transforms in the natural way.</span>
<span id="cb8-290"><a href="#cb8-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-291"><a href="#cb8-291" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Equivariance of the MLE"}</span>
<span id="cb8-292"><a href="#cb8-292" aria-hidden="true" tabindex="-1"></a>If $\hat{\theta}_n$ is the MLE of $\theta$, then for any function $g$:</span>
<span id="cb8-293"><a href="#cb8-293" aria-hidden="true" tabindex="-1"></a>$$\widehat{g(\theta)} = g(\hat{\theta}_n)$$</span>
<span id="cb8-294"><a href="#cb8-294" aria-hidden="true" tabindex="-1"></a>That is, the MLE of $\tau = g(\theta)$ is $\hat{\tau}_n = g(\hat{\theta}_n)$.</span>
<span id="cb8-295"><a href="#cb8-295" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-296"><a href="#cb8-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-297"><a href="#cb8-297" aria-hidden="true" tabindex="-1"></a>**Proof**: Let $\tau = g(\theta)$ where $g$ has inverse $h$, so $\theta = h(\tau)$. For any $\tau$:</span>
<span id="cb8-298"><a href="#cb8-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-299"><a href="#cb8-299" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(\tau) = \prod_{i=1}^n f(X_i; h(\tau)) = \prod_{i=1}^n f(X_i; \theta) = \mathcal{L}_n(\theta)$$</span>
<span id="cb8-300"><a href="#cb8-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-301"><a href="#cb8-301" aria-hidden="true" tabindex="-1"></a>where $\theta = h(\tau)$. Therefore:</span>
<span id="cb8-302"><a href="#cb8-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-303"><a href="#cb8-303" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(\tau) = \mathcal{L}_n(\theta) \leq \mathcal{L}_n(\hat{\theta}_n) = \mathcal{L}_n(\hat{\tau}_n)$$</span>
<span id="cb8-304"><a href="#cb8-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-305"><a href="#cb8-305" aria-hidden="true" tabindex="-1"></a>Since this holds for any $\tau$, we have $\hat{\tau}_n = g(\hat{\theta}_n)$. □</span>
<span id="cb8-306"><a href="#cb8-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-307"><a href="#cb8-307" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-308"><a href="#cb8-308" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Equivariance in Practice</span></span>
<span id="cb8-309"><a href="#cb8-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-310"><a href="#cb8-310" aria-hidden="true" tabindex="-1"></a>Consider $X_1, \ldots, X_n \sim \mathcal{N}(\theta, 1)$ where we're interested in both:</span>
<span id="cb8-311"><a href="#cb8-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-312"><a href="#cb8-312" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The mean $\theta$</span>
<span id="cb8-313"><a href="#cb8-313" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The parameter $\tau = e^\theta$ (perhaps $\theta$ is log-income and $\tau$ is income)</span>
<span id="cb8-314"><a href="#cb8-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-315"><a href="#cb8-315" aria-hidden="true" tabindex="-1"></a>The MLE for $\theta$ is $\hat{\theta}_n = \bar{X}_n$. By equivariance:</span>
<span id="cb8-316"><a href="#cb8-316" aria-hidden="true" tabindex="-1"></a>$$\hat{\tau}_n = e^{\hat{\theta}_n} = e^{\bar{X}_n}$$</span>
<span id="cb8-317"><a href="#cb8-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-318"><a href="#cb8-318" aria-hidden="true" tabindex="-1"></a>No need to rederive from scratch! This is particularly convenient when dealing with complex transformations.</span>
<span id="cb8-319"><a href="#cb8-319" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-320"><a href="#cb8-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-321"><a href="#cb8-321" aria-hidden="true" tabindex="-1"></a>**Why Equivariance Matters**: </span>
<span id="cb8-322"><a href="#cb8-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-323"><a href="#cb8-323" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Convenience**: We can work in whatever parameterization is most natural for finding the MLE, then transform to the parameterization of interest.</span>
<span id="cb8-324"><a href="#cb8-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-325"><a href="#cb8-325" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Consistency across parameterizations**: Different researchers might parameterize the same model differently (e.g., variance vs. precision in a normal distribution). Equivariance ensures they'll get equivalent results.</span>
<span id="cb8-326"><a href="#cb8-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-327"><a href="#cb8-327" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Not universal**: This property is special to MLEs! Other estimators, like the Bayesian posterior mode (also known as MAP or <span class="co">[</span><span class="ot">*maximum a posteriori*</span><span class="co">](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)</span> estimate), generally lack this property. For instance, if $\theta$ has a uniform prior, $\tau = \theta^2$ does not have a uniform prior, leading to different posterior modes.</span>
<span id="cb8-328"><a href="#cb8-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-329"><a href="#cb8-329" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb8-330"><a href="#cb8-330" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Common Misconception</span></span>
<span id="cb8-331"><a href="#cb8-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-332"><a href="#cb8-332" aria-hidden="true" tabindex="-1"></a>Equivariance does NOT mean that:</span>
<span id="cb8-333"><a href="#cb8-333" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">g(\hat{\theta}_n)</span><span class="co">]</span> = g(\mathbb{E}<span class="co">[</span><span class="ot">\hat{\theta}_n</span><span class="co">]</span>)$$</span>
<span id="cb8-334"><a href="#cb8-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-335"><a href="#cb8-335" aria-hidden="true" tabindex="-1"></a>In general, $g(\hat{\theta}_n)$ is a biased estimator of $g(\theta)$ even if $\hat{\theta}_n$ is unbiased for $\theta$ (unless $g$ is linear). Equivariance is about what parameter value maximizes the likelihood, not about expected values.</span>
<span id="cb8-336"><a href="#cb8-336" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-337"><a href="#cb8-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-338"><a href="#cb8-338" aria-hidden="true" tabindex="-1"></a><span class="fu">### Asymptotic Normality &amp; Optimality</span></span>
<span id="cb8-339"><a href="#cb8-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-340"><a href="#cb8-340" aria-hidden="true" tabindex="-1"></a>The consistency and equivariance properties are nice, but they don't tell us about the *distribution* of the MLE. How much uncertainty is there in our estimate? How efficient is it compared to other estimators?</span>
<span id="cb8-341"><a href="#cb8-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-342"><a href="#cb8-342" aria-hidden="true" tabindex="-1"></a>The remarkable answer is that the MLE is approximately normally distributed with the smallest possible variance. We'll explore these twin properties -- asymptotic normality and efficiency -- in detail in the next section, as they require us to first understand a fundamental concept: the Fisher Information.</span>
<span id="cb8-343"><a href="#cb8-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-344"><a href="#cb8-344" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fisher Information and Confidence Intervals</span></span>
<span id="cb8-345"><a href="#cb8-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-346"><a href="#cb8-346" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fisher Information: Quantifying What Data Can Tell Us</span></span>
<span id="cb8-347"><a href="#cb8-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-348"><a href="#cb8-348" aria-hidden="true" tabindex="-1"></a>To understand the precision of the MLE, we need to introduce one of the most important concepts in statistical theory: the **Fisher Information**. Named after statistician and polymath <span class="co">[</span><span class="ot">R.A. Fisher</span><span class="co">](https://en.wikipedia.org/wiki/Ronald_Fisher)</span>, this quantity measures how much "information" about a parameter is contained in the data.</span>
<span id="cb8-349"><a href="#cb8-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-350"><a href="#cb8-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-351"><a href="#cb8-351" aria-hidden="true" tabindex="-1"></a>The Fisher Information is formally defined through the **score function** and its variance.</span>
<span id="cb8-352"><a href="#cb8-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-353"><a href="#cb8-353" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb8-354"><a href="#cb8-354" aria-hidden="true" tabindex="-1"></a>The **score function** is the gradient of the log-likelihood:</span>
<span id="cb8-355"><a href="#cb8-355" aria-hidden="true" tabindex="-1"></a>$$s(X; \theta) = \frac{\partial \log f(X; \theta)}{\partial \theta}$$</span>
<span id="cb8-356"><a href="#cb8-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-357"><a href="#cb8-357" aria-hidden="true" tabindex="-1"></a>The **Fisher Information** is:</span>
<span id="cb8-358"><a href="#cb8-358" aria-hidden="true" tabindex="-1"></a>$$I_n(\theta) = \mathbb{V}_\theta\left(\sum_{i=1}^n s(X_i; \theta)\right) = \sum_{i=1}^n \mathbb{V}_\theta(s(X_i; \theta))$$</span>
<span id="cb8-359"><a href="#cb8-359" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-360"><a href="#cb8-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-361"><a href="#cb8-361" aria-hidden="true" tabindex="-1"></a>For a single observation ($n=1$), we often write $I(\theta) = I_1(\theta)$.</span>
<span id="cb8-362"><a href="#cb8-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-363"><a href="#cb8-363" aria-hidden="true" tabindex="-1"></a>The computation of the Fisher information can be simplified using the following results:</span>
<span id="cb8-364"><a href="#cb8-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-365"><a href="#cb8-365" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb8-366"><a href="#cb8-366" aria-hidden="true" tabindex="-1"></a>For an IID sample of size $n$:</span>
<span id="cb8-367"><a href="#cb8-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-368"><a href="#cb8-368" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$I_n(\theta) = n \cdot I(\theta)$ (information accumulates linearly)</span>
<span id="cb8-369"><a href="#cb8-369" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\mathbb{E}_\theta<span class="co">[</span><span class="ot">s(X; \theta)</span><span class="co">]</span> = 0$ (expected score is zero at the true parameter)</span>
<span id="cb8-370"><a href="#cb8-370" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$I(\theta) = -\mathbb{E}_\theta\left<span class="co">[</span><span class="ot">\frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}\right</span><span class="co">]</span>$ (expected negative curvature)</span>
<span id="cb8-371"><a href="#cb8-371" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-372"><a href="#cb8-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-373"><a href="#cb8-373" aria-hidden="true" tabindex="-1"></a>The last property shows that Fisher Information is literally the expected curvature of the log-likelihood -- confirming our intuition about "sharpness"!</span>
<span id="cb8-374"><a href="#cb8-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-375"><a href="#cb8-375" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-376"><a href="#cb8-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Properties of Fisher Information and Score</span></span>
<span id="cb8-377"><a href="#cb8-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-378"><a href="#cb8-378" aria-hidden="true" tabindex="-1"></a>**Property 1: Linear accumulation for IID samples**</span>
<span id="cb8-379"><a href="#cb8-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-380"><a href="#cb8-380" aria-hidden="true" tabindex="-1"></a>For IID samples $X_1, \ldots, X_n$:</span>
<span id="cb8-381"><a href="#cb8-381" aria-hidden="true" tabindex="-1"></a>$$I_n(\theta) = \mathbb{V}_\theta\left(\sum_{i=1}^n s(X_i; \theta)\right)$$</span>
<span id="cb8-382"><a href="#cb8-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-383"><a href="#cb8-383" aria-hidden="true" tabindex="-1"></a>Since the $X_i$ are independent and $\mathbb{V}(\sum Y_i) = \sum \mathbb{V}(Y_i)$ for independent random variables:</span>
<span id="cb8-384"><a href="#cb8-384" aria-hidden="true" tabindex="-1"></a>$$I_n(\theta) = \sum_{i=1}^n \mathbb{V}_\theta(s(X_i; \theta)) = n \cdot \mathbb{V}_\theta(s(X; \theta)) = n \cdot I(\theta)$$</span>
<span id="cb8-385"><a href="#cb8-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-386"><a href="#cb8-386" aria-hidden="true" tabindex="-1"></a>**Property 2: Expected score is zero**</span>
<span id="cb8-387"><a href="#cb8-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-388"><a href="#cb8-388" aria-hidden="true" tabindex="-1"></a>Under regularity conditions that allow interchange of derivative and integral:</span>
<span id="cb8-389"><a href="#cb8-389" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}_\theta[s(X; \theta)] = \mathbb{E}_\theta\left<span class="co">[</span><span class="ot">\frac{\partial \log f(X; \theta)}{\partial \theta}\right</span><span class="co">]</span> = \int \frac{\partial \log f(x; \theta)}{\partial \theta} f(x; \theta) dx$$</span>
<span id="cb8-390"><a href="#cb8-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-391"><a href="#cb8-391" aria-hidden="true" tabindex="-1"></a>Using $\frac{\partial \log f}{\partial \theta} = \frac{1}{f} \frac{\partial f}{\partial \theta}$:</span>
<span id="cb8-392"><a href="#cb8-392" aria-hidden="true" tabindex="-1"></a>$$= \int \frac{\partial f(x; \theta)}{\partial \theta} dx = \frac{\partial}{\partial \theta} \int f(x; \theta) dx = \frac{\partial}{\partial \theta} (1) = 0$$</span>
<span id="cb8-393"><a href="#cb8-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-394"><a href="#cb8-394" aria-hidden="true" tabindex="-1"></a>**Property 3: Alternative formula using second derivative**</span>
<span id="cb8-395"><a href="#cb8-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-396"><a href="#cb8-396" aria-hidden="true" tabindex="-1"></a>We start with Property 2: $\mathbb{E}_\theta<span class="co">[</span><span class="ot">s(X; \theta)</span><span class="co">]</span> = 0$, which we can write explicitly as:</span>
<span id="cb8-397"><a href="#cb8-397" aria-hidden="true" tabindex="-1"></a>$$\int s(x; \theta) f(x; \theta) dx = \int \frac{\partial \log f(x; \theta)}{\partial \theta} f(x; \theta) dx = 0$$</span>
<span id="cb8-398"><a href="#cb8-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-399"><a href="#cb8-399" aria-hidden="true" tabindex="-1"></a>Since this holds for all $\theta$, we can differentiate both sides with respect to $\theta$:</span>
<span id="cb8-400"><a href="#cb8-400" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial}{\partial \theta} \int s(x; \theta) f(x; \theta) dx = 0$$</span>
<span id="cb8-401"><a href="#cb8-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-402"><a href="#cb8-402" aria-hidden="true" tabindex="-1"></a>Under regularity conditions (allowing interchange of derivative and integral):</span>
<span id="cb8-403"><a href="#cb8-403" aria-hidden="true" tabindex="-1"></a>$$\int \frac{\partial}{\partial \theta}<span class="co">[</span><span class="ot">s(x; \theta) f(x; \theta)</span><span class="co">]</span> dx = 0$$</span>
<span id="cb8-404"><a href="#cb8-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-405"><a href="#cb8-405" aria-hidden="true" tabindex="-1"></a>Using the product rule:</span>
<span id="cb8-406"><a href="#cb8-406" aria-hidden="true" tabindex="-1"></a>$$\int \left<span class="co">[</span><span class="ot">\frac{\partial s(x; \theta)}{\partial \theta} f(x; \theta) + s(x; \theta) \frac{\partial f(x; \theta)}{\partial \theta}\right</span><span class="co">]</span> dx = 0$$</span>
<span id="cb8-407"><a href="#cb8-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-408"><a href="#cb8-408" aria-hidden="true" tabindex="-1"></a>Now, note that $\frac{\partial f(x; \theta)}{\partial \theta} = f(x; \theta) \cdot \frac{\partial \log f(x; \theta)}{\partial \theta} = f(x; \theta) \cdot s(x; \theta)$</span>
<span id="cb8-409"><a href="#cb8-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-410"><a href="#cb8-410" aria-hidden="true" tabindex="-1"></a>Substituting:</span>
<span id="cb8-411"><a href="#cb8-411" aria-hidden="true" tabindex="-1"></a>$$\int \left<span class="co">[</span><span class="ot">\frac{\partial s(x; \theta)}{\partial \theta} f(x; \theta) + s(x; \theta)^2 f(x; \theta)\right</span><span class="co">]</span> dx = 0$$</span>
<span id="cb8-412"><a href="#cb8-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-413"><a href="#cb8-413" aria-hidden="true" tabindex="-1"></a>This can be rewritten as:</span>
<span id="cb8-414"><a href="#cb8-414" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}_\theta\left[\frac{\partial s(X; \theta)}{\partial \theta}\right] + \mathbb{E}_\theta<span class="co">[</span><span class="ot">s(X; \theta)^2</span><span class="co">]</span> = 0$$</span>
<span id="cb8-415"><a href="#cb8-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-416"><a href="#cb8-416" aria-hidden="true" tabindex="-1"></a>Since $s(X; \theta) = \frac{\partial \log f(X; \theta)}{\partial \theta}$, we have:</span>
<span id="cb8-417"><a href="#cb8-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-418"><a href="#cb8-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\frac{\partial s(X; \theta)}{\partial \theta} = \frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}$</span>
<span id="cb8-419"><a href="#cb8-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}_\theta[s(X; \theta)^2] = \mathbb{V}_\theta[s(X; \theta)] = I(\theta)$ (since $\mathbb{E}_\theta<span class="co">[</span><span class="ot">s(X; \theta)</span><span class="co">]</span> = 0$)</span>
<span id="cb8-420"><a href="#cb8-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-421"><a href="#cb8-421" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb8-422"><a href="#cb8-422" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}_\theta\left<span class="co">[</span><span class="ot">\frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}\right</span><span class="co">]</span> + I(\theta) = 0$$</span>
<span id="cb8-423"><a href="#cb8-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-424"><a href="#cb8-424" aria-hidden="true" tabindex="-1"></a>Which gives us: $I(\theta) = -\mathbb{E}_\theta\left<span class="co">[</span><span class="ot">\frac{\partial^2 \log f(X; \theta)}{\partial \theta^2}\right</span><span class="co">]</span>$</span>
<span id="cb8-425"><a href="#cb8-425" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-426"><a href="#cb8-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-427"><a href="#cb8-427" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb8-428"><a href="#cb8-428" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb8-429"><a href="#cb8-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-430"><a href="#cb8-430" aria-hidden="true" tabindex="-1"></a>Imagine you're trying to estimate a parameter by maximizing the log-likelihood function. Picture this function as a hill that you're climbing to find the peak (the MLE).</span>
<span id="cb8-431"><a href="#cb8-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-432"><a href="#cb8-432" aria-hidden="true" tabindex="-1"></a>Now think about two scenarios:</span>
<span id="cb8-433"><a href="#cb8-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-434"><a href="#cb8-434" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Sharp, pointy peak**: The log-likelihood drops off steeply as you move away from the maximum. Even a small change in the parameter makes the data much less likely. This means the data is very "informative" -- it strongly prefers one specific parameter value.</span>
<span id="cb8-435"><a href="#cb8-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-436"><a href="#cb8-436" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Flat, broad peak**: The log-likelihood changes slowly near the maximum. Many different parameter values give similar likelihoods. The data isn't very informative about the exact parameter value.</span>
<span id="cb8-437"><a href="#cb8-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-438"><a href="#cb8-438" aria-hidden="true" tabindex="-1"></a>The **Fisher Information** measures the "sharpness" or curvature of the log-likelihood at its peak:</span>
<span id="cb8-439"><a href="#cb8-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-440"><a href="#cb8-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sharp peak** → High Fisher Information → Low variance for $\hat{\theta}$ → Narrow confidence interval</span>
<span id="cb8-441"><a href="#cb8-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Flat peak** → Low Fisher Information → High variance for $\hat{\theta}$ → Wide confidence interval</span>
<span id="cb8-442"><a href="#cb8-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-443"><a href="#cb8-443" aria-hidden="true" tabindex="-1"></a>This is the key insight: the same curvature that makes optimization easy (sharp peak = clear maximum) also makes estimation precise (sharp peak = low uncertainty)!</span>
<span id="cb8-444"><a href="#cb8-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-445"><a href="#cb8-445" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb8-446"><a href="#cb8-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-447"><a href="#cb8-447" aria-hidden="true" tabindex="-1"></a>The meaning of $\mathbb{E}_\theta<span class="co">[</span><span class="ot">s(X; \theta)</span><span class="co">]</span> = 0$ is subtle and often misunderstood. It does NOT mean:</span>
<span id="cb8-448"><a href="#cb8-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-449"><a href="#cb8-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The derivative is zero (that's what happens at the MLE for a specific dataset)</span>
<span id="cb8-450"><a href="#cb8-450" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The log-likelihood is maximized at the true parameter</span>
<span id="cb8-451"><a href="#cb8-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-452"><a href="#cb8-452" aria-hidden="true" tabindex="-1"></a>What it DOES mean:</span>
<span id="cb8-453"><a href="#cb8-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-454"><a href="#cb8-454" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When data is generated from $f(x; \theta_*)$, the score evaluated at $\theta_*$ (true parameter) averages to zero across all possible datasets</span>
<span id="cb8-455"><a href="#cb8-455" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If you're at the true parameter value and observe a random data point, it's equally likely to suggest increasing or decreasing $\theta$</span>
<span id="cb8-456"><a href="#cb8-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is why the true parameter is "stable" -- random samples don't systematically pull the estimate away from the truth</span>
<span id="cb8-457"><a href="#cb8-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-458"><a href="#cb8-458" aria-hidden="true" tabindex="-1"></a>Think of it as a balance point: at the true parameter, the data provides no systematic evidence to move in either direction, even though any individual sample might suggest moving up or down.</span>
<span id="cb8-459"><a href="#cb8-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-460"><a href="#cb8-460" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb8-461"><a href="#cb8-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-462"><a href="#cb8-462" aria-hidden="true" tabindex="-1"></a>Let's calculate and visualize the Fisher Information for a concrete example. We'll examine the <span class="co">[</span><span class="ot">Bernoulli distribution</span><span class="co">](https://en.wikipedia.org/wiki/Bernoulli_distribution)</span>, where the Fisher Information has a particularly interesting form, which we will derive later: </span>
<span id="cb8-463"><a href="#cb8-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-464"><a href="#cb8-464" aria-hidden="true" tabindex="-1"></a>$$I(p) = 1/(p(1-p))$$</span>
<span id="cb8-465"><a href="#cb8-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-466"><a href="#cb8-466" aria-hidden="true" tabindex="-1"></a>This shows how the precision of estimating a probability depends on the true probability value:</span>
<span id="cb8-467"><a href="#cb8-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-470"><a href="#cb8-470" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-471"><a href="#cb8-471" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb8-472"><a href="#cb8-472" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb8-473"><a href="#cb8-473" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-474"><a href="#cb8-474" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-475"><a href="#cb8-475" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb8-476"><a href="#cb8-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-477"><a href="#cb8-477" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Fisher Information for Bernoulli(p)</span></span>
<span id="cb8-478"><a href="#cb8-478" aria-hidden="true" tabindex="-1"></a><span class="co"># For Bernoulli: I(p) = 1/(p(1-p))</span></span>
<span id="cb8-479"><a href="#cb8-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-480"><a href="#cb8-480" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">200</span>)</span>
<span id="cb8-481"><a href="#cb8-481" aria-hidden="true" tabindex="-1"></a>fisher_info <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (p_values <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_values))</span>
<span id="cb8-482"><a href="#cb8-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-483"><a href="#cb8-483" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb8-484"><a href="#cb8-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-485"><a href="#cb8-485" aria-hidden="true" tabindex="-1"></a><span class="co"># Top plot: Fisher Information</span></span>
<span id="cb8-486"><a href="#cb8-486" aria-hidden="true" tabindex="-1"></a>ax1.plot(p_values, fisher_info, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-487"><a href="#cb8-487" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb8-488"><a href="#cb8-488" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'I(p)'</span>)</span>
<span id="cb8-489"><a href="#cb8-489" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Fisher Information for Bernoulli(p)'</span>)</span>
<span id="cb8-490"><a href="#cb8-490" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-491"><a href="#cb8-491" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb8-492"><a href="#cb8-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-493"><a href="#cb8-493" aria-hidden="true" tabindex="-1"></a><span class="co"># Middle plot: Standard error (1/sqrt(nI(p))) for different n</span></span>
<span id="cb8-494"><a href="#cb8-494" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>]</span>
<span id="cb8-495"><a href="#cb8-495" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'orange'</span>, <span class="st">'green'</span>, <span class="st">'blue'</span>]</span>
<span id="cb8-496"><a href="#cb8-496" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(n_values, colors):</span>
<span id="cb8-497"><a href="#cb8-497" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(n <span class="op">*</span> fisher_info)</span>
<span id="cb8-498"><a href="#cb8-498" aria-hidden="true" tabindex="-1"></a>    ax2.plot(p_values, se, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-499"><a href="#cb8-499" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb8-500"><a href="#cb8-500" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Standard Error'</span>)</span>
<span id="cb8-501"><a href="#cb8-501" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Standard Error of MLE'</span>)</span>
<span id="cb8-502"><a href="#cb8-502" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb8-503"><a href="#cb8-503" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-504"><a href="#cb8-504" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="dv">0</span>, <span class="fl">0.2</span>)</span>
<span id="cb8-505"><a href="#cb8-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-506"><a href="#cb8-506" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom plot: Log-likelihood curves for different p values</span></span>
<span id="cb8-507"><a href="#cb8-507" aria-hidden="true" tabindex="-1"></a>n_obs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb8-508"><a href="#cb8-508" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb8-509"><a href="#cb8-509" aria-hidden="true" tabindex="-1"></a>theta_range <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">200</span>)</span>
<span id="cb8-510"><a href="#cb8-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-511"><a href="#cb8-511" aria-hidden="true" tabindex="-1"></a><span class="co"># Show log-likelihood for different true p values</span></span>
<span id="cb8-512"><a href="#cb8-512" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p_true, color <span class="kw">in</span> [(<span class="fl">0.5</span>, <span class="st">'red'</span>), (<span class="fl">0.6</span>, <span class="st">'green'</span>), (<span class="fl">0.8</span>, <span class="st">'blue'</span>)]:</span>
<span id="cb8-513"><a href="#cb8-513" aria-hidden="true" tabindex="-1"></a>    log_lik <span class="op">=</span> successes <span class="op">*</span> np.log(theta_range) <span class="op">+</span> (n_obs <span class="op">-</span> successes) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> theta_range)</span>
<span id="cb8-514"><a href="#cb8-514" aria-hidden="true" tabindex="-1"></a>    ax3.plot(theta_range, log_lik, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb8-515"><a href="#cb8-515" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'Data from p = </span><span class="sc">{</span>p_true<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-516"><a href="#cb8-516" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-517"><a href="#cb8-517" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark the MLE</span></span>
<span id="cb8-518"><a href="#cb8-518" aria-hidden="true" tabindex="-1"></a>    mle <span class="op">=</span> successes <span class="op">/</span> n_obs</span>
<span id="cb8-519"><a href="#cb8-519" aria-hidden="true" tabindex="-1"></a>    mle_ll <span class="op">=</span> successes <span class="op">*</span> np.log(mle) <span class="op">+</span> (n_obs <span class="op">-</span> successes) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> mle)</span>
<span id="cb8-520"><a href="#cb8-520" aria-hidden="true" tabindex="-1"></a>    ax3.plot(mle, mle_ll, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb8-521"><a href="#cb8-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-522"><a href="#cb8-522" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'θ'</span>)</span>
<span id="cb8-523"><a href="#cb8-523" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'Log-likelihood'</span>)</span>
<span id="cb8-524"><a href="#cb8-524" aria-hidden="true" tabindex="-1"></a>ax3.set_title(<span class="ss">f'Log-likelihood curves (n=</span><span class="sc">{</span>n_obs<span class="sc">}</span><span class="ss">, observed </span><span class="sc">{</span>successes<span class="sc">}</span><span class="ss"> successes)'</span>)</span>
<span id="cb8-525"><a href="#cb8-525" aria-hidden="true" tabindex="-1"></a>ax3.legend()</span>
<span id="cb8-526"><a href="#cb8-526" aria-hidden="true" tabindex="-1"></a>ax3.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-527"><a href="#cb8-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-528"><a href="#cb8-528" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-529"><a href="#cb8-529" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-530"><a href="#cb8-530" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-531"><a href="#cb8-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-532"><a href="#cb8-532" aria-hidden="true" tabindex="-1"></a>**Key insights from the plots:**</span>
<span id="cb8-533"><a href="#cb8-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-534"><a href="#cb8-534" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Fisher Information is lowest at $p=0.5$ (hardest to estimate a fair coin)</span>
<span id="cb8-535"><a href="#cb8-535" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Fisher Information $\rightarrow \infty$ as $p \rightarrow 0$ or $p \rightarrow 1$ (extreme probabilities are 'easier' to pin down)</span>
<span id="cb8-536"><a href="#cb8-536" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Standard error decreases with both $n$ and $I(p)$</span>
<span id="cb8-537"><a href="#cb8-537" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>The curvature of the log-likelihood reflects the Fisher Information</span>
<span id="cb8-538"><a href="#cb8-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-539"><a href="#cb8-539" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-540"><a href="#cb8-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-541"><a href="#cb8-541" aria-hidden="true" tabindex="-1"></a><span class="fu">### Asymptotic Normality of the MLE</span></span>
<span id="cb8-542"><a href="#cb8-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-543"><a href="#cb8-543" aria-hidden="true" tabindex="-1"></a>Now we can state the key theorem that connects Fisher Information to the distribution of the MLE:</span>
<span id="cb8-544"><a href="#cb8-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-545"><a href="#cb8-545" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Asymptotic Normality of the MLE"}</span>
<span id="cb8-546"><a href="#cb8-546" aria-hidden="true" tabindex="-1"></a>Let $\text{se} = \sqrt{\mathbb{V}(\hat{\theta}_n)}$. Under appropriate regularity conditions we have the following:</span>
<span id="cb8-547"><a href="#cb8-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-548"><a href="#cb8-548" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\text{se} \approx \sqrt{1 / I_n(\theta)}$ and</span>
<span id="cb8-549"><a href="#cb8-549" aria-hidden="true" tabindex="-1"></a>   $$\frac{(\hat{\theta}_n - \theta)}{\text{se}} \rightsquigarrow \mathcal{N}(0,1)$$</span>
<span id="cb8-550"><a href="#cb8-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-551"><a href="#cb8-551" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Let $\widehat{\text{se}} = \sqrt{1 / I_n(\hat{\theta}_n)}$. Then</span>
<span id="cb8-552"><a href="#cb8-552" aria-hidden="true" tabindex="-1"></a>   $$\frac{(\hat{\theta}_n - \theta)}{\widehat{\text{se}}} \rightsquigarrow \mathcal{N}(0,1)$$</span>
<span id="cb8-553"><a href="#cb8-553" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-554"><a href="#cb8-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-555"><a href="#cb8-555" aria-hidden="true" tabindex="-1"></a>According to this theorem, the distribution of the MLE is approximately $\mathcal{N}(\theta, \widehat{\text{se}}^2)$. This is one of the most important results in statistics: it tells us not just that the MLE converges to the true value (consistency), but also gives us its approximate distribution for finite samples.</span>
<span id="cb8-556"><a href="#cb8-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-557"><a href="#cb8-557" aria-hidden="true" tabindex="-1"></a>The key insight is that the same Fisher Information that measures how "sharp" the likelihood is also determines the precision of our estimate. Sharp likelihood → High Fisher Information → Small standard error → Precise estimate.</span>
<span id="cb8-558"><a href="#cb8-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-559"><a href="#cb8-559" aria-hidden="true" tabindex="-1"></a><span class="fu">### Constructing Confidence Intervals for the MLE</span></span>
<span id="cb8-560"><a href="#cb8-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-561"><a href="#cb8-561" aria-hidden="true" tabindex="-1"></a>With the asymptotic normality result in hand, we can now construct confidence intervals for our parameter estimates.</span>
<span id="cb8-562"><a href="#cb8-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-563"><a href="#cb8-563" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb8-564"><a href="#cb8-564" aria-hidden="true" tabindex="-1"></a>The interval</span>
<span id="cb8-565"><a href="#cb8-565" aria-hidden="true" tabindex="-1"></a>$$C_n = \left( \hat{\theta}_n - z_{\alpha/2}\widehat{\text{se}}, \hat{\theta}_n + z_{\alpha/2}\widehat{\text{se}} \right)$$</span>
<span id="cb8-566"><a href="#cb8-566" aria-hidden="true" tabindex="-1"></a>is an asymptotically valid $(1-\alpha)$ confidence interval for $\theta$.</span>
<span id="cb8-567"><a href="#cb8-567" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-568"><a href="#cb8-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-569"><a href="#cb8-569" aria-hidden="true" tabindex="-1"></a>For a 95% confidence interval, $z_{0.025} \approx 1.96 \approx 2$, giving the simple rule:</span>
<span id="cb8-570"><a href="#cb8-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-571"><a href="#cb8-571" aria-hidden="true" tabindex="-1"></a>$$\text{95\% CI} \approx \hat{\theta}_n \pm 2 \cdot \widehat{\text{se}}$$</span>
<span id="cb8-572"><a href="#cb8-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-573"><a href="#cb8-573" aria-hidden="true" tabindex="-1"></a>This type of confidence interval is used as the basis for statements about expected error in opinion polls and many other applications.</span>
<span id="cb8-574"><a href="#cb8-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-575"><a href="#cb8-575" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-576"><a href="#cb8-576" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Confidence Interval for a Proportion</span></span>
<span id="cb8-577"><a href="#cb8-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-578"><a href="#cb8-578" aria-hidden="true" tabindex="-1"></a>Let's work through the complete derivation for the Bernoulli case, which gives us confidence intervals for proportions -- one of the most common applications in practice.</span>
<span id="cb8-579"><a href="#cb8-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-580"><a href="#cb8-580" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$, we have $f(x; p) = p^x (1-p)^{1-x}$ for $x \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$.</span>
<span id="cb8-581"><a href="#cb8-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-582"><a href="#cb8-582" aria-hidden="true" tabindex="-1"></a>**Step 1: Find the MLE**</span>
<span id="cb8-583"><a href="#cb8-583" aria-hidden="true" tabindex="-1"></a>The likelihood for $n$ observations is:</span>
<span id="cb8-584"><a href="#cb8-584" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(p) = \prod_{i=1}^n p^{X_i} (1-p)^{1-X_i} = p^{S} (1-p)^{n-S}$$</span>
<span id="cb8-585"><a href="#cb8-585" aria-hidden="true" tabindex="-1"></a>where $S = \sum_{i=1}^n X_i$ is the total number of successes.</span>
<span id="cb8-586"><a href="#cb8-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-587"><a href="#cb8-587" aria-hidden="true" tabindex="-1"></a>Taking the log: $\ell_n(p) = S \log p + (n-S) \log(1-p)$</span>
<span id="cb8-588"><a href="#cb8-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-589"><a href="#cb8-589" aria-hidden="true" tabindex="-1"></a>Setting $\frac{d\ell_n}{dp} = \frac{S}{p} - \frac{n-S}{1-p} = 0$ gives $\hat{p}_n = \frac{S}{n} = \bar{X}_n$</span>
<span id="cb8-590"><a href="#cb8-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-591"><a href="#cb8-591" aria-hidden="true" tabindex="-1"></a>**Step 2: Compute the score function**</span>
<span id="cb8-592"><a href="#cb8-592" aria-hidden="true" tabindex="-1"></a>For a single observation:</span>
<span id="cb8-593"><a href="#cb8-593" aria-hidden="true" tabindex="-1"></a>$$\log f(X; p) = X \log p + (1-X) \log(1-p)$$</span>
<span id="cb8-594"><a href="#cb8-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-595"><a href="#cb8-595" aria-hidden="true" tabindex="-1"></a>The score function is:</span>
<span id="cb8-596"><a href="#cb8-596" aria-hidden="true" tabindex="-1"></a>$$s(X; p) = \frac{\partial \log f(X; p)}{\partial p} = \frac{X}{p} - \frac{1-X}{1-p}$$</span>
<span id="cb8-597"><a href="#cb8-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-598"><a href="#cb8-598" aria-hidden="true" tabindex="-1"></a>Let's verify $\mathbb{E}_p<span class="co">[</span><span class="ot">s(X; p)</span><span class="co">]</span> = 0$:</span>
<span id="cb8-599"><a href="#cb8-599" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}_p<span class="co">[</span><span class="ot">s(X; p)</span><span class="co">]</span> = \mathbb{E}_p\left<span class="co">[</span><span class="ot">\frac{X}{p} - \frac{1-X}{1-p}\right</span><span class="co">]</span> = \frac{p}{p} - \frac{1-p}{1-p} = 1 - 1 = 0 \checkmark$$</span>
<span id="cb8-600"><a href="#cb8-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-601"><a href="#cb8-601" aria-hidden="true" tabindex="-1"></a>**Step 3: Compute the second derivative**</span>
<span id="cb8-602"><a href="#cb8-602" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial^2 \log f(X; p)}{\partial p^2} = \frac{\partial}{\partial p}\left<span class="co">[</span><span class="ot">\frac{X}{p} - \frac{1-X}{1-p}\right</span><span class="co">]</span> = -\frac{X}{p^2} - \frac{1-X}{(1-p)^2}$$</span>
<span id="cb8-603"><a href="#cb8-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-604"><a href="#cb8-604" aria-hidden="true" tabindex="-1"></a>**Step 4: Find the Fisher Information**</span>
<span id="cb8-605"><a href="#cb8-605" aria-hidden="true" tabindex="-1"></a>Using the formula $I(p) = -\mathbb{E}_p\left<span class="co">[</span><span class="ot">\frac{\partial^2 \log f(X; p)}{\partial p^2}\right</span><span class="co">]</span>$:</span>
<span id="cb8-606"><a href="#cb8-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-607"><a href="#cb8-607" aria-hidden="true" tabindex="-1"></a>$$I(p) = -\mathbb{E}_p\left<span class="co">[</span><span class="ot">-\frac{X}{p^2} - \frac{1-X}{(1-p)^2}\right</span><span class="co">]</span> = \mathbb{E}_p\left<span class="co">[</span><span class="ot">\frac{X}{p^2}\right</span><span class="co">]</span> + \mathbb{E}_p\left<span class="co">[</span><span class="ot">\frac{1-X}{(1-p)^2}\right</span><span class="co">]</span>$$</span>
<span id="cb8-608"><a href="#cb8-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-609"><a href="#cb8-609" aria-hidden="true" tabindex="-1"></a>Since $\mathbb{E}_p<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = p$ and $\mathbb{E}_p<span class="co">[</span><span class="ot">1-X</span><span class="co">]</span> = 1-p$:</span>
<span id="cb8-610"><a href="#cb8-610" aria-hidden="true" tabindex="-1"></a>$$I(p) = \frac{p}{p^2} + \frac{1-p}{(1-p)^2} = \frac{1}{p} + \frac{1}{1-p} = \frac{1-p+p}{p(1-p)} = \frac{1}{p(1-p)}$$</span>
<span id="cb8-611"><a href="#cb8-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-612"><a href="#cb8-612" aria-hidden="true" tabindex="-1"></a>**Step 5: Derive the standard error**</span>
<span id="cb8-613"><a href="#cb8-613" aria-hidden="true" tabindex="-1"></a>For $n$ observations, $I_n(p) = n \cdot I(p) = \frac{n}{p(1-p)}$</span>
<span id="cb8-614"><a href="#cb8-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-615"><a href="#cb8-615" aria-hidden="true" tabindex="-1"></a>The standard error of $\hat{p}_n$ is:</span>
<span id="cb8-616"><a href="#cb8-616" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}} = \frac{1}{\sqrt{I_n(\hat{p}_n)}} = \frac{1}{\sqrt{\frac{n}{\hat{p}_n(1-\hat{p}_n)}}} = \sqrt{\frac{\hat{p}_n(1-\hat{p}_n)}{n}}$$</span>
<span id="cb8-617"><a href="#cb8-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-618"><a href="#cb8-618" aria-hidden="true" tabindex="-1"></a>**Step 6: Construct the confidence interval**</span>
<span id="cb8-619"><a href="#cb8-619" aria-hidden="true" tabindex="-1"></a>From the asymptotic normality theorem: $\frac{\hat{p}_n - p}{\widehat{\text{se}}} \rightsquigarrow \mathcal{N}(0,1)$</span>
<span id="cb8-620"><a href="#cb8-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-621"><a href="#cb8-621" aria-hidden="true" tabindex="-1"></a>For a $(1-\alpha)$ confidence interval:</span>
<span id="cb8-622"><a href="#cb8-622" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}\left(-z_{\alpha/2} \leq \frac{\hat{p}_n - p}{\widehat{\text{se}}} \leq z_{\alpha/2}\right) \approx 1-\alpha$$</span>
<span id="cb8-623"><a href="#cb8-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-624"><a href="#cb8-624" aria-hidden="true" tabindex="-1"></a>Rearranging:</span>
<span id="cb8-625"><a href="#cb8-625" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}\left(\hat{p}_n - z_{\alpha/2} \cdot \widehat{\text{se}} \leq p \leq \hat{p}_n + z_{\alpha/2} \cdot \widehat{\text{se}}\right) \approx 1-\alpha$$</span>
<span id="cb8-626"><a href="#cb8-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-627"><a href="#cb8-627" aria-hidden="true" tabindex="-1"></a>Therefore, the $(1-\alpha)$ confidence interval is:</span>
<span id="cb8-628"><a href="#cb8-628" aria-hidden="true" tabindex="-1"></a>$$\hat{p}_n \pm z_{\alpha/2} \sqrt{\frac{\hat{p}_n(1-\hat{p}_n)}{n}}$$</span>
<span id="cb8-629"><a href="#cb8-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-630"><a href="#cb8-630" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-631"><a href="#cb8-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-632"><a href="#cb8-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-633"><a href="#cb8-633" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-634"><a href="#cb8-634" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Political Opinion Polling</span></span>
<span id="cb8-635"><a href="#cb8-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-636"><a href="#cb8-636" aria-hidden="true" tabindex="-1"></a>Let's apply the confidence interval for a proportion from the previous worked example to understand how political polls work and what their "margin of error" really means.</span>
<span id="cb8-637"><a href="#cb8-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-640"><a href="#cb8-640" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-641"><a href="#cb8-641" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb8-642"><a href="#cb8-642" aria-hidden="true" tabindex="-1"></a><span class="co"># Political polling example</span></span>
<span id="cb8-643"><a href="#cb8-643" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose we poll n=1000 randomly selected likely voters about a referendum</span></span>
<span id="cb8-644"><a href="#cb8-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-645"><a href="#cb8-645" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Sample size</span></span>
<span id="cb8-646"><a href="#cb8-646" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">520</span>  <span class="co"># Number who support the referendum</span></span>
<span id="cb8-647"><a href="#cb8-647" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> successes <span class="op">/</span> n  <span class="co"># Sample proportion = 0.52 (52%)</span></span>
<span id="cb8-648"><a href="#cb8-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-649"><a href="#cb8-649" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard error using the formula we derived</span></span>
<span id="cb8-650"><a href="#cb8-650" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p_hat <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_hat) <span class="op">/</span> n)</span>
<span id="cb8-651"><a href="#cb8-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-652"><a href="#cb8-652" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% confidence interval (using z_{0.025} = 1.96)</span></span>
<span id="cb8-653"><a href="#cb8-653" aria-hidden="true" tabindex="-1"></a>z_critical <span class="op">=</span> <span class="fl">1.96</span></span>
<span id="cb8-654"><a href="#cb8-654" aria-hidden="true" tabindex="-1"></a>ci_lower <span class="op">=</span> p_hat <span class="op">-</span> z_critical <span class="op">*</span> se</span>
<span id="cb8-655"><a href="#cb8-655" aria-hidden="true" tabindex="-1"></a>ci_upper <span class="op">=</span> p_hat <span class="op">+</span> z_critical <span class="op">*</span> se</span>
<span id="cb8-656"><a href="#cb8-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-657"><a href="#cb8-657" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"POLL RESULTS:"</span>)</span>
<span id="cb8-658"><a href="#cb8-658" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> voters"</span>)</span>
<span id="cb8-659"><a href="#cb8-659" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Support for referendum: </span><span class="sc">{</span>p_hat<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% (</span><span class="sc">{</span>successes<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb8-660"><a href="#cb8-660" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard error: </span><span class="sc">{</span>se<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss"> percentage points"</span>)</span>
<span id="cb8-661"><a href="#cb8-661" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"95% confidence interval: (</span><span class="sc">{</span>ci_lower<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%, </span><span class="sc">{</span>ci_upper<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%)"</span>)</span>
<span id="cb8-662"><a href="#cb8-662" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Margin of error: ±</span><span class="sc">{</span>z_critical<span class="op">*</span>se<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss"> percentage points"</span>)</span>
<span id="cb8-663"><a href="#cb8-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-664"><a href="#cb8-664" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpretation</span></span>
<span id="cb8-665"><a href="#cb8-665" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">INTERPRETATION:"</span>)</span>
<span id="cb8-666"><a href="#cb8-666" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> ci_lower <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb8-667"><a href="#cb8-667" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The referendum has statistically significant majority support."</span>)</span>
<span id="cb8-668"><a href="#cb8-668" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> ci_upper <span class="op">&lt;</span> <span class="fl">0.5</span>:</span>
<span id="cb8-669"><a href="#cb8-669" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The referendum has statistically significant minority support."</span>)</span>
<span id="cb8-670"><a href="#cb8-670" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-671"><a href="#cb8-671" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The poll cannot determine if there's majority support (CI includes 50%)."</span>)</span>
<span id="cb8-672"><a href="#cb8-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-673"><a href="#cb8-673" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare different scenarios to understand margin of error</span></span>
<span id="cb8-674"><a href="#cb8-674" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-675"><a href="#cb8-675" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"HOW MARGIN OF ERROR VARIES:"</span>)</span>
<span id="cb8-676"><a href="#cb8-676" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-677"><a href="#cb8-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-678"><a href="#cb8-678" aria-hidden="true" tabindex="-1"></a>scenarios <span class="op">=</span> [</span>
<span id="cb8-679"><a href="#cb8-679" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.50</span>, <span class="dv">500</span>, <span class="st">"50</span><span class="sc">% s</span><span class="st">upport, n=500"</span>),</span>
<span id="cb8-680"><a href="#cb8-680" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.50</span>, <span class="dv">1000</span>, <span class="st">"50</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>), </span>
<span id="cb8-681"><a href="#cb8-681" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.50</span>, <span class="dv">2000</span>, <span class="st">"50</span><span class="sc">% s</span><span class="st">upport, n=2000"</span>),</span>
<span id="cb8-682"><a href="#cb8-682" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.20</span>, <span class="dv">1000</span>, <span class="st">"20</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>),</span>
<span id="cb8-683"><a href="#cb8-683" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.10</span>, <span class="dv">1000</span>, <span class="st">"10</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>),</span>
<span id="cb8-684"><a href="#cb8-684" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.01</span>, <span class="dv">1000</span>, <span class="st">"1</span><span class="sc">% s</span><span class="st">upport, n=1000"</span>)</span>
<span id="cb8-685"><a href="#cb8-685" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb8-686"><a href="#cb8-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-687"><a href="#cb8-687" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Scenario'</span><span class="sc">:&lt;25}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Margin of Error'</span><span class="sc">:&gt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'95% CI'</span><span class="sc">:&gt;20}</span><span class="ss">"</span>)</span>
<span id="cb8-688"><a href="#cb8-688" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">65</span>)</span>
<span id="cb8-689"><a href="#cb8-689" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p, n, desc <span class="kw">in</span> scenarios:</span>
<span id="cb8-690"><a href="#cb8-690" aria-hidden="true" tabindex="-1"></a>    margin <span class="op">=</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">/</span> n) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb8-691"><a href="#cb8-691" aria-hidden="true" tabindex="-1"></a>    ci_low <span class="op">=</span> (p <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">/</span> n)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb8-692"><a href="#cb8-692" aria-hidden="true" tabindex="-1"></a>    ci_high <span class="op">=</span> (p <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">/</span> n)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb8-693"><a href="#cb8-693" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>desc<span class="sc">:&lt;25}</span><span class="ss"> ±</span><span class="sc">{</span>margin<span class="sc">:&gt;6.1f}</span><span class="ss"> percentage pts  (</span><span class="sc">{</span>ci_low<span class="sc">:&gt;5.1f}</span><span class="ss">%, </span><span class="sc">{</span>ci_high<span class="sc">:&gt;5.1f}</span><span class="ss">%)"</span>)</span>
<span id="cb8-694"><a href="#cb8-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-695"><a href="#cb8-695" aria-hidden="true" tabindex="-1"></a><span class="co"># Key assumptions and limitations</span></span>
<span id="cb8-696"><a href="#cb8-696" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">KEY ASSUMPTIONS:"</span>)</span>
<span id="cb8-697"><a href="#cb8-697" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"1. Random sampling from the population of interest"</span>)</span>
<span id="cb8-698"><a href="#cb8-698" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"2. Respondents answer truthfully"</span>)</span>
<span id="cb8-699"><a href="#cb8-699" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"3. The sample size is large enough for normal approximation (np ≥ 10 and n(1-p) ≥ 10)"</span>)</span>
<span id="cb8-700"><a href="#cb8-700" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"4. No systematic bias in who responds to the poll"</span>)</span>
<span id="cb8-701"><a href="#cb8-701" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-702"><a href="#cb8-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-703"><a href="#cb8-703" aria-hidden="true" tabindex="-1"></a>**Important insights about polling:**</span>
<span id="cb8-704"><a href="#cb8-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-705"><a href="#cb8-705" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**The margin of error is largest when p = 0.5**: This is why close races are hardest to call. When support is near 50%, we have maximum uncertainty about which side has the majority.</span>
<span id="cb8-706"><a href="#cb8-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-707"><a href="#cb8-707" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Sample size matters, but with diminishing returns**: To halve the margin of error, you need to quadruple the sample size (since margin ∝ 1/√n).</span>
<span id="cb8-708"><a href="#cb8-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-709"><a href="#cb8-709" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Extreme proportions have smaller margins**: If only 1% support something, we can estimate that quite precisely even with modest sample sizes.</span>
<span id="cb8-710"><a href="#cb8-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-711"><a href="#cb8-711" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**"Statistical ties"**: When the confidence interval includes 50%, we cannot conclude which side has majority support. This is often called a "statistical tie" in media reports.</span>
<span id="cb8-712"><a href="#cb8-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-713"><a href="#cb8-713" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**The stated margin usually assumes 95% confidence**: When polls report "±3 percentage points," they typically mean the 95% confidence interval extends 3 points in each direction.</span>
<span id="cb8-714"><a href="#cb8-714" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-715"><a href="#cb8-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-716"><a href="#cb8-716" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning collapse="true"}</span>
<span id="cb8-717"><a href="#cb8-717" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Limitations of the Wald Interval</span></span>
<span id="cb8-718"><a href="#cb8-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-719"><a href="#cb8-719" aria-hidden="true" tabindex="-1"></a>The confidence interval we derived above, known as the **Wald interval**, has poor coverage when $p$ is near 0 or 1, or when $n$ is small. In practice, consider using:</span>
<span id="cb8-720"><a href="#cb8-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-721"><a href="#cb8-721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Agresti-Coull interval**: Add 2 successes and 2 failures before computing: $\tilde{p} = (S+2)/(n+4)$.</span>
<span id="cb8-722"><a href="#cb8-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wilson score interval**: More complex but widely recommended as the default choice.</span>
<span id="cb8-723"><a href="#cb8-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-724"><a href="#cb8-724" aria-hidden="true" tabindex="-1"></a>The Wald interval remains important for understanding the theory, but these alternatives perform better in practice.</span>
<span id="cb8-725"><a href="#cb8-725" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-726"><a href="#cb8-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-727"><a href="#cb8-727" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-728"><a href="#cb8-728" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: The Cramér-Rao Lower Bound</span></span>
<span id="cb8-729"><a href="#cb8-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-730"><a href="#cb8-730" aria-hidden="true" tabindex="-1"></a>We mentioned that the MLE achieves the smallest possible variance. This is formalized by the **Cramér-Rao Lower Bound**:</span>
<span id="cb8-731"><a href="#cb8-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-732"><a href="#cb8-732" aria-hidden="true" tabindex="-1"></a>For any unbiased estimator $\tilde{\theta}$:</span>
<span id="cb8-733"><a href="#cb8-733" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(\tilde{\theta}) \geq \frac{1}{I_n(\theta)}$$</span>
<span id="cb8-734"><a href="#cb8-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-735"><a href="#cb8-735" aria-hidden="true" tabindex="-1"></a>Since the MLE asymptotically achieves variance $1/I_n(\theta)$, it is **asymptotically efficient** -- no consistent estimator can do better!</span>
<span id="cb8-736"><a href="#cb8-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-737"><a href="#cb8-737" aria-hidden="true" tabindex="-1"></a>This is why we call the MLE "optimal": it extracts all possible information from the data about the parameter.</span>
<span id="cb8-738"><a href="#cb8-738" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-739"><a href="#cb8-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-740"><a href="#cb8-740" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Topics</span></span>
<span id="cb8-741"><a href="#cb8-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-742"><a href="#cb8-742" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Delta Method: Confidence Intervals for Transformations</span></span>
<span id="cb8-743"><a href="#cb8-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-744"><a href="#cb8-744" aria-hidden="true" tabindex="-1"></a>Often we're not interested in the parameter $\theta$ itself, but in some transformation $\tau = g(\theta)$. For example:</span>
<span id="cb8-745"><a href="#cb8-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-746"><a href="#cb8-746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $\theta$ is log-odds, we might want odds $\tau = e^\theta$</span>
<span id="cb8-747"><a href="#cb8-747" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $\theta$ is standard deviation, we might want variance $\tau = \theta^2$</span>
<span id="cb8-748"><a href="#cb8-748" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $\theta$ is a rate parameter, we might want mean lifetime $\tau = 1/\theta$</span>
<span id="cb8-749"><a href="#cb8-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-750"><a href="#cb8-750" aria-hidden="true" tabindex="-1"></a>Due to the **equivariance property** seen earlier, if we know the MLE $\hat{\theta}$, we know also that the MLE of $\tau$ is $\hat{\tau} = g(\hat{\theta})$.</span>
<span id="cb8-751"><a href="#cb8-751" aria-hidden="true" tabindex="-1"></a>However, what about the confidence intervals of $\hat{\tau}$?</span>
<span id="cb8-752"><a href="#cb8-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-753"><a href="#cb8-753" aria-hidden="true" tabindex="-1"></a>The **Delta Method** provides a way to find confidence intervals for transformed parameters.</span>
<span id="cb8-754"><a href="#cb8-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-755"><a href="#cb8-755" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="The Delta Method"}</span>
<span id="cb8-756"><a href="#cb8-756" aria-hidden="true" tabindex="-1"></a>If $\tau = g(\theta)$ where $g$ is differentiable and $g'(\theta) \neq 0$, then:</span>
<span id="cb8-757"><a href="#cb8-757" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}}(\hat{\tau}_n) \approx |g'(\hat{\theta}_n)| \cdot \widehat{\text{se}}(\hat{\theta}_n)$$</span>
<span id="cb8-758"><a href="#cb8-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-759"><a href="#cb8-759" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb8-760"><a href="#cb8-760" aria-hidden="true" tabindex="-1"></a>$$\frac{(\hat{\tau}_n - \tau)}{\widehat{\text{se}}(\hat{\tau}_n)} \rightsquigarrow \mathcal{N}(0,1)$$</span>
<span id="cb8-761"><a href="#cb8-761" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-762"><a href="#cb8-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-763"><a href="#cb8-763" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-764"><a href="#cb8-764" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuition Behind the Delta Method</span></span>
<span id="cb8-765"><a href="#cb8-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-766"><a href="#cb8-766" aria-hidden="true" tabindex="-1"></a>If we zoom in enough, any smooth function looks linear. The Delta Method says that when we transform our estimate through a function $g$, the standard error gets multiplied by $|g'(\hat{\theta}_n)|$ -- the absolute slope of the function at our estimate. </span>
<span id="cb8-767"><a href="#cb8-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-768"><a href="#cb8-768" aria-hidden="true" tabindex="-1"></a>This approximation becomes exact as $n \to \infty$ because the variability of $\hat{\theta}_n$ shrinks, effectively "zooming in" on the region where $g$ is effectively linear.</span>
<span id="cb8-769"><a href="#cb8-769" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-770"><a href="#cb8-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-771"><a href="#cb8-771" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-772"><a href="#cb8-772" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Full Delta Method Workflow</span></span>
<span id="cb8-773"><a href="#cb8-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-774"><a href="#cb8-774" aria-hidden="true" tabindex="-1"></a>Let's work through estimating $\psi = \log \sigma$ for a Normal distribution with known mean. This example demonstrates every step of the Delta Method process.</span>
<span id="cb8-775"><a href="#cb8-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-776"><a href="#cb8-776" aria-hidden="true" tabindex="-1"></a>**Setup**: $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ with $\mu$ known.</span>
<span id="cb8-777"><a href="#cb8-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-778"><a href="#cb8-778" aria-hidden="true" tabindex="-1"></a>**Step 1: Find MLE for $\sigma$**</span>
<span id="cb8-779"><a href="#cb8-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-780"><a href="#cb8-780" aria-hidden="true" tabindex="-1"></a>The log-likelihood is:</span>
<span id="cb8-781"><a href="#cb8-781" aria-hidden="true" tabindex="-1"></a>$$\ell(\sigma) = -n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2$$</span>
<span id="cb8-782"><a href="#cb8-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-783"><a href="#cb8-783" aria-hidden="true" tabindex="-1"></a>Taking the derivative and setting to zero:</span>
<span id="cb8-784"><a href="#cb8-784" aria-hidden="true" tabindex="-1"></a>$$\frac{d\ell}{d\sigma} = -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^n (X_i - \mu)^2 = 0$$</span>
<span id="cb8-785"><a href="#cb8-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-786"><a href="#cb8-786" aria-hidden="true" tabindex="-1"></a>Solving: $\hat{\sigma}_n = \sqrt{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2}$</span>
<span id="cb8-787"><a href="#cb8-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-788"><a href="#cb8-788" aria-hidden="true" tabindex="-1"></a>**Step 2: Find Fisher Information for $\sigma$**</span>
<span id="cb8-789"><a href="#cb8-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-790"><a href="#cb8-790" aria-hidden="true" tabindex="-1"></a>The log density for a single observation is:</span>
<span id="cb8-791"><a href="#cb8-791" aria-hidden="true" tabindex="-1"></a>$$\log f(X; \sigma) = -\log \sigma - \frac{(X-\mu)^2}{2\sigma^2}$$</span>
<span id="cb8-792"><a href="#cb8-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-793"><a href="#cb8-793" aria-hidden="true" tabindex="-1"></a>First derivative:</span>
<span id="cb8-794"><a href="#cb8-794" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \log f(X; \sigma)}{\partial \sigma} = -\frac{1}{\sigma} + \frac{(X-\mu)^2}{\sigma^3}$$</span>
<span id="cb8-795"><a href="#cb8-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-796"><a href="#cb8-796" aria-hidden="true" tabindex="-1"></a>Second derivative:</span>
<span id="cb8-797"><a href="#cb8-797" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial^2 \log f(X; \sigma)}{\partial \sigma^2} = \frac{1}{\sigma^2} - \frac{3(X-\mu)^2}{\sigma^4}$$</span>
<span id="cb8-798"><a href="#cb8-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-799"><a href="#cb8-799" aria-hidden="true" tabindex="-1"></a>Fisher Information:</span>
<span id="cb8-800"><a href="#cb8-800" aria-hidden="true" tabindex="-1"></a>$$I(\sigma) = -\mathbb{E}\left<span class="co">[</span><span class="ot">\frac{\partial^2 \log f(X; \sigma)}{\partial \sigma^2}\right</span><span class="co">]</span> = -\frac{1}{\sigma^2} + \frac{3\sigma^2}{\sigma^4} = \frac{2}{\sigma^2}$$</span>
<span id="cb8-801"><a href="#cb8-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-802"><a href="#cb8-802" aria-hidden="true" tabindex="-1"></a>**Step 3: Standard Error for $\hat{\sigma}$**</span>
<span id="cb8-803"><a href="#cb8-803" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}}(\hat{\sigma}_n) = \frac{1}{\sqrt{nI(\hat{\sigma}_n)}} = \frac{\hat{\sigma}_n}{\sqrt{2n}}$$</span>
<span id="cb8-804"><a href="#cb8-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-805"><a href="#cb8-805" aria-hidden="true" tabindex="-1"></a>**Step 4: Apply the Delta Method**</span>
<span id="cb8-806"><a href="#cb8-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-807"><a href="#cb8-807" aria-hidden="true" tabindex="-1"></a>For $\psi = g(\sigma) = \log \sigma$, we have $g'(\sigma) = 1/\sigma$. Therefore:</span>
<span id="cb8-808"><a href="#cb8-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-809"><a href="#cb8-809" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}}(\hat{\psi}_n) = |g'(\hat{\sigma}_n)| \cdot \widehat{\text{se}}(\hat{\sigma}_n) = \frac{1}{\hat{\sigma}_n} \cdot \frac{\hat{\sigma}_n}{\sqrt{2n}} = \frac{1}{\sqrt{2n}}$$</span>
<span id="cb8-810"><a href="#cb8-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-811"><a href="#cb8-811" aria-hidden="true" tabindex="-1"></a>**Step 5: Confidence Interval**</span>
<span id="cb8-812"><a href="#cb8-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-813"><a href="#cb8-813" aria-hidden="true" tabindex="-1"></a>A 95% confidence interval for $\psi = \log \sigma$ is:</span>
<span id="cb8-814"><a href="#cb8-814" aria-hidden="true" tabindex="-1"></a>$$\hat{\psi}_n \pm \frac{2}{\sqrt{2n}} = \log \hat{\sigma}_n \pm \frac{2}{\sqrt{2n}}$$</span>
<span id="cb8-815"><a href="#cb8-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-816"><a href="#cb8-816" aria-hidden="true" tabindex="-1"></a>Notice something remarkable: the standard error for $\log \sigma$ doesn't depend on $\sigma$ itself! This is one reason why log-transformations are often used for scale parameters.</span>
<span id="cb8-817"><a href="#cb8-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-818"><a href="#cb8-818" aria-hidden="true" tabindex="-1"></a>**Verification Via Simulation**</span>
<span id="cb8-819"><a href="#cb8-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-820"><a href="#cb8-820" aria-hidden="true" tabindex="-1"></a>Let's verify the Delta Method through simulation. We'll generate many samples, compute both $\hat{\sigma}$ and $\log \hat{\sigma}$ for each, and check if their empirical standard errors match the theoretical predictions:</span>
<span id="cb8-821"><a href="#cb8-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-824"><a href="#cb8-824" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-825"><a href="#cb8-825" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb8-826"><a href="#cb8-826" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb8-827"><a href="#cb8-827" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate the Delta Method with simulations</span></span>
<span id="cb8-828"><a href="#cb8-828" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-829"><a href="#cb8-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-830"><a href="#cb8-830" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameters</span></span>
<span id="cb8-831"><a href="#cb8-831" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb8-832"><a href="#cb8-832" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb8-833"><a href="#cb8-833" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-834"><a href="#cb8-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-835"><a href="#cb8-835" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data and compute MLEs</span></span>
<span id="cb8-836"><a href="#cb8-836" aria-hidden="true" tabindex="-1"></a>n_sims <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-837"><a href="#cb8-837" aria-hidden="true" tabindex="-1"></a>sigma_mles <span class="op">=</span> []</span>
<span id="cb8-838"><a href="#cb8-838" aria-hidden="true" tabindex="-1"></a>log_sigma_mles <span class="op">=</span> []</span>
<span id="cb8-839"><a href="#cb8-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-840"><a href="#cb8-840" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_sims):</span>
<span id="cb8-841"><a href="#cb8-841" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.random.normal(mu_true, sigma_true, n)</span>
<span id="cb8-842"><a href="#cb8-842" aria-hidden="true" tabindex="-1"></a>    sigma_hat <span class="op">=</span> np.sqrt(np.mean((data <span class="op">-</span> mu_true)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb8-843"><a href="#cb8-843" aria-hidden="true" tabindex="-1"></a>    sigma_mles.append(sigma_hat)</span>
<span id="cb8-844"><a href="#cb8-844" aria-hidden="true" tabindex="-1"></a>    log_sigma_mles.append(np.log(sigma_hat))</span>
<span id="cb8-845"><a href="#cb8-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-846"><a href="#cb8-846" aria-hidden="true" tabindex="-1"></a>sigma_mles <span class="op">=</span> np.array(sigma_mles)</span>
<span id="cb8-847"><a href="#cb8-847" aria-hidden="true" tabindex="-1"></a>log_sigma_mles <span class="op">=</span> np.array(log_sigma_mles)</span>
<span id="cb8-848"><a href="#cb8-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-849"><a href="#cb8-849" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical values</span></span>
<span id="cb8-850"><a href="#cb8-850" aria-hidden="true" tabindex="-1"></a>theoretical_se_sigma <span class="op">=</span> sigma_true <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>n)</span>
<span id="cb8-851"><a href="#cb8-851" aria-hidden="true" tabindex="-1"></a>theoretical_se_log_sigma <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>n)</span>
<span id="cb8-852"><a href="#cb8-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-853"><a href="#cb8-853" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plots</span></span>
<span id="cb8-854"><a href="#cb8-854" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb8-855"><a href="#cb8-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-856"><a href="#cb8-856" aria-hidden="true" tabindex="-1"></a><span class="co"># Left: Distribution of sigma MLE</span></span>
<span id="cb8-857"><a href="#cb8-857" aria-hidden="true" tabindex="-1"></a>ax1.hist(sigma_mles, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb8-858"><a href="#cb8-858" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(sigma_mles.<span class="bu">min</span>(), sigma_mles.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb8-859"><a href="#cb8-859" aria-hidden="true" tabindex="-1"></a>ax1.plot(x, stats.norm.pdf(x, sigma_true, theoretical_se_sigma), </span>
<span id="cb8-860"><a href="#cb8-860" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Theoretical'</span>)</span>
<span id="cb8-861"><a href="#cb8-861" aria-hidden="true" tabindex="-1"></a>ax1.axvline(sigma_true, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True value'</span>)</span>
<span id="cb8-862"><a href="#cb8-862" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'$\hat{\sigma}$'</span>)</span>
<span id="cb8-863"><a href="#cb8-863" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb8-864"><a href="#cb8-864" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Distribution of $\hat{\sigma}$'</span>)</span>
<span id="cb8-865"><a href="#cb8-865" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb8-866"><a href="#cb8-866" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-867"><a href="#cb8-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-868"><a href="#cb8-868" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: Distribution of log(sigma) MLE</span></span>
<span id="cb8-869"><a href="#cb8-869" aria-hidden="true" tabindex="-1"></a>ax2.hist(log_sigma_mles, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb8-870"><a href="#cb8-870" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(log_sigma_mles.<span class="bu">min</span>(), log_sigma_mles.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb8-871"><a href="#cb8-871" aria-hidden="true" tabindex="-1"></a>ax2.plot(x, stats.norm.pdf(x, np.log(sigma_true), theoretical_se_log_sigma), </span>
<span id="cb8-872"><a href="#cb8-872" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Theoretical'</span>)</span>
<span id="cb8-873"><a href="#cb8-873" aria-hidden="true" tabindex="-1"></a>ax2.axvline(np.log(sigma_true), color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True value'</span>)</span>
<span id="cb8-874"><a href="#cb8-874" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'$\hat{\psi} = \log \hat{\sigma}$'</span>)</span>
<span id="cb8-875"><a href="#cb8-875" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb8-876"><a href="#cb8-876" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Distribution of $\log \hat{\sigma}$'</span>)</span>
<span id="cb8-877"><a href="#cb8-877" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb8-878"><a href="#cb8-878" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-879"><a href="#cb8-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-880"><a href="#cb8-880" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-881"><a href="#cb8-881" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-882"><a href="#cb8-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-883"><a href="#cb8-883" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Standard Error Comparison:"</span>)</span>
<span id="cb8-884"><a href="#cb8-884" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical SE($\hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>np<span class="sc">.</span>std(sigma_mles)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-885"><a href="#cb8-885" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical SE($\hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>theoretical_se_sigma<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-886"><a href="#cb8-886" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical SE($\log \hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>np<span class="sc">.</span>std(log_sigma_mles)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-887"><a href="#cb8-887" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical SE($\log \hat</span><span class="ch">{{</span><span class="ss">\sigma</span><span class="ch">}}</span><span class="ss">$): </span><span class="sc">{</span>theoretical_se_log_sigma<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-888"><a href="#cb8-888" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">SE ratio (empirical): </span><span class="sc">{</span>np<span class="sc">.</span>std(log_sigma_mles)<span class="op">/</span>np<span class="sc">.</span>std(sigma_mles)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-889"><a href="#cb8-889" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SE ratio (Delta method): </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>sigma_true<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-890"><a href="#cb8-890" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-891"><a href="#cb8-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-892"><a href="#cb8-892" aria-hidden="true" tabindex="-1"></a>**Key takeaways**: The simulation confirms that the Delta Method works perfectly! The empirical standard error ratio between $\log \hat{\sigma}$ and $\hat{\sigma}$ matches exactly what the Delta Method predicts: $1/\sigma$. Both distributions are well-approximated by normal distributions, validating the asymptotic normality of MLEs.</span>
<span id="cb8-893"><a href="#cb8-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-894"><a href="#cb8-894" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-895"><a href="#cb8-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-896"><a href="#cb8-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-897"><a href="#cb8-897" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiparameter Models</span></span>
<span id="cb8-898"><a href="#cb8-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-899"><a href="#cb8-899" aria-hidden="true" tabindex="-1"></a>Real-world problems often involve multiple parameters. The theory we saw in this chapter extends naturally, with matrices replacing scalars.</span>
<span id="cb8-900"><a href="#cb8-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-901"><a href="#cb8-901" aria-hidden="true" tabindex="-1"></a>For $\theta = (\theta_1, \ldots, \theta_k)$, let $\hat{\theta} = (\hat{\theta}_1, \ldots, \hat{\theta}_k)$ be the MLE. The key concepts become:</span>
<span id="cb8-902"><a href="#cb8-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-903"><a href="#cb8-903" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb8-904"><a href="#cb8-904" aria-hidden="true" tabindex="-1"></a>Let $\ell_n(\theta) = \sum_{i=1}^n \log f(X_i; \theta)$ and define:</span>
<span id="cb8-905"><a href="#cb8-905" aria-hidden="true" tabindex="-1"></a>$$H_{jk} = \frac{\partial^2 \ell_n}{\partial \theta_j \partial \theta_k}$$</span>
<span id="cb8-906"><a href="#cb8-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-907"><a href="#cb8-907" aria-hidden="true" tabindex="-1"></a>The **Fisher Information Matrix** is:</span>
<span id="cb8-908"><a href="#cb8-908" aria-hidden="true" tabindex="-1"></a>$$I_n(\theta) = -</span>
<span id="cb8-909"><a href="#cb8-909" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb8-910"><a href="#cb8-910" aria-hidden="true" tabindex="-1"></a>  \mathbb{E}_\theta(H_{11}) &amp; \mathbb{E}_\theta(H_{12}) &amp; \cdots &amp; \mathbb{E}_\theta(H_{1k}) <span class="sc">\\</span></span>
<span id="cb8-911"><a href="#cb8-911" aria-hidden="true" tabindex="-1"></a>  \mathbb{E}_\theta(H_{21}) &amp; \mathbb{E}_\theta(H_{22}) &amp; \cdots &amp; \mathbb{E}_\theta(H_{2k}) <span class="sc">\\</span></span>
<span id="cb8-912"><a href="#cb8-912" aria-hidden="true" tabindex="-1"></a>    \vdots           &amp;    \vdots           &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb8-913"><a href="#cb8-913" aria-hidden="true" tabindex="-1"></a>  \mathbb{E}_\theta(H_{k1}) &amp; \mathbb{E}_\theta(H_{k2}) &amp; \cdots &amp; \mathbb{E}_\theta(H_{kk})</span>
<span id="cb8-914"><a href="#cb8-914" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}$$</span>
<span id="cb8-915"><a href="#cb8-915" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-916"><a href="#cb8-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-917"><a href="#cb8-917" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb8-918"><a href="#cb8-918" aria-hidden="true" tabindex="-1"></a>Under regularity conditions:</span>
<span id="cb8-919"><a href="#cb8-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-920"><a href="#cb8-920" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The MLE is approximately multivariate normal:</span>
<span id="cb8-921"><a href="#cb8-921" aria-hidden="true" tabindex="-1"></a>   $$(\hat{\theta} - \theta) \approx \mathcal{N}(0, J_n)$$</span>
<span id="cb8-922"><a href="#cb8-922" aria-hidden="true" tabindex="-1"></a>   where $J_n = I_n^{-1}(\theta)$ is the inverse Fisher Information matrix.</span>
<span id="cb8-923"><a href="#cb8-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-924"><a href="#cb8-924" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For parameter $\theta_j$, the standard error is:</span>
<span id="cb8-925"><a href="#cb8-925" aria-hidden="true" tabindex="-1"></a>   $$\widehat{\text{se}}_j = \sqrt{J_n(j,j)}$$</span>
<span id="cb8-926"><a href="#cb8-926" aria-hidden="true" tabindex="-1"></a>   (the square root of the $j$-th diagonal element of $J_n$)</span>
<span id="cb8-927"><a href="#cb8-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-928"><a href="#cb8-928" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The covariance between $\hat{\theta}_j$ and $\hat{\theta}_k$ is $J_n(j,k)$.</span>
<span id="cb8-929"><a href="#cb8-929" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-930"><a href="#cb8-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-931"><a href="#cb8-931" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-932"><a href="#cb8-932" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Multiparameter Normal Distribution</span></span>
<span id="cb8-933"><a href="#cb8-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-934"><a href="#cb8-934" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ with both parameters unknown:</span>
<span id="cb8-935"><a href="#cb8-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-936"><a href="#cb8-936" aria-hidden="true" tabindex="-1"></a>The Fisher Information matrix is:</span>
<span id="cb8-937"><a href="#cb8-937" aria-hidden="true" tabindex="-1"></a>$$I_n(\mu, \sigma) = n \begin{pmatrix}</span>
<span id="cb8-938"><a href="#cb8-938" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sigma^2} &amp; 0 <span class="sc">\\</span></span>
<span id="cb8-939"><a href="#cb8-939" aria-hidden="true" tabindex="-1"></a>0 &amp; \frac{2}{\sigma^2}</span>
<span id="cb8-940"><a href="#cb8-940" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}$$</span>
<span id="cb8-941"><a href="#cb8-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-942"><a href="#cb8-942" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb8-943"><a href="#cb8-943" aria-hidden="true" tabindex="-1"></a>$$J_n = \frac{1}{n} \begin{pmatrix}</span>
<span id="cb8-944"><a href="#cb8-944" aria-hidden="true" tabindex="-1"></a>\sigma^2 &amp; 0 <span class="sc">\\</span></span>
<span id="cb8-945"><a href="#cb8-945" aria-hidden="true" tabindex="-1"></a>0 &amp; \frac{\sigma^2}{2}</span>
<span id="cb8-946"><a href="#cb8-946" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}$$</span>
<span id="cb8-947"><a href="#cb8-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-948"><a href="#cb8-948" aria-hidden="true" tabindex="-1"></a>This tells us:</span>
<span id="cb8-949"><a href="#cb8-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-950"><a href="#cb8-950" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\widehat{\text{se}}(\hat{\mu}) = \sigma/\sqrt{n}$ (familiar formula!)</span>
<span id="cb8-951"><a href="#cb8-951" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\widehat{\text{se}}(\hat{\sigma}) = \sigma/\sqrt{2n}$</span>
<span id="cb8-952"><a href="#cb8-952" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Cov}(\hat{\mu}, \hat{\sigma}) = 0$ (they're independent!)</span>
<span id="cb8-953"><a href="#cb8-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-954"><a href="#cb8-954" aria-hidden="true" tabindex="-1"></a>The zero off-diagonal terms mean that uncertainty about $\mu$ doesn't affect our estimate of $\sigma$, and vice versa. This orthogonality is special to the normal distribution.</span>
<span id="cb8-955"><a href="#cb8-955" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-956"><a href="#cb8-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-957"><a href="#cb8-957" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-958"><a href="#cb8-958" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Multiparameter Delta Method</span></span>
<span id="cb8-959"><a href="#cb8-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-960"><a href="#cb8-960" aria-hidden="true" tabindex="-1"></a>For a function $\tau = g(\theta_1, \ldots, \theta_k)$, the Delta Method generalizes to:</span>
<span id="cb8-961"><a href="#cb8-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-962"><a href="#cb8-962" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}}(\hat{\tau}) = \sqrt{(\widehat{\nabla} g)^T \widehat{J}_n (\widehat{\nabla} g)}$$</span>
<span id="cb8-963"><a href="#cb8-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-964"><a href="#cb8-964" aria-hidden="true" tabindex="-1"></a>where $\widehat{\nabla} g$ is the gradient of $g$ evaluated at $\hat{\theta}$.</span>
<span id="cb8-965"><a href="#cb8-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-966"><a href="#cb8-966" aria-hidden="true" tabindex="-1"></a>**Example**: For the Normal distribution, if we're interested in the coefficient of variation $\tau = \sigma/\mu$:</span>
<span id="cb8-967"><a href="#cb8-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-968"><a href="#cb8-968" aria-hidden="true" tabindex="-1"></a>$$\nabla g = \begin{pmatrix} -\sigma/\mu^2 <span class="sc">\\</span> 1/\mu \end{pmatrix}$$</span>
<span id="cb8-969"><a href="#cb8-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-970"><a href="#cb8-970" aria-hidden="true" tabindex="-1"></a>The standard error involves both parameter uncertainties and their covariance (though the covariance is zero for the normal case).</span>
<span id="cb8-971"><a href="#cb8-971" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-972"><a href="#cb8-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-973"><a href="#cb8-973" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sufficient Statistics</span></span>
<span id="cb8-974"><a href="#cb8-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-975"><a href="#cb8-975" aria-hidden="true" tabindex="-1"></a>A **statistic** is a function $T(X^n)$ of the data. A **sufficient statistic** is a statistic that contains all the information in the data about the parameter.</span>
<span id="cb8-976"><a href="#cb8-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-977"><a href="#cb8-977" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb8-978"><a href="#cb8-978" aria-hidden="true" tabindex="-1"></a>A statistic $T(X^n)$ is **sufficient** for parameter $\theta$ if the conditional distribution of the data given $T$ doesn't depend on $\theta$.</span>
<span id="cb8-979"><a href="#cb8-979" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-980"><a href="#cb8-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-981"><a href="#cb8-981" aria-hidden="true" tabindex="-1"></a>::: {.example}</span>
<span id="cb8-982"><a href="#cb8-982" aria-hidden="true" tabindex="-1"></a>Let $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$. Then $\mathcal{L}(p) = p^S(1-p)^{n-S}$ where $S = \sum_{i=1}^n X_i$. </span>
<span id="cb8-983"><a href="#cb8-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-984"><a href="#cb8-984" aria-hidden="true" tabindex="-1"></a>The likelihood depends on the data only through $S$, so $S$ is sufficient. This means that once we know the total number of successes, the individual outcomes provide no additional information about $p$.</span>
<span id="cb8-985"><a href="#cb8-985" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-986"><a href="#cb8-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-987"><a href="#cb8-987" aria-hidden="true" tabindex="-1"></a>A sufficient statistic is **minimal** if it provides the most compressed summary possible while still retaining all information about the parameter. More precisely: any other sufficient statistic can be "reduced" to the minimal one, but not vice versa.</span>
<span id="cb8-988"><a href="#cb8-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-989"><a href="#cb8-989" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-990"><a href="#cb8-990" aria-hidden="true" tabindex="-1"></a><span class="fu">## Connection Between MLE and Sufficiency</span></span>
<span id="cb8-991"><a href="#cb8-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-992"><a href="#cb8-992" aria-hidden="true" tabindex="-1"></a>When a non-trivial sufficient statistic exists, the MLE depends on the data only through that statistic. Examples:</span>
<span id="cb8-993"><a href="#cb8-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-994"><a href="#cb8-994" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bernoulli**: MLE $\hat{p} = S/n$ depends only on sufficient statistic $S = \sum X_i$</span>
<span id="cb8-995"><a href="#cb8-995" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Normal**: MLE $(\hat{\mu}, \hat{\sigma}^2)$ depends only on sufficient statistics $(\bar{X}, S^2)$</span>
<span id="cb8-996"><a href="#cb8-996" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Uniform(0,$\theta$)**: MLE is exactly the sufficient statistic $\max<span class="sc">\{</span>X_i<span class="sc">\}</span>$</span>
<span id="cb8-997"><a href="#cb8-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-998"><a href="#cb8-998" aria-hidden="true" tabindex="-1"></a>This provides theoretical justification for data reduction: when sufficient statistics exist, we can compress our data without losing any information about the parameter. However, not all models have nice sufficient statistics beyond the trivial one (the entire dataset).</span>
<span id="cb8-999"><a href="#cb8-999" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1000"><a href="#cb8-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1001"><a href="#cb8-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1002"><a href="#cb8-1002" aria-hidden="true" tabindex="-1"></a><span class="fu">## Connection to Machine Learning: Cross-Entropy as MLE</span></span>
<span id="cb8-1003"><a href="#cb8-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1004"><a href="#cb8-1004" aria-hidden="true" tabindex="-1"></a>One of the most profound connections between statistics and modern machine learning is that many "loss functions" used in ML are actually negative log-likelihoods in disguise. This isn't a coincidence -- it reflects the deep statistical foundations of machine learning. Let's consider the case of the cross-entropy loss used in classification tasks.</span>
<span id="cb8-1005"><a href="#cb8-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1006"><a href="#cb8-1006" aria-hidden="true" tabindex="-1"></a>Consider a classification problem where we predict probabilities for $K$ classes using a machine learning model $f(X; \theta)$ parameterized by $\theta$. </span>
<span id="cb8-1007"><a href="#cb8-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1008"><a href="#cb8-1008" aria-hidden="true" tabindex="-1"></a>The **cross-entropy loss** is:</span>
<span id="cb8-1009"><a href="#cb8-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1010"><a href="#cb8-1010" aria-hidden="true" tabindex="-1"></a>$$H(p, q) = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^K p(Y_i = j) \log q_\theta(Y_i = j)$$</span>
<span id="cb8-1011"><a href="#cb8-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1012"><a href="#cb8-1012" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb8-1013"><a href="#cb8-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1014"><a href="#cb8-1014" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(Y_i = j)$ is the observed distribution (1 if observation $i$ is class $j$, 0 otherwise)</span>
<span id="cb8-1015"><a href="#cb8-1015" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$q_\theta(Y_i = j) = f(X_i; \theta)_j$ is the predicted probability from our model with parameters $\theta$</span>
<span id="cb8-1016"><a href="#cb8-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1017"><a href="#cb8-1017" aria-hidden="true" tabindex="-1"></a>Let's show this is exactly the negative log-likelihood for a categorical distribution.</span>
<span id="cb8-1018"><a href="#cb8-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1019"><a href="#cb8-1019" aria-hidden="true" tabindex="-1"></a>The likelihood for categorical data is:</span>
<span id="cb8-1020"><a href="#cb8-1020" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(\theta) = \prod_{i=1}^n \text{Categorical}(Y_i; f(X_i; \theta))$$</span>
<span id="cb8-1021"><a href="#cb8-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1022"><a href="#cb8-1022" aria-hidden="true" tabindex="-1"></a>The categorical probability for observation $i$ is:</span>
<span id="cb8-1023"><a href="#cb8-1023" aria-hidden="true" tabindex="-1"></a>$$\text{Categorical}(Y_i; f(X_i; \theta)) = \prod_{j=1}^K f(X_i; \theta)_j^{p(Y_i = j)}$$</span>
<span id="cb8-1024"><a href="#cb8-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1025"><a href="#cb8-1025" aria-hidden="true" tabindex="-1"></a>Taking the log:</span>
<span id="cb8-1026"><a href="#cb8-1026" aria-hidden="true" tabindex="-1"></a>$$\ell_n(\theta) = \sum_{i=1}^n \sum_{j=1}^K p(Y_i = j) \log f(X_i; \theta)_j$$</span>
<span id="cb8-1027"><a href="#cb8-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1028"><a href="#cb8-1028" aria-hidden="true" tabindex="-1"></a>This is exactly $-n \cdot H(p, q)$. Thus: **Minimizing cross-entropy = Maximizing likelihood!**</span>
<span id="cb8-1029"><a href="#cb8-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1030"><a href="#cb8-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1031"><a href="#cb8-1031" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-1032"><a href="#cb8-1032" aria-hidden="true" tabindex="-1"></a><span class="fu">## Other Common ML Loss Functions as MLEs</span></span>
<span id="cb8-1033"><a href="#cb8-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1034"><a href="#cb8-1034" aria-hidden="true" tabindex="-1"></a>This pattern appears throughout machine learning:</span>
<span id="cb8-1035"><a href="#cb8-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1036"><a href="#cb8-1036" aria-hidden="true" tabindex="-1"></a>| ML Loss Function | Statistical Model | MLE Interpretation |</span>
<span id="cb8-1037"><a href="#cb8-1037" aria-hidden="true" tabindex="-1"></a>|-----------------|-------------------|-------------------|</span>
<span id="cb8-1038"><a href="#cb8-1038" aria-hidden="true" tabindex="-1"></a>| Mean Squared Error (MSE) | $Y \sim \mathcal{N}(\mu(x), \sigma^2)$ | Gaussian likelihood |</span>
<span id="cb8-1039"><a href="#cb8-1039" aria-hidden="true" tabindex="-1"></a>| Mean Absolute Error (MAE) | $Y \sim \text{Laplace}(\mu(x), b)$ | Laplace likelihood |</span>
<span id="cb8-1040"><a href="#cb8-1040" aria-hidden="true" tabindex="-1"></a>| Huber Loss | Hybrid of $L_1$ and $L_2$ | Robust to outliers |</span>
<span id="cb8-1041"><a href="#cb8-1041" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1042"><a href="#cb8-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1043"><a href="#cb8-1043" aria-hidden="true" tabindex="-1"></a>Understanding that common ML losses are negative log-likelihoods provides:</span>
<span id="cb8-1044"><a href="#cb8-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1045"><a href="#cb8-1045" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Principled loss design**: When facing a new problem, you can derive an appropriate loss function by specifying a probabilistic model for your data.</span>
<span id="cb8-1046"><a href="#cb8-1046" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Historical context**: It explains why these particular loss functions became standard -- they weren't arbitrary choices but emerged from statistical principles.</span>
<span id="cb8-1047"><a href="#cb8-1047" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Intuition**: Knowing the probabilistic interpretation helps understand when each loss is appropriate (e.g., MAE for heavy-tailed errors, MSE for Gaussian noise).</span>
<span id="cb8-1048"><a href="#cb8-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1049"><a href="#cb8-1049" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb8-1050"><a href="#cb8-1050" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Deriving a Custom Loss</span></span>
<span id="cb8-1051"><a href="#cb8-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1052"><a href="#cb8-1052" aria-hidden="true" tabindex="-1"></a>Suppose you believe errors in your regression problem approximately follow a <span class="co">[</span><span class="ot">Student-t distribution</span><span class="co">](https://en.wikipedia.org/wiki/Student%27s_t-distribution)</span> (heavy tails for robustness). The negative log-likelihood gives you the loss function:</span>
<span id="cb8-1053"><a href="#cb8-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1054"><a href="#cb8-1054" aria-hidden="true" tabindex="-1"></a>$$\text{Loss}(y, \hat{y}) = \frac{\nu + 1}{2} \log\left(1 + \frac{(y - \hat{y})^2}{\nu s^2}\right)$$</span>
<span id="cb8-1055"><a href="#cb8-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1056"><a href="#cb8-1056" aria-hidden="true" tabindex="-1"></a>where $\nu$ is degrees of freedom and $s$ is scale. This naturally down-weights outliers!</span>
<span id="cb8-1057"><a href="#cb8-1057" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1058"><a href="#cb8-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1059"><a href="#cb8-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1060"><a href="#cb8-1060" aria-hidden="true" tabindex="-1"></a><span class="fu">## MLE for Latent Variable Models: The EM Algorithm</span></span>
<span id="cb8-1061"><a href="#cb8-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1062"><a href="#cb8-1062" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Challenge of Latent Variables</span></span>
<span id="cb8-1063"><a href="#cb8-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1064"><a href="#cb8-1064" aria-hidden="true" tabindex="-1"></a>So far, we've assumed that maximizing the likelihood is straightforward -- we take derivatives, set them to zero, and solve (possibly numerically). But what if the likelihood itself is intractable?</span>
<span id="cb8-1065"><a href="#cb8-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1066"><a href="#cb8-1066" aria-hidden="true" tabindex="-1"></a>This often happens when our model involves **latent (unobserved) variables** -- hidden quantities that would make the problem easy if only we could observe them.</span>
<span id="cb8-1067"><a href="#cb8-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1068"><a href="#cb8-1068" aria-hidden="true" tabindex="-1"></a>**Common examples with latent variables:**</span>
<span id="cb8-1069"><a href="#cb8-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1070"><a href="#cb8-1070" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mixture models**: Which component generated each observation?</span>
<span id="cb8-1071"><a href="#cb8-1071" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hidden Markov models**: What's the hidden state sequence?</span>
<span id="cb8-1072"><a href="#cb8-1072" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Factor analysis**: What are the values of the latent factors?</span>
<span id="cb8-1073"><a href="#cb8-1073" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Missing data**: What would the missing values have been?</span>
<span id="cb8-1074"><a href="#cb8-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1075"><a href="#cb8-1075" aria-hidden="true" tabindex="-1"></a>The canonical example is the **Gaussian Mixture Model (GMM)**:^[Also called **Mixture of Gaussians (MoG)**.]</span>
<span id="cb8-1076"><a href="#cb8-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1077"><a href="#cb8-1077" aria-hidden="true" tabindex="-1"></a>$$f(y; \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(y; \mu_k, \sigma_k^2)$$</span>
<span id="cb8-1078"><a href="#cb8-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1079"><a href="#cb8-1079" aria-hidden="true" tabindex="-1"></a>where $\theta = (\pi_1, \ldots, \pi_K, \mu_1, \ldots, \mu_K, \sigma_1, \ldots, \sigma_K)$.</span>
<span id="cb8-1080"><a href="#cb8-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1081"><a href="#cb8-1081" aria-hidden="true" tabindex="-1"></a>The likelihood involves a sum inside the logarithm:</span>
<span id="cb8-1082"><a href="#cb8-1082" aria-hidden="true" tabindex="-1"></a>$$\ell_n(\theta) = \sum_{i=1}^n \log\left<span class="co">[</span><span class="ot">\sum_{k=1}^K \pi_k \mathcal{N}(y_i; \mu_k, \sigma_k^2)\right</span><span class="co">]</span>$$</span>
<span id="cb8-1083"><a href="#cb8-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1084"><a href="#cb8-1084" aria-hidden="true" tabindex="-1"></a>This is a nightmare to differentiate! The derivatives involve ratios of sums -- messy and hard to work with.</span>
<span id="cb8-1085"><a href="#cb8-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1086"><a href="#cb8-1086" aria-hidden="true" tabindex="-1"></a>**The key insight**: If we knew which component $Z_i \in <span class="sc">\{</span>1, \ldots, K<span class="sc">\}</span>$ generated each observation $Y_i$, the problem would become trivial -- we'd just fit separate Gaussians to each group.</span>
<span id="cb8-1087"><a href="#cb8-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1088"><a href="#cb8-1088" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-1089"><a href="#cb8-1089" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Two-Component Gaussian Mixture ($K=2$)</span></span>
<span id="cb8-1090"><a href="#cb8-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1091"><a href="#cb8-1091" aria-hidden="true" tabindex="-1"></a>For the special case of $K=2$ (two components), the mixture model simplifies to:</span>
<span id="cb8-1092"><a href="#cb8-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1093"><a href="#cb8-1093" aria-hidden="true" tabindex="-1"></a>$$f(y; \theta) = \pi \mathcal{N}(y; \mu_1, \sigma_1^2) + (1-\pi) \mathcal{N}(y; \mu_2, \sigma_2^2)$$</span>
<span id="cb8-1094"><a href="#cb8-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1095"><a href="#cb8-1095" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb8-1096"><a href="#cb8-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1097"><a href="#cb8-1097" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\pi \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ is the mixing weight (probability of component 1)</span>
<span id="cb8-1098"><a href="#cb8-1098" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$(1-\pi)$ is the probability of component 2</span>
<span id="cb8-1099"><a href="#cb8-1099" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_1, \mu_2$ are the means of the two Gaussian components</span>
<span id="cb8-1100"><a href="#cb8-1100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma_1^2, \sigma_2^2$ are the variances of the two components</span>
<span id="cb8-1101"><a href="#cb8-1101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The parameter vector is $\theta = (\pi, \mu_1, \mu_2, \sigma_1, \sigma_2)$</span>
<span id="cb8-1102"><a href="#cb8-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1103"><a href="#cb8-1103" aria-hidden="true" tabindex="-1"></a>**Interpretation**: Each observation $Y_i$ is generated by:</span>
<span id="cb8-1104"><a href="#cb8-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1105"><a href="#cb8-1105" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>First, flip a biased coin with probability $\pi$ of heads</span>
<span id="cb8-1106"><a href="#cb8-1106" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If heads: draw $Y_i \sim \mathcal{N}(\mu_1, \sigma_1^2)$</span>
<span id="cb8-1107"><a href="#cb8-1107" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If tails: draw $Y_i \sim \mathcal{N}(\mu_2, \sigma_2^2)$</span>
<span id="cb8-1108"><a href="#cb8-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1109"><a href="#cb8-1109" aria-hidden="true" tabindex="-1"></a>The **latent variables** $Z_1, \ldots, Z_n \in <span class="sc">\{</span>1, 2<span class="sc">\}</span>$ indicate which component generated each observation. If we knew these $Z_i$ values, estimation would be trivial -- we'd simply:</span>
<span id="cb8-1110"><a href="#cb8-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1111"><a href="#cb8-1111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimate $\pi$ as the proportion of observations from component 1</span>
<span id="cb8-1112"><a href="#cb8-1112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimate $\mu_1, \sigma_1$ using only observations where $Z_i = 1$</span>
<span id="cb8-1113"><a href="#cb8-1113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimate $\mu_2, \sigma_2$ using only observations where $Z_i = 2$</span>
<span id="cb8-1114"><a href="#cb8-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1115"><a href="#cb8-1115" aria-hidden="true" tabindex="-1"></a>But since the $Z_i$ are unobserved, we need a method to estimate both the component assignments and the parameters.</span>
<span id="cb8-1116"><a href="#cb8-1116" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1117"><a href="#cb8-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1118"><a href="#cb8-1118" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Expectation-Maximization (EM) Algorithm</span></span>
<span id="cb8-1119"><a href="#cb8-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1120"><a href="#cb8-1120" aria-hidden="true" tabindex="-1"></a>The EM algorithm is a clever iterative procedure that alternates between:</span>
<span id="cb8-1121"><a href="#cb8-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1122"><a href="#cb8-1122" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Guessing the values of the latent variables (E-step)</span>
<span id="cb8-1123"><a href="#cb8-1123" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Updating parameters as if those guesses were correct (M-step)</span>
<span id="cb8-1124"><a href="#cb8-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1125"><a href="#cb8-1125" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb8-1126"><a href="#cb8-1126" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb8-1127"><a href="#cb8-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1128"><a href="#cb8-1128" aria-hidden="true" tabindex="-1"></a>Think of the EM algorithm as a "chicken and egg" problem solver for mixture models.</span>
<span id="cb8-1129"><a href="#cb8-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1130"><a href="#cb8-1130" aria-hidden="true" tabindex="-1"></a>**The dilemma**: </span>
<span id="cb8-1131"><a href="#cb8-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1132"><a href="#cb8-1132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If we knew which cluster each point belonged to, we could easily estimate the cluster parameters (just compute means and variances for each group)</span>
<span id="cb8-1133"><a href="#cb8-1133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If we knew the cluster parameters, we could easily assign points to clusters (pick the most likely cluster for each point)</span>
<span id="cb8-1134"><a href="#cb8-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1135"><a href="#cb8-1135" aria-hidden="true" tabindex="-1"></a>**The EM solution**: Start with a guess and alternate!</span>
<span id="cb8-1136"><a href="#cb8-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1137"><a href="#cb8-1137" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**E-step (Expectation)**: Given current cluster parameters, compute "soft assignments" -- the probability that each point belongs to each cluster. These are called "responsibilities."</span>
<span id="cb8-1138"><a href="#cb8-1138" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Point near cluster 1 center: Maybe 90% probability for cluster 1, 10% for cluster 2</span>
<span id="cb8-1139"><a href="#cb8-1139" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Point between clusters: Maybe 50-50 split</span>
<span id="cb8-1140"><a href="#cb8-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1141"><a href="#cb8-1141" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**M-step (Maximization)**: Update cluster parameters using weighted calculations, where weights are the responsibilities.</span>
<span id="cb8-1142"><a href="#cb8-1142" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>New mean for cluster 1: Weighted average of all points, using their cluster 1 responsibilities as weights</span>
<span id="cb8-1143"><a href="#cb8-1143" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Points with high responsibility contribute more to the cluster's parameters</span>
<span id="cb8-1144"><a href="#cb8-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1145"><a href="#cb8-1145" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Iterate**: Keep alternating until convergence.</span>
<span id="cb8-1146"><a href="#cb8-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1147"><a href="#cb8-1147" aria-hidden="true" tabindex="-1"></a>It's like a dance where clusters and assignments gradually find their proper configuration, each step making the other more accurate.</span>
<span id="cb8-1148"><a href="#cb8-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1149"><a href="#cb8-1149" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb8-1150"><a href="#cb8-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1151"><a href="#cb8-1151" aria-hidden="true" tabindex="-1"></a>The EM algorithm maximizes the expected complete-data log-likelihood. Let:</span>
<span id="cb8-1152"><a href="#cb8-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1153"><a href="#cb8-1153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Y$ = observed data</span>
<span id="cb8-1154"><a href="#cb8-1154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Z$ = latent/missing data</span>
<span id="cb8-1155"><a href="#cb8-1155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$(Y, Z)$ = complete data</span>
<span id="cb8-1156"><a href="#cb8-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1157"><a href="#cb8-1157" aria-hidden="true" tabindex="-1"></a>The algorithm iterates between:</span>
<span id="cb8-1158"><a href="#cb8-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1159"><a href="#cb8-1159" aria-hidden="true" tabindex="-1"></a>**E-step**: Compute the expected complete-data log-likelihood:</span>
<span id="cb8-1160"><a href="#cb8-1160" aria-hidden="true" tabindex="-1"></a>$$Q(\theta | \theta^{(t)}) = \mathbb{E}_{Z|Y,\theta^{(t)}}<span class="co">[</span><span class="ot">\log P(Y, Z | \theta)</span><span class="co">]</span>$$</span>
<span id="cb8-1161"><a href="#cb8-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1162"><a href="#cb8-1162" aria-hidden="true" tabindex="-1"></a>This expectation is over the distribution of $Z$ given the observed data $Y$ and current parameters $\theta^{(t)}$.</span>
<span id="cb8-1163"><a href="#cb8-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1164"><a href="#cb8-1164" aria-hidden="true" tabindex="-1"></a>**M-step**: Maximize with respect to $\theta$:</span>
<span id="cb8-1165"><a href="#cb8-1165" aria-hidden="true" tabindex="-1"></a>$$\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})$$</span>
<span id="cb8-1166"><a href="#cb8-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1167"><a href="#cb8-1167" aria-hidden="true" tabindex="-1"></a>**Key property**: The likelihood is guaranteed to increase (or stay the same) at each iteration:</span>
<span id="cb8-1168"><a href="#cb8-1168" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}(\theta^{(t+1)}) \geq \mathcal{L}(\theta^{(t)})$$</span>
<span id="cb8-1169"><a href="#cb8-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1170"><a href="#cb8-1170" aria-hidden="true" tabindex="-1"></a>This monotonic improvement ensures convergence to a local maximum.</span>
<span id="cb8-1171"><a href="#cb8-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1172"><a href="#cb8-1172" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb8-1173"><a href="#cb8-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1174"><a href="#cb8-1174" aria-hidden="true" tabindex="-1"></a>Let's implement and visualize the EM algorithm for a 2D Gaussian mixture:</span>
<span id="cb8-1175"><a href="#cb8-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1178"><a href="#cb8-1178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-1179"><a href="#cb8-1179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb8-1180"><a href="#cb8-1180" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 10</span></span>
<span id="cb8-1181"><a href="#cb8-1181" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-1182"><a href="#cb8-1182" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-1183"><a href="#cb8-1183" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Ellipse</span>
<span id="cb8-1184"><a href="#cb8-1184" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb8-1185"><a href="#cb8-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1186"><a href="#cb8-1186" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data from a mixture of 3 Gaussians</span></span>
<span id="cb8-1187"><a href="#cb8-1187" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-1188"><a href="#cb8-1188" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb8-1189"><a href="#cb8-1189" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-1190"><a href="#cb8-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1191"><a href="#cb8-1191" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameters</span></span>
<span id="cb8-1192"><a href="#cb8-1192" aria-hidden="true" tabindex="-1"></a>true_means <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>], [<span class="dv">0</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb8-1193"><a href="#cb8-1193" aria-hidden="true" tabindex="-1"></a>true_covs <span class="op">=</span> [np.array([[<span class="fl">0.5</span>, <span class="fl">0.2</span>], [<span class="fl">0.2</span>, <span class="fl">0.5</span>]]),</span>
<span id="cb8-1194"><a href="#cb8-1194" aria-hidden="true" tabindex="-1"></a>             np.array([[<span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.3</span>], [<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.8</span>]]),</span>
<span id="cb8-1195"><a href="#cb8-1195" aria-hidden="true" tabindex="-1"></a>             np.array([[<span class="fl">0.6</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="fl">0.3</span>]])]</span>
<span id="cb8-1196"><a href="#cb8-1196" aria-hidden="true" tabindex="-1"></a>true_weights <span class="op">=</span> np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>])</span>
<span id="cb8-1197"><a href="#cb8-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1198"><a href="#cb8-1198" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb8-1199"><a href="#cb8-1199" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb8-1200"><a href="#cb8-1200" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb8-1201"><a href="#cb8-1201" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb8-1202"><a href="#cb8-1202" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Choose component</span></span>
<span id="cb8-1203"><a href="#cb8-1203" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> np.random.choice(n_components, p<span class="op">=</span>true_weights)</span>
<span id="cb8-1204"><a href="#cb8-1204" aria-hidden="true" tabindex="-1"></a>    true_labels.append(k)</span>
<span id="cb8-1205"><a href="#cb8-1205" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate point from that component</span></span>
<span id="cb8-1206"><a href="#cb8-1206" aria-hidden="true" tabindex="-1"></a>    data.append(np.random.multivariate_normal(true_means[k], true_covs[k]))</span>
<span id="cb8-1207"><a href="#cb8-1207" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(data)</span>
<span id="cb8-1208"><a href="#cb8-1208" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> np.array(true_labels)</span>
<span id="cb8-1209"><a href="#cb8-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1210"><a href="#cb8-1210" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize EM with random parameters</span></span>
<span id="cb8-1211"><a href="#cb8-1211" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> np.random.randn(n_components, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb8-1212"><a href="#cb8-1212" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> [np.eye(<span class="dv">2</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_components)]</span>
<span id="cb8-1213"><a href="#cb8-1213" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.ones(n_components) <span class="op">/</span> n_components</span>
<span id="cb8-1214"><a href="#cb8-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1215"><a href="#cb8-1215" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to plot current state</span></span>
<span id="cb8-1216"><a href="#cb8-1216" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_em_state(data, means, covs, weights, responsibilities, iteration, log_likelihood_history):</span>
<span id="cb8-1217"><a href="#cb8-1217" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb8-1218"><a href="#cb8-1218" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1219"><a href="#cb8-1219" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Left plot: data colored by responsibilities</span></span>
<span id="cb8-1220"><a href="#cb8-1220" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> responsibilities <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb8-1221"><a href="#cb8-1221" aria-hidden="true" tabindex="-1"></a>    ax1.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span>colors, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb8-1222"><a href="#cb8-1222" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1223"><a href="#cb8-1223" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot component centers and ellipses</span></span>
<span id="cb8-1224"><a href="#cb8-1224" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb8-1225"><a href="#cb8-1225" aria-hidden="true" tabindex="-1"></a>        ax1.plot(means[k, <span class="dv">0</span>], means[k, <span class="dv">1</span>], <span class="st">'k*'</span>, markersize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb8-1226"><a href="#cb8-1226" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-1227"><a href="#cb8-1227" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw ellipse representing covariance</span></span>
<span id="cb8-1228"><a href="#cb8-1228" aria-hidden="true" tabindex="-1"></a>        eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eigh(covs[k])</span>
<span id="cb8-1229"><a href="#cb8-1229" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> np.degrees(np.arctan2(eigenvectors[<span class="dv">1</span>, <span class="dv">0</span>], eigenvectors[<span class="dv">0</span>, <span class="dv">0</span>]))</span>
<span id="cb8-1230"><a href="#cb8-1230" aria-hidden="true" tabindex="-1"></a>        width, height <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(eigenvalues)</span>
<span id="cb8-1231"><a href="#cb8-1231" aria-hidden="true" tabindex="-1"></a>        ellipse <span class="op">=</span> Ellipse(means[k], width, height, angle<span class="op">=</span>angle,</span>
<span id="cb8-1232"><a href="#cb8-1232" aria-hidden="true" tabindex="-1"></a>                         facecolor<span class="op">=</span><span class="st">'none'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-1233"><a href="#cb8-1233" aria-hidden="true" tabindex="-1"></a>        ax1.add_patch(ellipse)</span>
<span id="cb8-1234"><a href="#cb8-1234" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1235"><a href="#cb8-1235" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="ss">f'Iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">: Soft Assignments'</span>)</span>
<span id="cb8-1236"><a href="#cb8-1236" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb8-1237"><a href="#cb8-1237" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb8-1238"><a href="#cb8-1238" aria-hidden="true" tabindex="-1"></a>    ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-1239"><a href="#cb8-1239" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1240"><a href="#cb8-1240" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Right plot: log-likelihood history</span></span>
<span id="cb8-1241"><a href="#cb8-1241" aria-hidden="true" tabindex="-1"></a>    ax2.plot(<span class="bu">range</span>(<span class="bu">len</span>(log_likelihood_history)), log_likelihood_history, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-1242"><a href="#cb8-1242" aria-hidden="true" tabindex="-1"></a>    ax2.plot(iteration, log_likelihood_history[iteration], <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Mark current iteration</span></span>
<span id="cb8-1243"><a href="#cb8-1243" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb8-1244"><a href="#cb8-1244" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Log-likelihood'</span>)</span>
<span id="cb8-1245"><a href="#cb8-1245" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'Convergence'</span>)</span>
<span id="cb8-1246"><a href="#cb8-1246" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">9.5</span>)  <span class="co"># Always show full range</span></span>
<span id="cb8-1247"><a href="#cb8-1247" aria-hidden="true" tabindex="-1"></a>    ax2.set_xticks(<span class="bu">range</span>(<span class="dv">10</span>))  <span class="co"># Show only integer ticks 0-9</span></span>
<span id="cb8-1248"><a href="#cb8-1248" aria-hidden="true" tabindex="-1"></a>    ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-1249"><a href="#cb8-1249" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1250"><a href="#cb8-1250" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb8-1251"><a href="#cb8-1251" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb8-1252"><a href="#cb8-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1253"><a href="#cb8-1253" aria-hidden="true" tabindex="-1"></a><span class="co"># EM iterations</span></span>
<span id="cb8-1254"><a href="#cb8-1254" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-1255"><a href="#cb8-1255" aria-hidden="true" tabindex="-1"></a>log_likelihood_history <span class="op">=</span> []</span>
<span id="cb8-1256"><a href="#cb8-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1257"><a href="#cb8-1257" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb8-1258"><a href="#cb8-1258" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E-step: compute responsibilities</span></span>
<span id="cb8-1259"><a href="#cb8-1259" aria-hidden="true" tabindex="-1"></a>    responsibilities <span class="op">=</span> np.zeros((n_samples, n_components))</span>
<span id="cb8-1260"><a href="#cb8-1260" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb8-1261"><a href="#cb8-1261" aria-hidden="true" tabindex="-1"></a>        responsibilities[:, k] <span class="op">=</span> weights[k] <span class="op">*</span> multivariate_normal.pdf(</span>
<span id="cb8-1262"><a href="#cb8-1262" aria-hidden="true" tabindex="-1"></a>            data, means[k], covs[k])</span>
<span id="cb8-1263"><a href="#cb8-1263" aria-hidden="true" tabindex="-1"></a>    responsibilities <span class="op">/=</span> responsibilities.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-1264"><a href="#cb8-1264" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1265"><a href="#cb8-1265" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute log-likelihood</span></span>
<span id="cb8-1266"><a href="#cb8-1266" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> np.<span class="bu">sum</span>(np.log(np.<span class="bu">sum</span>([</span>
<span id="cb8-1267"><a href="#cb8-1267" aria-hidden="true" tabindex="-1"></a>        weights[k] <span class="op">*</span> multivariate_normal.pdf(data, means[k], covs[k])</span>
<span id="cb8-1268"><a href="#cb8-1268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components)], axis<span class="op">=</span><span class="dv">0</span>)))</span>
<span id="cb8-1269"><a href="#cb8-1269" aria-hidden="true" tabindex="-1"></a>    log_likelihood_history.append(log_likelihood)</span>
<span id="cb8-1270"><a href="#cb8-1270" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1271"><a href="#cb8-1271" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot current state (only show a few iterations)</span></span>
<span id="cb8-1272"><a href="#cb8-1272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>]:</span>
<span id="cb8-1273"><a href="#cb8-1273" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plot_em_state(data, means, covs, weights, responsibilities, iteration, log_likelihood_history)</span>
<span id="cb8-1274"><a href="#cb8-1274" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb8-1275"><a href="#cb8-1275" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1276"><a href="#cb8-1276" aria-hidden="true" tabindex="-1"></a>    <span class="co"># M-step: update parameters</span></span>
<span id="cb8-1277"><a href="#cb8-1277" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb8-1278"><a href="#cb8-1278" aria-hidden="true" tabindex="-1"></a>        resp_k <span class="op">=</span> responsibilities[:, k]</span>
<span id="cb8-1279"><a href="#cb8-1279" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weight</span></span>
<span id="cb8-1280"><a href="#cb8-1280" aria-hidden="true" tabindex="-1"></a>        weights[k] <span class="op">=</span> resp_k.<span class="bu">sum</span>() <span class="op">/</span> n_samples</span>
<span id="cb8-1281"><a href="#cb8-1281" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update mean</span></span>
<span id="cb8-1282"><a href="#cb8-1282" aria-hidden="true" tabindex="-1"></a>        means[k] <span class="op">=</span> (resp_k[:, np.newaxis] <span class="op">*</span> data).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb8-1283"><a href="#cb8-1283" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update covariance</span></span>
<span id="cb8-1284"><a href="#cb8-1284" aria-hidden="true" tabindex="-1"></a>        diff <span class="op">=</span> data <span class="op">-</span> means[k]</span>
<span id="cb8-1285"><a href="#cb8-1285" aria-hidden="true" tabindex="-1"></a>        covs[k] <span class="op">=</span> (resp_k[:, np.newaxis, np.newaxis] <span class="op">*</span> </span>
<span id="cb8-1286"><a href="#cb8-1286" aria-hidden="true" tabindex="-1"></a>                   diff[:, :, np.newaxis] <span class="op">@</span> diff[:, np.newaxis, :]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb8-1287"><a href="#cb8-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1288"><a href="#cb8-1288" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final parameters:"</span>)</span>
<span id="cb8-1289"><a href="#cb8-1289" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weights: </span><span class="sc">{</span>weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-1290"><a href="#cb8-1290" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Means:</span><span class="ch">\n</span><span class="sc">{</span>means<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-1291"><a href="#cb8-1291" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-1292"><a href="#cb8-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1293"><a href="#cb8-1293" aria-hidden="true" tabindex="-1"></a>The visualizations show:</span>
<span id="cb8-1294"><a href="#cb8-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1295"><a href="#cb8-1295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Left plots**: Data points colored by their soft assignments (RGB = probabilities for 3 clusters)</span>
<span id="cb8-1296"><a href="#cb8-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Black stars**: Cluster centers</span>
<span id="cb8-1297"><a href="#cb8-1297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ellipses**: Covariance structure of each component</span>
<span id="cb8-1298"><a href="#cb8-1298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Right plot**: Log-likelihood increasing monotonically</span>
<span id="cb8-1299"><a href="#cb8-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1300"><a href="#cb8-1300" aria-hidden="true" tabindex="-1"></a>Notice how the algorithm gradually discovers the true cluster structure!</span>
<span id="cb8-1301"><a href="#cb8-1301" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1302"><a href="#cb8-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1303"><a href="#cb8-1303" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties and Practical Considerations</span></span>
<span id="cb8-1304"><a href="#cb8-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1305"><a href="#cb8-1305" aria-hidden="true" tabindex="-1"></a>**Strengths of EM:**</span>
<span id="cb8-1306"><a href="#cb8-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1307"><a href="#cb8-1307" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Guaranteed improvement**: The likelihood never decreases</span>
<span id="cb8-1308"><a href="#cb8-1308" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**No step size tuning**: Unlike gradient descent, no learning rate to choose</span>
<span id="cb8-1309"><a href="#cb8-1309" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Naturally handles constraints**: Probabilities automatically sum to 1</span>
<span id="cb8-1310"><a href="#cb8-1310" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Interpretable intermediate results**: Soft assignments have meaning</span>
<span id="cb8-1311"><a href="#cb8-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1312"><a href="#cb8-1312" aria-hidden="true" tabindex="-1"></a>**Limitations and solutions:**</span>
<span id="cb8-1313"><a href="#cb8-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1314"><a href="#cb8-1314" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Local optima**: EM only finds a local maximum</span>
<span id="cb8-1315"><a href="#cb8-1315" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Solution**: Run from multiple random initializations</span>
<span id="cb8-1316"><a href="#cb8-1316" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Smart initialization**: Use <span class="co">[</span><span class="ot">k-means++</span><span class="co">](https://en.wikipedia.org/wiki/K-means%2B%2B)</span> or similar</span>
<span id="cb8-1317"><a href="#cb8-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1318"><a href="#cb8-1318" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Slow convergence**: Can take many iterations near the optimum</span>
<span id="cb8-1319"><a href="#cb8-1319" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Solution**: Switch to Newton's method near convergence</span>
<span id="cb8-1320"><a href="#cb8-1320" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Early stopping**: Monitor log-likelihood changes</span>
<span id="cb8-1321"><a href="#cb8-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1322"><a href="#cb8-1322" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Choosing number of components**: How many clusters?</span>
<span id="cb8-1323"><a href="#cb8-1323" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Solution**: Use information criteria (AIC, BIC)</span>
<span id="cb8-1324"><a href="#cb8-1324" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Cross-validation**: Evaluate on held-out data</span>
<span id="cb8-1325"><a href="#cb8-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1326"><a href="#cb8-1326" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb8-1327"><a href="#cb8-1327" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Initialization Trap</span></span>
<span id="cb8-1328"><a href="#cb8-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1329"><a href="#cb8-1329" aria-hidden="true" tabindex="-1"></a>The EM algorithm is extremely sensitive to initialization. Poor starting values can lead to:</span>
<span id="cb8-1330"><a href="#cb8-1330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1331"><a href="#cb8-1331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence to inferior local optima</span>
<span id="cb8-1332"><a href="#cb8-1332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One component "eating" all the data</span>
<span id="cb8-1333"><a href="#cb8-1333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Empty components (singularities)</span>
<span id="cb8-1334"><a href="#cb8-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1335"><a href="#cb8-1335" aria-hidden="true" tabindex="-1"></a>Always run EM multiple times with different initializations and choose the result with the highest likelihood!</span>
<span id="cb8-1336"><a href="#cb8-1336" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1337"><a href="#cb8-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1338"><a href="#cb8-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1339"><a href="#cb8-1339" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb8-1340"><a href="#cb8-1340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1341"><a href="#cb8-1341" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb8-1342"><a href="#cb8-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1343"><a href="#cb8-1343" aria-hidden="true" tabindex="-1"></a>We've explored the remarkable properties that make the Maximum Likelihood Estimator the "gold standard" of parametric estimation:</span>
<span id="cb8-1344"><a href="#cb8-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1345"><a href="#cb8-1345" aria-hidden="true" tabindex="-1"></a>**Core Properties of the MLE**:</span>
<span id="cb8-1346"><a href="#cb8-1346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1347"><a href="#cb8-1347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Consistency**: $\hat{\theta}_n \xrightarrow{P} \theta_*$ -- the MLE converges to the truth as $n \to \infty$.</span>
<span id="cb8-1348"><a href="#cb8-1348" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Equivariance**: $\widehat{g(\theta)} = g(\hat{\theta}_n)$ -- reparameterization doesn't affect the MLE.</span>
<span id="cb8-1349"><a href="#cb8-1349" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Asymptotic Normality**: $\hat{\theta}_n \approx \mathcal{N}(\theta, 1/I_n(\theta))$ -- enabling confidence intervals.</span>
<span id="cb8-1350"><a href="#cb8-1350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Asymptotic Efficiency**: Achieves the Cramér-Rao lower bound -- optimal variance.</span>
<span id="cb8-1351"><a href="#cb8-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1352"><a href="#cb8-1352" aria-hidden="true" tabindex="-1"></a>**Fisher Information**:</span>
<span id="cb8-1353"><a href="#cb8-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1354"><a href="#cb8-1354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measures the "information" about a parameter contained in data.</span>
<span id="cb8-1355"><a href="#cb8-1355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Equals the expected curvature of the log-likelihood: $I(\theta) = -\mathbb{E}<span class="co">[</span><span class="ot">\frac{\partial^2 \log f(X;\theta)}{\partial \theta^2}</span><span class="co">]</span>$.</span>
<span id="cb8-1356"><a href="#cb8-1356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Determines the precision of the MLE: $\text{Var}(\hat{\theta}_n) \approx 1/(nI(\theta))$.</span>
<span id="cb8-1357"><a href="#cb8-1357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sharp likelihood peak → High information → Precise estimates.</span>
<span id="cb8-1358"><a href="#cb8-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1359"><a href="#cb8-1359" aria-hidden="true" tabindex="-1"></a>**Deep Connections**:</span>
<span id="cb8-1360"><a href="#cb8-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1361"><a href="#cb8-1361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cross-entropy loss in ML = Negative log-likelihood.</span>
<span id="cb8-1362"><a href="#cb8-1362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Many ML algorithms are secretly performing MLE.</span>
<span id="cb8-1363"><a href="#cb8-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1364"><a href="#cb8-1364" aria-hidden="true" tabindex="-1"></a>**Practical Tools**:</span>
<span id="cb8-1365"><a href="#cb8-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1366"><a href="#cb8-1366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Confidence Intervals**: $\hat{\theta}_n \pm z_{\alpha/2} \cdot \widehat{\text{se}}$ where $\widehat{\text{se}} = 1/\sqrt{I_n(\hat{\theta}_n)}$.</span>
<span id="cb8-1367"><a href="#cb8-1367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Delta Method**: For transformed parameters $\tau = g(\theta)$: $\widehat{\text{se}}(\hat{\tau}) \approx |g'(\hat{\theta})| \cdot \widehat{\text{se}}(\hat{\theta})$.</span>
<span id="cb8-1368"><a href="#cb8-1368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**EM Algorithm**: Iterative method for MLEs with latent variables -- alternates between E-step (soft assignments) and M-step (parameter updates).</span>
<span id="cb8-1369"><a href="#cb8-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1370"><a href="#cb8-1370" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Big Picture</span></span>
<span id="cb8-1371"><a href="#cb8-1371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1372"><a href="#cb8-1372" aria-hidden="true" tabindex="-1"></a>We have moved from simply *finding* estimators (Chapter 5) to *evaluating their quality*. This chapter revealed why the MLE is so widely used:</span>
<span id="cb8-1373"><a href="#cb8-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1374"><a href="#cb8-1374" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**It works**: Consistency ensures we get the right answer with enough data.</span>
<span id="cb8-1375"><a href="#cb8-1375" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**It's optimal**: No other estimator has smaller asymptotic variance.</span>
<span id="cb8-1376"><a href="#cb8-1376" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**It's practical**: To a degree, we can quantify uncertainty through Fisher Information.^<span class="co">[</span><span class="ot">As we will see in future lectures, a Bayesian approach is often more suitable for proper uncertainty quantitication.</span><span class="co">]</span></span>
<span id="cb8-1377"><a href="#cb8-1377" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**It's flexible**: Equivariance means we can work in convenient parameterizations.</span>
<span id="cb8-1378"><a href="#cb8-1378" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**It extends**: The EM algorithm handles complex models with latent structure.</span>
<span id="cb8-1379"><a href="#cb8-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1380"><a href="#cb8-1380" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb8-1381"><a href="#cb8-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1382"><a href="#cb8-1382" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Misinterpreting Confidence Intervals**: A 95% CI does NOT mean:</span>
<span id="cb8-1383"><a href="#cb8-1383" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>"95% probability the parameter is in this interval" (that's Bayesian!).</span>
<span id="cb8-1384"><a href="#cb8-1384" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>"95% of the data falls in this interval" (that's a prediction interval).</span>
<span id="cb8-1385"><a href="#cb8-1385" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Correct: "95% of such intervals constructed from repeated samples would contain the true parameter".</span>
<span id="cb8-1386"><a href="#cb8-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1387"><a href="#cb8-1387" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Assuming Asymptotic Results for Small Samples**: </span>
<span id="cb8-1388"><a href="#cb8-1388" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Asymptotic normality may not hold for small $n$.</span>
<span id="cb8-1389"><a href="#cb8-1389" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fisher Information formulas are approximate for finite samples.</span>
<span id="cb8-1390"><a href="#cb8-1390" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bootstrap or exact methods may be preferable when $n &lt; 30$.^<span class="co">[</span><span class="ot">This is just a rule of thumb.</span><span class="co">]</span></span>
<span id="cb8-1391"><a href="#cb8-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1392"><a href="#cb8-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Getting Stuck in Local Optima with EM**:</span>
<span id="cb8-1393"><a href="#cb8-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>EM only guarantees local, not global, maximum.</span>
<span id="cb8-1394"><a href="#cb8-1394" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Always run from multiple starting points.</span>
<span id="cb8-1395"><a href="#cb8-1395" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Monitor log-likelihood to ensure proper convergence.</span>
<span id="cb8-1396"><a href="#cb8-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1397"><a href="#cb8-1397" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Forgetting About Model Assumptions**:</span>
<span id="cb8-1398"><a href="#cb8-1398" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>MLE theory assumes the model is correctly specified (i.e., the true distribution belongs to our model family).</span>
<span id="cb8-1399"><a href="#cb8-1399" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>We briefly mentioned that parameters must be identifiable (Section on Consistency) -- different parameter values must give different distributions.</span>
<span id="cb8-1400"><a href="#cb8-1400" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>The asymptotic results (normality, efficiency) require "regularity conditions" that we haven't detailed but include smoothness of the likelihood.</span>
<span id="cb8-1401"><a href="#cb8-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1402"><a href="#cb8-1402" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Over-interpreting Asymptotic Results**:</span>
<span id="cb8-1403"><a href="#cb8-1403" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>All our results (asymptotic normality, efficiency) are for large samples.</span>
<span id="cb8-1404"><a href="#cb8-1404" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>With small samples, the MLE might not be normally distributed and confidence intervals may be inaccurate.</span>
<span id="cb8-1405"><a href="#cb8-1405" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>The MLE's optimality is theoretical -- in practice, we might prefer more robust methods when data contains outliers or model assumptions are questionable.</span>
<span id="cb8-1406"><a href="#cb8-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1407"><a href="#cb8-1407" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb8-1408"><a href="#cb8-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1409"><a href="#cb8-1409" aria-hidden="true" tabindex="-1"></a>**Building on Previous Chapters**:</span>
<span id="cb8-1410"><a href="#cb8-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1411"><a href="#cb8-1411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chapter 3 (Statistical Inference)**: Provided the framework for evaluating estimators. Now we have seen that MLEs have optimal properties within this framework.</span>
<span id="cb8-1412"><a href="#cb8-1412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chapter 4 (Bootstrap)**: Offers a computational alternative to the analytical standard errors we derived here. When Fisher Information is hard to compute, bootstrap it!</span>
<span id="cb8-1413"><a href="#cb8-1413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chapter 5 (Finding Estimators)**: Showed how to find MLEs. This chapter justifies why we bother -- they have provably good properties.</span>
<span id="cb8-1414"><a href="#cb8-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1415"><a href="#cb8-1415" aria-hidden="true" tabindex="-1"></a>**Looking Ahead**:</span>
<span id="cb8-1416"><a href="#cb8-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1417"><a href="#cb8-1417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bayesian Inference**: While MLE obtains point estimates and confidence intervals, Bayesian methods provide full probability distributions over parameters. The MLE appears as the mode of the posterior with uniform priors.</span>
<span id="cb8-1418"><a href="#cb8-1418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regression Models**: The theory in this chapter extends directly -- least squares is MLE for normal errors, logistic regression is MLE for binary outcomes.</span>
<span id="cb8-1419"><a href="#cb8-1419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model Selection**: Information criteria (AIC, BIC) build on the likelihood framework we've developed.</span>
<span id="cb8-1420"><a href="#cb8-1420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Robust Statistics**: When MLE assumptions fail, we need methods that sacrifice some efficiency for robustness.</span>
<span id="cb8-1421"><a href="#cb8-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1422"><a href="#cb8-1422" aria-hidden="true" tabindex="-1"></a>The properties we've studied -- consistency, asymptotic normality, efficiency -- will reappear throughout statistics. Whether you're fitting neural networks or analyzing clinical trials, you're likely using some form of MLE, and the theory in this chapter explains why it works.</span>
<span id="cb8-1423"><a href="#cb8-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1424"><a href="#cb8-1424" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb8-1425"><a href="#cb8-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1426"><a href="#cb8-1426" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Fisher Information**: For $X_1, \ldots, X_n \sim \text{Exponential}(\lambda)$ with PDF $f(x; \lambda) = \lambda e^{-\lambda x}$ for $x &gt; 0$:</span>
<span id="cb8-1427"><a href="#cb8-1427" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Find the Fisher Information $I(\lambda)$ for a single observation.</span>
<span id="cb8-1428"><a href="#cb8-1428" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>What is the approximate standard error of the MLE $\hat{\lambda}_n$?</span>
<span id="cb8-1429"><a href="#cb8-1429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1430"><a href="#cb8-1430" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Confidence Intervals**: You flip a coin 100 times and observe 58 heads. Use Fisher Information to construct an approximate 95% confidence interval for the probability of heads.</span>
<span id="cb8-1431"><a href="#cb8-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1432"><a href="#cb8-1432" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Delta Method**: If $\hat{p} = 0.4$ with standard error 0.05, find the standard error of $\hat{\tau} = \log(p/(1-p))$ using the Delta Method.</span>
<span id="cb8-1433"><a href="#cb8-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1434"><a href="#cb8-1434" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**EM Intuition**: In a two-component Gaussian mixture, the E-step computes "responsibilities" for each data point. What do these represent? Why can't we just assign each point to its most likely cluster?</span>
<span id="cb8-1435"><a href="#cb8-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1436"><a href="#cb8-1436" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**MLE Properties**: True or False (with brief explanation):</span>
<span id="cb8-1437"><a href="#cb8-1437" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>If $\hat{\theta}_n$ is the MLE of $\theta$, then $\hat{\theta}_n^2$ is the MLE of $\theta^2$.</span>
<span id="cb8-1438"><a href="#cb8-1438" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>The MLE is always unbiased.</span>
<span id="cb8-1439"><a href="#cb8-1439" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fisher Information measures how "peaked" the likelihood function is.</span>
<span id="cb8-1440"><a href="#cb8-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1441"><a href="#cb8-1441" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb8-1442"><a href="#cb8-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1443"><a href="#cb8-1443" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb8-1444"><a href="#cb8-1444" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb8-1445"><a href="#cb8-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1446"><a href="#cb8-1446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb8-1447"><a href="#cb8-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1448"><a href="#cb8-1448" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb8-1449"><a href="#cb8-1449" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb8-1450"><a href="#cb8-1450" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-1451"><a href="#cb8-1451" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb8-1452"><a href="#cb8-1452" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize <span class="im">as</span> optimize</span>
<span id="cb8-1453"><a href="#cb8-1453" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb8-1454"><a href="#cb8-1454" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb8-1455"><a href="#cb8-1455" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb8-1456"><a href="#cb8-1456" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb8-1457"><a href="#cb8-1457" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb8-1458"><a href="#cb8-1458" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb8-1459"><a href="#cb8-1459" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.scipy.stats <span class="im">as</span> jstats</span>
<span id="cb8-1460"><a href="#cb8-1460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1461"><a href="#cb8-1461" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information calculation (symbolic)</span></span>
<span id="cb8-1462"><a href="#cb8-1462" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fisher_info_symbolic():</span>
<span id="cb8-1463"><a href="#cb8-1463" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example: Fisher Information for Exponential distribution"""</span></span>
<span id="cb8-1464"><a href="#cb8-1464" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define symbols</span></span>
<span id="cb8-1465"><a href="#cb8-1465" aria-hidden="true" tabindex="-1"></a>    x, lam <span class="op">=</span> sp.symbols(<span class="st">'x lambda'</span>, positive<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-1466"><a href="#cb8-1466" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1467"><a href="#cb8-1467" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define log pdf</span></span>
<span id="cb8-1468"><a href="#cb8-1468" aria-hidden="true" tabindex="-1"></a>    log_pdf <span class="op">=</span> sp.log(lam) <span class="op">-</span> lam <span class="op">*</span> x</span>
<span id="cb8-1469"><a href="#cb8-1469" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1470"><a href="#cb8-1470" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Score function (first derivative)</span></span>
<span id="cb8-1471"><a href="#cb8-1471" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> sp.diff(log_pdf, lam)</span>
<span id="cb8-1472"><a href="#cb8-1472" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1473"><a href="#cb8-1473" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second derivative</span></span>
<span id="cb8-1474"><a href="#cb8-1474" aria-hidden="true" tabindex="-1"></a>    second_deriv <span class="op">=</span> sp.diff(score, lam)</span>
<span id="cb8-1475"><a href="#cb8-1475" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1476"><a href="#cb8-1476" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fisher Information (negative expectation)</span></span>
<span id="cb8-1477"><a href="#cb8-1477" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For exponential: E[X] = 1/lambda</span></span>
<span id="cb8-1478"><a href="#cb8-1478" aria-hidden="true" tabindex="-1"></a>    fisher_info <span class="op">=</span> <span class="op">-</span>second_deriv.subs(x, <span class="dv">1</span><span class="op">/</span>lam)</span>
<span id="cb8-1479"><a href="#cb8-1479" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sp.simplify(fisher_info)</span>
<span id="cb8-1480"><a href="#cb8-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1481"><a href="#cb8-1481" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information using JAX (automatic differentiation)</span></span>
<span id="cb8-1482"><a href="#cb8-1482" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fisher_info_jax(log_pdf, theta, n_samples<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb8-1483"><a href="#cb8-1483" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Estimate Fisher Information using automatic differentiation"""</span></span>
<span id="cb8-1484"><a href="#cb8-1484" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the Hessian (second derivatives)</span></span>
<span id="cb8-1485"><a href="#cb8-1485" aria-hidden="true" tabindex="-1"></a>    hessian <span class="op">=</span> jax.hessian(log_pdf)</span>
<span id="cb8-1486"><a href="#cb8-1486" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1487"><a href="#cb8-1487" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate samples and compute expectation</span></span>
<span id="cb8-1488"><a href="#cb8-1488" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb8-1489"><a href="#cb8-1489" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is problem-specific - would need appropriate sampler</span></span>
<span id="cb8-1490"><a href="#cb8-1490" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example continues with general structure</span></span>
<span id="cb8-1491"><a href="#cb8-1491" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1492"><a href="#cb8-1492" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>jnp.mean(hessian(theta))</span>
<span id="cb8-1493"><a href="#cb8-1493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1494"><a href="#cb8-1494" aria-hidden="true" tabindex="-1"></a><span class="co"># Confidence intervals with Fisher Information</span></span>
<span id="cb8-1495"><a href="#cb8-1495" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mle_confidence_interval(mle, fisher_info_n, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb8-1496"><a href="#cb8-1496" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Construct CI using Fisher Information"""</span></span>
<span id="cb8-1497"><a href="#cb8-1497" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(fisher_info_n)</span>
<span id="cb8-1498"><a href="#cb8-1498" aria-hidden="true" tabindex="-1"></a>    z_crit <span class="op">=</span> stats.norm.ppf(<span class="dv">1</span> <span class="op">-</span> alpha<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb8-1499"><a href="#cb8-1499" aria-hidden="true" tabindex="-1"></a>    ci_lower <span class="op">=</span> mle <span class="op">-</span> z_crit <span class="op">*</span> se</span>
<span id="cb8-1500"><a href="#cb8-1500" aria-hidden="true" tabindex="-1"></a>    ci_upper <span class="op">=</span> mle <span class="op">+</span> z_crit <span class="op">*</span> se</span>
<span id="cb8-1501"><a href="#cb8-1501" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ci_lower, ci_upper</span>
<span id="cb8-1502"><a href="#cb8-1502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1503"><a href="#cb8-1503" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta Method implementation</span></span>
<span id="cb8-1504"><a href="#cb8-1504" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> delta_method_se(mle, se_mle, g_derivative):</span>
<span id="cb8-1505"><a href="#cb8-1505" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-1506"><a href="#cb8-1506" aria-hidden="true" tabindex="-1"></a><span class="co">    Apply Delta Method to find SE of transformed parameter</span></span>
<span id="cb8-1507"><a href="#cb8-1507" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-1508"><a href="#cb8-1508" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb8-1509"><a href="#cb8-1509" aria-hidden="true" tabindex="-1"></a><span class="co">    - mle: MLE of original parameter</span></span>
<span id="cb8-1510"><a href="#cb8-1510" aria-hidden="true" tabindex="-1"></a><span class="co">    - se_mle: Standard error of MLE</span></span>
<span id="cb8-1511"><a href="#cb8-1511" aria-hidden="true" tabindex="-1"></a><span class="co">    - g_derivative: Derivative of transformation g evaluated at MLE</span></span>
<span id="cb8-1512"><a href="#cb8-1512" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-1513"><a href="#cb8-1513" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">abs</span>(g_derivative) <span class="op">*</span> se_mle</span>
<span id="cb8-1514"><a href="#cb8-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1515"><a href="#cb8-1515" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Poisson rate to mean waiting time</span></span>
<span id="cb8-1516"><a href="#cb8-1516" aria-hidden="true" tabindex="-1"></a>lam_hat <span class="op">=</span> <span class="fl">3.0</span>  <span class="co"># MLE for rate</span></span>
<span id="cb8-1517"><a href="#cb8-1517" aria-hidden="true" tabindex="-1"></a>se_lam <span class="op">=</span> <span class="fl">0.3</span>   <span class="co"># Standard error</span></span>
<span id="cb8-1518"><a href="#cb8-1518" aria-hidden="true" tabindex="-1"></a><span class="co"># For tau = 1/lambda, g'(lambda) = -1/lambda^2</span></span>
<span id="cb8-1519"><a href="#cb8-1519" aria-hidden="true" tabindex="-1"></a>g_prime <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">/</span> lam_hat<span class="op">**</span><span class="dv">2</span></span>
<span id="cb8-1520"><a href="#cb8-1520" aria-hidden="true" tabindex="-1"></a>se_tau <span class="op">=</span> delta_method_se(lam_hat, se_lam, g_prime)</span>
<span id="cb8-1521"><a href="#cb8-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1522"><a href="#cb8-1522" aria-hidden="true" tabindex="-1"></a><span class="co"># EM Algorithm using sklearn</span></span>
<span id="cb8-1523"><a href="#cb8-1523" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_gaussian_mixture(data, n_components<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb8-1524"><a href="#cb8-1524" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Fit Gaussian Mixture Model using EM"""</span></span>
<span id="cb8-1525"><a href="#cb8-1525" aria-hidden="true" tabindex="-1"></a>    gmm <span class="op">=</span> GaussianMixture(</span>
<span id="cb8-1526"><a href="#cb8-1526" aria-hidden="true" tabindex="-1"></a>        n_components<span class="op">=</span>n_components,</span>
<span id="cb8-1527"><a href="#cb8-1527" aria-hidden="true" tabindex="-1"></a>        n_init<span class="op">=</span>n_init,  <span class="co"># Number of initializations</span></span>
<span id="cb8-1528"><a href="#cb8-1528" aria-hidden="true" tabindex="-1"></a>        init_params<span class="op">=</span><span class="st">'k-means++'</span>,  <span class="co"># Smart initialization</span></span>
<span id="cb8-1529"><a href="#cb8-1529" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb8-1530"><a href="#cb8-1530" aria-hidden="true" tabindex="-1"></a>        tol<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb8-1531"><a href="#cb8-1531" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb8-1532"><a href="#cb8-1532" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-1533"><a href="#cb8-1533" aria-hidden="true" tabindex="-1"></a>    gmm.fit(data)</span>
<span id="cb8-1534"><a href="#cb8-1534" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1535"><a href="#cb8-1535" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb8-1536"><a href="#cb8-1536" aria-hidden="true" tabindex="-1"></a>        <span class="st">'means'</span>: gmm.means_,</span>
<span id="cb8-1537"><a href="#cb8-1537" aria-hidden="true" tabindex="-1"></a>        <span class="st">'covariances'</span>: gmm.covariances_,</span>
<span id="cb8-1538"><a href="#cb8-1538" aria-hidden="true" tabindex="-1"></a>        <span class="st">'weights'</span>: gmm.weights_,</span>
<span id="cb8-1539"><a href="#cb8-1539" aria-hidden="true" tabindex="-1"></a>        <span class="st">'log_likelihood'</span>: gmm.score(data) <span class="op">*</span> <span class="bu">len</span>(data),</span>
<span id="cb8-1540"><a href="#cb8-1540" aria-hidden="true" tabindex="-1"></a>        <span class="st">'converged'</span>: gmm.converged_</span>
<span id="cb8-1541"><a href="#cb8-1541" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-1542"><a href="#cb8-1542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1543"><a href="#cb8-1543" aria-hidden="true" tabindex="-1"></a><span class="co"># Advanced models with confidence intervals</span></span>
<span id="cb8-1544"><a href="#cb8-1544" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_model_with_cis(model, data):</span>
<span id="cb8-1545"><a href="#cb8-1545" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example using statsmodels for automatic CIs"""</span></span>
<span id="cb8-1546"><a href="#cb8-1546" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: Poisson regression</span></span>
<span id="cb8-1547"><a href="#cb8-1547" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.fit()</span>
<span id="cb8-1548"><a href="#cb8-1548" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1549"><a href="#cb8-1549" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract MLEs and standard errors</span></span>
<span id="cb8-1550"><a href="#cb8-1550" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> results.params</span>
<span id="cb8-1551"><a href="#cb8-1551" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> results.bse</span>
<span id="cb8-1552"><a href="#cb8-1552" aria-hidden="true" tabindex="-1"></a>    conf_int <span class="op">=</span> results.conf_int(alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb8-1553"><a href="#cb8-1553" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1554"><a href="#cb8-1554" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fisher Information matrix</span></span>
<span id="cb8-1555"><a href="#cb8-1555" aria-hidden="true" tabindex="-1"></a>    fisher_info <span class="op">=</span> results.cov_params()</span>
<span id="cb8-1556"><a href="#cb8-1556" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1557"><a href="#cb8-1557" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb8-1558"><a href="#cb8-1558" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mle'</span>: params,</span>
<span id="cb8-1559"><a href="#cb8-1559" aria-hidden="true" tabindex="-1"></a>        <span class="st">'se'</span>: se,</span>
<span id="cb8-1560"><a href="#cb8-1560" aria-hidden="true" tabindex="-1"></a>        <span class="st">'confidence_intervals'</span>: conf_int,</span>
<span id="cb8-1561"><a href="#cb8-1561" aria-hidden="true" tabindex="-1"></a>        <span class="st">'fisher_info_inverse'</span>: fisher_info</span>
<span id="cb8-1562"><a href="#cb8-1562" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-1563"><a href="#cb8-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1564"><a href="#cb8-1564" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual EM implementation for mixture of normals</span></span>
<span id="cb8-1565"><a href="#cb8-1565" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianMixtureEM:</span>
<span id="cb8-1566"><a href="#cb8-1566" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_components<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb8-1567"><a href="#cb8-1567" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_components <span class="op">=</span> n_components</span>
<span id="cb8-1568"><a href="#cb8-1568" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-1569"><a href="#cb8-1569" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> e_step(<span class="va">self</span>, X, means, covs, weights):</span>
<span id="cb8-1570"><a href="#cb8-1570" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute responsibilities (soft assignments)"""</span></span>
<span id="cb8-1571"><a href="#cb8-1571" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb8-1572"><a href="#cb8-1572" aria-hidden="true" tabindex="-1"></a>        resp <span class="op">=</span> np.zeros((n_samples, <span class="va">self</span>.n_components))</span>
<span id="cb8-1573"><a href="#cb8-1573" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-1574"><a href="#cb8-1574" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_components):</span>
<span id="cb8-1575"><a href="#cb8-1575" aria-hidden="true" tabindex="-1"></a>            resp[:, k] <span class="op">=</span> weights[k] <span class="op">*</span> multivariate_normal.pdf(</span>
<span id="cb8-1576"><a href="#cb8-1576" aria-hidden="true" tabindex="-1"></a>                X, means[k], covs[k]</span>
<span id="cb8-1577"><a href="#cb8-1577" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-1578"><a href="#cb8-1578" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-1579"><a href="#cb8-1579" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize</span></span>
<span id="cb8-1580"><a href="#cb8-1580" aria-hidden="true" tabindex="-1"></a>        resp <span class="op">/=</span> resp.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-1581"><a href="#cb8-1581" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> resp</span>
<span id="cb8-1582"><a href="#cb8-1582" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1583"><a href="#cb8-1583" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> m_step(<span class="va">self</span>, X, resp):</span>
<span id="cb8-1584"><a href="#cb8-1584" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Update parameters given responsibilities"""</span></span>
<span id="cb8-1585"><a href="#cb8-1585" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb8-1586"><a href="#cb8-1586" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-1587"><a href="#cb8-1587" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights</span></span>
<span id="cb8-1588"><a href="#cb8-1588" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> resp.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> n_samples</span>
<span id="cb8-1589"><a href="#cb8-1589" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-1590"><a href="#cb8-1590" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update means</span></span>
<span id="cb8-1591"><a href="#cb8-1591" aria-hidden="true" tabindex="-1"></a>        means <span class="op">=</span> []</span>
<span id="cb8-1592"><a href="#cb8-1592" aria-hidden="true" tabindex="-1"></a>        covs <span class="op">=</span> []</span>
<span id="cb8-1593"><a href="#cb8-1593" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_components):</span>
<span id="cb8-1594"><a href="#cb8-1594" aria-hidden="true" tabindex="-1"></a>            resp_k <span class="op">=</span> resp[:, k]</span>
<span id="cb8-1595"><a href="#cb8-1595" aria-hidden="true" tabindex="-1"></a>            mean_k <span class="op">=</span> (resp_k[:, np.newaxis] <span class="op">*</span> X).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb8-1596"><a href="#cb8-1596" aria-hidden="true" tabindex="-1"></a>            means.append(mean_k)</span>
<span id="cb8-1597"><a href="#cb8-1597" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb8-1598"><a href="#cb8-1598" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update covariances</span></span>
<span id="cb8-1599"><a href="#cb8-1599" aria-hidden="true" tabindex="-1"></a>            diff <span class="op">=</span> X <span class="op">-</span> mean_k</span>
<span id="cb8-1600"><a href="#cb8-1600" aria-hidden="true" tabindex="-1"></a>            cov_k <span class="op">=</span> (resp_k[:, np.newaxis, np.newaxis] <span class="op">*</span> </span>
<span id="cb8-1601"><a href="#cb8-1601" aria-hidden="true" tabindex="-1"></a>                    diff[:, :, np.newaxis] <span class="op">@</span> diff[:, np.newaxis, :]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-1602"><a href="#cb8-1602" aria-hidden="true" tabindex="-1"></a>            cov_k <span class="op">/=</span> resp_k.<span class="bu">sum</span>()</span>
<span id="cb8-1603"><a href="#cb8-1603" aria-hidden="true" tabindex="-1"></a>            covs.append(cov_k)</span>
<span id="cb8-1604"><a href="#cb8-1604" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb8-1605"><a href="#cb8-1605" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(means), covs, weights</span>
<span id="cb8-1606"><a href="#cb8-1606" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-1607"><a href="#cb8-1607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1608"><a href="#cb8-1608" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb8-1609"><a href="#cb8-1609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1610"><a href="#cb8-1610" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb8-1611"><a href="#cb8-1611" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb8-1612"><a href="#cb8-1612" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)      <span class="co"># For fitdistr</span></span>
<span id="cb8-1613"><a href="#cb8-1613" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stats4)    <span class="co"># For mle function</span></span>
<span id="cb8-1614"><a href="#cb8-1614" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(numDeriv)  <span class="co"># For numerical derivatives</span></span>
<span id="cb8-1615"><a href="#cb8-1615" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mclust)    <span class="co"># For Gaussian mixture models</span></span>
<span id="cb8-1616"><a href="#cb8-1616" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(msm)       <span class="co"># For deltamethod function</span></span>
<span id="cb8-1617"><a href="#cb8-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1618"><a href="#cb8-1618" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information calculation</span></span>
<span id="cb8-1619"><a href="#cb8-1619" aria-hidden="true" tabindex="-1"></a>fisher_info_exponential <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda) {</span>
<span id="cb8-1620"><a href="#cb8-1620" aria-hidden="true" tabindex="-1"></a>  <span class="co"># For Exponential(lambda): I(lambda) = 1/lambda^2</span></span>
<span id="cb8-1621"><a href="#cb8-1621" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> lambda<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-1622"><a href="#cb8-1622" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1623"><a href="#cb8-1623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1624"><a href="#cb8-1624" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher Information via numerical methods</span></span>
<span id="cb8-1625"><a href="#cb8-1625" aria-hidden="true" tabindex="-1"></a>fisher_info_numerical <span class="ot">&lt;-</span> <span class="cf">function</span>(loglik_fn, theta, ...) {</span>
<span id="cb8-1626"><a href="#cb8-1626" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute negative expected Hessian</span></span>
<span id="cb8-1627"><a href="#cb8-1627" aria-hidden="true" tabindex="-1"></a>  hess <span class="ot">&lt;-</span> <span class="fu">hessian</span>(loglik_fn, theta, ...)</span>
<span id="cb8-1628"><a href="#cb8-1628" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>hess)</span>
<span id="cb8-1629"><a href="#cb8-1629" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1630"><a href="#cb8-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1631"><a href="#cb8-1631" aria-hidden="true" tabindex="-1"></a><span class="co"># MLE with standard errors using fitdistr</span></span>
<span id="cb8-1632"><a href="#cb8-1632" aria-hidden="true" tabindex="-1"></a>fit_with_se <span class="ot">&lt;-</span> <span class="cf">function</span>(data, distribution) {</span>
<span id="cb8-1633"><a href="#cb8-1633" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(data, distribution)</span>
<span id="cb8-1634"><a href="#cb8-1634" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1635"><a href="#cb8-1635" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract estimates and standard errors</span></span>
<span id="cb8-1636"><a href="#cb8-1636" aria-hidden="true" tabindex="-1"></a>  mle <span class="ot">&lt;-</span> fit<span class="sc">$</span>estimate</span>
<span id="cb8-1637"><a href="#cb8-1637" aria-hidden="true" tabindex="-1"></a>  se <span class="ot">&lt;-</span> fit<span class="sc">$</span>sd</span>
<span id="cb8-1638"><a href="#cb8-1638" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1639"><a href="#cb8-1639" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Construct confidence intervals</span></span>
<span id="cb8-1640"><a href="#cb8-1640" aria-hidden="true" tabindex="-1"></a>  ci_lower <span class="ot">&lt;-</span> mle <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se</span>
<span id="cb8-1641"><a href="#cb8-1641" aria-hidden="true" tabindex="-1"></a>  ci_upper <span class="ot">&lt;-</span> mle <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se</span>
<span id="cb8-1642"><a href="#cb8-1642" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1643"><a href="#cb8-1643" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb8-1644"><a href="#cb8-1644" aria-hidden="true" tabindex="-1"></a>    <span class="at">mle =</span> mle,</span>
<span id="cb8-1645"><a href="#cb8-1645" aria-hidden="true" tabindex="-1"></a>    <span class="at">se =</span> se,</span>
<span id="cb8-1646"><a href="#cb8-1646" aria-hidden="true" tabindex="-1"></a>    <span class="at">ci =</span> <span class="fu">cbind</span>(<span class="at">lower =</span> ci_lower, <span class="at">upper =</span> ci_upper),</span>
<span id="cb8-1647"><a href="#cb8-1647" aria-hidden="true" tabindex="-1"></a>    <span class="at">loglik =</span> fit<span class="sc">$</span>loglik</span>
<span id="cb8-1648"><a href="#cb8-1648" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-1649"><a href="#cb8-1649" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1650"><a href="#cb8-1650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1651"><a href="#cb8-1651" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta Method implementation</span></span>
<span id="cb8-1652"><a href="#cb8-1652" aria-hidden="true" tabindex="-1"></a>apply_delta_method <span class="ot">&lt;-</span> <span class="cf">function</span>(mle, var_mle, g_expr, param_name) {</span>
<span id="cb8-1653"><a href="#cb8-1653" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Using msm::deltamethod</span></span>
<span id="cb8-1654"><a href="#cb8-1654" aria-hidden="true" tabindex="-1"></a>  <span class="co"># g_expr: expression for g(theta) as a string</span></span>
<span id="cb8-1655"><a href="#cb8-1655" aria-hidden="true" tabindex="-1"></a>  <span class="co"># param_name: name of parameter in expression</span></span>
<span id="cb8-1656"><a href="#cb8-1656" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1657"><a href="#cb8-1657" aria-hidden="true" tabindex="-1"></a>  se_transformed <span class="ot">&lt;-</span> <span class="fu">deltamethod</span>(</span>
<span id="cb8-1658"><a href="#cb8-1658" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, g_expr)), </span>
<span id="cb8-1659"><a href="#cb8-1659" aria-hidden="true" tabindex="-1"></a>    mle, </span>
<span id="cb8-1660"><a href="#cb8-1660" aria-hidden="true" tabindex="-1"></a>    var_mle</span>
<span id="cb8-1661"><a href="#cb8-1661" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-1662"><a href="#cb8-1662" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1663"><a href="#cb8-1663" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(se_transformed)</span>
<span id="cb8-1664"><a href="#cb8-1664" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1665"><a href="#cb8-1665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1666"><a href="#cb8-1666" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Delta method for Poisson</span></span>
<span id="cb8-1667"><a href="#cb8-1667" aria-hidden="true" tabindex="-1"></a>lambda_hat <span class="ot">&lt;-</span> <span class="fl">3.0</span></span>
<span id="cb8-1668"><a href="#cb8-1668" aria-hidden="true" tabindex="-1"></a>se_lambda <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb8-1669"><a href="#cb8-1669" aria-hidden="true" tabindex="-1"></a>var_lambda <span class="ot">&lt;-</span> se_lambda<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb8-1670"><a href="#cb8-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1671"><a href="#cb8-1671" aria-hidden="true" tabindex="-1"></a><span class="co"># For tau = 1/lambda</span></span>
<span id="cb8-1672"><a href="#cb8-1672" aria-hidden="true" tabindex="-1"></a>se_tau <span class="ot">&lt;-</span> <span class="fu">deltamethod</span>(<span class="sc">~</span> <span class="dv">1</span><span class="sc">/</span>x1, lambda_hat, var_lambda)</span>
<span id="cb8-1673"><a href="#cb8-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1674"><a href="#cb8-1674" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual Delta Method</span></span>
<span id="cb8-1675"><a href="#cb8-1675" aria-hidden="true" tabindex="-1"></a>delta_method_manual <span class="ot">&lt;-</span> <span class="cf">function</span>(mle, se_mle, g_deriv) {</span>
<span id="cb8-1676"><a href="#cb8-1676" aria-hidden="true" tabindex="-1"></a>  <span class="co"># g_deriv: derivative of g evaluated at MLE</span></span>
<span id="cb8-1677"><a href="#cb8-1677" aria-hidden="true" tabindex="-1"></a>  se_transformed <span class="ot">&lt;-</span> <span class="fu">abs</span>(g_deriv) <span class="sc">*</span> se_mle</span>
<span id="cb8-1678"><a href="#cb8-1678" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(se_transformed)</span>
<span id="cb8-1679"><a href="#cb8-1679" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1680"><a href="#cb8-1680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1681"><a href="#cb8-1681" aria-hidden="true" tabindex="-1"></a><span class="co"># EM Algorithm using mclust</span></span>
<span id="cb8-1682"><a href="#cb8-1682" aria-hidden="true" tabindex="-1"></a>fit_gaussian_mixture <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">G =</span> <span class="dv">2</span>) {</span>
<span id="cb8-1683"><a href="#cb8-1683" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit Gaussian mixture model</span></span>
<span id="cb8-1684"><a href="#cb8-1684" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(data, <span class="at">G =</span> G)</span>
<span id="cb8-1685"><a href="#cb8-1685" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1686"><a href="#cb8-1686" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb8-1687"><a href="#cb8-1687" aria-hidden="true" tabindex="-1"></a>    <span class="at">means =</span> fit<span class="sc">$</span>parameters<span class="sc">$</span>mean,</span>
<span id="cb8-1688"><a href="#cb8-1688" aria-hidden="true" tabindex="-1"></a>    <span class="at">covariances =</span> fit<span class="sc">$</span>parameters<span class="sc">$</span>variance<span class="sc">$</span>sigma,</span>
<span id="cb8-1689"><a href="#cb8-1689" aria-hidden="true" tabindex="-1"></a>    <span class="at">weights =</span> fit<span class="sc">$</span>parameters<span class="sc">$</span>pro,</span>
<span id="cb8-1690"><a href="#cb8-1690" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_likelihood =</span> fit<span class="sc">$</span>loglik,</span>
<span id="cb8-1691"><a href="#cb8-1691" aria-hidden="true" tabindex="-1"></a>    <span class="at">classification =</span> fit<span class="sc">$</span>classification,</span>
<span id="cb8-1692"><a href="#cb8-1692" aria-hidden="true" tabindex="-1"></a>    <span class="at">uncertainty =</span> fit<span class="sc">$</span>uncertainty</span>
<span id="cb8-1693"><a href="#cb8-1693" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-1694"><a href="#cb8-1694" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1695"><a href="#cb8-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1696"><a href="#cb8-1696" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual implementation of MLE with custom likelihood</span></span>
<span id="cb8-1697"><a href="#cb8-1697" aria-hidden="true" tabindex="-1"></a>mle_custom <span class="ot">&lt;-</span> <span class="cf">function</span>(data, start, nll_function) {</span>
<span id="cb8-1698"><a href="#cb8-1698" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Negative log-likelihood function</span></span>
<span id="cb8-1699"><a href="#cb8-1699" aria-hidden="true" tabindex="-1"></a>  nll <span class="ot">&lt;-</span> <span class="cf">function</span>(params) {</span>
<span id="cb8-1700"><a href="#cb8-1700" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nll_function</span>(params, data)</span>
<span id="cb8-1701"><a href="#cb8-1701" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb8-1702"><a href="#cb8-1702" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1703"><a href="#cb8-1703" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Optimize</span></span>
<span id="cb8-1704"><a href="#cb8-1704" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb8-1705"><a href="#cb8-1705" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> start,</span>
<span id="cb8-1706"><a href="#cb8-1706" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> nll,</span>
<span id="cb8-1707"><a href="#cb8-1707" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>,</span>
<span id="cb8-1708"><a href="#cb8-1708" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>  <span class="co"># Get Hessian for Fisher Information</span></span>
<span id="cb8-1709"><a href="#cb8-1709" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-1710"><a href="#cb8-1710" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1711"><a href="#cb8-1711" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fisher Information is the Hessian at MLE</span></span>
<span id="cb8-1712"><a href="#cb8-1712" aria-hidden="true" tabindex="-1"></a>  fisher_info <span class="ot">&lt;-</span> fit<span class="sc">$</span>hessian</span>
<span id="cb8-1713"><a href="#cb8-1713" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1714"><a href="#cb8-1714" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Standard errors from inverse Fisher Information</span></span>
<span id="cb8-1715"><a href="#cb8-1715" aria-hidden="true" tabindex="-1"></a>  se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">solve</span>(fisher_info)))</span>
<span id="cb8-1716"><a href="#cb8-1716" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1717"><a href="#cb8-1717" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb8-1718"><a href="#cb8-1718" aria-hidden="true" tabindex="-1"></a>    <span class="at">mle =</span> fit<span class="sc">$</span>par,</span>
<span id="cb8-1719"><a href="#cb8-1719" aria-hidden="true" tabindex="-1"></a>    <span class="at">se =</span> se,</span>
<span id="cb8-1720"><a href="#cb8-1720" aria-hidden="true" tabindex="-1"></a>    <span class="at">fisher_info =</span> fisher_info,</span>
<span id="cb8-1721"><a href="#cb8-1721" aria-hidden="true" tabindex="-1"></a>    <span class="at">convergence =</span> fit<span class="sc">$</span>convergence,</span>
<span id="cb8-1722"><a href="#cb8-1722" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_likelihood =</span> <span class="sc">-</span>fit<span class="sc">$</span>value</span>
<span id="cb8-1723"><a href="#cb8-1723" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-1724"><a href="#cb8-1724" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1725"><a href="#cb8-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1726"><a href="#cb8-1726" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Beta distribution MLE</span></span>
<span id="cb8-1727"><a href="#cb8-1727" aria-hidden="true" tabindex="-1"></a>beta_nll <span class="ot">&lt;-</span> <span class="cf">function</span>(params, data) {</span>
<span id="cb8-1728"><a href="#cb8-1728" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> params[<span class="dv">1</span>]</span>
<span id="cb8-1729"><a href="#cb8-1729" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> params[<span class="dv">2</span>]</span>
<span id="cb8-1730"><a href="#cb8-1730" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(alpha <span class="sc">&lt;=</span> <span class="dv">0</span> <span class="sc">||</span> beta <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="cn">Inf</span>)</span>
<span id="cb8-1731"><a href="#cb8-1731" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dbeta</span>(data, alpha, beta, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb8-1732"><a href="#cb8-1732" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1733"><a href="#cb8-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1734"><a href="#cb8-1734" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Beta distribution</span></span>
<span id="cb8-1735"><a href="#cb8-1735" aria-hidden="true" tabindex="-1"></a><span class="co"># data &lt;- rbeta(100, 1.5, 0.5)</span></span>
<span id="cb8-1736"><a href="#cb8-1736" aria-hidden="true" tabindex="-1"></a><span class="co"># fit &lt;- mle_custom(data, c(1, 1), beta_nll)</span></span>
<span id="cb8-1737"><a href="#cb8-1737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1738"><a href="#cb8-1738" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing profile likelihood confidence intervals</span></span>
<span id="cb8-1739"><a href="#cb8-1739" aria-hidden="true" tabindex="-1"></a>profile_ci <span class="ot">&lt;-</span> <span class="cf">function</span>(data, param_index, mle_full, nll_function, <span class="at">level =</span> <span class="fl">0.95</span>) {</span>
<span id="cb8-1740"><a href="#cb8-1740" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Critical value for likelihood ratio test</span></span>
<span id="cb8-1741"><a href="#cb8-1741" aria-hidden="true" tabindex="-1"></a>  crit <span class="ot">&lt;-</span> <span class="fu">qchisq</span>(level, <span class="at">df =</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb8-1742"><a href="#cb8-1742" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1743"><a href="#cb8-1743" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Profile negative log-likelihood</span></span>
<span id="cb8-1744"><a href="#cb8-1744" aria-hidden="true" tabindex="-1"></a>  profile_nll <span class="ot">&lt;-</span> <span class="cf">function</span>(param_value, other_params) {</span>
<span id="cb8-1745"><a href="#cb8-1745" aria-hidden="true" tabindex="-1"></a>    params <span class="ot">&lt;-</span> mle_full</span>
<span id="cb8-1746"><a href="#cb8-1746" aria-hidden="true" tabindex="-1"></a>    params[param_index] <span class="ot">&lt;-</span> param_value</span>
<span id="cb8-1747"><a href="#cb8-1747" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optimize over other parameters...</span></span>
<span id="cb8-1748"><a href="#cb8-1748" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (simplified here)</span></span>
<span id="cb8-1749"><a href="#cb8-1749" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nll_function</span>(params, data)</span>
<span id="cb8-1750"><a href="#cb8-1750" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb8-1751"><a href="#cb8-1751" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1752"><a href="#cb8-1752" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find confidence interval boundaries</span></span>
<span id="cb8-1753"><a href="#cb8-1753" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (simplified - would need root finding in practice)</span></span>
<span id="cb8-1754"><a href="#cb8-1754" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1755"><a href="#cb8-1755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1756"><a href="#cb8-1756" aria-hidden="true" tabindex="-1"></a><span class="co"># EM Algorithm manual implementation</span></span>
<span id="cb8-1757"><a href="#cb8-1757" aria-hidden="true" tabindex="-1"></a>em_gaussian_mixture <span class="ot">&lt;-</span> <span class="cf">function</span>(X, <span class="at">n_components =</span> <span class="dv">2</span>, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">tol =</span> <span class="fl">1e-4</span>) {</span>
<span id="cb8-1758"><a href="#cb8-1758" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb8-1759"><a href="#cb8-1759" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb8-1760"><a href="#cb8-1760" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1761"><a href="#cb8-1761" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize parameters</span></span>
<span id="cb8-1762"><a href="#cb8-1762" aria-hidden="true" tabindex="-1"></a>  km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(X, n_components)</span>
<span id="cb8-1763"><a href="#cb8-1763" aria-hidden="true" tabindex="-1"></a>  means <span class="ot">&lt;-</span> km<span class="sc">$</span>centers</span>
<span id="cb8-1764"><a href="#cb8-1764" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> <span class="fu">table</span>(km<span class="sc">$</span>cluster) <span class="sc">/</span> n</span>
<span id="cb8-1765"><a href="#cb8-1765" aria-hidden="true" tabindex="-1"></a>  covs <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb8-1766"><a href="#cb8-1766" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_components) {</span>
<span id="cb8-1767"><a href="#cb8-1767" aria-hidden="true" tabindex="-1"></a>    covs[[k]] <span class="ot">&lt;-</span> <span class="fu">cov</span>(X[km<span class="sc">$</span>cluster <span class="sc">==</span> k, ])</span>
<span id="cb8-1768"><a href="#cb8-1768" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb8-1769"><a href="#cb8-1769" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1770"><a href="#cb8-1770" aria-hidden="true" tabindex="-1"></a>  log_lik_old <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb8-1771"><a href="#cb8-1771" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1772"><a href="#cb8-1772" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_iter) {</span>
<span id="cb8-1773"><a href="#cb8-1773" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E-step: compute responsibilities</span></span>
<span id="cb8-1774"><a href="#cb8-1774" aria-hidden="true" tabindex="-1"></a>    resp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n, n_components)</span>
<span id="cb8-1775"><a href="#cb8-1775" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_components) {</span>
<span id="cb8-1776"><a href="#cb8-1776" aria-hidden="true" tabindex="-1"></a>      resp[, k] <span class="ot">&lt;-</span> weights[k] <span class="sc">*</span> <span class="fu">dmvnorm</span>(X, means[k, ], covs[[k]])</span>
<span id="cb8-1777"><a href="#cb8-1777" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-1778"><a href="#cb8-1778" aria-hidden="true" tabindex="-1"></a>    resp <span class="ot">&lt;-</span> resp <span class="sc">/</span> <span class="fu">rowSums</span>(resp)</span>
<span id="cb8-1779"><a href="#cb8-1779" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1780"><a href="#cb8-1780" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log-likelihood</span></span>
<span id="cb8-1781"><a href="#cb8-1781" aria-hidden="true" tabindex="-1"></a>    log_lik <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">rowSums</span>(resp)))</span>
<span id="cb8-1782"><a href="#cb8-1782" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">abs</span>(log_lik <span class="sc">-</span> log_lik_old) <span class="sc">&lt;</span> tol) <span class="cf">break</span></span>
<span id="cb8-1783"><a href="#cb8-1783" aria-hidden="true" tabindex="-1"></a>    log_lik_old <span class="ot">&lt;-</span> log_lik</span>
<span id="cb8-1784"><a href="#cb8-1784" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-1785"><a href="#cb8-1785" aria-hidden="true" tabindex="-1"></a>    <span class="co"># M-step: update parameters</span></span>
<span id="cb8-1786"><a href="#cb8-1786" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_components) {</span>
<span id="cb8-1787"><a href="#cb8-1787" aria-hidden="true" tabindex="-1"></a>      nk <span class="ot">&lt;-</span> <span class="fu">sum</span>(resp[, k])</span>
<span id="cb8-1788"><a href="#cb8-1788" aria-hidden="true" tabindex="-1"></a>      weights[k] <span class="ot">&lt;-</span> nk <span class="sc">/</span> n</span>
<span id="cb8-1789"><a href="#cb8-1789" aria-hidden="true" tabindex="-1"></a>      means[k, ] <span class="ot">&lt;-</span> <span class="fu">colSums</span>(resp[, k] <span class="sc">*</span> X) <span class="sc">/</span> nk</span>
<span id="cb8-1790"><a href="#cb8-1790" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb8-1791"><a href="#cb8-1791" aria-hidden="true" tabindex="-1"></a>      X_centered <span class="ot">&lt;-</span> <span class="fu">sweep</span>(X, <span class="dv">2</span>, means[k, ])</span>
<span id="cb8-1792"><a href="#cb8-1792" aria-hidden="true" tabindex="-1"></a>      covs[[k]] <span class="ot">&lt;-</span> <span class="fu">t</span>(X_centered) <span class="sc">%*%</span> (resp[, k] <span class="sc">*</span> X_centered) <span class="sc">/</span> nk</span>
<span id="cb8-1793"><a href="#cb8-1793" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-1794"><a href="#cb8-1794" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb8-1795"><a href="#cb8-1795" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-1796"><a href="#cb8-1796" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb8-1797"><a href="#cb8-1797" aria-hidden="true" tabindex="-1"></a>    <span class="at">means =</span> means,</span>
<span id="cb8-1798"><a href="#cb8-1798" aria-hidden="true" tabindex="-1"></a>    <span class="at">covariances =</span> covs,</span>
<span id="cb8-1799"><a href="#cb8-1799" aria-hidden="true" tabindex="-1"></a>    <span class="at">weights =</span> weights,</span>
<span id="cb8-1800"><a href="#cb8-1800" aria-hidden="true" tabindex="-1"></a>    <span class="at">responsibilities =</span> resp,</span>
<span id="cb8-1801"><a href="#cb8-1801" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_likelihood =</span> log_lik,</span>
<span id="cb8-1802"><a href="#cb8-1802" aria-hidden="true" tabindex="-1"></a>    <span class="at">iterations =</span> iter</span>
<span id="cb8-1803"><a href="#cb8-1803" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-1804"><a href="#cb8-1804" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-1805"><a href="#cb8-1805" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-1806"><a href="#cb8-1806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1807"><a href="#cb8-1807" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1808"><a href="#cb8-1808" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1809"><a href="#cb8-1809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1810"><a href="#cb8-1810" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb8-1811"><a href="#cb8-1811" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb8-1812"><a href="#cb8-1812" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb8-1813"><a href="#cb8-1813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1814"><a href="#cb8-1814" aria-hidden="true" tabindex="-1"></a>Python and R code examples for Fisher Information, confidence intervals, the Delta Method, and the EM algorithm can be found in the HTML version of these notes. The code includes:</span>
<span id="cb8-1815"><a href="#cb8-1815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1816"><a href="#cb8-1816" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computing Fisher Information both symbolically and numerically</span>
<span id="cb8-1817"><a href="#cb8-1817" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Constructing confidence intervals using Fisher Information</span>
<span id="cb8-1818"><a href="#cb8-1818" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implementing the Delta Method for transformed parameters</span>
<span id="cb8-1819"><a href="#cb8-1819" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using built-in packages for Gaussian mixture models (sklearn in Python, mclust in R)</span>
<span id="cb8-1820"><a href="#cb8-1820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Manual implementations of the EM algorithm</span>
<span id="cb8-1821"><a href="#cb8-1821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1822"><a href="#cb8-1822" aria-hidden="true" tabindex="-1"></a>Key packages:</span>
<span id="cb8-1823"><a href="#cb8-1823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Python**: <span class="in">`statsmodels`</span>, <span class="in">`sympy`</span>, <span class="in">`sklearn.mixture`</span>, <span class="in">`jax`</span></span>
<span id="cb8-1824"><a href="#cb8-1824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**R**: <span class="in">`MASS`</span>, <span class="in">`mclust`</span>, <span class="in">`msm`</span>, <span class="in">`numDeriv`</span></span>
<span id="cb8-1825"><a href="#cb8-1825" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1826"><a href="#cb8-1826" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1827"><a href="#cb8-1827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1828"><a href="#cb8-1828" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb8-1829"><a href="#cb8-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1830"><a href="#cb8-1830" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb8-1831"><a href="#cb8-1831" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb8-1832"><a href="#cb8-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1833"><a href="#cb8-1833" aria-hidden="true" tabindex="-1"></a>This table maps sections in these lecture notes to the corresponding sections in @wasserman2013all ("All of Statistics" or AoS).</span>
<span id="cb8-1834"><a href="#cb8-1834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1835"><a href="#cb8-1835" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding AoS Section(s) |</span>
<span id="cb8-1836"><a href="#cb8-1836" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb8-1837"><a href="#cb8-1837" aria-hidden="true" tabindex="-1"></a>| **Introduction: How Good Are Our Estimators?** | From slides and general context from AoS §9.4 introduction on properties of estimators. |</span>
<span id="cb8-1838"><a href="#cb8-1838" aria-hidden="true" tabindex="-1"></a>| **Warm-up: Beta Distribution MLE** | Example from slides; similar numerical optimization examples in AoS §9.13.4. |</span>
<span id="cb8-1839"><a href="#cb8-1839" aria-hidden="true" tabindex="-1"></a>| **Core Properties of the MLE** | |</span>
<span id="cb8-1840"><a href="#cb8-1840" aria-hidden="true" tabindex="-1"></a>| ↳ Overview | AoS §9.4 (list of properties). |</span>
<span id="cb8-1841"><a href="#cb8-1841" aria-hidden="true" tabindex="-1"></a>| ↳ Consistency | AoS §9.5 (Theorem 9.13 and related discussion). |</span>
<span id="cb8-1842"><a href="#cb8-1842" aria-hidden="true" tabindex="-1"></a>| ↳ Equivariance | AoS §9.6 (Theorem and Example). |</span>
<span id="cb8-1843"><a href="#cb8-1843" aria-hidden="true" tabindex="-1"></a>| ↳ Asymptotic Normality &amp; Optimality | AoS §9.7 introduction, §9.8. |</span>
<span id="cb8-1844"><a href="#cb8-1844" aria-hidden="true" tabindex="-1"></a>| **Fisher Information and Confidence Intervals** | |</span>
<span id="cb8-1845"><a href="#cb8-1845" aria-hidden="true" tabindex="-1"></a>| ↳ Fisher Information | AoS §9.7 (Definitions and Theorem 9.17). |</span>
<span id="cb8-1846"><a href="#cb8-1846" aria-hidden="true" tabindex="-1"></a>| ↳ Constructing Confidence Intervals | AoS §9.7 (Theorem 9.19 and Examples). |</span>
<span id="cb8-1847"><a href="#cb8-1847" aria-hidden="true" tabindex="-1"></a>| ↳ Cramér-Rao Lower Bound | Mentioned in AoS §9.8. |</span>
<span id="cb8-1848"><a href="#cb8-1848" aria-hidden="true" tabindex="-1"></a>| **Additional Topics** | |</span>
<span id="cb8-1849"><a href="#cb8-1849" aria-hidden="true" tabindex="-1"></a>| ↳ The Delta Method | AoS §9.9 (Theorem 9.24 and Examples). |</span>
<span id="cb8-1850"><a href="#cb8-1850" aria-hidden="true" tabindex="-1"></a>| ↳ Multiparameter Models | AoS §9.10 (Fisher Information Matrix and related theorems). |</span>
<span id="cb8-1851"><a href="#cb8-1851" aria-hidden="true" tabindex="-1"></a>| ↳ Sufficient Statistics | AoS §9.13.2 (brief treatment from slides). |</span>
<span id="cb8-1852"><a href="#cb8-1852" aria-hidden="true" tabindex="-1"></a>| **Connection to Machine Learning: Cross-Entropy as MLE** | Expanded from slides. |</span>
<span id="cb8-1853"><a href="#cb8-1853" aria-hidden="true" tabindex="-1"></a>| **MLE for Latent Variable Models: The EM Algorithm** | AoS §9.13.4 (Appendix on computing MLEs). |</span>
<span id="cb8-1854"><a href="#cb8-1854" aria-hidden="true" tabindex="-1"></a>| **Self-Test Problems** | Inspired by AoS §9.14 exercises. |</span>
<span id="cb8-1855"><a href="#cb8-1855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1856"><a href="#cb8-1856" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-1857"><a href="#cb8-1857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1858"><a href="#cb8-1858" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Materials</span></span>
<span id="cb8-1859"><a href="#cb8-1859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1860"><a href="#cb8-1860" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Connection to machine learning**: Murphy (2022), "Probabilistic Machine Learning: An Introduction", Chapter 4</span>
<span id="cb8-1861"><a href="#cb8-1861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1862"><a href="#cb8-1862" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb8-1863"><a href="#cb8-1863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1864"><a href="#cb8-1864" aria-hidden="true" tabindex="-1"></a>*Remember: The MLE isn't just a computational procedure -- it's a principled approach to estimation with deep theoretical foundations. The properties we've studied explain why maximum likelihood appears everywhere from simple coin flips to complex neural networks. These concepts represent the statistical foundations of modern machine learning!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>