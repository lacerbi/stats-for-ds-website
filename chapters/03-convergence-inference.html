<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 3&nbsp; Convergence and The Basics of Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/04-nonparametric-bootstrap.html" rel="next">
<link href="../chapters/02-expectation.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/03-convergence-inference.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">3.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-and-motivation" id="toc-introduction-and-motivation" class="nav-link" data-scroll-target="#introduction-and-motivation"><span class="header-section-number">3.2</span> Introduction and Motivation</a>
  <ul class="collapse">
  <li><a href="#convergence-matters-to-understand-machine-learning-algorithms" id="toc-convergence-matters-to-understand-machine-learning-algorithms" class="nav-link" data-scroll-target="#convergence-matters-to-understand-machine-learning-algorithms"><span class="header-section-number">3.2.1</span> Convergence Matters to Understand Machine Learning Algorithms</a></li>
  </ul></li>
  <li><a href="#inequalities-bounding-the-unknown" id="toc-inequalities-bounding-the-unknown" class="nav-link" data-scroll-target="#inequalities-bounding-the-unknown"><span class="header-section-number">3.3</span> Inequalities: Bounding the Unknown</a>
  <ul class="collapse">
  <li><a href="#why-we-need-inequalities" id="toc-why-we-need-inequalities" class="nav-link" data-scroll-target="#why-we-need-inequalities"><span class="header-section-number">3.3.1</span> Why We Need Inequalities</a></li>
  <li><a href="#markovs-inequality" id="toc-markovs-inequality" class="nav-link" data-scroll-target="#markovs-inequality"><span class="header-section-number">3.3.2</span> Markov’s Inequality</a></li>
  <li><a href="#chebyshevs-inequality" id="toc-chebyshevs-inequality" class="nav-link" data-scroll-target="#chebyshevs-inequality"><span class="header-section-number">3.3.3</span> Chebyshev’s Inequality</a></li>
  </ul></li>
  <li><a href="#convergence-of-random-variables" id="toc-convergence-of-random-variables" class="nav-link" data-scroll-target="#convergence-of-random-variables"><span class="header-section-number">3.4</span> Convergence of Random Variables</a>
  <ul class="collapse">
  <li><a href="#the-need-for-probabilistic-convergence" id="toc-the-need-for-probabilistic-convergence" class="nav-link" data-scroll-target="#the-need-for-probabilistic-convergence"><span class="header-section-number">3.4.1</span> The Need for Probabilistic Convergence</a></li>
  <li><a href="#convergence-in-probability" id="toc-convergence-in-probability" class="nav-link" data-scroll-target="#convergence-in-probability"><span class="header-section-number">3.4.2</span> Convergence in Probability</a></li>
  <li><a href="#convergence-in-distribution" id="toc-convergence-in-distribution" class="nav-link" data-scroll-target="#convergence-in-distribution"><span class="header-section-number">3.4.3</span> Convergence in Distribution</a></li>
  <li><a href="#comparing-modes-of-convergence" id="toc-comparing-modes-of-convergence" class="nav-link" data-scroll-target="#comparing-modes-of-convergence"><span class="header-section-number">3.4.4</span> Comparing Modes of Convergence</a></li>
  <li><a href="#properties-and-transformations" id="toc-properties-and-transformations" class="nav-link" data-scroll-target="#properties-and-transformations"><span class="header-section-number">3.4.5</span> Properties and Transformations</a></li>
  </ul></li>
  <li><a href="#the-two-fundamental-theorems-of-statistics" id="toc-the-two-fundamental-theorems-of-statistics" class="nav-link" data-scroll-target="#the-two-fundamental-theorems-of-statistics"><span class="header-section-number">3.5</span> The Two Fundamental Theorems of Statistics</a>
  <ul class="collapse">
  <li><a href="#the-law-of-large-numbers-lln" id="toc-the-law-of-large-numbers-lln" class="nav-link" data-scroll-target="#the-law-of-large-numbers-lln"><span class="header-section-number">3.5.1</span> The Law of Large Numbers (LLN)</a></li>
  <li><a href="#the-central-limit-theorem-clt" id="toc-the-central-limit-theorem-clt" class="nav-link" data-scroll-target="#the-central-limit-theorem-clt"><span class="header-section-number">3.5.2</span> The Central Limit Theorem (CLT)</a></li>
  </ul></li>
  <li><a href="#the-language-of-statistical-inference" id="toc-the-language-of-statistical-inference" class="nav-link" data-scroll-target="#the-language-of-statistical-inference"><span class="header-section-number">3.6</span> The Language of Statistical Inference</a>
  <ul class="collapse">
  <li><a href="#from-probability-to-inference" id="toc-from-probability-to-inference" class="nav-link" data-scroll-target="#from-probability-to-inference"><span class="header-section-number">3.6.1</span> From Probability to Inference</a></li>
  <li><a href="#statistical-models" id="toc-statistical-models" class="nav-link" data-scroll-target="#statistical-models"><span class="header-section-number">3.6.2</span> Statistical Models</a></li>
  <li><a href="#point-estimation" id="toc-point-estimation" class="nav-link" data-scroll-target="#point-estimation"><span class="header-section-number">3.6.3</span> Point Estimation</a></li>
  <li><a href="#how-to-evaluate-estimators" id="toc-how-to-evaluate-estimators" class="nav-link" data-scroll-target="#how-to-evaluate-estimators"><span class="header-section-number">3.6.4</span> How to Evaluate Estimators</a></li>
  <li><a href="#the-bias-variance-tradeoff" id="toc-the-bias-variance-tradeoff" class="nav-link" data-scroll-target="#the-bias-variance-tradeoff"><span class="header-section-number">3.6.5</span> The Bias-Variance Tradeoff</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">3.7</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">3.7.1</span> Key Concepts Review</a></li>
  <li><a href="#why-these-concepts-matter" id="toc-why-these-concepts-matter" class="nav-link" data-scroll-target="#why-these-concepts-matter"><span class="header-section-number">3.7.2</span> Why These Concepts Matter</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">3.7.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">3.7.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">3.7.5</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">3.7.6</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">3.7.7</span> Connections to Source Material</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">3.7.8</span> Further Reading</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">3.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li>Explain how probability inequalities provide bounds on uncertainty.</li>
<li>Define concepts of probabilistic convergence and apply the Law of Large Numbers and Central Limit Theorem.</li>
<li>Define the core vocabulary of statistical inference (models, parameters, estimators).</li>
<li>Evaluate an estimator’s quality using its standard error, bias, and variance.</li>
<li>Explain the bias-variance tradeoff in the context of Mean Squared Error (MSE).</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter covers probability inequalities, convergence concepts, and the foundations of statistical inference. The material is adapted from Chapters 4, 5, and 6 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span>, supplemented with additional examples and perspectives relevant to data science applications.</p>
</div>
</div>
</section>
<section id="introduction-and-motivation" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="introduction-and-motivation"><span class="header-section-number">3.2</span> Introduction and Motivation</h2>
<section id="convergence-matters-to-understand-machine-learning-algorithms" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="convergence-matters-to-understand-machine-learning-algorithms"><span class="header-section-number">3.2.1</span> Convergence Matters to Understand Machine Learning Algorithms</h3>
<p>Deep learning models are trained with <em>stochastic</em> optimization algorithms. These algorithms produce a sequence of parameter estimates <span class="math display"> \theta_1, \theta_2, \theta_3, \ldots</span></p>
<p>as they iterate through the data. But here’s the fundamental question: do these estimates eventually <em>converge</em> to a good solution, and how do we establish that?</p>
<p>The challenge is that these parameter estimates are random variables – they depend on random initialization, random mini-batch selection, and random data shuffling. We can’t use the simple definition of convergence where <span class="math inline">|x_n - x| &lt; \epsilon</span> for all large <span class="math inline">n</span>, which you may remember from calculus. We need new mathematical tools.</p>
<p>This chapter develops the language of <em>probabilistic</em> convergence to understand and analyze such algorithms. We’ll then use these tools to build the foundation of statistical inference – the science of drawing conclusions about populations from samples.</p>
<p>Consider a concrete example: training a neural network for image classification. At each iteration <span class="math inline">t</span>:</p>
<ol type="1">
<li>Pick a random subset <span class="math inline">S</span> of training images</li>
<li>Compute the gradient <span class="math inline">g = \sum_{x_i \in S} \nabla_\theta L(\theta_t; x_i)</span> of the loss<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>Compute next estimate of the model parameters, <span class="math inline">\theta_{t+1}</span>, using <span class="math inline">g</span> and the current parameters<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ol>
<p>The randomness in batch selection makes each <span class="math inline">\theta_t</span> a random variable. As mentioned before, ideally we would want <span class="math inline">\theta_1, \theta_2, \ldots</span> to converge to a good solution. But what does it even mean to say the algorithm “converges”? This chapter provides the answer.</p>
<p>Convergence isn’t just about optimization algorithms. It’s central to all of statistics:</p>
<ul>
<li>When we compute a sample mean with increasing amount of data, does it converge to the population mean?</li>
<li>More generally, when we estimate a model parameter, does our estimate improve with more data?</li>
<li>When we approximate a distribution, does the approximation get better?</li>
</ul>
<p>The remarkable answers to these questions – provided by the Law of Large Numbers and Central Limit Theorem – form the theoretical backbone of statistical inference and machine learning.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here’s a reference table of key terms in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Markov’s inequality</td>
<td>Markovin epäyhtälö</td>
<td>Bounds probability of large values</td>
</tr>
<tr class="even">
<td>Chebyshev’s inequality</td>
<td>Tšebyšovin epäyhtälö</td>
<td>Uses variance to bound deviations</td>
</tr>
<tr class="odd">
<td>Convergence in probability</td>
<td>Stokastinen suppeneminen</td>
<td>Random variable settling to a value</td>
</tr>
<tr class="even">
<td>Convergence in distribution</td>
<td>Jakaumasuppeneminen</td>
<td>Distribution shape converging</td>
</tr>
<tr class="odd">
<td>Law of Large Numbers</td>
<td>Suurten lukujen laki</td>
<td>Sample mean → population mean</td>
</tr>
<tr class="even">
<td>Central Limit Theorem</td>
<td>Keskeinen raja-arvolause</td>
<td>Sums become normally distributed</td>
</tr>
<tr class="odd">
<td>Statistical model</td>
<td>Tilastollinen malli</td>
<td>Set of possible distributions</td>
</tr>
<tr class="even">
<td>Parametric model</td>
<td>Parametrinen malli</td>
<td>Finite-dimensional parameter space</td>
</tr>
<tr class="odd">
<td>Nonparametric model</td>
<td>Epäparametrinen malli</td>
<td>Infinite-dimensional space</td>
</tr>
<tr class="even">
<td>Nuisance parameter</td>
<td>Kiusaparametri</td>
<td>Parameter not of primary interest</td>
</tr>
<tr class="odd">
<td>Point estimation</td>
<td>Piste-estimointi</td>
<td>Single best guess of parameter</td>
</tr>
<tr class="even">
<td>Estimator</td>
<td>Estimaattori</td>
<td>Function of data estimating parameter</td>
</tr>
<tr class="odd">
<td>Bias</td>
<td>Harha</td>
<td>Expected error of estimator</td>
</tr>
<tr class="even">
<td>Unbiased</td>
<td>Harhaton</td>
<td>Zero expected error</td>
</tr>
<tr class="odd">
<td>Consistent</td>
<td>Tarkentuva</td>
<td>Converges to true value</td>
</tr>
<tr class="even">
<td>Standard error</td>
<td>Keskivirhe</td>
<td>Standard deviation of estimator</td>
</tr>
<tr class="odd">
<td>Mean Squared Error (MSE)</td>
<td>Keskimääräinen neliövirhe</td>
<td>Average squared error</td>
</tr>
<tr class="even">
<td>Sampling distribution</td>
<td>Otantajakauma</td>
<td>Distribution of the estimator</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="inequalities-bounding-the-unknown" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="inequalities-bounding-the-unknown"><span class="header-section-number">3.3</span> Inequalities: Bounding the Unknown</h2>
<section id="why-we-need-inequalities" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="why-we-need-inequalities"><span class="header-section-number">3.3.1</span> Why We Need Inequalities</h3>
<p>In probability and statistics, we often encounter quantities that are difficult or impossible to compute exactly. Inequalities provide <em>bounds</em> – upper or lower limits – that give us useful information even when exact calculations are intractable. They serve three critical purposes:</p>
<ol type="1">
<li><strong>Bounding quantities</strong>: When we can’t compute a probability exactly, an upper bound tells us it’s “at most this large”</li>
<li><strong>Proving theorems</strong>: The Law of Large Numbers and Central Limit Theorem rely on inequalities in their proofs</li>
<li><strong>Practical guarantees</strong>: In machine learning, we use inequalities to create bounds on critical quantities such as generalization error<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
</ol>
<p>Think of inequalities as providing universal statistical guarantees. They tell us that no matter how complicated the underlying distribution, certain bounds will always hold.</p>
</section>
<section id="markovs-inequality" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="markovs-inequality"><span class="header-section-number">3.3.2</span> Markov’s Inequality</h3>
<div class="theorem">
<p><strong>Markov’s Inequality</strong>: For a non-negative random variable <span class="math inline">X</span> with finite expectation: <span class="math display">\mathbb{P}(X \geq t) \leq \frac{\mathbb{E}(X)}{t} \quad \text{for all } t &gt; 0</span></p>
</div>
<p>This remarkably simple inequality says that – no matter what – the probability of a non-negative random variable exceeding a threshold <span class="math inline">t</span> is bounded by its mean divided by <span class="math inline">t</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Since <span class="math inline">X \geq 0</span>: <span class="math display">\begin{align}
\mathbb{E}(X) &amp;= \int_0^{\infty} x f(x) \, dx \\
&amp;= \int_0^t x f(x) \, dx + \int_t^{\infty} x f(x) \, dx \\
&amp;\geq \int_t^{\infty} x f(x) \, dx \\
&amp;\geq t \int_t^{\infty} f(x) \, dx \\
&amp;= t \mathbb{P}(X \geq t)
\end{align}</span></p>
<p>Rearranging gives the result.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Exceeding a Multiple of the Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X</span> be a non-negative random variable with mean <span class="math inline">\mathbb{E}(X) = \mu</span>. What can we say about the probability that <span class="math inline">X</span> exceeds <span class="math inline">k</span> times its mean, for some <span class="math inline">k &gt; 1</span>?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using Markov’s inequality by setting <span class="math inline">t = k\mu</span>: <span class="math display">\mathbb{P}(X \geq k\mu) \leq \frac{\mathbb{E}(X)}{k\mu} = \frac{\mu}{k\mu} = \frac{1}{k}</span></p>
<p>For example, the probability of a non-negative random variable exceeding twice its mean is at most <span class="math inline">1/2</span>. The probability of it exceeding 10 times its mean is at most <span class="math inline">1/10</span>. This universal bound is surprisingly useful.</p>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Exam Scores
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the average exam score is 50 points, what’s the maximum probability that a randomly selected student scored 90 or more?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using Markov’s inequality: <span class="math display">\mathbb{P}(X \geq 90) \leq \frac{50}{90} = \frac{5}{9} \approx 0.556</span></p>
<p>At most 55.6% of students can score 90 or more. This bound requires only knowing the average – no other information about the distribution!</p>
</div>
</div>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255588-76-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-76-1" role="tab" aria-controls="tabset-1757255588-76-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-76-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-76-2" role="tab" aria-controls="tabset-1757255588-76-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-76-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-76-3" role="tab" aria-controls="tabset-1757255588-76-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255588-76-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255588-76-1-tab"><p>Markov’s inequality captures a fundamental truth: <strong>averages
constrain extremes</strong>.</p><p>Imagine a village where the average wealth is €50,000. What fraction
of villagers could be millionaires? If everyone were a millionaire, the
average would be at least €1,000,000. Since the average is only €50,000,
at most 5% can be millionaires:
<span class="math display">\[\text{Fraction of millionaires} \leq \frac{€50,000}{€1,000,000} = 0.05\]</span></p><p>This reasoning works for any non-negative quantity: test scores,
waiting times, file sizes, or loss values in machine learning. The
average puts a hard limit on how often extreme values can occur.</p></div><div id="tabset-1757255588-76-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-76-2-tab"><p>Markov’s inequality is the foundation for many other inequalities.
Its power lies in its generality—it applies to any non-negative random
variable with finite expectation.</p><p>The inequality is tight (best possible) for certain distributions.
Consider: <span class="math display">\[X = \begin{cases}
0 &amp; \text{with probability } 1 - \frac{\mu}{t} \\
t &amp; \text{with probability } \frac{\mu}{t}
\end{cases}\]</span></p><p>Then <span class="math inline">\(\mathbb{E}(X) = \mu\)</span> and
<span class="math inline">\(\mathbb{P}(X \geq t) = \frac{\mu}{t}\)</span>,
achieving equality in Markov’s inequality.</p></div><div id="tabset-1757255588-76-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-76-3-tab"><p>Let’s visualize Markov’s inequality by comparing the true tail
probability with the bound for an exponential distribution.</p><div id="1678fd05" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the exponential distribution</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="dv">2</span>  <span class="co"># mean = 2</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> stats.expon.pdf(x, scale<span class="op">=</span>beta)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute true probabilities and Markov bounds</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>t_values <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>true_probs <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(t_values, scale<span class="op">=</span>beta)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>markov_bounds <span class="op">=</span> np.minimum(beta <span class="op">/</span> t_values, <span class="dv">1</span>)  <span class="co"># E[X]/t, capped at 1</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: PDF with shaded tail</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>t_example <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ax1.plot(x, pdf, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Exponential(2) PDF'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax1.fill_between(x[x <span class="op">&gt;=</span> t_example], pdf[x <span class="op">&gt;=</span> t_example], alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'red'</span>, </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>                 label<span class="op">=</span><span class="ss">f'P(X ≥ </span><span class="sc">{</span>t_example<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>ax1.axvline(beta, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'E[X] = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Exponential Distribution'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: True probability vs Markov bound</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax2.plot(t_values, true_probs, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True P(X ≥ t)'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax2.plot(t_values, markov_bounds, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Markov bound E[X]/t'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'t'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Markov</span><span class="ch">\'</span><span class="st">s Inequality for Exponential(2)'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="dv">0</span>, <span class="fl">1.1</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical comparison at specific points</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Comparison of true probability vs Markov bound:"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>]:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    true_p <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(t, scale<span class="op">=</span>beta)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    markov_p <span class="op">=</span> beta <span class="op">/</span> t</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"t = </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: True P(X ≥ </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span>true_p<span class="sc">:.4f}</span><span class="ss">, Markov bound = </span><span class="sc">{</span>markov_p<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="03-convergence-inference_files/figure-html/cell-2-output-1.png" width="679" height="468"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Comparison of true probability vs Markov bound:
t = 1: True P(X ≥ 1) = 0.6065, Markov bound = 2.0000
t = 2: True P(X ≥ 2) = 0.3679, Markov bound = 1.0000
t = 4: True P(X ≥ 4) = 0.1353, Markov bound = 0.5000
t = 8: True P(X ≥ 8) = 0.0183, Markov bound = 0.2500</code></pre>
</div>
</div><p>Notice that the Markov bound is always valid but often loose. It
becomes tighter as <span class="math inline">\(t\)</span> increases
relative to the mean.</p></div></div></div>
</section>
<section id="chebyshevs-inequality" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="chebyshevs-inequality"><span class="header-section-number">3.3.3</span> Chebyshev’s Inequality</h3>
<p>While Markov’s inequality uses only the mean, Chebyshev’s inequality leverages the variance to provide a tighter bound on deviations from the mean.</p>
<div class="theorem">
<p><strong>Chebyshev’s Inequality</strong>: Let <span class="math inline">X</span> have mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2</span>. Then: <span class="math display">\mathbb{P}(|X - \mu| \geq t) \leq \frac{\sigma^2}{t^2} \quad \text{for all } t &gt; 0</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Apply Markov’s inequality to the non-negative random variable <span class="math inline">(X - \mu)^2</span>: <span class="math display">\mathbb{P}(|X - \mu| \geq t) = \mathbb{P}((X - \mu)^2 \geq t^2) \leq \frac{\mathbb{E}[(X - \mu)^2]}{t^2} = \frac{\sigma^2}{t^2}</span></p>
</div>
</div>
</div>
<p>Equivalently, in terms of standard deviations: <span class="math display">\mathbb{P}(|X - \mu| \geq k\sigma) \leq \frac{\sigma^2}{k^2 \sigma^2 } = \frac{1}{k^2} \quad \text{for all } k &gt; 0</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Universal Two-Sigma Rule
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <em>any</em> distribution (not just normal!), Chebyshev’s inequality tells us:</p>
<p><span class="math display"> \mathbb{P}(|X - \mu| &lt; k \sigma) \ge 1 - \frac{1}{k^2}. </span></p>
<p>Thus, for <span class="math inline">k=2</span> and <span class="math inline">k=3</span> we find:</p>
<ul>
<li>At least 75% of the data lies within 2 standard deviations of the mean: <span class="math inline">\mathbb{P}(|X - \mu| &lt; 2\sigma) \geq 1 - \frac{1}{4} = 0.75</span></li>
<li>At least 89% lies within 3 standard deviations: <span class="math inline">\mathbb{P}(|X - \mu| &lt; 3\sigma) \geq 1 - \frac{1}{9} \approx 0.889</span></li>
</ul>
<p>Compare this to the normal distribution where about 95% lies within <span class="math inline">2\sigma</span> and 99.7% within <span class="math inline">3\sigma</span>. Chebyshev’s bounds are weaker but universal.</p>
</div>
</div>
<p>We show below the Chebyshev’s bound compared to the actual tail probabilities of a few famous distributions (normal, uniform and exponential).</p>
<div id="3b4032e1" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="2">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing Chebyshev's inequality</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up comparison for different distributions</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>chebyshev_bound <span class="op">=</span> np.minimum(<span class="dv">1</span> <span class="op">/</span> k_values<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute actual probabilities for different distributions</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>normal_probs <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> stats.norm.cdf(k_values))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>uniform_probs <span class="op">=</span> np.maximum(<span class="dv">0</span>, <span class="dv">1</span> <span class="op">-</span> k_values <span class="op">/</span> np.sqrt(<span class="dv">3</span>))  <span class="co"># Uniform on [-sqrt(3), sqrt(3)]</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>exp_probs <span class="op">=</span> []</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For exponential with mean 1, mu=1, sigma=1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    exp_probs.append(stats.expon.cdf(<span class="dv">1</span> <span class="op">-</span> k, scale<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(<span class="dv">1</span> <span class="op">+</span> k, scale<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, chebyshev_bound, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Chebyshev bound'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, normal_probs, <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Normal'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, uniform_probs, <span class="st">'g--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Uniform'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, exp_probs, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Exponential'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of standard deviations (k)'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(|X - μ| ≥ kσ)'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Chebyshev</span><span class="ch">\'</span><span class="st">s Inequality vs Actual Tail Probabilities'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="fl">0.5</span>, <span class="dv">4</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Print specific values</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being more than k standard deviations from the mean:"</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"k</span><span class="ch">\t</span><span class="st">Chebyshev</span><span class="ch">\t</span><span class="st">Normal</span><span class="ch">\t\t</span><span class="st">Uniform</span><span class="ch">\t\t</span><span class="st">Exponential"</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    cheby <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>k<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> stats.norm.cdf(k))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    uniform <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, <span class="dv">1</span> <span class="op">-</span> k<span class="op">/</span>np.sqrt(<span class="dv">3</span>))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    exp_val <span class="op">=</span> stats.expon.cdf(<span class="dv">1</span> <span class="op">-</span> k, scale<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(<span class="dv">1</span> <span class="op">+</span> k, scale<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\t</span><span class="sc">{</span>cheby<span class="sc">:.4f}</span><span class="ch">\t\t</span><span class="sc">{</span>normal<span class="sc">:.4f}</span><span class="ch">\t\t</span><span class="sc">{</span>uniform<span class="sc">:.4f}</span><span class="ch">\t\t</span><span class="sc">{</span>exp_val<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-convergence-inference_files/figure-html/cell-3-output-1.png" width="661" height="372" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability of being more than k standard deviations from the mean:
k   Chebyshev   Normal      Uniform     Exponential
1   1.0000      0.3173      0.4226      0.1353
2   0.2500      0.0455      0.0000      0.0498
3   0.1111      0.0027      0.0000      0.0183</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Hoeffding’s Inequality
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>While Chebyshev’s inequality is universal, it can be quite loose. For bounded random variables, Hoeffding’s inequality provides an exponentially decaying bound that’s much sharper.</p>
<div class="theorem" name="Hoeffding's Inequality">
<p>Let <span class="math inline">X_1, \ldots, X_n</span> be independent random variables with <span class="math inline">X_i \in [a_i, b_i]</span>. Let <span class="math inline">S_n = \sum_{i=1}^n X_i</span>. Then for any <span class="math inline">t &gt; 0</span>: <span class="math display">\mathbb{P}(S_n - \mathbb{E}[S_n] \geq t) \leq \exp\left(-\frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)</span></p>
<p>For the special case of <span class="math inline">n</span> independent Bernoulli(<span class="math inline">p</span>) random variables: <span class="math display">\mathbb{P}\left(|\bar{X}_n - p| &gt; \epsilon\right) \leq 2e^{-2n\epsilon^2}</span> where <span class="math inline">\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i</span>.</p>
</div>
<p>The key insight is the exponential decay in <span class="math inline">n</span>. This makes Hoeffding’s inequality the foundation for many machine learning generalization bounds.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Comparing Bounds
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider estimating a probability <span class="math inline">p</span> from <span class="math inline">n = 100</span> Bernoulli trials. How likely is our estimate to be off by more than <span class="math inline">\epsilon = 0.2</span>?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Chebyshev’s bound</strong>: Since <span class="math inline">\mathbb{V}(\bar{X}_n) = p(1-p)/n \leq 1/(4n)</span>: <span class="math display">\mathbb{P}(|\bar{X}_n - p| &gt; 0.2) \leq \frac{1/(4 \times 100)}{0.2^2} = \frac{1/400}{0.04} = 0.0625</span></p>
<p><strong>Hoeffding’s bound</strong>: <span class="math display">\mathbb{P}(|\bar{X}_n - p| &gt; 0.2) \leq 2e^{-2 \times 100 \times 0.2^2} = 2e^{-8} \approx 0.00067</span></p>
<p>Hoeffding’s bound is nearly 100 times tighter! This exponential improvement is crucial for machine learning theory.</p>
</div>
</div>
</div>
</div>
</div>
<p>The proof of Hoeffding’s inequality uses moment generating functions and is beyond the scope of this course, but the intuition is that bounded random variables have light tails, allowing for much stronger concentration.</p>
</div>
</div>
</div>
</section>
</section>
<section id="convergence-of-random-variables" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="convergence-of-random-variables"><span class="header-section-number">3.4</span> Convergence of Random Variables</h2>
<section id="the-need-for-probabilistic-convergence" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="the-need-for-probabilistic-convergence"><span class="header-section-number">3.4.1</span> The Need for Probabilistic Convergence</h3>
<p>In calculus, we say a sequence <span class="math inline">x_n</span> converges to <span class="math inline">x</span> if for every <span class="math inline">\epsilon &gt; 0</span>, we have <span class="math inline">|x_n - x| &lt; \epsilon</span> for all sufficiently large <span class="math inline">n</span>. But what about sequences of random variables?</p>
<p>There are multiple scenarios:</p>
<ol type="1">
<li><p><strong>Concentrating distribution</strong>: Let <span class="math inline">X_n \sim \mathcal{N}(0, 1/n)</span>. As <span class="math inline">n</span> increases, the distribution concentrates more tightly around 0. Intuitively, <span class="math inline">X_n</span> is “converging” to 0.</p></li>
<li><p><strong>Tracking outcomes</strong>: The case above can be generalized where <span class="math inline">X_n</span> does not converge to a constant (such as 0), but converges to <strong>the values taken by another random variable</strong> <span class="math inline">X</span>.</p></li>
</ol>
<p>The problem is that for any specific <span class="math inline">x</span>, <span class="math inline">\mathbb{P}(X_n = x) = 0</span> for all <span class="math inline">n</span>: continuous random variables never exactly equal any specific value.</p>
<p>There is then a completely different kind of convergence.</p>
<ol start="3" type="1">
<li><strong>Stable distribution</strong>: Let <span class="math inline">X_n \sim \mathcal{N}(0, 1)</span> for all <span class="math inline">n</span>. Each <span class="math inline">X_n</span> has the same distribution, but they’re different random variables. Is there a broader sense in which this sequence “converges”?</li>
</ol>
<p>In sum, we need new definitions that capture different notions of what it means for random variables to converge.</p>
</section>
<section id="convergence-in-probability" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="convergence-in-probability"><span class="header-section-number">3.4.2</span> Convergence in Probability</h3>
<p>We consider the first two cases mentioned earlier: convergence of <strong>outcomes</strong> of a sequence of random variables to a constant or to the outcomes of another random variable, known as <strong>convergence in probability</strong>.</p>
<div class="definition">
<p>A sequence of random variables <span class="math inline">X_n</span> <strong>converges in probability</strong> to a random variable <span class="math inline">X</span>, written <span class="math inline">X_n \xrightarrow{P} X</span>, if for every <span class="math inline">\epsilon &gt; 0</span>: <span class="math display">\mathbb{P}(|X_n - X| &gt; \epsilon) \to 0 \text{ as } n \to \infty</span></p>
<p>When <span class="math inline">X = c</span> (a constant), we write <span class="math inline">X_n \xrightarrow{P} c</span>.</p>
</div>
<p>This definition captures the idea that <span class="math inline">X_n</span> becomes increasingly likely to be close to <span class="math inline">X</span> as <span class="math inline">n</span> grows. The probability of <span class="math inline">X_n</span> being “far” from <span class="math inline">X</span> (more than <span class="math inline">\epsilon</span> away) vanishes. In other words, the sequence of outcomes of the random variable <span class="math inline">X_n</span> “track” the outcomes of <span class="math inline">X</span> with ever-increasing accuracy as <span class="math inline">n</span> increases.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Convergence to Zero
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X_n \sim \mathcal{N}(0, 1/n)</span>. We’ll show that <span class="math inline">X_n \xrightarrow{P} 0</span>.</p>
<p>For any <span class="math inline">\epsilon &gt; 0</span>, using Chebyshev’s inequality: <span class="math display">\mathbb{P}(|X_n - 0| &gt; \epsilon) = \mathbb{P}(|X_n| &gt; \epsilon) \leq \frac{\mathbb{V}(X_n)}{\epsilon^2} = \frac{1/n}{\epsilon^2} = \frac{1}{n\epsilon^2}</span></p>
<p>Since <span class="math inline">\frac{1}{n\epsilon^2} \to 0</span> as <span class="math inline">n \to \infty</span>, we have <span class="math inline">X_n \xrightarrow{P} 0</span>.</p>
</div>
</div>
</section>
<section id="convergence-in-distribution" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="convergence-in-distribution"><span class="header-section-number">3.4.3</span> Convergence in Distribution</h3>
<p>We now consider the other case, where it’s not the random variables to converge but their distribution.</p>
<div class="definition">
<p>A sequence of random variables <span class="math inline">X_n</span> <strong>converges in distribution</strong> to a random variable <span class="math inline">X</span>, written <span class="math inline">X_n \rightsquigarrow X</span>, if: <span class="math display">\lim_{n \to \infty} F_n(t) = F(t)</span> at all points <span class="math inline">t</span> where <span class="math inline">F</span> is continuous. Here <span class="math inline">F_n</span> is the CDF of <span class="math inline">X_n</span> and <span class="math inline">F</span> is the CDF of <span class="math inline">X</span>.</p>
</div>
<p>This captures the idea that the <em>distribution</em> (or “shape”) of <span class="math inline">X_n</span> becomes increasingly similar to that of <span class="math inline">X</span>. We’re not saying the random variables themselves are close – just their overall probability distributions.</p>
<p>If <span class="math inline">X</span> is a point mass at <span class="math inline">c</span>, we denote <span class="math inline">X_n \rightsquigarrow c</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Key Distinction</strong>:</p>
<ul>
<li>Convergence in probability: The random variables themselves get close</li>
<li>Convergence in distribution: Only the distributions get close</li>
</ul>
</div>
</div>
<p>Let’s visualize this with the <span class="math inline">X_n \sim \mathcal{N}(0, 1/n)</span> example:</p>
<div id="a386b215" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="3">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the figure</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: PDFs converging to a spike at 0</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1000</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> plt.cm.Blues(np.linspace(<span class="fl">0.3</span>, <span class="fl">0.9</span>, <span class="bu">len</span>(n_values)))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(n_values, colors):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    pdf <span class="op">=</span> stats.norm.pdf(x, loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>np.sqrt(n))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x, pdf, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>ax1.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'PDFs of N(0, 1/n)'</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">0</span>, <span class="fl">2.5</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: CDFs converging to step function</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(n_values, colors):</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    cdf <span class="op">=</span> stats.norm.cdf(x, loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>np.sqrt(n))</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    ax2.plot(x, cdf, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot limiting step function</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>ax2.plot(x[x <span class="op">&lt;</span> <span class="dv">0</span>], np.zeros(<span class="bu">sum</span>(x <span class="op">&lt;</span> <span class="dv">0</span>)), <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Limit'</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>ax2.plot(x[x <span class="op">&gt;=</span> <span class="dv">0</span>], np.ones(<span class="bu">sum</span>(x <span class="op">&gt;=</span> <span class="dv">0</span>)), <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>ax2.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>, fillstyle<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>ax2.plot([<span class="dv">0</span>], [<span class="dv">1</span>], <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Cumulative probability'</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'CDFs converging to step function'</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-convergence-inference_files/figure-html/cell-4-output-1.png" width="662" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As <span class="math inline">n</span> increases:</p>
<ul>
<li>The PDF becomes more concentrated at 0 (spike)</li>
<li>The CDF approaches a step function jumping from 0 to 1 at <span class="math inline">x=0</span></li>
<li>This is convergence in distribution to a point mass at 0</li>
</ul>
</section>
<section id="comparing-modes-of-convergence" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="comparing-modes-of-convergence"><span class="header-section-number">3.4.4</span> Comparing Modes of Convergence</h3>
<div class="theorem">
<p><strong>Relationships Between Convergence Types</strong></p>
<ol type="1">
<li>If <span class="math inline">X_n \xrightarrow{P} X</span> then <span class="math inline">X_n \rightsquigarrow X</span> (always)</li>
<li>If <span class="math inline">X</span> is a point mass at <span class="math inline">c</span> and <span class="math inline">X_n \rightsquigarrow X</span>, then <span class="math inline">X_n \xrightarrow{P} c</span></li>
</ol>
</div>
<p>Convergence in probability implies convergence in distribution, but the converse holds only for constants.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255588-676-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-676-1" role="tab" aria-controls="tabset-1757255588-676-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-676-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-676-2" role="tab" aria-controls="tabset-1757255588-676-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-676-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-676-3" role="tab" aria-controls="tabset-1757255588-676-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255588-676-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255588-676-1-tab"><p><strong>Convergence in Probability: The Perfect Weather
Forecast</strong></p><p>Let <span class="math inline">\(X\)</span> be the actual temperature
tomorrow and <span class="math inline">\(X_n\)</span> be its forecast
from an ever-improving machine learning model where
<span class="math inline">\(n\)</span> is the model version, as we make
it bigger and feed it more data.</p><p><strong>Convergence in probability
(<span class="math inline">\(X_n \xrightarrow{P} X\)</span>)</strong>
means the forecast becomes more and more accurate as the model gets
better and better. Eventually, the temperature prediction
<span class="math inline">\(X_n\)</span> gets so close to the actual
temperature <span class="math inline">\(X\)</span> that the forecast
error, <span class="math inline">\(|X_n - X|\)</span>, becomes
negligible. The individual outcomes match.</p><p><strong>Convergence in Distribution: The Perfect Climate
Model</strong></p><p>A climate model doesn’t predict a specific day’s temperature; it
captures the statistical “character” of a season. Let
<span class="math inline">\(X\)</span> be the random variable for daily
temperature, and <span class="math inline">\(X_n\)</span> be a model’s
simulation of a typical day.</p><p><strong>Convergence in distribution
(<span class="math inline">\(X_n \rightsquigarrow X\)</span>)</strong>
means the model’s simulated statistics (e.g., its histogram of
temperatures) become identical to the real climate’s statistics. The
<strong>patterns match</strong>, but the individual <strong>outcomes
don’t</strong>.</p><p><strong>The Takeaway:</strong></p><ul>
<li><strong>Probability implies Distribution:</strong> A perfect daily
forecast naturally captures the climate’s long-term statistics.</li>
<li><strong>Distribution does NOT imply Probability:</strong> A perfect
climate model can’t predict the actual temperature on next Friday.</li>
</ul></div><div id="tabset-1757255588-676-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-676-2-tab"><p>We can construct a counterexample showing that convergence in
distribution does NOT imply convergence in probability.</p><p><strong>Counterexample</strong>: Let
<span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span> and define
<span class="math inline">\(X_n = -X\)</span> for all
<span class="math inline">\(n\)</span>. Then:</p><ul>
<li>Each <span class="math inline">\(X_n \sim \mathcal{N}(0,1)\)</span>,
so trivially
<span class="math inline">\(X_n \rightsquigarrow X\)</span></li>
<li>But <span class="math inline">\(|X_n - X| = |2X|\)</span>, so
<span class="math inline">\(\mathbb{P}(|X_n - X| &gt; \epsilon) = \mathbb{P}(|X| &gt; \epsilon/2) \not\to 0\)</span></li>
<li>Therefore <span class="math inline">\(X_n\)</span> does NOT converge
to <span class="math inline">\(X\)</span> in probability!</li>
</ul><p>The random variables have the same distribution but are perfectly
anti-correlated.</p></div><div id="tabset-1757255588-676-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-676-3-tab"><p>Let’s demonstrate both types of convergence and the counterexample
computationally.</p><div id="b9d71a31" class="cell" data-fig-height="8" data-fig-width="7" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 1: X_n ~ N(0, 1/n) → 0 (both types of convergence)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n <span class="kw">in</span> <span class="bu">enumerate</span>(n_values):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span>np.sqrt(n), n_samples)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    ax.hist(samples, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, density<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>, <span class="bu">range</span><span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ax.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Value'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Case 1: N(0,1/n) → 0</span><span class="ch">\n</span><span class="st">(Converges in both senses)'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 1 continued: Show |X_n - 0| for different epsilon</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>prob_far <span class="op">=</span> []</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>):</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span>np.sqrt(n), n_samples)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    prob_far.append(np.mean(np.<span class="bu">abs</span>(samples) <span class="op">&gt;</span> epsilon))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>), prob_far, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'n'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="ss">f'P(|X_n| &gt; </span><span class="sc">{</span>epsilon<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Convergence in Probability to 0'</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 2: X_n = -X counterexample</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n_samples)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>X_n <span class="op">=</span> <span class="op">-</span>X  <span class="co"># X_n for all n</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot distributions (they're identical!)</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>ax.hist(X, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">'X ~ N(0,1)'</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>ax.hist(X_n, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">'X_n = -X ~ N(0,1)'</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Value'</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Case 2: Same distribution</span><span class="ch">\n</span><span class="st">(Converges in distribution)'</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"># But |X_n - X| = |2X| doesn't converge to 0</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.<span class="bu">abs</span>(X_n <span class="op">-</span> X)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>ax.hist(diff, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'|X_n - X| = |2X|'</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'But NOT convergence in probability!</span><span class="ch">\n</span><span class="st">|X_n - X| doesn</span><span class="ch">\'</span><span class="st">t concentrate at 0'</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Add theoretical chi distribution (|2X| where X~N(0,1))</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>x_theory <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">1000</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>y_theory <span class="op">=</span> stats.chi.pdf(x_theory <span class="op">*</span> <span class="fl">0.5</span>, df<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.5</span>  <span class="co"># Scale for |2X|</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>ax.plot(x_theory, y_theory, <span class="st">'k--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Theory'</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="03-convergence-inference_files/figure-html/cell-5-output-1.png" width="678" height="757"></p>
</div>
</div><p><strong>Summary:</strong></p><ul>
<li>Case 1:
<span class="math inline">\(X_n ~ \mathcal{N}\left(0, 1/n\right)\)</span>
converges to 0 in BOTH senses</li>
<li>Case 2: <span class="math inline">\(X_n = -X\)</span> has same
distribution as X but does NOT converge in probability</li>
<li>Convergence in distribution is weaker than convergence in
probability</li>
</ul></div></div></div>
</section>
<section id="properties-and-transformations" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="properties-and-transformations"><span class="header-section-number">3.4.5</span> Properties and Transformations</h3>
<p>Understanding how convergence behaves under various operations is crucial for statistical theory. Here are the key properties:</p>
<div class="theorem">
<p><strong>Operations Under Convergence in Probability</strong></p>
<p>If <span class="math inline">X_n \xrightarrow{P} X</span> and <span class="math inline">Y_n \xrightarrow{P} Y</span>, then:</p>
<ol type="1">
<li><span class="math inline">X_n + Y_n \xrightarrow{P} X + Y</span></li>
<li><span class="math inline">X_n Y_n \xrightarrow{P} XY</span></li>
<li><span class="math inline">X_n / Y_n \xrightarrow{P} X/Y</span> (if <span class="math inline">\mathbb{P}(Y = 0) = 0</span>)</li>
</ol>
</div>
<p>This shows that convergence in probability is well-beheaved under standard operations of sum, product, and division not-by-zero.</p>
<div class="theorem">
<p><strong>Slutsky’s Theorem</strong></p>
<p>If <span class="math inline">X_n \rightsquigarrow X</span> and <span class="math inline">Y_n \xrightarrow{P} c</span> (constant), then:</p>
<ol type="1">
<li><span class="math inline">X_n + Y_n \rightsquigarrow X + c</span></li>
<li><span class="math inline">X_n Y_n \rightsquigarrow cX</span></li>
<li><span class="math inline">X_n / Y_n \rightsquigarrow X/c</span> (if <span class="math inline">c \neq 0</span>)</li>
</ol>
</div>
<p>Slutsky’s theorem tells us that convergence in distribution behaves nicely when paired with random variables that converge to a constant (this is not true in general!).</p>
<div class="theorem">
<p><strong>Continuous Mapping Theorem</strong></p>
<p>If <span class="math inline">g</span> is a continuous function:</p>
<ol type="1">
<li><span class="math inline">X_n \xrightarrow{P} X \implies g(X_n) \xrightarrow{P} g(X)</span></li>
<li><span class="math inline">X_n \rightsquigarrow X \implies g(X_n) \rightsquigarrow g(X)</span></li>
</ol>
</div>
<p>Finally, we see that continuous mappings behave nicely for both types of convergence.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Important limitation</strong>: In general, if <span class="math inline">X_n \rightsquigarrow X</span> and <span class="math inline">Y_n \rightsquigarrow Y</span>, we <strong>cannot</strong> conclude that <span class="math inline">X_n + Y_n \rightsquigarrow X + Y</span>. Convergence in distribution does not preserve sums unless one component converges to a constant!</p>
<p><strong>Counterexample</strong>: Let <span class="math inline">X \sim \mathcal{N}(0,1)</span> and define <span class="math inline">Y_n = -X</span> for all <span class="math inline">n</span>. Then <span class="math inline">Y_n \sim \mathcal{N}(0,1)</span>, so <span class="math inline">Y_n \rightsquigarrow Y \sim \mathcal{N}(0,1)</span>. But <span class="math inline">X + Y_n = X - X = 0</span>, which does not converge in distribution to <span class="math inline">X + Y \sim \mathcal{N}(0,2)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key takeaway
</div>
</div>
<div class="callout-body-container callout-body">
<p>The rules for convergence are subtle. Generally speaking, convergence in probability behaves nicely under algebraic operations, but convergence in distribution requires more care. Always verify which type of convergence you have before applying these properties!</p>
</div>
</div>
</section>
</section>
<section id="the-two-fundamental-theorems-of-statistics" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="the-two-fundamental-theorems-of-statistics"><span class="header-section-number">3.5</span> The Two Fundamental Theorems of Statistics</h2>
<section id="the-law-of-large-numbers-lln" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="the-law-of-large-numbers-lln"><span class="header-section-number">3.5.1</span> The Law of Large Numbers (LLN)</h3>
<p>The Law of Large Numbers formalizes one of our most basic intuitions about probability: averages stabilize as we collect more data. When we flip a fair coin many times, the proportion of heads approaches 1/2. When we measure heights of many people, the sample mean approaches the population mean. This isn’t just intuition – it’s a mathematical theorem.</p>
<div class="theorem" name="Weak Law of Large Numbers">
<p>Let <span class="math inline">X_1, X_2, \ldots, X_n</span> be independent and identically distributed (IID) random variables with <span class="math inline">\mathbb{E}(X_i) = \mu</span> and <span class="math inline">\mathbb{V}(X_i) = \sigma^2 &lt; \infty</span>. Define the <strong>sample mean</strong>: <span class="math display">\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i</span></p>
<p>Then <span class="math inline">\bar{X}_n \xrightarrow{P} \mu</span>.</p>
</div>
<p><strong>Interpretation</strong>: The sample mean converges in probability to the population mean. As we collect more data, our estimate gets arbitrarily close to the true value with high probability.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We’ll use Chebyshev’s inequality. First, compute the mean and variance of <span class="math inline">\bar{X}_n</span>:</p>
<p><span class="math display">\mathbb{E}(\bar{X}_n) = \mathbb{E}\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X_i) = \frac{1}{n} \cdot n\mu = \mu</span></p>
<p><span class="math display">\mathbb{V}(\bar{X}_n) = \mathbb{V}\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n \mathbb{V}(X_i) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}</span></p>
<p>(We used independence for the variance calculation.)</p>
<p>Now apply Chebyshev’s inequality: for any <span class="math inline">\epsilon &gt; 0</span>, <span class="math display">\mathbb{P}(|\bar{X}_n - \mu| &gt; \epsilon) \leq \frac{\mathbb{V}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0 \text{ as } n \to \infty</span></p>
<p>Therefore <span class="math inline">\bar{X}_n \xrightarrow{P} \mu</span>.</p>
</div>
</div>
</div>
<p>Let’s visualize the Law of Large Numbers in action by simulating repeated rolls of a standard six-sided die and computing the mean of all rolls until that point.</p>
<p>We show this in two plots: on a normal scale (top) and on a log-scale (bottom) for the number of rolls on the <span class="math inline">x</span> axis. The bottom plot also zooms in on the <span class="math inline">y</span>-axis around <span class="math inline">3.5</span>.</p>
<p>Note how the sample mean starts with high variability but converges to the true mean (<span class="math inline">3.5</span>) as the number of rolls increases.</p>
<div id="8080a09b" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="5">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate die rolls</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>n_max <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>die_rolls <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">7</span>, n_max)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>cumulative_mean <span class="op">=</span> np.cumsum(die_rolls) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_max <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="fl">3.5</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot without shared x-axis</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Top plot: Full scale with linear x-axis</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>ax1.plot(cumulative_mean, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span>true_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'True mean = </span><span class="sc">{</span>true_mean<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Law of Large Numbers: Sample Mean of Die Rolls'</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom plot: Zoomed in to show convergence with log scale</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.arange(<span class="dv">100</span>, n_max)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_values, cumulative_mean[<span class="dv">100</span>:], linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span>true_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Number of rolls'</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>ax2.set_xscale(<span class="st">'log'</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="fl">3.2</span>, <span class="fl">3.8</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Show convergence at specific sample sizes</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> [<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>]:</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"After </span><span class="sc">{</span>n<span class="sc">:5d}</span><span class="ss"> rolls: sample mean = </span><span class="sc">{</span>cumulative_mean[n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">, "</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"error = </span><span class="sc">{</span><span class="bu">abs</span>(cumulative_mean[n<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> true_mean)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-convergence-inference_files/figure-html/cell-6-output-1.png" width="661" height="467" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After    10 rolls: sample mean = 3.8000, error = 0.3000
After   100 rolls: sample mean = 3.6900, error = 0.1900
After  1000 rolls: sample mean = 3.4570, error = 0.0430
After 10000 rolls: sample mean = 3.4999, error = 0.0001</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Weak vs Strong Laws of Large Numbers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The theorem above is known as the “Weak” Law of Large Numbers because it guarantees convergence in probability. There exists a stronger version that guarantees <strong>almost sure convergence</strong>: <span class="math inline">\mathbb{P}(\bar{X}_n \to \mu) = 1</span>. The “Strong” LLN says that with probability 1, the sample mean will eventually get arbitrarily close to <span class="math inline">\mu</span> and <em>stay</em> close, while the Weak LLN only guarantees that the probability of being far from <span class="math inline">\mu</span> goes to zero. The Weak LLN requires only finite variance, while the Strong LLN typically needs additional assumptions (like finite fourth moments) but delivers a more powerful conclusion. We present the Weak version as it has minimal assumptions and suffices for most statistical applications.</p>
</div>
</div>
</div>
</section>
<section id="the-central-limit-theorem-clt" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="the-central-limit-theorem-clt"><span class="header-section-number">3.5.2</span> The Central Limit Theorem (CLT)</h3>
<p>While the Law of Large Numbers tells us that sample means converge to the population mean, it doesn’t tell us about the <em>distribution</em> of the sample mean. The Central Limit Theorem fills this gap with a remarkable result: properly scaled sample means are approximately normal, regardless of the underlying distribution!</p>
<div class="theorem" name="Central Limit Theorem">
<p>Let <span class="math inline">X_1, X_2, \ldots, X_n</span> be IID random variables with <span class="math inline">\mathbb{E}(X_i) = \mu</span> and <span class="math inline">\mathbb{V}(X_i) = \sigma^2 &lt; \infty</span>. Define: <span class="math display">Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma}</span></p>
<p>Then <span class="math inline">Z_n \rightsquigarrow Z</span> where <span class="math inline">Z \sim \mathcal{N}(0, 1)</span>.</p>
</div>
<p><strong>Alternative notations</strong> (all mean the same thing):</p>
<ul>
<li><span class="math inline">\bar{X}_n \approx \mathcal{N}(\mu, \sigma^2/n)</span> for large <span class="math inline">n</span></li>
<li><span class="math inline">\sqrt{n}(\bar{X}_n - \mu) \rightsquigarrow \mathcal{N}(0, \sigma^2)</span></li>
<li><span class="math inline">(\bar{X}_n - \mu)/(\sigma/\sqrt{n}) \rightsquigarrow \mathcal{N}(0, 1)</span></li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Critical Point</strong>: The CLT is about the distribution of the <em>sample mean</em>, not the data itself! The original data doesn’t become normal—only the sampling distribution of <span class="math inline">\bar{X}_n</span> does.</p>
</div>
</div>
<section id="interactive-demonstration-of-the-clt" class="level4">
<h4 class="anchored" data-anchor-id="interactive-demonstration-of-the-clt">Interactive Demonstration of the CLT</h4>
<p>Let’s visualize how the CLT works for different distributions from continuous to discrete and skewed (asymmetrical).</p>
<p>The interactive visualization below allows you to see this convergence in action. You can change the underlying distribution and adjust the sample size <code>n</code> to see how the distribution of the standardized sample mean approaches a standard normal distribution (red curves).</p>
<div class="cell">
<details class="code-fold hidden">
<summary>Show code</summary>
<div class="sourceCode cell-code hidden" id="cb6" data-startfrom="779" data-source-offset="-0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 778;"><span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> {cltDemo} <span class="im">from</span> <span class="st">"../js/clt-demo.js"</span></span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a>d3 <span class="op">=</span> <span class="pp">require</span>(<span class="st">"d3@7"</span>)</span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a>Inputs <span class="op">=</span> <span class="pp">require</span>(<span class="st">"https://cdn.jsdelivr.net/npm/@observablehq/inputs@0.10.6/dist/inputs.min.js"</span>)</span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> <span class="fu">cltDemo</span>(d3)<span class="op">;</span></span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a>viewof distName <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">select</span>(</span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a>  [<span class="st">"Uniform"</span><span class="op">,</span> <span class="st">"Exponential"</span><span class="op">,</span> <span class="st">"Bernoulli"</span><span class="op">,</span> <span class="st">"Skewed Discrete"</span>]<span class="op">,</span> </span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a>  {<span class="dt">label</span><span class="op">:</span> <span class="st">"Population Distribution"</span>}</span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a>viewof sampleSize <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(</span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span><span class="op">,</span> <span class="dv">100</span>]<span class="op">,</span> </span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a>  {<span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Sample Size (n)"</span>}</span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a>viewof numSimulations <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(</span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">100</span><span class="op">,</span> <span class="dv">10000</span>]<span class="op">,</span> </span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a>  {<span class="dt">step</span><span class="op">:</span> <span class="dv">100</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">10000</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Number of Simulations"</span>}</span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> plot <span class="op">=</span> demo<span class="op">.</span><span class="fu">createVisualization</span>({</span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a>    <span class="dt">distName</span><span class="op">:</span> distName<span class="op">,</span></span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a>    <span class="dt">sampleSize</span><span class="op">:</span> sampleSize<span class="op">,</span></span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a>    <span class="dt">numSimulations</span><span class="op">:</span> numSimulations<span class="op">,</span></span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a>  })<span class="op">;</span></span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> plot<span class="op">;</span> </span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="expression">

</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: CLT in Practice
</div>
</div>
<div class="callout-body-container callout-body">
<p>A factory produces bolts with mean length <span class="math inline">\mu = 5</span> cm and standard deviation <span class="math inline">\sigma = 0.1</span> cm. If we randomly sample 100 bolts, what’s the probability their average length exceeds 5.02 cm?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>By the CLT, <span class="math inline">\bar{X}_{100} \approx \mathcal{N}(5, 0.1^2/100) = \mathcal{N}(5, 0.0001)</span>.</p>
<p>We want: <span class="math display">\mathbb{P}(\bar{X}_{100} &gt; 5.02) = \mathbb{P}\left(\frac{\bar{X}_{100} - 5}{0.01} &gt; \frac{5.02 - 5}{0.01}\right) = \mathbb{P}(Z &gt; 2)</span></p>
<p>where <span class="math inline">Z \sim \mathcal{N}(0,1)</span>. From standard normal tables: <span class="math inline">\mathbb{P}(Z &gt; 2) \approx 0.0228</span>.</p>
<p>So there’s about a 2.3% chance the sample mean exceeds 5.02 cm.</p>
</div>
</div>
</div>
</div>
</div>
<div class="theorem">
<p><strong>CLT with Unknown Variance</strong>: If we replace <span class="math inline">\sigma</span> with the sample standard deviation <span class="math display">S_n = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2}</span></p>
<p>then we still have: <span class="math display">\frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n} \rightsquigarrow \mathcal{N}(0, 1)</span></p>
</div>
<p>This version is crucial for practice since we rarely know the true variance!</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rejoinder: Understanding Algorithms
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember the sequence of random variables <span class="math inline">\theta_1, \theta_2, \ldots</span> from our stochastic optimization algorithm at the beginning of this chapter? We can now answer what kind of convergence we should expect:</p>
<p><strong>Convergence in probability</strong>: We want <span class="math inline">\theta_n \xrightarrow{P} \theta^*</span> where <span class="math inline">\theta^*</span> is the true optimal solution. This means the probability of <span class="math inline">\theta_n</span> being far from the optimum vanishes as iterations increase.</p>
<p>The tools we’ve covered – probability inequalities (to bound deviations), convergence concepts (to formalize what “converges” means), and limit theorems (to understand averaging behavior) – are the foundation for analyzing when and why algorithms like stochastic gradient descent converge to good solutions. Modern machine learning theory relies heavily on these concepts to provide theoretical guarantees about algorithm performance!</p>
</div>
</div>
</section>
</section>
</section>
<section id="the-language-of-statistical-inference" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="the-language-of-statistical-inference"><span class="header-section-number">3.6</span> The Language of Statistical Inference</h2>
<section id="from-probability-to-inference" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="from-probability-to-inference"><span class="header-section-number">3.6.1</span> From Probability to Inference</h3>
<p>We’ve developed powerful tools: inequalities that bound uncertainty, convergence concepts that describe limiting behavior, and fundamental theorems that guarantee nice properties of averages. Now we flip the perspective.</p>
<p><strong>Probability</strong>: Given a known distribution, what can we say about the data we’ll observe?</p>
<p><strong>Statistical Inference</strong>: Given observed data, what can we infer about the unknown distribution that generated it? More formally: given a sample <span class="math inline">X_1, \ldots, X_n \sim F</span>, how do we infer <span class="math inline">F</span>?</p>
<p>This process – often called “learning” in computer science – is at the core of both classical statistics and modern machine learning. Sometimes we want to infer the entire distribution <span class="math inline">F</span>, but often we focus on specific features like its mean, variance, or other parameters.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Modeling Uncertainty in Real Decisions
</div>
</div>
<div class="callout-body-container callout-body">
<p>An online retailer tests a new ad campaign. Out of 1000 users who see the ad, 30 make a purchase (3% conversion rate). But this raises critical questions:</p>
<p><strong>Immediate questions:</strong></p>
<ul>
<li>What can we say about the true conversion rate? Is it exactly 3%?</li>
<li>How likely is it that at least 25 out of the next 1000 users will convert?</li>
</ul>
<p><strong>Comparative questions:</strong></p>
<ul>
<li>A competing ad had 290 conversions out of 10,000 users (2.9% rate). Which is better?</li>
<li>How confident can we be that the 3% ad truly outperforms the 2.9% ad – could the difference just be due to random chance?</li>
</ul>
<p><strong>Long-term questions:</strong></p>
<ul>
<li>What’s the probability that the long-run conversion rate exceeds 2.5%?</li>
<li>How many more users do we need to test to be 95% confident about the true rate?</li>
</ul>
<p>These questions – about uncertainty, confidence, and decision-making with limited data – are at the heart of statistical inference.</p>
</div>
</div>
</section>
<section id="statistical-models" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="statistical-models"><span class="header-section-number">3.6.2</span> Statistical Models</h3>
<div class="definition">
<p>A <strong>statistical model</strong> <span class="math inline">\mathfrak{F}</span> is a set of probability distributions (or densities or regression functions).</p>
</div>
<p>In the context of inference, we use models to represent our assumptions about which distributions could have generated our observed data. The model defines the “universe of possibilities” we’re considering – we then use data to identify which specific distribution within <span class="math inline">\mathfrak{F}</span> is most plausible.</p>
<p>Models come in two main flavors, <strong>parametric</strong> and <strong>nonparametric</strong>.</p>
<section id="parametric-models" class="level4">
<h4 class="anchored" data-anchor-id="parametric-models">Parametric Models</h4>
<div class="definition">
<p>A <strong>parametric model</strong> is indexed by a finite number of parameters. We write it as: <span class="math display">\mathfrak{F} = \{f(x; \theta) : \theta \in \Theta\}</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\theta</span> (<a href="https://en.wikipedia.org/wiki/Theta">theta</a>) is the <strong>parameter</strong> (possibly vector-valued)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
<li><span class="math inline">\Theta</span> (capital theta) is the <strong>parameter space</strong> (the set of all possible parameter values)</li>
<li><span class="math inline">f(x; \theta)</span> is the density or distribution function indexed by <span class="math inline">\theta</span></li>
</ul>
</div>
<p>Typically, the parameters <span class="math inline">\theta</span> are unknown quantities we want to estimate. If there are elements of the vector <span class="math inline">\theta</span> that we are not interested in, those are called <strong>nuisance parameters</strong>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Feature Performance as a Parametric Model
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A product manager at a tech company launches a new “AI Recap” feature in their app. To determine if the feature is a success, they track the number of daily views over the first month. They hypothesize that the daily view count approximately follows a normal distribution.</p>
<p>The model for daily views is a parametric family <span class="math inline">\mathfrak{F}</span>: <span class="math display">\mathfrak{F} = \{f(x; \theta) : \theta = (\mu, \sigma^2), \mu \in \mathbb{R}, \sigma^2 &gt; 0\}</span></p>
<p>where the density function is:<span class="math display">f(x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)</span></p>
<p>This is a 2-dimensional parametric model with:</p>
<ul>
<li><strong>Parameter vector</strong>: <span class="math inline">\theta = (\mu, \sigma^2)</span></li>
<li><strong>Parameter space</strong>: <span class="math inline">\Theta = \mathbb{R} \times (0, \infty)</span></li>
</ul>
<p><strong>Nuisance parameters in action</strong>: The company has set a target: the feature will be considered successful and receive further development only if it can reliably generate more than 100,000 views per day.</p>
<ul>
<li>The <strong>parameter of interest</strong> is the average daily views, <span class="math inline">\mu</span>. The entire business decision hinges on testing the hypothesis that <span class="math inline">\mu &gt; 100,000</span>.</li>
<li>The <strong>nuisance parameter</strong> is the variance, <span class="math inline">\sigma^2</span>. The day-to-day fluctuation in views is critical for assessing the statistical certainty of our estimate for <span class="math inline">\mu</span>, but it’s not the primary metric for success. The product manager needs to account for this variability, but their core question is about the average performance, not the variability itself.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="nonparametric-models" class="level4">
<h4 class="anchored" data-anchor-id="nonparametric-models">Nonparametric Models</h4>
<p><strong>Nonparametric Models</strong> cannot be parameterized by a finite number of parameters. These models make minimal assumptions about the distribution. For example: <span class="math display">\mathfrak{F}_{\text{ALL}} = \{\text{all continuous CDFs}\}</span></p>
<p>or with some constraints: <span class="math display">\mathfrak{F} = \{\text{all distributions with finite variance}\}</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How can we work with “all distributions”?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This seems impossibly broad! In practice, we don’t explicitly enumerate all possible distributions. Instead, nonparametric methods <strong>directly use the data</strong> without assuming a specific functional form or parameter to be estimated. We will see multiple concrete example of nonparametric techniques in the next chapter. So, in theory the model space is infinite-dimensional, but in practice nonparametric estimation procedures are still concrete and computable.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Choosing a Model
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Scenario 1</strong>: Heights of adult males in Finland.</p>
<ul>
<li><strong>Parametric choice</strong>: <span class="math inline">\mathfrak{F} = \{\mathcal{N}(\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma &gt; 0\}</span></li>
<li><strong>Justification</strong>: Heights are often approximately normal due to many small genetic and environmental factors (CLT in action!)</li>
</ul>
<p><strong>Scenario 2</strong>: Time between website visits.</p>
<ul>
<li><strong>Parametric choice</strong>: <span class="math inline">\mathfrak{F} = \{\text{Exponential}(\lambda) : \lambda &gt; 0\}</span></li>
<li><strong>Justification</strong>: Exponential models “memoryless” waiting times</li>
</ul>
<p><strong>Scenario 3</strong>: Unknown distribution shape.</p>
<ul>
<li><strong>Nonparametric choice</strong>: <span class="math inline">\mathfrak{F} = \{\text{all distributions with finite variance}\}</span></li>
<li><strong>Justification</strong>: Make minimal assumptions, let data speak</li>
</ul>
</div>
</div>
</section>
</section>
<section id="point-estimation" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="point-estimation"><span class="header-section-number">3.6.3</span> Point Estimation</h3>
<div class="definition">
<p><strong>Point estimation</strong> is the task of providing a single “best guess” for an unknown quantity based on data.</p>
</div>
<p>This quantity can be a single parameter, a full vector of parameters, even a full CDF or PDF, or prediction for a future value of some random variable.</p>
<p>A point estimate of <span class="math inline">\theta</span> is denoted by <span class="math inline">\hat{\theta}</span>.</p>
<div class="definition">
<p>Given data <span class="math inline">X_1, \ldots, X_n</span>, a <strong>point estimator</strong> is a function: <span class="math display">\hat{\theta}_n = g(X_1, \ldots, X_n)</span></p>
</div>
<p>The “hat” notation <span class="math inline">\hat{\theta}</span> can indicate both an estimator and the estimate.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Critical Distinction</strong>:</p>
<ul>
<li><strong>Parameter</strong> <span class="math inline">\theta</span>: Fixed, unknown number we want to learn</li>
<li><strong>Estimator</strong> <span class="math inline">\hat{\theta}_n</span>: Random variable (before seeing data)</li>
<li><strong>Estimate</strong> <span class="math inline">\hat{\theta}_n</span>: Specific number (after seeing data) – notation can be overlapping</li>
</ul>
<p>For example, <span class="math inline">\bar{X}_n</span> is an estimator; <span class="math inline">\bar{x}_n = 3.7</span> is an estimate.</p>
</div>
</div>
<p>The distribution of <span class="math inline">\hat{\theta}_n</span> is called the <strong>sampling distribution</strong>. The standard deviation of this distribution is the <strong>standard error</strong>: <span class="math display">\text{se}(\hat{\theta}_n) = \sqrt{\mathbb{V}(\hat{\theta}_n)}</span></p>
<p>When the standard error depends on unknown parameters, we use the <strong>estimated standard error</strong> <span class="math inline">\widehat{\text{se}}</span>.</p>
<p>A particularly common standard error in stastistics is the <a href="https://en.wikipedia.org/wiki/Standard_error">standard error of the mean (SEM)</a>.</p>
</section>
<section id="how-to-evaluate-estimators" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="how-to-evaluate-estimators"><span class="header-section-number">3.6.4</span> How to Evaluate Estimators</h3>
<p>How do we judge if an estimator is “good”? Several criteria have emerged:</p>
<div class="definition">
<p><strong>Bias</strong>: The systematic error of an estimator. <span class="math display">\text{bias}(\hat{\theta}_n) = \mathbb{E}(\hat{\theta}_n) - \theta</span></p>
<p>An estimator is <strong>unbiased</strong> if <span class="math inline">\mathbb{E}(\hat{\theta}_n) = \theta</span>.</p>
</div>
<div class="definition">
<p><strong>Consistency</strong>: An estimator is consistent if it converges to the true value. <span class="math display">\hat{\theta}_n \xrightarrow{P} \theta \text{ as } n \to \infty</span></p>
</div>
<div class="definition">
<p><strong>Mean Squared Error (MSE)</strong>: The average squared distance from the truth. <span class="math display">\text{MSE}(\hat{\theta}_n) = \mathbb{E}[(\hat{\theta}_n - \theta)^2]</span></p>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Evaluating Estimators for the Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\theta, \sigma^2)</span> where <span class="math inline">\theta</span> is unknown. Consider three estimators:</p>
<ol type="1">
<li><strong>Constant estimator</strong>: <span class="math inline">\hat{\theta}_n^{(1)} = 3</span>
<ul>
<li>Bias: <span class="math inline">\mathbb{E}(3) - \theta = 3 - \theta</span> (biased unless <span class="math inline">\theta = 3</span>)</li>
<li>Variance: <span class="math inline">\mathbb{V}(3) = 0</span></li>
<li>Consistent: No, always equals 3</li>
<li>MSE: <span class="math inline">(3 - \theta)^2</span></li>
</ul></li>
<li><strong>First observation</strong>: <span class="math inline">\hat{\theta}_n^{(2)} = X_1</span>
<ul>
<li>Bias: <span class="math inline">\mathbb{E}(X_1) - \theta = 0</span> (unbiased!)</li>
<li>Variance: <span class="math inline">\mathbb{V}(X_1) = \sigma^2</span></li>
<li>Consistent: No, variance doesn’t shrink</li>
<li>MSE: <span class="math inline">\sigma^2</span></li>
</ul></li>
<li><strong>Sample mean</strong>: <span class="math inline">\hat{\theta}_n^{(3)} = \bar{X}_n</span>
<ul>
<li>Bias: <span class="math inline">\mathbb{E}(\bar{X}_n) - \theta = 0</span> (unbiased!)</li>
<li>Variance: <span class="math inline">\mathbb{V}(\bar{X}_n) = \sigma^2/n</span></li>
<li>Consistent: Yes! (by LLN)</li>
<li>MSE: <span class="math inline">\sigma^2/n \to 0</span></li>
</ul></li>
</ol>
<p>The sample mean is unbiased AND consistent—it improves with more data!</p>
</div>
</div>
<p>A classic example shows that unbiased isn’t everything:</p>
<p><strong>Sample variance</strong>: Two common estimators for population variance <span class="math inline">\sigma^2</span>:</p>
<ol type="1">
<li><strong>Unbiased version</strong>: <span class="math inline">S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2</span>
<ul>
<li><span class="math inline">\mathbb{E}(S^2) = \sigma^2</span> (unbiased by design)</li>
</ul></li>
<li><strong>Maximum likelihood estimator</strong>: <span class="math inline">\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X})^2</span>
<ul>
<li><span class="math inline">\mathbb{E}(\hat{\sigma}^2) = \frac{n-1}{n}\sigma^2</span> (biased!)</li>
</ul></li>
</ol>
<p>Which is better? It depends on the criterion!</p>
</section>
<section id="the-bias-variance-tradeoff" class="level3" data-number="3.6.5">
<h3 data-number="3.6.5" class="anchored" data-anchor-id="the-bias-variance-tradeoff"><span class="header-section-number">3.6.5</span> The Bias-Variance Tradeoff</h3>
<div class="theorem">
<p>The MSE decomposes as: <span class="math display">\text{MSE} = \text{bias}^2(\hat{\theta}_n) + \mathbb{V}(\hat{\theta}_n)</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\bar{\theta}_n = \mathbb{E}(\hat{\theta}_n)</span>. Then: <span class="math display">\begin{align}
\mathbb{E}[(\hat{\theta}_n - \theta)^2]
&amp;= \mathbb{E}[(\hat{\theta}_n - \bar{\theta}_n + \bar{\theta}_n - \theta)^2] \\
&amp;= \mathbb{E}[(\hat{\theta}_n - \bar{\theta}_n)^2] + 2(\bar{\theta}_n - \theta)\mathbb{E}[\hat{\theta}_n - \bar{\theta}_n] + (\bar{\theta}_n - \theta)^2 \\
&amp;= \mathbb{E}[(\hat{\theta}_n - \bar{\theta}_n)^2] + 2(\bar{\theta}_n - \theta) \cdot 0 + (\bar{\theta}_n - \theta)^2 \\
&amp;= \mathbb{V}(\hat{\theta}_n) + \text{bias}^2(\hat{\theta}_n)
\end{align}</span></p>
<p>where we used that <span class="math inline">\mathbb{E}[\hat{\theta}_n - \bar{\theta}_n] = \mathbb{E}[\hat{\theta}_n] - \bar{\theta}_n = \bar{\theta}_n - \bar{\theta}_n = 0</span>.</p>
</div>
</div>
</div>
<p>This decomposition reveals a fundamental tradeoff in statistics.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255588-470-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-470-1" role="tab" aria-controls="tabset-1757255588-470-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-470-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-470-2" role="tab" aria-controls="tabset-1757255588-470-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-470-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-470-3" role="tab" aria-controls="tabset-1757255588-470-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255588-470-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255588-470-1-tab"><p>Imagine you’re an archer trying to hit a target. Your performance
depends on two things:</p><p><strong>Bias</strong>: How far your average shot is from the
bullseye. A biased archer consistently aims too high or too far
left.</p><p><strong>Variance</strong>: How spread out your shots are. A
high-variance archer is inconsistent—sometimes dead on, sometimes way
off.</p><p>The best archer has low bias AND low variance. But here’s the key
insight: sometimes accepting a little bias can dramatically reduce
variance, improving overall accuracy!</p><p>Think of it this way:</p><ul>
<li>A complex model (like memorizing training data) has low bias but
high variance</li>
<li>A simple model (like always predicting the average) has higher bias
but low variance</li>
<li>The sweet spot balances both</li>
</ul><p>This tradeoff is why regularization works in machine learning – we
accept a bit of bias to gain a lot in variance reduction.</p></div><div id="tabset-1757255588-470-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-470-2-tab"><p>The bias-variance decomposition gives us a precise way to understand
prediction error:</p><p><span class="math display">\[\text{MSE}(\hat{\theta}_n) = \text{bias}^2(\hat{\theta}_n) + \mathbb{V}(\hat{\theta}_n)\]</span></p><p>This isn’t just algebra – it reveals the two fundamental sources of
error:</p><ol type="1">
<li><strong>Systematic error</strong> (bias): Being consistently
wrong</li>
<li><strong>Random error</strong> (variance): Being inconsistently
wrong</li>
</ol><p>For prediction problems where
<span class="math inline">\(\hat{f}(x)\)</span> estimates
<span class="math inline">\(f(x)\)</span>:
<span class="math display">\[\mathbb{E}[(\hat{f}(x) - f(x))^2] = \underbrace{(E[\hat{f}(x)] - f(x))^2}_{\text{bias}^2} + \underbrace{\mathbb{V}(\hat{f}(x))}_{\text{variance}}\]</span></p><p>The optimal predictor minimizes their sum. In machine learning:</p><ul>
<li>Increasing model complexity typically decreases bias but increases
variance</li>
<li>The art is finding the right complexity for your data</li>
</ul></div><div id="tabset-1757255588-470-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-470-3-tab"><p>Let’s visualize the bias-variance tradeoff by comparing estimators
for population variance.</p><div id="b49b31fd" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="6">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>true_variance <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># True σ² = 1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> np.arange(<span class="dv">5</span>, <span class="dv">101</span>, <span class="dv">5</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for results</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>bias_unbiased <span class="op">=</span> []</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>bias_mle <span class="op">=</span> []</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>variance_unbiased <span class="op">=</span> []</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>variance_mle <span class="op">=</span> []</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>mse_unbiased <span class="op">=</span> []</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>mse_mle <span class="op">=</span> []</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> n_values:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate many samples and compute both estimators</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    unbiased_estimates <span class="op">=</span> []</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    mle_estimates <span class="op">=</span> []</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate sample from N(0, 1)</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        sample_mean <span class="op">=</span> np.mean(sample)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Unbiased estimator (n-1 denominator)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        s_squared <span class="op">=</span> np.<span class="bu">sum</span>((sample <span class="op">-</span> sample_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (n <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        unbiased_estimates.append(s_squared)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MLE (n denominator)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        sigma_hat_squared <span class="op">=</span> np.<span class="bu">sum</span>((sample <span class="op">-</span> sample_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> n</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        mle_estimates.append(sigma_hat_squared)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate bias, variance, and MSE</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    unbiased_estimates <span class="op">=</span> np.array(unbiased_estimates)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    mle_estimates <span class="op">=</span> np.array(mle_estimates)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    bias_unbiased.append(np.mean(unbiased_estimates) <span class="op">-</span> true_variance)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    bias_mle.append(np.mean(mle_estimates) <span class="op">-</span> true_variance)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Variance</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    variance_unbiased.append(np.var(unbiased_estimates))</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    variance_mle.append(np.var(mle_estimates))</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># MSE</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    mse_unbiased.append(np.mean((unbiased_estimates <span class="op">-</span> true_variance)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    mse_mle.append(np.mean((mle_estimates <span class="op">-</span> true_variance)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plots</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Bias plot</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>ax1.plot(n_values, bias_unbiased, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Unbiased (S²)'</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>ax1.plot(n_values, bias_mle, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE (σ̂²)'</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Sample size (n)'</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Bias'</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Bias'</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance plot  </span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>ax2.plot(n_values, variance_unbiased, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Unbiased (S²)'</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>ax2.plot(n_values, variance_mle, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE (σ̂²)'</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Sample size (n)'</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Variance'</span>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Variance of Estimator'</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE plot</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>ax3.plot(n_values, mse_unbiased, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Unbiased (S²)'</span>)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>ax3.plot(n_values, mse_mle, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE (σ̂²)'</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'Sample size (n)'</span>)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'MSE'</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>ax3.set_title(<span class="st">'Mean Squared Error'</span>)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>ax3.legend()</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>ax3.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">'Bias-Variance Tradeoff: Variance Estimators'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Print numerical comparison for n=10</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>n_idx <span class="op">=</span> <span class="dv">1</span>  <span class="co"># n=10</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For n=10:"</span>)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbiased (S²): Bias = </span><span class="sc">{</span>bias_unbiased[n_idx]<span class="sc">:.4f}</span><span class="ss">, Variance = </span><span class="sc">{</span>variance_unbiased[n_idx]<span class="sc">:.4f}</span><span class="ss">, MSE = </span><span class="sc">{</span>mse_unbiased[n_idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE (σ̂²):      Bias = </span><span class="sc">{</span>bias_mle[n_idx]<span class="sc">:.4f}</span><span class="ss">, Variance = </span><span class="sc">{</span>variance_mle[n_idx]<span class="sc">:.4f}</span><span class="ss">, MSE = </span><span class="sc">{</span>mse_mle[n_idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">The MLE has lower MSE despite being biased!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="03-convergence-inference_files/figure-html/cell-7-output-1.png" width="664" height="473"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>For n=10:
Unbiased (S²): Bias = -0.0013, Variance = 0.2219, MSE = 0.2219
MLE (σ̂²):      Bias = -0.1012, Variance = 0.1797, MSE = 0.1899

The MLE has lower MSE despite being biased!</code></pre>
</div>
</div><p>Key insights from the visualization:</p><ul>
<li>The unbiased estimator <span class="math inline">\(S^2\)</span> has
zero bias (blue solid line at 0)</li>
<li>The MLE <span class="math inline">\(\hat{\sigma}^2\)</span> has
negative bias that shrinks as <span class="math inline">\(n\)</span>
grows</li>
<li>The MLE has lower variance than the unbiased estimator</li>
<li><strong>For finite samples, the biased MLE has lower
MSE!</strong></li>
</ul><p>This demonstrates a profound principle: the “best” estimator depends
on your criterion. Unbiasedness is nice, but minimizing MSE often
matters more in practice.</p></div></div></div>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">3.7</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">3.7.1</span> Key Concepts Review</h3>
<p>We’ve built a complete framework for understanding randomness and inference:</p>
<p><strong>Inequalities</strong> bound the unknown:</p>
<ul>
<li><strong>Markov</strong>: <span class="math inline">\mathbb{P}(X \geq t) \leq \mathbb{E}(X)/t</span> — averages constrain extremes</li>
<li><strong>Chebyshev</strong>: <span class="math inline">\mathbb{P}(|X - \mu| \geq k\sigma) \leq 1/k^2</span> — universal bounds using variance</li>
</ul>
<p><strong>Convergence</strong> describes limiting behavior:</p>
<ul>
<li><strong>In probability</strong>: <span class="math inline">X_n \xrightarrow{P} X</span> — the random variable itself settles down</li>
<li><strong>In distribution</strong>: <span class="math inline">X_n \rightsquigarrow X</span> — the shape of the distribution stabilizes</li>
<li><strong>Key relationship</strong>: Convergence in probability implies convergence in distribution</li>
</ul>
<p><strong>Fundamental theorems</strong> guarantee nice behavior:</p>
<ul>
<li><strong>Law of Large Numbers</strong>: <span class="math inline">\bar{X}_n \xrightarrow{P} \mu</span> — sample means converge to population means</li>
<li><strong>Central Limit Theorem</strong>: <span class="math inline">\sqrt{n}(\bar{X}_n - \mu)/\sigma \rightsquigarrow \mathcal{N}(0,1)</span> — sample means are approximately normal</li>
</ul>
<p><strong>Statistical inference</strong> flips the perspective:</p>
<ul>
<li><strong>Models</strong>: Parametric (finite-dimensional) vs nonparametric (infinite-dimensional)</li>
<li><strong>Estimators</strong>: Functions of data that guess parameters</li>
<li><strong>Standard error</strong>: Standard deviation of an estimator’s sampling distribution</li>
<li><strong>Evaluation criteria</strong>: Bias, variance, consistency, MSE</li>
<li><strong>Bias-variance tradeoff</strong>: <span class="math inline">\text{MSE} = \text{bias}^2 + \text{variance}</span></li>
</ul>
</section>
<section id="why-these-concepts-matter" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="why-these-concepts-matter"><span class="header-section-number">3.7.2</span> Why These Concepts Matter</h3>
<p><strong>For Statistical Inference</strong>:</p>
<ul>
<li>LLN justifies using sample statistics to estimate population parameters</li>
<li>CLT enables confidence intervals and hypothesis tests (to be seen in the next chapters)</li>
<li>Bias-variance tradeoff guides choice of estimators</li>
<li>Consistency ensures our methods improve with more data</li>
</ul>
<p><strong>For Machine Learning</strong>:</p>
<ul>
<li>Convergence concepts analyze iterative algorithms such as stochastic optimization</li>
<li>Bias-variance tradeoff explains overfitting vs underfitting</li>
<li>CLT justifies bootstrap and cross-validation, as we will see in the next chapters</li>
</ul>
<p><strong>For Data Science Practice</strong>:</p>
<ul>
<li>Understanding variability in estimates prevents overconfidence</li>
<li>Recognizing when CLT applies (and when it doesn’t)</li>
<li>Choosing between simple and complex models</li>
<li>Interpreting A/B test results correctly</li>
</ul>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">3.7.3</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><p><strong>Confusing convergence types</strong>:</p>
<ul>
<li>“My algorithm converged” – in what sense?</li>
<li>Convergence in distribution does <em>not</em> imply convergence in probability!</li>
</ul></li>
<li><p><strong>Misapplying the CLT</strong>:</p>
<ul>
<li>CLT is about <em>sample means</em>, not individual observations</li>
<li>Need large enough <span class="math inline">n</span> (depends on skewness)</li>
<li>Doesn’t work without finite variance (Cauchy!)</li>
</ul></li>
<li><p><strong>Overvaluing unbiasedness</strong>:</p>
<ul>
<li>Unbiased doesn’t mean good (e.g., using just <span class="math inline">X_1</span>)</li>
<li>Biased can be better in statistics (regularization, priors)</li>
</ul></li>
<li><p><strong>Ignoring assumptions</strong>:</p>
<ul>
<li>Independence matters for variance calculations</li>
<li>Finite variance required for CLT</li>
</ul></li>
<li><p><strong>Misinterpreting bounds</strong>:</p>
<ul>
<li>Markov/Chebyshev give worst-case bounds</li>
<li>Often very loose in practice</li>
<li>Tighter bounds exist for specific distributions</li>
</ul></li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">3.7.4</span> Chapter Connections</h3>
<p>This chapter connects fundamental probability theory to practical statistical inference:</p>
<ul>
<li><strong>From Previous Chapters</strong>: We’ve applied Chapter 1’s probability framework and Chapter 2’s expectation/variance concepts to prove convergence theorems (LLN, CLT) that explain why sample statistics work as estimators</li>
<li><strong>Next - Chapter 4 (Bootstrap)</strong>: While this chapter gave us theoretical tools for inference (CLT-based confidence intervals), the bootstrap will provide a computational approach that works even when theoretical distributions are intractable</li>
<li><strong>Statistical Modeling (Chapters 5+)</strong>: The bias-variance tradeoff introduced here becomes central to model selection, while MSE serves as our primary tool for comparing estimators in regression and machine learning</li>
<li><strong>Throughout the Course</strong>: The convergence concepts (especially CLT) and inference framework established here underpin virtually every statistical method—from hypothesis testing to Bayesian inference</li>
</ul>
</section>
<section id="self-test-problems" class="level3" data-number="3.7.5">
<h3 data-number="3.7.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">3.7.5</span> Self-Test Problems</h3>
<ol type="1">
<li><p><strong>Applying Chebyshev</strong>: A website’s daily visitors have mean 10,000 and standard deviation 2,000. Without assuming any distribution, what can you say about the probability of getting fewer than 4,000 or more than 16,000 visitors?</p></li>
<li><p><strong>CLT Application</strong>: A casino’s slot machine pays out €1 with probability 0.4 and €0 otherwise. If someone plays 400 times, approximate the probability their total winnings exceed €170.</p></li>
<li><p><strong>Comparing Estimators</strong>: Given <span class="math inline">X_1, \ldots, X_n \sim \text{Uniform}(0, \theta)</span>, consider two estimators:</p>
<ul>
<li><span class="math inline">\hat{\theta}_1 = 2\bar{X}_n</span></li>
<li><span class="math inline">\hat{\theta}_2 = \frac{n+1}{n} \max\{X_1, \ldots, X_n\}</span></li>
</ul>
<p>Calculate bias and variance for each. Which has lower MSE?</p></li>
<li><p><strong>Convergence Concepts</strong>: Let <span class="math inline">X_n</span> have PMF: <span class="math display">P(X_n = 0) = 1 - 1/n, \quad P(X_n = n) = 1/n</span></p>
<ul>
<li>Does <span class="math inline">X_n \xrightarrow{P} 0</span>?</li>
<li>Does <span class="math inline">X_n \rightsquigarrow 0</span>?</li>
<li>Does <span class="math inline">\mathbb{E}(X_n) \to 0</span>?</li>
</ul></li>
</ol>
</section>
<section id="python-and-r-reference" class="level3" data-number="3.7.6">
<h3 data-number="3.7.6" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">3.7.6</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255588-108-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-108-1" role="tab" aria-controls="tabset-1757255588-108-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255588-108-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255588-108-2" role="tab" aria-controls="tabset-1757255588-108-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255588-108-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255588-108-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability inequalities</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> markov_bound(mean, t):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Markov inequality bound P(X &gt;= t)"""</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">min</span>(mean <span class="op">/</span> t, <span class="dv">1</span>) <span class="cf">if</span> t <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chebyshev_bound(k):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Chebyshev bound P(|X - μ| &gt;= kσ)"""</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">min</span>(<span class="dv">1</span> <span class="op">/</span> k<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>) <span class="cf">if</span> k <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulating convergence</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_lln(dist, n_max<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show Law of Large Numbers"""</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> dist.rvs(n_max)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    cumulative_mean <span class="op">=</span> np.cumsum(samples) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_max <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cumulative_mean</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_clt(dist, n, n_simulations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show Central Limit Theorem"""</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    sample_means <span class="op">=</span> []</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> dist.rvs(n)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        sample_means.append(np.mean(sample))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(sample_means)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimator evaluation</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_estimator(estimator_func, true_value, n, sample_func, n_simulations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute bias, variance, and MSE of an estimator via simulation."""</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    estimates <span class="op">=</span> []</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> sample_func(n)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        estimate <span class="op">=</span> estimator_func(sample)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        estimates.append(estimate)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    estimates <span class="op">=</span> np.array(estimates)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> np.mean(estimates) <span class="op">-</span> true_value</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> np.var(estimates)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean((estimates <span class="op">-</span> true_value)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'bias'</span>: bias, <span class="st">'variance'</span>: variance, <span class="st">'mse'</span>: mse}</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Common distributions for examples</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>normal_dist <span class="op">=</span> stats.norm(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>exp_dist <span class="op">=</span> stats.expon(scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>uniform_dist <span class="op">=</span> stats.uniform(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_mean(x):</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(x)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_variance_unbiased(x):</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.var(x, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># n-1 denominator</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_variance_mle(x):</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.var(x, ddof<span class="op">=</span><span class="dv">0</span>)  <span class="co"># n denominator</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Evaluate variance estimators for N(0,1) samples (true variance = 1)</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_normal_sample(n):</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>n)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"--- Evaluating Variance Estimators (n=10) ---"</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>mle_results <span class="op">=</span> evaluate_estimator(</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    estimator_func<span class="op">=</span>sample_variance_mle,</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    true_value<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    sample_func<span class="op">=</span>generate_normal_sample</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE Estimator: </span><span class="sc">{</span>mle_results<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>unbiased_results <span class="op">=</span> evaluate_estimator(</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    estimator_func<span class="op">=</span>sample_variance_unbiased,</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    true_value<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    sample_func<span class="op">=</span>generate_normal_sample</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbiased Estimator: </span><span class="sc">{</span>unbiased_results<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255588-108-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255588-108-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability inequalities</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>markov_bound <span class="ot">&lt;-</span> <span class="cf">function</span>(mean_val, t) {</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (t <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min</span>(mean_val <span class="sc">/</span> t, <span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>chebyshev_bound <span class="ot">&lt;-</span> <span class="cf">function</span>(k) {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (k <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min</span>(<span class="dv">1</span> <span class="sc">/</span> k<span class="sc">^</span><span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulating convergence</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>demonstrate_lln <span class="ot">&lt;-</span> <span class="cf">function</span>(dist_func, <span class="at">n_max =</span> <span class="dv">10000</span>, ...) {</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># dist_func should be a function like rnorm, rexp, etc.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  samples <span class="ot">&lt;-</span> <span class="fu">dist_func</span>(n_max, ...)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  cumulative_mean <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(samples) <span class="sc">/</span> <span class="fu">seq_len</span>(n_max)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(cumulative_mean)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>demonstrate_clt <span class="ot">&lt;-</span> <span class="cf">function</span>(dist_func, n, <span class="at">n_simulations =</span> <span class="dv">10000</span>, ...) {</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  sample_means <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_simulations, {</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">dist_func</span>(n, ...)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(sample)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(sample_means)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimator evaluation</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>evaluate_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(estimator_func, true_value, n, </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>                              sample_func, <span class="at">n_simulations =</span> <span class="dv">10000</span>) {</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  estimates <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_simulations, {</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">sample_func</span>(n)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="fu">estimator_func</span>(sample)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>  bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(estimates) <span class="sc">-</span> true_value</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  variance <span class="ot">&lt;-</span> <span class="fu">var</span>(estimates)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>  mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((estimates <span class="sc">-</span> true_value)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">bias =</span> bias, <span class="at">variance =</span> variance, <span class="at">mse =</span> mse)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># LLN for exponential</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>lln_exp <span class="ot">&lt;-</span> <span class="fu">demonstrate_lln</span>(rexp, <span class="at">n_max =</span> <span class="dv">10000</span>, <span class="at">rate =</span> <span class="dv">1</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lln_exp, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">1.5</span>), <span class="at">main =</span> <span class="st">"LLN for Exponential(1)"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="co"># True mean is 1</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co"># CLT for uniform</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>clt_unif <span class="ot">&lt;-</span> <span class="fu">demonstrate_clt</span>(runif, <span class="at">n =</span> <span class="dv">30</span>, <span class="at">n_simulations =</span> <span class="dv">10000</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(clt_unif, <span class="at">breaks =</span> <span class="dv">50</span>, <span class="at">probability =</span> <span class="cn">TRUE</span>, <span class="at">main =</span> <span class="st">"CLT for Uniform(0,1)"</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance of U(0,1) is 1/12. SD of sample mean = sqrt(Var(X)/n)</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>((<span class="dv">1</span><span class="sc">/</span><span class="dv">12</span>)<span class="sc">/</span><span class="dv">30</span>)), <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare variance estimators</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>mle_var <span class="ot">&lt;-</span> <span class="cf">function</span>(x) { <span class="fu">var</span>(x) <span class="sc">*</span> (<span class="fu">length</span>(x) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">length</span>(x) }</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>compare_var_estimators <span class="ot">&lt;-</span> <span class="cf">function</span>(n) {</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>  sample_func <span class="ot">&lt;-</span> <span class="cf">function</span>(k) <span class="fu">rnorm</span>(k, <span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># True variance = 1</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>  unbiased_res <span class="ot">&lt;-</span> <span class="fu">evaluate_estimator</span>(</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    var,  <span class="co"># R's var() is the unbiased (n-1) version</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="at">true_value =</span> <span class="dv">1</span>, <span class="at">n =</span> n, <span class="at">sample_func =</span> sample_func</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>  mle_res <span class="ot">&lt;-</span> <span class="fu">evaluate_estimator</span>(</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    mle_var,</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    <span class="at">true_value =</span> <span class="dv">1</span>, <span class="at">n =</span> n, <span class="at">sample_func =</span> sample_func</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">unbiased =</span> unbiased_res, <span class="at">mle =</span> mle_res)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"--- Comparing Variance Estimators (n=10) ---"</span>)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">compare_var_estimators</span>(<span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="3.7.7">
<h3 data-number="3.7.7" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">3.7.7</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This table maps sections in these lecture notes to the corresponding sections in <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> (“All of Statistics” or AoS).</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding AoS Section(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction and Motivation</strong></td>
<td style="text-align: left;">Expanded material from the slides, contextualizing convergence for machine learning.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Inequalities: Bounding the Unknown</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Markov’s Inequality</td>
<td style="text-align: left;">AoS §4.1 (Theorem 4.1)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Chebyshev’s Inequality</td>
<td style="text-align: left;">AoS §4.1 (Theorem 4.2)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Hoeffding’s Inequality</td>
<td style="text-align: left;">AoS §4.1 (Theorems 4.4 &amp; 4.5)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Convergence of Random Variables</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Need for Probabilistic Convergence</td>
<td style="text-align: left;">AoS §5.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Convergence in Probability</td>
<td style="text-align: left;">AoS §5.2 (Definition 5.1)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Convergence in Distribution</td>
<td style="text-align: left;">AoS §5.2 (Definition 5.1)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Comparing Modes of Convergence</td>
<td style="text-align: left;">AoS §5.2 (Theorem 5.4)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Properties and Transformations</td>
<td style="text-align: left;">AoS §5.2 (Theorem 5.5, including Slutsky’s Theorem)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>The Two Fundamental Theorems of Statistics</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Law of Large Numbers (LLN)</td>
<td style="text-align: left;">AoS §5.3 (The Weak Law of Large Numbers, Theorem 5.6)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Central Limit Theorem (CLT)</td>
<td style="text-align: left;">AoS §5.4 (Theorems 5.8 &amp; 5.10)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>The Language of Statistical Inference</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ From Probability to Inference</td>
<td style="text-align: left;">AoS §6.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Statistical Models</td>
<td style="text-align: left;">AoS §6.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Point Estimation</td>
<td style="text-align: left;">AoS §6.3.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ How to Evaluate Estimators</td>
<td style="text-align: left;">AoS §6.3.1 (covers bias, consistency, MSE)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Bias-Variance Tradeoff</td>
<td style="text-align: left;">AoS §6.3.1 (MSE decomposition, Theorem 6.9)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Chapter Summary and Connections</strong></td>
<td style="text-align: left;">New summary material.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-reading" class="level3" data-number="3.7.8">
<h3 data-number="3.7.8" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">3.7.8</span> Further Reading</h3>
<ul>
<li><strong>Statistical inference</strong>: Casella &amp; Berger, “Statistical Inference”</li>
<li><strong>Machine learning perspective</strong>: Shalev-Shwartz &amp; Ben-David, “Understanding Machine Learning: From Theory to Algorithms”</li>
</ul>
<hr>
<p><em>Remember: Convergence and inference concepts are the bedrock of statistics. The Law of Large Numbers tells us why sampling works. The Central Limit Theorem tells us how to quantify uncertainty. The bias-variance tradeoff tells us how to choose good estimators. Master these ideas – they’re the key to everything that follows!</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Remember that <span class="math inline">\nabla_\theta f</span> denotes the gradient of function <span class="math inline">f (\theta; x)</span> with respect to <span class="math inline">\theta</span> – its “vector derivative” with respect to <span class="math inline">\theta</span> in more than dimension.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>For example, via a simple gradient descent step: <span class="math display">\theta_{t+1} = \theta_t - \alpha_t g</span> where <span class="math inline">\alpha_t &gt; 0</span> is the learning rate at step <span class="math inline">t</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For example, in the framework known as <a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">probably approximately correct (PAC) learning</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The symbol <span class="math inline">\theta</span> is almost universally reserved to represent generic “parameters” of a model in statistics and machine learning.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6ImltcG9ydCB7Y2x0RGVtb30gZnJvbSBcIi4uL2pzL2NsdC1kZW1vLmpzXCJcbmQzID0gcmVxdWlyZShcImQzQDdcIilcbklucHV0cyA9IHJlcXVpcmUoXCJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvbnBtL0BvYnNlcnZhYmxlaHEvaW5wdXRzQDAuMTAuNi9kaXN0L2lucHV0cy5taW4uanNcIilcblxuZGVtbyA9IGNsdERlbW8oZDMpO1xuXG52aWV3b2YgZGlzdE5hbWUgPSBJbnB1dHMuc2VsZWN0KFxuICBbXCJVbmlmb3JtXCIsIFwiRXhwb25lbnRpYWxcIiwgXCJCZXJub3VsbGlcIiwgXCJTa2V3ZWQgRGlzY3JldGVcIl0sIFxuICB7bGFiZWw6IFwiUG9wdWxhdGlvbiBEaXN0cmlidXRpb25cIn1cbilcbnZpZXdvZiBzYW1wbGVTaXplID0gSW5wdXRzLnJhbmdlKFxuICBbMSwgMTAwXSwgXG4gIHtzdGVwOiAxLCB2YWx1ZTogMSwgbGFiZWw6IFwiU2FtcGxlIFNpemUgKG4pXCJ9XG4pXG52aWV3b2YgbnVtU2ltdWxhdGlvbnMgPSBJbnB1dHMucmFuZ2UoXG4gIFsxMDAsIDEwMDAwXSwgXG4gIHtzdGVwOiAxMDAsIHZhbHVlOiAxMDAwMCwgbGFiZWw6IFwiTnVtYmVyIG9mIFNpbXVsYXRpb25zXCJ9XG4pXG5cbntcbiAgY29uc3QgcGxvdCA9IGRlbW8uY3JlYXRlVmlzdWFsaXphdGlvbih7XG4gICAgZGlzdE5hbWU6IGRpc3ROYW1lLFxuICAgIHNhbXBsZVNpemU6IHNhbXBsZVNpemUsXG4gICAgbnVtU2ltdWxhdGlvbnM6IG51bVNpbXVsYXRpb25zLFxuICB9KTtcbiAgcmV0dXJuIHBsb3Q7IFxufVxuIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdkaXN0TmFtZScpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdzYW1wbGVTaXplJykifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ251bVNpbXVsYXRpb25zJykifV19
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../chapters";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/02-expectation.html" class="pagination-link" aria-label="Expectation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/04-nonparametric-bootstrap.html" class="pagination-link" aria-label="Nonparametric Estimation and The Bootstrap">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Convergence and The Basics of Inference</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain how probability inequalities provide bounds on uncertainty.</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define concepts of probabilistic convergence and apply the Law of Large Numbers and Central Limit Theorem.</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define the core vocabulary of statistical inference (models, parameters, estimators).</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluate an estimator's quality using its standard error, bias, and variance.</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain the bias-variance tradeoff in the context of Mean Squared Error (MSE).</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>This chapter covers probability inequalities, convergence concepts, and the foundations of statistical inference. The material is adapted from Chapters 4, 5, and 6 of @wasserman2013all, supplemented with additional examples and perspectives relevant to data science applications.</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction and Motivation</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence Matters to Understand Machine Learning Algorithms</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>Deep learning models are trained with *stochastic* optimization algorithms. These algorithms produce a sequence of parameter estimates </span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>$$ \theta_1, \theta_2, \theta_3, \ldots$$ </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>as they iterate through the data. But here's the fundamental question: do these estimates eventually *converge* to a good solution, and how do we establish that?</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>The challenge is that these parameter estimates are random variables -- they depend on random initialization, random mini-batch selection, and random data shuffling. We can't use the simple definition of convergence where $|x_n - x| &lt; \epsilon$ for all large $n$, which you may remember from calculus. We need new mathematical tools.</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>This chapter develops the language of *probabilistic* convergence to understand and analyze such algorithms.</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>We'll then use these tools to build the foundation of statistical inference -- the science of drawing conclusions about populations from samples.</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>Consider a concrete example: training a neural network for image classification. At each iteration $t$:</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Pick a random subset $S$ of training images</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the gradient $g = \sum_{x_i \in S} \nabla_\theta L(\theta_t; x_i)$ of the loss^[Remember that $\nabla_\theta f$ denotes the gradient of function $f (\theta; x)$ with respect to $\theta$ -- its "vector derivative" with respect to $\theta$ in more than dimension.]</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compute next estimate of the model parameters, $\theta_{t+1}$, using $g$ and the current parameters^[For example, via a simple gradient descent step: $$\theta_{t+1} = \theta_t - \alpha_t g$$ where $\alpha_t &gt; 0$ is the learning rate at step $t$.]</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>The randomness in batch selection makes each $\theta_t$ a random variable. </span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>As mentioned before, ideally we would want $\theta_1, \theta_2, \ldots$ to converge to a good solution.</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>But what does it even mean to say the algorithm "converges"? This chapter provides the answer.</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>Convergence isn't just about optimization algorithms. It's central to all of statistics:</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When we compute a sample mean with increasing amount of data, does it converge to the population mean?</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More generally, when we estimate a model parameter, does our estimate improve with more data?</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When we approximate a distribution, does the approximation get better?</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>The remarkable answers to these questions -- provided by the Law of Large Numbers and Central Limit Theorem -- form the theoretical backbone of statistical inference and machine learning.</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here's a reference table of key terms in this chapter:</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>| Markov's inequality | Markovin epäyhtälö | Bounds probability of large values |</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>| Chebyshev's inequality | Tšebyšovin epäyhtälö | Uses variance to bound deviations |</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>| Convergence in probability | Stokastinen suppeneminen | Random variable settling to a value |</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>| Convergence in distribution | Jakaumasuppeneminen | Distribution shape converging |</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>| Law of Large Numbers | Suurten lukujen laki | Sample mean → population mean |</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>| Central Limit Theorem | Keskeinen raja-arvolause | Sums become normally distributed |</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>| Statistical model | Tilastollinen malli | Set of possible distributions |</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>| Parametric model | Parametrinen malli | Finite-dimensional parameter space |</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>| Nonparametric model | Epäparametrinen malli | Infinite-dimensional space |</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>| Nuisance parameter | Kiusaparametri | Parameter not of primary interest |</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>| Point estimation | Piste-estimointi | Single best guess of parameter |</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>| Estimator | Estimaattori | Function of data estimating parameter |</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>| Bias | Harha | Expected error of estimator |</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>| Unbiased | Harhaton | Zero expected error |</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>| Consistent | Tarkentuva | Converges to true value |</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>| Standard error | Keskivirhe | Standard deviation of estimator |</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>| Mean Squared Error (MSE) | Keskimääräinen neliövirhe | Average squared error |</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>| Sampling distribution | Otantajakauma | Distribution of the estimator |</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inequalities: Bounding the Unknown</span></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why We Need Inequalities</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>In probability and statistics, we often encounter quantities that are difficult or impossible to compute exactly.</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>Inequalities provide *bounds* -- upper or lower limits -- that give us useful information even when exact calculations are intractable. They serve three critical purposes:</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bounding quantities**: When we can't compute a probability exactly, an upper bound tells us it's "at most this large"</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Proving theorems**: The Law of Large Numbers and Central Limit Theorem rely on inequalities in their proofs</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Practical guarantees**: In machine learning, we use inequalities to create bounds on critical quantities such as generalization error^<span class="co">[</span><span class="ot">For example, in the framework known as [probably approximately correct (PAC) learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning).</span><span class="co">]</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>Think of inequalities as providing universal statistical guarantees.</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>They tell us that no matter how complicated the underlying distribution, certain bounds will always hold.</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### Markov's Inequality</span></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>**Markov's Inequality**: For a non-negative random variable $X$ with finite expectation:</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(X \geq t) \leq \frac{\mathbb{E}(X)}{t} \quad \text{for all } t &gt; 0$$</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>This remarkably simple inequality says that -- no matter what -- the probability of a non-negative random variable exceeding a threshold $t$ is bounded by its mean divided by $t$.</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>Since $X \geq 0$:</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(X) &amp;= \int_0^{\infty} x f(x) \, dx <span class="sc">\\</span></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>&amp;= \int_0^t x f(x) \, dx + \int_t^{\infty} x f(x) \, dx <span class="sc">\\</span></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>&amp;\geq \int_t^{\infty} x f(x) \, dx <span class="sc">\\</span></span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>&amp;\geq t \int_t^{\infty} f(x) \, dx <span class="sc">\\</span></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>&amp;= t \mathbb{P}(X \geq t)</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>Rearranging gives the result.</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Exceeding a Multiple of the Mean</span></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>Let $X$ be a non-negative random variable with mean $\mathbb{E}(X) = \mu$. What can we say about the probability that $X$ exceeds $k$ times its mean, for some $k &gt; 1$?</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>Using Markov's inequality by setting $t = k\mu$:</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(X \geq k\mu) \leq \frac{\mathbb{E}(X)}{k\mu} = \frac{\mu}{k\mu} = \frac{1}{k}$$</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>For example, the probability of a non-negative random variable exceeding twice its mean is at most $1/2$. The probability of it exceeding 10 times its mean is at most $1/10$. This universal bound is surprisingly useful.</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Exam Scores</span></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>If the average exam score is 50 points, what's the maximum probability that a randomly selected student scored 90 or more?</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>Using Markov's inequality:</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(X \geq 90) \leq \frac{50}{90} = \frac{5}{9} \approx 0.556$$</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>At most 55.6% of students can score 90 or more. This bound requires only knowing the average -- no other information about the distribution!</span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>Markov's inequality captures a fundamental truth: **averages constrain extremes**.</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>Imagine a village where the average wealth is €50,000. What fraction of villagers could be millionaires? If everyone were a millionaire, the average would be at least €1,000,000. Since the average is only €50,000, at most 5% can be millionaires:</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>$$\text{Fraction of millionaires} \leq \frac{€50,000}{€1,000,000} = 0.05$$</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>This reasoning works for any non-negative quantity: test scores, waiting times, file sizes, or loss values in machine learning. The average puts a hard limit on how often extreme values can occur.</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a>Markov's inequality is the foundation for many other inequalities. Its power lies in its generality—it applies to any non-negative random variable with finite expectation.</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>The inequality is tight (best possible) for certain distributions. Consider:</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a>$$X = \begin{cases}</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{with probability } 1 - \frac{\mu}{t} <span class="sc">\\</span></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>t &amp; \text{with probability } \frac{\mu}{t}</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>Then $\mathbb{E}(X) = \mu$ and $\mathbb{P}(X \geq t) = \frac{\mu}{t}$, achieving equality in Markov's inequality.</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>Let's visualize Markov's inequality by comparing the true tail probability with the bound for an exponential distribution.</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the exponential distribution</span></span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="dv">2</span>  <span class="co"># mean = 2</span></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> stats.expon.pdf(x, scale<span class="op">=</span>beta)</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute true probabilities and Markov bounds</span></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>t_values <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>true_probs <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(t_values, scale<span class="op">=</span>beta)</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>markov_bounds <span class="op">=</span> np.minimum(beta <span class="op">/</span> t_values, <span class="dv">1</span>)  <span class="co"># E[X]/t, capped at 1</span></span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: PDF with shaded tail</span></span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>t_example <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a>ax1.plot(x, pdf, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Exponential(2) PDF'</span>)</span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>ax1.fill_between(x[x <span class="op">&gt;=</span> t_example], pdf[x <span class="op">&gt;=</span> t_example], alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'red'</span>, </span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>                 label<span class="op">=</span><span class="ss">f'P(X ≥ </span><span class="sc">{</span>t_example<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>ax1.axvline(beta, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'E[X] = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Exponential Distribution'</span>)</span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: True probability vs Markov bound</span></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>ax2.plot(t_values, true_probs, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True P(X ≥ t)'</span>)</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>ax2.plot(t_values, markov_bounds, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Markov bound E[X]/t'</span>)</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'t'</span>)</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Markov</span><span class="ch">\'</span><span class="st">s Inequality for Exponential(2)'</span>)</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="dv">0</span>, <span class="fl">1.1</span>)</span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical comparison at specific points</span></span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Comparison of true probability vs Markov bound:"</span>)</span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>]:</span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a>    true_p <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(t, scale<span class="op">=</span>beta)</span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a>    markov_p <span class="op">=</span> beta <span class="op">/</span> t</span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"t = </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: True P(X ≥ </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span>true_p<span class="sc">:.4f}</span><span class="ss">, Markov bound = </span><span class="sc">{</span>markov_p<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a>Notice that the Markov bound is always valid but often loose. It becomes tighter as $t$ increases relative to the mean.</span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chebyshev's Inequality</span></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>While Markov's inequality uses only the mean, Chebyshev's inequality leverages the variance to provide a tighter bound on deviations from the mean.</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a>**Chebyshev's Inequality**: Let $X$ have mean $\mu$ and variance $\sigma^2$. Then:</span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|X - \mu| \geq t) \leq \frac{\sigma^2}{t^2} \quad \text{for all } t &gt; 0$$</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a>Apply Markov's inequality to the non-negative random variable $(X - \mu)^2$:</span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|X - \mu| \geq t) = \mathbb{P}((X - \mu)^2 \geq t^2) \leq \frac{\mathbb{E}<span class="co">[</span><span class="ot">(X - \mu)^2</span><span class="co">]</span>}{t^2} = \frac{\sigma^2}{t^2}$$</span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a>Equivalently, in terms of standard deviations:</span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|X - \mu| \geq k\sigma) \leq \frac{\sigma^2}{k^2 \sigma^2 } = \frac{1}{k^2} \quad \text{for all } k &gt; 0$$</span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Universal Two-Sigma Rule</span></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a>For *any* distribution (not just normal!), Chebyshev's inequality tells us:</span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a>$$ \mathbb{P}(|X - \mu| &lt; k \sigma) \ge 1 - \frac{1}{k^2}. $$</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a>Thus, for $k=2$ and $k=3$ we find:</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At least 75% of the data lies within 2 standard deviations of the mean: $\mathbb{P}(|X - \mu| &lt; 2\sigma) \geq 1 - \frac{1}{4} = 0.75$</span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At least 89% lies within 3 standard deviations: $\mathbb{P}(|X - \mu| &lt; 3\sigma) \geq 1 - \frac{1}{9} \approx 0.889$</span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a>Compare this to the normal distribution where about 95% lies within $2\sigma$ and 99.7% within $3\sigma$. Chebyshev's bounds are weaker but universal.</span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a>We show below the Chebyshev's bound compared to the actual tail probabilities of a few famous distributions (normal, uniform and exponential).</span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing Chebyshev's inequality</span></span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up comparison for different distributions</span></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a>chebyshev_bound <span class="op">=</span> np.minimum(<span class="dv">1</span> <span class="op">/</span> k_values<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute actual probabilities for different distributions</span></span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a>normal_probs <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> stats.norm.cdf(k_values))</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a>uniform_probs <span class="op">=</span> np.maximum(<span class="dv">0</span>, <span class="dv">1</span> <span class="op">-</span> k_values <span class="op">/</span> np.sqrt(<span class="dv">3</span>))  <span class="co"># Uniform on [-sqrt(3), sqrt(3)]</span></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a>exp_probs <span class="op">=</span> []</span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For exponential with mean 1, mu=1, sigma=1</span></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a>    exp_probs.append(stats.expon.cdf(<span class="dv">1</span> <span class="op">-</span> k, scale<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(<span class="dv">1</span> <span class="op">+</span> k, scale<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, chebyshev_bound, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Chebyshev bound'</span>)</span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, normal_probs, <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Normal'</span>)</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, uniform_probs, <span class="st">'g--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Uniform'</span>)</span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, exp_probs, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Exponential'</span>)</span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of standard deviations (k)'</span>)</span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(|X - μ| ≥ kσ)'</span>)</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Chebyshev</span><span class="ch">\'</span><span class="st">s Inequality vs Actual Tail Probabilities'</span>)</span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="fl">0.5</span>, <span class="dv">4</span>)</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a><span class="co"># Print specific values</span></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability of being more than k standard deviations from the mean:"</span>)</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"k</span><span class="ch">\t</span><span class="st">Chebyshev</span><span class="ch">\t</span><span class="st">Normal</span><span class="ch">\t\t</span><span class="st">Uniform</span><span class="ch">\t\t</span><span class="st">Exponential"</span>)</span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]:</span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a>    cheby <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>k<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> stats.norm.cdf(k))</span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a>    uniform <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, <span class="dv">1</span> <span class="op">-</span> k<span class="op">/</span>np.sqrt(<span class="dv">3</span>))</span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a>    exp_val <span class="op">=</span> stats.expon.cdf(<span class="dv">1</span> <span class="op">-</span> k, scale<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> stats.expon.cdf(<span class="dv">1</span> <span class="op">+</span> k, scale<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\t</span><span class="sc">{</span>cheby<span class="sc">:.4f}</span><span class="ch">\t\t</span><span class="sc">{</span>normal<span class="sc">:.4f}</span><span class="ch">\t\t</span><span class="sc">{</span>uniform<span class="sc">:.4f}</span><span class="ch">\t\t</span><span class="sc">{</span>exp_val<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Hoeffding's Inequality</span></span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>While Chebyshev's inequality is universal, it can be quite loose. For bounded random variables, Hoeffding's inequality provides an exponentially decaying bound that's much sharper.</span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Hoeffding's Inequality"}</span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a>Let $X_1, \ldots, X_n$ be independent random variables with $X_i \in <span class="co">[</span><span class="ot">a_i, b_i</span><span class="co">]</span>$. Let $S_n = \sum_{i=1}^n X_i$. Then for any $t &gt; 0$:</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(S_n - \mathbb{E}<span class="co">[</span><span class="ot">S_n</span><span class="co">]</span> \geq t) \leq \exp\left(-\frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)$$</span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a>For the special case of $n$ independent Bernoulli($p$) random variables:</span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}\left(|\bar{X}_n - p| &gt; \epsilon\right) \leq 2e^{-2n\epsilon^2}$$</span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a>where $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.</span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a>The key insight is the exponential decay in $n$. This makes Hoeffding's inequality the foundation for many machine learning generalization bounds.</span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Comparing Bounds</span></span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a>Consider estimating a probability $p$ from $n = 100$ Bernoulli trials. How likely is our estimate to be off by more than $\epsilon = 0.2$?</span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>**Chebyshev's bound**: Since $\mathbb{V}(\bar{X}_n) = p(1-p)/n \leq 1/(4n)$:</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|\bar{X}_n - p| &gt; 0.2) \leq \frac{1/(4 \times 100)}{0.2^2} = \frac{1/400}{0.04} = 0.0625$$</span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a>**Hoeffding's bound**:</span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|\bar{X}_n - p| &gt; 0.2) \leq 2e^{-2 \times 100 \times 0.2^2} = 2e^{-8} \approx 0.00067$$</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a>Hoeffding's bound is nearly 100 times tighter! This exponential improvement is crucial for machine learning theory.</span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a>The proof of Hoeffding's inequality uses moment generating functions and is beyond the scope of this course, but the intuition is that bounded random variables have light tails, allowing for much stronger concentration.</span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence of Random Variables</span></span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Need for Probabilistic Convergence</span></span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>In calculus, we say a sequence $x_n$ converges to $x$ if for every $\epsilon &gt; 0$, we have $|x_n - x| &lt; \epsilon$ for all sufficiently large $n$. But what about sequences of random variables?</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a>There are multiple scenarios:</span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Concentrating distribution**: Let $X_n \sim \mathcal{N}(0, 1/n)$. As $n$ increases, the distribution concentrates more tightly around 0. Intuitively, $X_n$ is "converging" to 0. </span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Tracking outcomes**: The case above can be generalized where $X_n$ does not converge to a constant (such as 0), but converges to **the values taken by another random variable** $X$.</span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a>The problem is that for any specific $x$, $\mathbb{P}(X_n = x) = 0$ for all $n$: continuous random variables never exactly equal any specific value.</span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a>There is then a completely different kind of convergence.</span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Stable distribution**: Let $X_n \sim \mathcal{N}(0, 1)$ for all $n$. Each $X_n$ has the same distribution, but they're different random variables. Is there a broader sense in which this sequence  "converges"?</span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a>In sum, we need new definitions that capture different notions of what it means for random variables to converge.</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in Probability</span></span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a>We consider the first two cases mentioned earlier: convergence of **outcomes** of a sequence of random variables to a constant or to the outcomes of another random variable, known as **convergence in probability**.</span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ **converges in probability** to a random variable $X$, written $X_n \xrightarrow{P} X$, if for every $\epsilon &gt; 0$:</span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|X_n - X| &gt; \epsilon) \to 0 \text{ as } n \to \infty$$</span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a>When $X = c$ (a constant), we write $X_n \xrightarrow{P} c$.</span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a>This definition captures the idea that $X_n$ becomes increasingly likely to be close to $X$ as $n$ grows. </span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a>The probability of $X_n$ being "far" from $X$ (more than $\epsilon$ away) vanishes.</span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a>In other words, the sequence of outcomes of the random variable $X_n$ "track" the outcomes of $X$ with ever-increasing accuracy as $n$ increases.</span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Convergence to Zero</span></span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a>Let $X_n \sim \mathcal{N}(0, 1/n)$. We'll show that $X_n \xrightarrow{P} 0$.</span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a>For any $\epsilon &gt; 0$, using Chebyshev's inequality:</span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|X_n - 0| &gt; \epsilon) = \mathbb{P}(|X_n| &gt; \epsilon) \leq \frac{\mathbb{V}(X_n)}{\epsilon^2} = \frac{1/n}{\epsilon^2} = \frac{1}{n\epsilon^2}$$</span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>Since $\frac{1}{n\epsilon^2} \to 0$ as $n \to \infty$, we have $X_n \xrightarrow{P} 0$.</span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in Distribution</span></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>We now consider the other case, where it's not the random variables to converge but their distribution.</span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ **converges in distribution** to a random variable $X$, written $X_n \rightsquigarrow X$, if:</span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a>$$\lim_{n \to \infty} F_n(t) = F(t)$$</span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a>at all points $t$ where $F$ is continuous. Here $F_n$ is the CDF of $X_n$ and $F$ is the CDF of $X$.</span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a>This captures the idea that the *distribution* (or "shape") of $X_n$ becomes increasingly similar to that of $X$. We're not saying the random variables themselves are close -- just their overall probability distributions.</span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a>If $X$ is a point mass at $c$, we denote $X_n \rightsquigarrow c$.</span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a>**Key Distinction**: </span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence in probability: The random variables themselves get close</span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence in distribution: Only the distributions get close</span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a>Let's visualize this with the $X_n \sim \mathcal{N}(0, 1/n)$ example:</span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the figure</span></span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: PDFs converging to a spike at 0</span></span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1000</span>)</span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>]</span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> plt.cm.Blues(np.linspace(<span class="fl">0.3</span>, <span class="fl">0.9</span>, <span class="bu">len</span>(n_values)))</span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(n_values, colors):</span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a>    pdf <span class="op">=</span> stats.norm.pdf(x, loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>np.sqrt(n))</span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x, pdf, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a>ax1.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'PDFs of N(0, 1/n)'</span>)</span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">0</span>, <span class="fl">2.5</span>)</span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: CDFs converging to step function</span></span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(n_values, colors):</span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a>    cdf <span class="op">=</span> stats.norm.cdf(x, loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>np.sqrt(n))</span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a>    ax2.plot(x, cdf, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot limiting step function</span></span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a>ax2.plot(x[x <span class="op">&lt;</span> <span class="dv">0</span>], np.zeros(<span class="bu">sum</span>(x <span class="op">&lt;</span> <span class="dv">0</span>)), <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Limit'</span>)</span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a>ax2.plot(x[x <span class="op">&gt;=</span> <span class="dv">0</span>], np.ones(<span class="bu">sum</span>(x <span class="op">&gt;=</span> <span class="dv">0</span>)), <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a>ax2.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>, fillstyle<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a>ax2.plot([<span class="dv">0</span>], [<span class="dv">1</span>], <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Cumulative probability'</span>)</span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'CDFs converging to step function'</span>)</span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a>As $n$ increases:</span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The PDF becomes more concentrated at 0 (spike)</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The CDF approaches a step function jumping from 0 to 1 at $x=0$</span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is convergence in distribution to a point mass at 0</span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing Modes of Convergence</span></span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a>**Relationships Between Convergence Types**</span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If $X_n \xrightarrow{P} X$ then $X_n \rightsquigarrow X$ (always)</span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If $X$ is a point mass at $c$ and $X_n \rightsquigarrow X$, then $X_n \xrightarrow{P} c$</span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a>Convergence in probability implies convergence in distribution, but the converse holds only for constants.</span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a>**Convergence in Probability: The Perfect Weather Forecast**</span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a>Let $X$ be the actual temperature tomorrow and $X_n$ be its forecast from an ever-improving machine learning model where $n$ is the model version, as we make it bigger and feed it more data.</span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a>**Convergence in probability ($X_n \xrightarrow{P} X$)** means the forecast becomes more and more accurate as the model gets better and better. Eventually, the temperature prediction $X_n$ gets so close to the actual temperature $X$ that the forecast error, $|X_n - X|$, becomes negligible. The individual outcomes match.</span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a>**Convergence in Distribution: The Perfect Climate Model**</span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a>A climate model doesn't predict a specific day's temperature; it captures the statistical "character" of a season. Let $X$ be the random variable for daily temperature, and $X_n$ be a model's simulation of a typical day.</span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a>**Convergence in distribution ($X_n \rightsquigarrow X$)** means the model's simulated statistics (e.g., its histogram of temperatures) become identical to the real climate's statistics. The **patterns match**, but the individual **outcomes don't**.</span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a>**The Takeaway:**</span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Probability implies Distribution:** A perfect daily forecast naturally captures the climate's long-term statistics.</span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Distribution does NOT imply Probability:** A perfect climate model can't predict the actual temperature on next Friday.</span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-515"><a href="#cb7-515" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a>We can construct a counterexample showing that convergence in distribution does NOT imply convergence in probability.</span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a>**Counterexample**: Let $X \sim \mathcal{N}(0,1)$ and define $X_n = -X$ for all $n$. Then:</span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each $X_n \sim \mathcal{N}(0,1)$, so trivially $X_n \rightsquigarrow X$</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>But $|X_n - X| = |2X|$, so $\mathbb{P}(|X_n - X| &gt; \epsilon) = \mathbb{P}(|X| &gt; \epsilon/2) \not\to 0$</span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Therefore $X_n$ does NOT converge to $X$ in probability!</span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a>The random variables have the same distribution but are perfectly anti-correlated.</span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a>Let's demonstrate both types of convergence and the counterexample computationally.</span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 1: X_n ~ N(0, 1/n) → 0 (both types of convergence)</span></span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>]</span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n <span class="kw">in</span> <span class="bu">enumerate</span>(n_values):</span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span>np.sqrt(n), n_samples)</span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a>    ax.hist(samples, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, density<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>, <span class="bu">range</span><span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a>ax.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Value'</span>)</span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Case 1: N(0,1/n) → 0</span><span class="ch">\n</span><span class="st">(Converges in both senses)'</span>)</span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 1 continued: Show |X_n - 0| for different epsilon</span></span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a>prob_far <span class="op">=</span> []</span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>):</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span>np.sqrt(n), n_samples)</span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a>    prob_far.append(np.mean(np.<span class="bu">abs</span>(samples) <span class="op">&gt;</span> epsilon))</span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>), prob_far, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'n'</span>)</span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="ss">f'P(|X_n| &gt; </span><span class="sc">{</span>epsilon<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Convergence in Probability to 0'</span>)</span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 2: X_n = -X counterexample</span></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n_samples)</span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a>X_n <span class="op">=</span> <span class="op">-</span>X  <span class="co"># X_n for all n</span></span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot distributions (they're identical!)</span></span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a>ax.hist(X, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">'X ~ N(0,1)'</span>)</span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a>ax.hist(X_n, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">'X_n = -X ~ N(0,1)'</span>)</span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Value'</span>)</span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Case 2: Same distribution</span><span class="ch">\n</span><span class="st">(Converges in distribution)'</span>)</span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a><span class="co"># But |X_n - X| = |2X| doesn't converge to 0</span></span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.<span class="bu">abs</span>(X_n <span class="op">-</span> X)</span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a>ax.hist(diff, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'|X_n - X| = |2X|'</span>)</span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'But NOT convergence in probability!</span><span class="ch">\n</span><span class="st">|X_n - X| doesn</span><span class="ch">\'</span><span class="st">t concentrate at 0'</span>)</span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a><span class="co"># Add theoretical chi distribution (|2X| where X~N(0,1))</span></span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a>x_theory <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">1000</span>)</span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a>y_theory <span class="op">=</span> stats.chi.pdf(x_theory <span class="op">*</span> <span class="fl">0.5</span>, df<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.5</span>  <span class="co"># Scale for |2X|</span></span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a>ax.plot(x_theory, y_theory, <span class="st">'k--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Theory'</span>)</span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a>**Summary:**</span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Case 1: $X_n ~ \mathcal{N}\left(0, 1/n\right)$ converges to 0 in BOTH senses</span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Case 2: $X_n = -X$ has same distribution as X but does NOT converge in probability</span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence in distribution is weaker than convergence in probability</span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-613"><a href="#cb7-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-614"><a href="#cb7-614" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties and Transformations</span></span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a>Understanding how convergence behaves under various operations is crucial for statistical theory. Here are the key properties:</span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a>**Operations Under Convergence in Probability**</span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a>If $X_n \xrightarrow{P} X$ and $Y_n \xrightarrow{P} Y$, then:</span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$X_n + Y_n \xrightarrow{P} X + Y$</span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$X_n Y_n \xrightarrow{P} XY$</span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$X_n / Y_n \xrightarrow{P} X/Y$ (if $\mathbb{P}(Y = 0) = 0$)</span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a>This shows that convergence in probability is well-beheaved under standard operations of sum, product, and division not-by-zero.</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a>**Slutsky's Theorem**</span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a>If $X_n \rightsquigarrow X$ and $Y_n \xrightarrow{P} c$ (constant), then:</span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$X_n + Y_n \rightsquigarrow X + c$</span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$X_n Y_n \rightsquigarrow cX$</span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$X_n / Y_n \rightsquigarrow X/c$ (if $c \neq 0$)</span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a>Slutsky's theorem tells us that convergence in distribution behaves nicely when paired with random variables that converge to a constant (this is not true in general!).</span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a>**Continuous Mapping Theorem**</span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a>If $g$ is a continuous function:</span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$X_n \xrightarrow{P} X \implies g(X_n) \xrightarrow{P} g(X)$</span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$X_n \rightsquigarrow X \implies g(X_n) \rightsquigarrow g(X)$</span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a>Finally, we see that continuous mappings behave nicely for both types of convergence.</span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a>**Important limitation**: In general, if $X_n \rightsquigarrow X$ and $Y_n \rightsquigarrow Y$, we **cannot** conclude that $X_n + Y_n \rightsquigarrow X + Y$. Convergence in distribution does not preserve sums unless one component converges to a constant!</span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a>**Counterexample**: Let $X \sim \mathcal{N}(0,1)$ and define $Y_n = -X$ for all $n$. Then $Y_n \sim \mathcal{N}(0,1)$, so $Y_n \rightsquigarrow Y \sim \mathcal{N}(0,1)$. But $X + Y_n = X - X = 0$, which does not converge in distribution to $X + Y \sim \mathcal{N}(0,2)$.</span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key takeaway</span></span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a>The rules for convergence are subtle. Generally speaking, convergence in probability behaves nicely under algebraic operations, but convergence in distribution requires more care. Always verify which type of convergence you have before applying these properties!</span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Two Fundamental Theorems of Statistics</span></span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Law of Large Numbers (LLN)</span></span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a>The Law of Large Numbers formalizes one of our most basic intuitions about probability: averages stabilize as we collect more data. When we flip a fair coin many times, the proportion of heads approaches 1/2. When we measure heights of many people, the sample mean approaches the population mean. This isn't just intuition -- it's a mathematical theorem.</span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Weak Law of Large Numbers"}</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a>Let $X_1, X_2, \ldots, X_n$ be independent and identically distributed (IID) random variables with $\mathbb{E}(X_i) = \mu$ and $\mathbb{V}(X_i) = \sigma^2 &lt; \infty$. Define the **sample mean**:</span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a>$$\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$$</span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a>Then $\bar{X}_n \xrightarrow{P} \mu$.</span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a>**Interpretation**: The sample mean converges in probability to the population mean. As we collect more data, our estimate gets arbitrarily close to the true value with high probability.</span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a>We'll use Chebyshev's inequality. First, compute the mean and variance of $\bar{X}_n$:</span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(\bar{X}_n) = \mathbb{E}\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X_i) = \frac{1}{n} \cdot n\mu = \mu$$</span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(\bar{X}_n) = \mathbb{V}\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n \mathbb{V}(X_i) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$$</span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a>(We used independence for the variance calculation.)</span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a>Now apply Chebyshev's inequality: for any $\epsilon &gt; 0$,</span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(|\bar{X}_n - \mu| &gt; \epsilon) \leq \frac{\mathbb{V}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0 \text{ as } n \to \infty$$</span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a>Therefore $\bar{X}_n \xrightarrow{P} \mu$.</span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a>Let's visualize the Law of Large Numbers in action by simulating repeated rolls of a standard six-sided die and computing the mean of all rolls until that point. </span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a>We show this in two plots: on a normal scale (top) and on a log-scale (bottom) for the number of rolls on the $x$ axis. The bottom plot also zooms in on the $y$-axis around $3.5$.</span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a>Note how the sample mean starts with high variability but converges to the true mean ($3.5$) as the number of rolls increases.</span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate die rolls</span></span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a>n_max <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a>die_rolls <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">7</span>, n_max)</span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a>cumulative_mean <span class="op">=</span> np.cumsum(die_rolls) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_max <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="fl">3.5</span></span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot without shared x-axis</span></span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a><span class="co"># Top plot: Full scale with linear x-axis</span></span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a>ax1.plot(cumulative_mean, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span>true_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'True mean = </span><span class="sc">{</span>true_mean<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Law of Large Numbers: Sample Mean of Die Rolls'</span>)</span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom plot: Zoomed in to show convergence with log scale</span></span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.arange(<span class="dv">100</span>, n_max)</span>
<span id="cb7-736"><a href="#cb7-736" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_values, cumulative_mean[<span class="dv">100</span>:], linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb7-737"><a href="#cb7-737" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span>true_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-738"><a href="#cb7-738" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Number of rolls'</span>)</span>
<span id="cb7-739"><a href="#cb7-739" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Sample mean'</span>)</span>
<span id="cb7-740"><a href="#cb7-740" aria-hidden="true" tabindex="-1"></a>ax2.set_xscale(<span class="st">'log'</span>)</span>
<span id="cb7-741"><a href="#cb7-741" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-742"><a href="#cb7-742" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="fl">3.2</span>, <span class="fl">3.8</span>)</span>
<span id="cb7-743"><a href="#cb7-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-744"><a href="#cb7-744" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-745"><a href="#cb7-745" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-746"><a href="#cb7-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-747"><a href="#cb7-747" aria-hidden="true" tabindex="-1"></a><span class="co"># Show convergence at specific sample sizes</span></span>
<span id="cb7-748"><a href="#cb7-748" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> [<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>]:</span>
<span id="cb7-749"><a href="#cb7-749" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"After </span><span class="sc">{</span>n<span class="sc">:5d}</span><span class="ss"> rolls: sample mean = </span><span class="sc">{</span>cumulative_mean[n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">, "</span></span>
<span id="cb7-750"><a href="#cb7-750" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"error = </span><span class="sc">{</span><span class="bu">abs</span>(cumulative_mean[n<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> true_mean)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-751"><a href="#cb7-751" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-752"><a href="#cb7-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-753"><a href="#cb7-753" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-754"><a href="#cb7-754" aria-hidden="true" tabindex="-1"></a><span class="fu">## Weak vs Strong Laws of Large Numbers</span></span>
<span id="cb7-755"><a href="#cb7-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-756"><a href="#cb7-756" aria-hidden="true" tabindex="-1"></a>The theorem above is known as the "Weak" Law of Large Numbers because it guarantees convergence in probability. There exists a stronger version that guarantees **almost sure convergence**: $\mathbb{P}(\bar{X}_n \to \mu) = 1$. The "Strong" LLN says that with probability 1, the sample mean will eventually get arbitrarily close to $\mu$ and *stay* close, while the Weak LLN only guarantees that the probability of being far from $\mu$ goes to zero. The Weak LLN requires only finite variance, while the Strong LLN typically needs additional assumptions (like finite fourth moments) but delivers a more powerful conclusion. We present the Weak version as it has minimal assumptions and suffices for most statistical applications.</span>
<span id="cb7-757"><a href="#cb7-757" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-758"><a href="#cb7-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-759"><a href="#cb7-759" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Central Limit Theorem (CLT)</span></span>
<span id="cb7-760"><a href="#cb7-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-761"><a href="#cb7-761" aria-hidden="true" tabindex="-1"></a>While the Law of Large Numbers tells us that sample means converge to the population mean, it doesn't tell us about the *distribution* of the sample mean. The Central Limit Theorem fills this gap with a remarkable result: properly scaled sample means are approximately normal, regardless of the underlying distribution!</span>
<span id="cb7-762"><a href="#cb7-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-763"><a href="#cb7-763" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Central Limit Theorem"}</span>
<span id="cb7-764"><a href="#cb7-764" aria-hidden="true" tabindex="-1"></a>Let $X_1, X_2, \ldots, X_n$ be IID random variables with $\mathbb{E}(X_i) = \mu$ and $\mathbb{V}(X_i) = \sigma^2 &lt; \infty$. Define:</span>
<span id="cb7-765"><a href="#cb7-765" aria-hidden="true" tabindex="-1"></a>$$Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma}$$</span>
<span id="cb7-766"><a href="#cb7-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-767"><a href="#cb7-767" aria-hidden="true" tabindex="-1"></a>Then $Z_n \rightsquigarrow Z$ where $Z \sim \mathcal{N}(0, 1)$.</span>
<span id="cb7-768"><a href="#cb7-768" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-769"><a href="#cb7-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-770"><a href="#cb7-770" aria-hidden="true" tabindex="-1"></a>**Alternative notations** (all mean the same thing):</span>
<span id="cb7-771"><a href="#cb7-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-772"><a href="#cb7-772" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\bar{X}_n \approx \mathcal{N}(\mu, \sigma^2/n)$ for large $n$</span>
<span id="cb7-773"><a href="#cb7-773" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sqrt{n}(\bar{X}_n - \mu) \rightsquigarrow \mathcal{N}(0, \sigma^2)$</span>
<span id="cb7-774"><a href="#cb7-774" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$(\bar{X}_n - \mu)/(\sigma/\sqrt{n}) \rightsquigarrow \mathcal{N}(0, 1)$</span>
<span id="cb7-775"><a href="#cb7-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-776"><a href="#cb7-776" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb7-777"><a href="#cb7-777" aria-hidden="true" tabindex="-1"></a>**Critical Point**: The CLT is about the distribution of the *sample mean*, not the data itself! The original data doesn't become normal—only the sampling distribution of $\bar{X}_n$ does.</span>
<span id="cb7-778"><a href="#cb7-778" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-779"><a href="#cb7-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-780"><a href="#cb7-780" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb7-781"><a href="#cb7-781" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interactive Demonstration of the CLT</span></span>
<span id="cb7-782"><a href="#cb7-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-783"><a href="#cb7-783" aria-hidden="true" tabindex="-1"></a>Let's visualize how the CLT works for different distributions from continuous to discrete and skewed (asymmetrical). </span>
<span id="cb7-784"><a href="#cb7-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-785"><a href="#cb7-785" aria-hidden="true" tabindex="-1"></a>The interactive visualization below allows you to see this convergence in action. You can change the underlying distribution and adjust the sample size <span class="in">`n`</span> to see how the distribution of the standardized sample mean approaches a standard normal distribution (red curves). </span>
<span id="cb7-786"><a href="#cb7-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-789"><a href="#cb7-789" aria-hidden="true" tabindex="-1"></a><span class="in">```{ojs}</span></span>
<span id="cb7-790"><a href="#cb7-790" aria-hidden="true" tabindex="-1"></a><span class="in">//| echo: false</span></span>
<span id="cb7-791"><a href="#cb7-791" aria-hidden="true" tabindex="-1"></a><span class="in">import {cltDemo} from "../js/clt-demo.js"</span></span>
<span id="cb7-792"><a href="#cb7-792" aria-hidden="true" tabindex="-1"></a><span class="in">d3 = require("d3@7")</span></span>
<span id="cb7-793"><a href="#cb7-793" aria-hidden="true" tabindex="-1"></a><span class="in">Inputs = require("https://cdn.jsdelivr.net/npm/@observablehq/inputs@0.10.6/dist/inputs.min.js")</span></span>
<span id="cb7-794"><a href="#cb7-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-795"><a href="#cb7-795" aria-hidden="true" tabindex="-1"></a><span class="in">demo = cltDemo(d3);</span></span>
<span id="cb7-796"><a href="#cb7-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-797"><a href="#cb7-797" aria-hidden="true" tabindex="-1"></a><span class="in">viewof distName = Inputs.select(</span></span>
<span id="cb7-798"><a href="#cb7-798" aria-hidden="true" tabindex="-1"></a><span class="in">  ["Uniform", "Exponential", "Bernoulli", "Skewed Discrete"], </span></span>
<span id="cb7-799"><a href="#cb7-799" aria-hidden="true" tabindex="-1"></a><span class="in">  {label: "Population Distribution"}</span></span>
<span id="cb7-800"><a href="#cb7-800" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb7-801"><a href="#cb7-801" aria-hidden="true" tabindex="-1"></a><span class="in">viewof sampleSize = Inputs.range(</span></span>
<span id="cb7-802"><a href="#cb7-802" aria-hidden="true" tabindex="-1"></a><span class="in">  [1, 100], </span></span>
<span id="cb7-803"><a href="#cb7-803" aria-hidden="true" tabindex="-1"></a><span class="in">  {step: 1, value: 1, label: "Sample Size (n)"}</span></span>
<span id="cb7-804"><a href="#cb7-804" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb7-805"><a href="#cb7-805" aria-hidden="true" tabindex="-1"></a><span class="in">viewof numSimulations = Inputs.range(</span></span>
<span id="cb7-806"><a href="#cb7-806" aria-hidden="true" tabindex="-1"></a><span class="in">  [100, 10000], </span></span>
<span id="cb7-807"><a href="#cb7-807" aria-hidden="true" tabindex="-1"></a><span class="in">  {step: 100, value: 10000, label: "Number of Simulations"}</span></span>
<span id="cb7-808"><a href="#cb7-808" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb7-809"><a href="#cb7-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-810"><a href="#cb7-810" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb7-811"><a href="#cb7-811" aria-hidden="true" tabindex="-1"></a><span class="in">  const plot = demo.createVisualization({</span></span>
<span id="cb7-812"><a href="#cb7-812" aria-hidden="true" tabindex="-1"></a><span class="in">    distName: distName,</span></span>
<span id="cb7-813"><a href="#cb7-813" aria-hidden="true" tabindex="-1"></a><span class="in">    sampleSize: sampleSize,</span></span>
<span id="cb7-814"><a href="#cb7-814" aria-hidden="true" tabindex="-1"></a><span class="in">    numSimulations: numSimulations,</span></span>
<span id="cb7-815"><a href="#cb7-815" aria-hidden="true" tabindex="-1"></a><span class="in">  });</span></span>
<span id="cb7-816"><a href="#cb7-816" aria-hidden="true" tabindex="-1"></a><span class="in">  return plot; </span></span>
<span id="cb7-817"><a href="#cb7-817" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb7-818"><a href="#cb7-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-819"><a href="#cb7-819" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-820"><a href="#cb7-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-821"><a href="#cb7-821" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb7-822"><a href="#cb7-822" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-823"><a href="#cb7-823" aria-hidden="true" tabindex="-1"></a><span class="fu">## Online Interactive Demonstration of the CLT</span></span>
<span id="cb7-824"><a href="#cb7-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-825"><a href="#cb7-825" aria-hidden="true" tabindex="-1"></a>An interactive visualization of the Central Limit Theorem is available in the HTML version of these notes. It allows you to select different population distributions and adjust the sample size <span class="in">`n`</span> to see the convergence to normality in real-time.</span>
<span id="cb7-826"><a href="#cb7-826" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-827"><a href="#cb7-827" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-828"><a href="#cb7-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-829"><a href="#cb7-829" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-830"><a href="#cb7-830" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: CLT in Practice</span></span>
<span id="cb7-831"><a href="#cb7-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-832"><a href="#cb7-832" aria-hidden="true" tabindex="-1"></a>A factory produces bolts with mean length $\mu = 5$ cm and standard deviation $\sigma = 0.1$ cm. If we randomly sample 100 bolts, what's the probability their average length exceeds 5.02 cm?</span>
<span id="cb7-833"><a href="#cb7-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-834"><a href="#cb7-834" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-835"><a href="#cb7-835" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb7-836"><a href="#cb7-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-837"><a href="#cb7-837" aria-hidden="true" tabindex="-1"></a>By the CLT, $\bar{X}_{100} \approx \mathcal{N}(5, 0.1^2/100) = \mathcal{N}(5, 0.0001)$.</span>
<span id="cb7-838"><a href="#cb7-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-839"><a href="#cb7-839" aria-hidden="true" tabindex="-1"></a>We want:</span>
<span id="cb7-840"><a href="#cb7-840" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(\bar{X}_{100} &gt; 5.02) = \mathbb{P}\left(\frac{\bar{X}_{100} - 5}{0.01} &gt; \frac{5.02 - 5}{0.01}\right) = \mathbb{P}(Z &gt; 2)$$</span>
<span id="cb7-841"><a href="#cb7-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-842"><a href="#cb7-842" aria-hidden="true" tabindex="-1"></a>where $Z \sim \mathcal{N}(0,1)$. From standard normal tables: $\mathbb{P}(Z &gt; 2) \approx 0.0228$.</span>
<span id="cb7-843"><a href="#cb7-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-844"><a href="#cb7-844" aria-hidden="true" tabindex="-1"></a>So there's about a 2.3% chance the sample mean exceeds 5.02 cm.</span>
<span id="cb7-845"><a href="#cb7-845" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-846"><a href="#cb7-846" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-847"><a href="#cb7-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-848"><a href="#cb7-848" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-849"><a href="#cb7-849" aria-hidden="true" tabindex="-1"></a>**CLT with Unknown Variance**: If we replace $\sigma$ with the sample standard deviation</span>
<span id="cb7-850"><a href="#cb7-850" aria-hidden="true" tabindex="-1"></a>$$S_n = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2}$$</span>
<span id="cb7-851"><a href="#cb7-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-852"><a href="#cb7-852" aria-hidden="true" tabindex="-1"></a>then we still have:</span>
<span id="cb7-853"><a href="#cb7-853" aria-hidden="true" tabindex="-1"></a>$$\frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n} \rightsquigarrow \mathcal{N}(0, 1)$$</span>
<span id="cb7-854"><a href="#cb7-854" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-855"><a href="#cb7-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-856"><a href="#cb7-856" aria-hidden="true" tabindex="-1"></a>This version is crucial for practice since we rarely know the true variance!</span>
<span id="cb7-857"><a href="#cb7-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-858"><a href="#cb7-858" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-859"><a href="#cb7-859" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rejoinder: Understanding Algorithms</span></span>
<span id="cb7-860"><a href="#cb7-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-861"><a href="#cb7-861" aria-hidden="true" tabindex="-1"></a>Remember the sequence of random variables $\theta_1, \theta_2, \ldots$ from our stochastic optimization algorithm at the beginning of this chapter? We can now answer what kind of convergence we should expect:</span>
<span id="cb7-862"><a href="#cb7-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-863"><a href="#cb7-863" aria-hidden="true" tabindex="-1"></a>**Convergence in probability**: We want $\theta_n \xrightarrow{P} \theta^*$ where $\theta^*$ is the true optimal solution. This means the probability of $\theta_n$ being far from the optimum vanishes as iterations increase.</span>
<span id="cb7-864"><a href="#cb7-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-865"><a href="#cb7-865" aria-hidden="true" tabindex="-1"></a>The tools we've covered -- probability inequalities (to bound deviations), convergence concepts (to formalize what "converges" means), and limit theorems (to understand averaging behavior) -- are the foundation for analyzing when and why algorithms like stochastic gradient descent converge to good solutions. Modern machine learning theory relies heavily on these concepts to provide theoretical guarantees about algorithm performance!</span>
<span id="cb7-866"><a href="#cb7-866" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-867"><a href="#cb7-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-868"><a href="#cb7-868" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Language of Statistical Inference</span></span>
<span id="cb7-869"><a href="#cb7-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-870"><a href="#cb7-870" aria-hidden="true" tabindex="-1"></a><span class="fu">### From Probability to Inference</span></span>
<span id="cb7-871"><a href="#cb7-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-872"><a href="#cb7-872" aria-hidden="true" tabindex="-1"></a>We've developed powerful tools: inequalities that bound uncertainty, convergence concepts that describe limiting behavior, and fundamental theorems that guarantee nice properties of averages. Now we flip the perspective.</span>
<span id="cb7-873"><a href="#cb7-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-874"><a href="#cb7-874" aria-hidden="true" tabindex="-1"></a>**Probability**: Given a known distribution, what can we say about the data we'll observe?</span>
<span id="cb7-875"><a href="#cb7-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-876"><a href="#cb7-876" aria-hidden="true" tabindex="-1"></a>**Statistical Inference**: Given observed data, what can we infer about the unknown distribution that generated it? More formally: given a sample $X_1, \ldots, X_n \sim F$, how do we infer $F$?</span>
<span id="cb7-877"><a href="#cb7-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-878"><a href="#cb7-878" aria-hidden="true" tabindex="-1"></a>This process -- often called "learning" in computer science -- is at the core of both classical statistics and modern machine learning. Sometimes we want to infer the entire distribution $F$, but often we focus on specific features like its mean, variance, or other parameters.</span>
<span id="cb7-879"><a href="#cb7-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-880"><a href="#cb7-880" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-881"><a href="#cb7-881" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Modeling Uncertainty in Real Decisions</span></span>
<span id="cb7-882"><a href="#cb7-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-883"><a href="#cb7-883" aria-hidden="true" tabindex="-1"></a>An online retailer tests a new ad campaign. Out of 1000 users who see the ad, 30 make a purchase (3% conversion rate). But this raises critical questions:</span>
<span id="cb7-884"><a href="#cb7-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-885"><a href="#cb7-885" aria-hidden="true" tabindex="-1"></a>**Immediate questions:**</span>
<span id="cb7-886"><a href="#cb7-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-887"><a href="#cb7-887" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What can we say about the true conversion rate? Is it exactly 3%?</span>
<span id="cb7-888"><a href="#cb7-888" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How likely is it that at least 25 out of the next 1000 users will convert?</span>
<span id="cb7-889"><a href="#cb7-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-890"><a href="#cb7-890" aria-hidden="true" tabindex="-1"></a>**Comparative questions:**</span>
<span id="cb7-891"><a href="#cb7-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-892"><a href="#cb7-892" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A competing ad had 290 conversions out of 10,000 users (2.9% rate). Which is better?</span>
<span id="cb7-893"><a href="#cb7-893" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How confident can we be that the 3% ad truly outperforms the 2.9% ad -- could the difference just be due to random chance?</span>
<span id="cb7-894"><a href="#cb7-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-895"><a href="#cb7-895" aria-hidden="true" tabindex="-1"></a>**Long-term questions:**</span>
<span id="cb7-896"><a href="#cb7-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-897"><a href="#cb7-897" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's the probability that the long-run conversion rate exceeds 2.5%?</span>
<span id="cb7-898"><a href="#cb7-898" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How many more users do we need to test to be 95% confident about the true rate?</span>
<span id="cb7-899"><a href="#cb7-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-900"><a href="#cb7-900" aria-hidden="true" tabindex="-1"></a>These questions -- about uncertainty, confidence, and decision-making with limited data -- are at the heart of statistical inference.</span>
<span id="cb7-901"><a href="#cb7-901" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-902"><a href="#cb7-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-903"><a href="#cb7-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-904"><a href="#cb7-904" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistical Models</span></span>
<span id="cb7-905"><a href="#cb7-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-906"><a href="#cb7-906" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-907"><a href="#cb7-907" aria-hidden="true" tabindex="-1"></a>A **statistical model** $\mathfrak{F}$ is a set of probability distributions (or densities or regression functions).</span>
<span id="cb7-908"><a href="#cb7-908" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-909"><a href="#cb7-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-910"><a href="#cb7-910" aria-hidden="true" tabindex="-1"></a>In the context of inference, we use models to represent our assumptions about which distributions could have generated our observed data. The model defines the "universe of possibilities" we're considering -- we then use data to identify which specific distribution within $\mathfrak{F}$ is most plausible.</span>
<span id="cb7-911"><a href="#cb7-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-912"><a href="#cb7-912" aria-hidden="true" tabindex="-1"></a>Models come in two main flavors, **parametric** and **nonparametric**.</span>
<span id="cb7-913"><a href="#cb7-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-914"><a href="#cb7-914" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Parametric Models</span></span>
<span id="cb7-915"><a href="#cb7-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-916"><a href="#cb7-916" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-917"><a href="#cb7-917" aria-hidden="true" tabindex="-1"></a>A **parametric model** is indexed by a finite number of parameters. We write it as:</span>
<span id="cb7-918"><a href="#cb7-918" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{F} = <span class="sc">\{</span>f(x; \theta) : \theta \in \Theta<span class="sc">\}</span>$$</span>
<span id="cb7-919"><a href="#cb7-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-920"><a href="#cb7-920" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb7-921"><a href="#cb7-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-922"><a href="#cb7-922" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta$ (<span class="co">[</span><span class="ot">theta</span><span class="co">](https://en.wikipedia.org/wiki/Theta)</span>) is the **parameter** (possibly vector-valued)^<span class="co">[</span><span class="ot">The symbol $\theta$ is almost universally reserved to represent generic "parameters" of a model in statistics and machine learning.</span><span class="co">]</span></span>
<span id="cb7-923"><a href="#cb7-923" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Theta$ (capital theta) is the **parameter space** (the set of all possible parameter values)</span>
<span id="cb7-924"><a href="#cb7-924" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f(x; \theta)$ is the density or distribution function indexed by $\theta$</span>
<span id="cb7-925"><a href="#cb7-925" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-926"><a href="#cb7-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-927"><a href="#cb7-927" aria-hidden="true" tabindex="-1"></a>Typically, the parameters $\theta$ are unknown quantities we want to estimate. If there are elements of the vector $\theta$ that we are not interested in, those are called **nuisance parameters**.</span>
<span id="cb7-928"><a href="#cb7-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-929"><a href="#cb7-929" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false collapse="true"}</span>
<span id="cb7-930"><a href="#cb7-930" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Feature Performance as a Parametric Model</span></span>
<span id="cb7-931"><a href="#cb7-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-932"><a href="#cb7-932" aria-hidden="true" tabindex="-1"></a>A product manager at a tech company launches a new "AI Recap" feature in their app. To determine if the feature is a success, they track the number of daily views over the first month. They hypothesize that the daily view count approximately follows a normal distribution.</span>
<span id="cb7-933"><a href="#cb7-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-934"><a href="#cb7-934" aria-hidden="true" tabindex="-1"></a>The model for daily views is a parametric family $\mathfrak{F}$:</span>
<span id="cb7-935"><a href="#cb7-935" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{F} = <span class="sc">\{</span>f(x; \theta) : \theta = (\mu, \sigma^2), \mu \in \mathbb{R}, \sigma^2 &gt; 0<span class="sc">\}</span>$$</span>
<span id="cb7-936"><a href="#cb7-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-937"><a href="#cb7-937" aria-hidden="true" tabindex="-1"></a>where the density function is:$$f(x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$</span>
<span id="cb7-938"><a href="#cb7-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-939"><a href="#cb7-939" aria-hidden="true" tabindex="-1"></a>This is a 2-dimensional parametric model with:</span>
<span id="cb7-940"><a href="#cb7-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-941"><a href="#cb7-941" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Parameter vector**: $\theta = (\mu, \sigma^2)$</span>
<span id="cb7-942"><a href="#cb7-942" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Parameter space**: $\Theta = \mathbb{R} \times (0, \infty)$</span>
<span id="cb7-943"><a href="#cb7-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-944"><a href="#cb7-944" aria-hidden="true" tabindex="-1"></a>**Nuisance parameters in action**: The company has set a target: the feature will be considered successful and receive further development only if it can reliably generate more than 100,000 views per day.</span>
<span id="cb7-945"><a href="#cb7-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-946"><a href="#cb7-946" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The **parameter of interest** is the average daily views, $\mu$. The entire business decision hinges on testing the hypothesis that $\mu &gt; 100,000$.</span>
<span id="cb7-947"><a href="#cb7-947" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The **nuisance parameter** is the variance, $\sigma^2$. The day-to-day fluctuation in views is critical for assessing the statistical certainty of our estimate for $\mu$, but it's not the primary metric for success. The product manager needs to account for this variability, but their core question is about the average performance, not the variability itself.</span>
<span id="cb7-948"><a href="#cb7-948" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-949"><a href="#cb7-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-950"><a href="#cb7-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-951"><a href="#cb7-951" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Nonparametric Models</span></span>
<span id="cb7-952"><a href="#cb7-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-953"><a href="#cb7-953" aria-hidden="true" tabindex="-1"></a>**Nonparametric Models** cannot be parameterized by a finite number of parameters. These models make minimal assumptions about the distribution. For example:</span>
<span id="cb7-954"><a href="#cb7-954" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{F}_{\text{ALL}} = <span class="sc">\{</span>\text{all continuous CDFs}<span class="sc">\}</span>$$</span>
<span id="cb7-955"><a href="#cb7-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-956"><a href="#cb7-956" aria-hidden="true" tabindex="-1"></a>or with some constraints:</span>
<span id="cb7-957"><a href="#cb7-957" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{F} = <span class="sc">\{</span>\text{all distributions with finite variance}<span class="sc">\}</span>$$</span>
<span id="cb7-958"><a href="#cb7-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-959"><a href="#cb7-959" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-960"><a href="#cb7-960" aria-hidden="true" tabindex="-1"></a><span class="fu">## How can we work with "all distributions"?</span></span>
<span id="cb7-961"><a href="#cb7-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-962"><a href="#cb7-962" aria-hidden="true" tabindex="-1"></a>This seems impossibly broad! In practice, we don't explicitly enumerate all possible distributions. Instead, nonparametric methods **directly use the data** without assuming a specific functional form or parameter to be estimated. We will see multiple concrete example of nonparametric techniques in the next chapter. So, in theory the model space is infinite-dimensional, but in practice nonparametric estimation procedures are still concrete and computable.</span>
<span id="cb7-963"><a href="#cb7-963" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-964"><a href="#cb7-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-965"><a href="#cb7-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-966"><a href="#cb7-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-967"><a href="#cb7-967" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-968"><a href="#cb7-968" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Choosing a Model</span></span>
<span id="cb7-969"><a href="#cb7-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-970"><a href="#cb7-970" aria-hidden="true" tabindex="-1"></a>**Scenario 1**: Heights of adult males in Finland.</span>
<span id="cb7-971"><a href="#cb7-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-972"><a href="#cb7-972" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parametric choice**: $\mathfrak{F} = <span class="sc">\{</span>\mathcal{N}(\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma &gt; 0<span class="sc">\}</span>$</span>
<span id="cb7-973"><a href="#cb7-973" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Justification**: Heights are often approximately normal due to many small genetic and environmental factors (CLT in action!)</span>
<span id="cb7-974"><a href="#cb7-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-975"><a href="#cb7-975" aria-hidden="true" tabindex="-1"></a>**Scenario 2**: Time between website visits.</span>
<span id="cb7-976"><a href="#cb7-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-977"><a href="#cb7-977" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parametric choice**: $\mathfrak{F} = <span class="sc">\{</span>\text{Exponential}(\lambda) : \lambda &gt; 0<span class="sc">\}</span>$</span>
<span id="cb7-978"><a href="#cb7-978" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Justification**: Exponential models "memoryless" waiting times</span>
<span id="cb7-979"><a href="#cb7-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-980"><a href="#cb7-980" aria-hidden="true" tabindex="-1"></a>**Scenario 3**: Unknown distribution shape.</span>
<span id="cb7-981"><a href="#cb7-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-982"><a href="#cb7-982" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Nonparametric choice**: $\mathfrak{F} = <span class="sc">\{</span>\text{all distributions with finite variance}<span class="sc">\}</span>$</span>
<span id="cb7-983"><a href="#cb7-983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Justification**: Make minimal assumptions, let data speak</span>
<span id="cb7-984"><a href="#cb7-984" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-985"><a href="#cb7-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-986"><a href="#cb7-986" aria-hidden="true" tabindex="-1"></a><span class="fu">### Point Estimation</span></span>
<span id="cb7-987"><a href="#cb7-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-988"><a href="#cb7-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-989"><a href="#cb7-989" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-990"><a href="#cb7-990" aria-hidden="true" tabindex="-1"></a>**Point estimation** is the task of providing a single "best guess" for an unknown quantity based on data.</span>
<span id="cb7-991"><a href="#cb7-991" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-992"><a href="#cb7-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-993"><a href="#cb7-993" aria-hidden="true" tabindex="-1"></a>This quantity can be a single parameter, a full vector of parameters, even a full CDF or PDF, or prediction for a future value of some random variable.</span>
<span id="cb7-994"><a href="#cb7-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-995"><a href="#cb7-995" aria-hidden="true" tabindex="-1"></a>A point estimate of $\theta$ is denoted by $\hat{\theta}$.</span>
<span id="cb7-996"><a href="#cb7-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-997"><a href="#cb7-997" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-998"><a href="#cb7-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-999"><a href="#cb7-999" aria-hidden="true" tabindex="-1"></a>Given data $X_1, \ldots, X_n$, a **point estimator** is a function:</span>
<span id="cb7-1000"><a href="#cb7-1000" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_n = g(X_1, \ldots, X_n)$$</span>
<span id="cb7-1001"><a href="#cb7-1001" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1002"><a href="#cb7-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1003"><a href="#cb7-1003" aria-hidden="true" tabindex="-1"></a>The "hat" notation $\hat{\theta}$ can indicate both an estimator and the estimate.</span>
<span id="cb7-1004"><a href="#cb7-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1005"><a href="#cb7-1005" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb7-1006"><a href="#cb7-1006" aria-hidden="true" tabindex="-1"></a>**Critical Distinction**:</span>
<span id="cb7-1007"><a href="#cb7-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1008"><a href="#cb7-1008" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameter** $\theta$: Fixed, unknown number we want to learn</span>
<span id="cb7-1009"><a href="#cb7-1009" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Estimator** $\hat{\theta}_n$: Random variable (before seeing data)</span>
<span id="cb7-1010"><a href="#cb7-1010" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Estimate** $\hat{\theta}_n$: Specific number (after seeing data) -- notation can be overlapping</span>
<span id="cb7-1011"><a href="#cb7-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1012"><a href="#cb7-1012" aria-hidden="true" tabindex="-1"></a>For example, $\bar{X}_n$ is an estimator; $\bar{x}_n = 3.7$ is an estimate.</span>
<span id="cb7-1013"><a href="#cb7-1013" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1014"><a href="#cb7-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1015"><a href="#cb7-1015" aria-hidden="true" tabindex="-1"></a>The distribution of $\hat{\theta}_n$ is called the **sampling distribution**. The standard deviation of this distribution is the **standard error**:</span>
<span id="cb7-1016"><a href="#cb7-1016" aria-hidden="true" tabindex="-1"></a>$$\text{se}(\hat{\theta}_n) = \sqrt{\mathbb{V}(\hat{\theta}_n)}$$</span>
<span id="cb7-1017"><a href="#cb7-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1018"><a href="#cb7-1018" aria-hidden="true" tabindex="-1"></a>When the standard error depends on unknown parameters, we use the **estimated standard error** $\widehat{\text{se}}$.</span>
<span id="cb7-1019"><a href="#cb7-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1020"><a href="#cb7-1020" aria-hidden="true" tabindex="-1"></a>A particularly common standard error in stastistics is the <span class="co">[</span><span class="ot">standard error of the mean (SEM)</span><span class="co">](https://en.wikipedia.org/wiki/Standard_error)</span>.</span>
<span id="cb7-1021"><a href="#cb7-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1022"><a href="#cb7-1022" aria-hidden="true" tabindex="-1"></a><span class="fu">### How to Evaluate Estimators</span></span>
<span id="cb7-1023"><a href="#cb7-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1024"><a href="#cb7-1024" aria-hidden="true" tabindex="-1"></a>How do we judge if an estimator is "good"? Several criteria have emerged:</span>
<span id="cb7-1025"><a href="#cb7-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1026"><a href="#cb7-1026" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-1027"><a href="#cb7-1027" aria-hidden="true" tabindex="-1"></a>**Bias**: The systematic error of an estimator.</span>
<span id="cb7-1028"><a href="#cb7-1028" aria-hidden="true" tabindex="-1"></a>$$\text{bias}(\hat{\theta}_n) = \mathbb{E}(\hat{\theta}_n) - \theta$$</span>
<span id="cb7-1029"><a href="#cb7-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1030"><a href="#cb7-1030" aria-hidden="true" tabindex="-1"></a>An estimator is **unbiased** if $\mathbb{E}(\hat{\theta}_n) = \theta$.</span>
<span id="cb7-1031"><a href="#cb7-1031" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1032"><a href="#cb7-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1033"><a href="#cb7-1033" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-1034"><a href="#cb7-1034" aria-hidden="true" tabindex="-1"></a>**Consistency**: An estimator is consistent if it converges to the true value.</span>
<span id="cb7-1035"><a href="#cb7-1035" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_n \xrightarrow{P} \theta \text{ as } n \to \infty$$</span>
<span id="cb7-1036"><a href="#cb7-1036" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1037"><a href="#cb7-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1038"><a href="#cb7-1038" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb7-1039"><a href="#cb7-1039" aria-hidden="true" tabindex="-1"></a>**Mean Squared Error (MSE)**: The average squared distance from the truth.</span>
<span id="cb7-1040"><a href="#cb7-1040" aria-hidden="true" tabindex="-1"></a>$$\text{MSE}(\hat{\theta}_n) = \mathbb{E}<span class="co">[</span><span class="ot">(\hat{\theta}_n - \theta)^2</span><span class="co">]</span>$$</span>
<span id="cb7-1041"><a href="#cb7-1041" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1042"><a href="#cb7-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1043"><a href="#cb7-1043" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb7-1044"><a href="#cb7-1044" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Evaluating Estimators for the Mean</span></span>
<span id="cb7-1045"><a href="#cb7-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1046"><a href="#cb7-1046" aria-hidden="true" tabindex="-1"></a>Suppose $X_1, \ldots, X_n \sim \mathcal{N}(\theta, \sigma^2)$ where $\theta$ is unknown. Consider three estimators:</span>
<span id="cb7-1047"><a href="#cb7-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1048"><a href="#cb7-1048" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Constant estimator**: $\hat{\theta}_n^{(1)} = 3$</span>
<span id="cb7-1049"><a href="#cb7-1049" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bias: $\mathbb{E}(3) - \theta = 3 - \theta$ (biased unless $\theta = 3$)</span>
<span id="cb7-1050"><a href="#cb7-1050" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Variance: $\mathbb{V}(3) = 0$</span>
<span id="cb7-1051"><a href="#cb7-1051" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Consistent: No, always equals 3</span>
<span id="cb7-1052"><a href="#cb7-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>MSE: $(3 - \theta)^2$</span>
<span id="cb7-1053"><a href="#cb7-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1054"><a href="#cb7-1054" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**First observation**: $\hat{\theta}_n^{(2)} = X_1$</span>
<span id="cb7-1055"><a href="#cb7-1055" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bias: $\mathbb{E}(X_1) - \theta = 0$ (unbiased!)</span>
<span id="cb7-1056"><a href="#cb7-1056" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Variance: $\mathbb{V}(X_1) = \sigma^2$</span>
<span id="cb7-1057"><a href="#cb7-1057" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Consistent: No, variance doesn't shrink</span>
<span id="cb7-1058"><a href="#cb7-1058" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>MSE: $\sigma^2$</span>
<span id="cb7-1059"><a href="#cb7-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1060"><a href="#cb7-1060" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Sample mean**: $\hat{\theta}_n^{(3)} = \bar{X}_n$</span>
<span id="cb7-1061"><a href="#cb7-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bias: $\mathbb{E}(\bar{X}_n) - \theta = 0$ (unbiased!)</span>
<span id="cb7-1062"><a href="#cb7-1062" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Variance: $\mathbb{V}(\bar{X}_n) = \sigma^2/n$</span>
<span id="cb7-1063"><a href="#cb7-1063" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Consistent: Yes! (by LLN)</span>
<span id="cb7-1064"><a href="#cb7-1064" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>MSE: $\sigma^2/n \to 0$</span>
<span id="cb7-1065"><a href="#cb7-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1066"><a href="#cb7-1066" aria-hidden="true" tabindex="-1"></a>The sample mean is unbiased AND consistent—it improves with more data!</span>
<span id="cb7-1067"><a href="#cb7-1067" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1068"><a href="#cb7-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1069"><a href="#cb7-1069" aria-hidden="true" tabindex="-1"></a>A classic example shows that unbiased isn't everything:</span>
<span id="cb7-1070"><a href="#cb7-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1071"><a href="#cb7-1071" aria-hidden="true" tabindex="-1"></a>**Sample variance**: Two common estimators for population variance $\sigma^2$:</span>
<span id="cb7-1072"><a href="#cb7-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1073"><a href="#cb7-1073" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Unbiased version**: $S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2$</span>
<span id="cb7-1074"><a href="#cb7-1074" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\mathbb{E}(S^2) = \sigma^2$ (unbiased by design)</span>
<span id="cb7-1075"><a href="#cb7-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1076"><a href="#cb7-1076" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Maximum likelihood estimator**: $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X})^2$</span>
<span id="cb7-1077"><a href="#cb7-1077" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\mathbb{E}(\hat{\sigma}^2) = \frac{n-1}{n}\sigma^2$ (biased!)</span>
<span id="cb7-1078"><a href="#cb7-1078" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-1079"><a href="#cb7-1079" aria-hidden="true" tabindex="-1"></a>Which is better? It depends on the criterion!</span>
<span id="cb7-1080"><a href="#cb7-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1081"><a href="#cb7-1081" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Bias-Variance Tradeoff</span></span>
<span id="cb7-1082"><a href="#cb7-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1083"><a href="#cb7-1083" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb7-1084"><a href="#cb7-1084" aria-hidden="true" tabindex="-1"></a>The MSE decomposes as:</span>
<span id="cb7-1085"><a href="#cb7-1085" aria-hidden="true" tabindex="-1"></a>$$\text{MSE} = \text{bias}^2(\hat{\theta}_n) + \mathbb{V}(\hat{\theta}_n)$$</span>
<span id="cb7-1086"><a href="#cb7-1086" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1087"><a href="#cb7-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1088"><a href="#cb7-1088" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-1089"><a href="#cb7-1089" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb7-1090"><a href="#cb7-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1091"><a href="#cb7-1091" aria-hidden="true" tabindex="-1"></a>Let $\bar{\theta}_n = \mathbb{E}(\hat{\theta}_n)$. Then:</span>
<span id="cb7-1092"><a href="#cb7-1092" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb7-1093"><a href="#cb7-1093" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(\hat{\theta}_n - \theta)^2</span><span class="co">]</span> </span>
<span id="cb7-1094"><a href="#cb7-1094" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(\hat{\theta}_n - \bar{\theta}_n + \bar{\theta}_n - \theta)^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb7-1095"><a href="#cb7-1095" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(\hat{\theta}_n - \bar{\theta}_n)^2</span><span class="co">]</span> + 2(\bar{\theta}_n - \theta)\mathbb{E}<span class="co">[</span><span class="ot">\hat{\theta}_n - \bar{\theta}_n</span><span class="co">]</span> + (\bar{\theta}_n - \theta)^2 <span class="sc">\\</span></span>
<span id="cb7-1096"><a href="#cb7-1096" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(\hat{\theta}_n - \bar{\theta}_n)^2</span><span class="co">]</span> + 2(\bar{\theta}_n - \theta) \cdot 0 + (\bar{\theta}_n - \theta)^2 <span class="sc">\\</span></span>
<span id="cb7-1097"><a href="#cb7-1097" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{V}(\hat{\theta}_n) + \text{bias}^2(\hat{\theta}_n)</span>
<span id="cb7-1098"><a href="#cb7-1098" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb7-1099"><a href="#cb7-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1100"><a href="#cb7-1100" aria-hidden="true" tabindex="-1"></a>where we used that $\mathbb{E}<span class="co">[</span><span class="ot">\hat{\theta}_n - \bar{\theta}_n</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">\hat{\theta}_n</span><span class="co">]</span> - \bar{\theta}_n = \bar{\theta}_n - \bar{\theta}_n = 0$.</span>
<span id="cb7-1101"><a href="#cb7-1101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1102"><a href="#cb7-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1103"><a href="#cb7-1103" aria-hidden="true" tabindex="-1"></a>This decomposition reveals a fundamental tradeoff in statistics.</span>
<span id="cb7-1104"><a href="#cb7-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1105"><a href="#cb7-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1106"><a href="#cb7-1106" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb7-1107"><a href="#cb7-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1108"><a href="#cb7-1108" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb7-1109"><a href="#cb7-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1110"><a href="#cb7-1110" aria-hidden="true" tabindex="-1"></a>Imagine you're an archer trying to hit a target. Your performance depends on two things:</span>
<span id="cb7-1111"><a href="#cb7-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1112"><a href="#cb7-1112" aria-hidden="true" tabindex="-1"></a>**Bias**: How far your average shot is from the bullseye. A biased archer consistently aims too high or too far left.</span>
<span id="cb7-1113"><a href="#cb7-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1114"><a href="#cb7-1114" aria-hidden="true" tabindex="-1"></a>**Variance**: How spread out your shots are. A high-variance archer is inconsistent—sometimes dead on, sometimes way off.</span>
<span id="cb7-1115"><a href="#cb7-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1116"><a href="#cb7-1116" aria-hidden="true" tabindex="-1"></a>The best archer has low bias AND low variance. But here's the key insight: sometimes accepting a little bias can dramatically reduce variance, improving overall accuracy!</span>
<span id="cb7-1117"><a href="#cb7-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1118"><a href="#cb7-1118" aria-hidden="true" tabindex="-1"></a>Think of it this way:</span>
<span id="cb7-1119"><a href="#cb7-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1120"><a href="#cb7-1120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A complex model (like memorizing training data) has low bias but high variance</span>
<span id="cb7-1121"><a href="#cb7-1121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A simple model (like always predicting the average) has higher bias but low variance</span>
<span id="cb7-1122"><a href="#cb7-1122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The sweet spot balances both</span>
<span id="cb7-1123"><a href="#cb7-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1124"><a href="#cb7-1124" aria-hidden="true" tabindex="-1"></a>This tradeoff is why regularization works in machine learning -- we accept a bit of bias to gain a lot in variance reduction.</span>
<span id="cb7-1125"><a href="#cb7-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1126"><a href="#cb7-1126" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb7-1127"><a href="#cb7-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1128"><a href="#cb7-1128" aria-hidden="true" tabindex="-1"></a>The bias-variance decomposition gives us a precise way to understand prediction error:</span>
<span id="cb7-1129"><a href="#cb7-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1130"><a href="#cb7-1130" aria-hidden="true" tabindex="-1"></a>$$\text{MSE}(\hat{\theta}_n) = \text{bias}^2(\hat{\theta}_n) + \mathbb{V}(\hat{\theta}_n)$$</span>
<span id="cb7-1131"><a href="#cb7-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1132"><a href="#cb7-1132" aria-hidden="true" tabindex="-1"></a>This isn't just algebra -- it reveals the two fundamental sources of error:</span>
<span id="cb7-1133"><a href="#cb7-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1134"><a href="#cb7-1134" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Systematic error** (bias): Being consistently wrong</span>
<span id="cb7-1135"><a href="#cb7-1135" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Random error** (variance): Being inconsistently wrong</span>
<span id="cb7-1136"><a href="#cb7-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1137"><a href="#cb7-1137" aria-hidden="true" tabindex="-1"></a>For prediction problems where $\hat{f}(x)$ estimates $f(x)$:</span>
<span id="cb7-1138"><a href="#cb7-1138" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">(\hat{f}(x) - f(x))^2</span><span class="co">]</span> = \underbrace{(E<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> - f(x))^2}_{\text{bias}^2} + \underbrace{\mathbb{V}(\hat{f}(x))}_{\text{variance}}$$</span>
<span id="cb7-1139"><a href="#cb7-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1140"><a href="#cb7-1140" aria-hidden="true" tabindex="-1"></a>The optimal predictor minimizes their sum. In machine learning:</span>
<span id="cb7-1141"><a href="#cb7-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1142"><a href="#cb7-1142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Increasing model complexity typically decreases bias but increases variance</span>
<span id="cb7-1143"><a href="#cb7-1143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The art is finding the right complexity for your data</span>
<span id="cb7-1144"><a href="#cb7-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1145"><a href="#cb7-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb7-1146"><a href="#cb7-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1147"><a href="#cb7-1147" aria-hidden="true" tabindex="-1"></a>Let's visualize the bias-variance tradeoff by comparing estimators for population variance.</span>
<span id="cb7-1148"><a href="#cb7-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1151"><a href="#cb7-1151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-1152"><a href="#cb7-1152" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb7-1153"><a href="#cb7-1153" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb7-1154"><a href="#cb7-1154" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-1155"><a href="#cb7-1155" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-1156"><a href="#cb7-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1157"><a href="#cb7-1157" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-1158"><a href="#cb7-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1159"><a href="#cb7-1159" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb7-1160"><a href="#cb7-1160" aria-hidden="true" tabindex="-1"></a>true_variance <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># True σ² = 1</span></span>
<span id="cb7-1161"><a href="#cb7-1161" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> np.arange(<span class="dv">5</span>, <span class="dv">101</span>, <span class="dv">5</span>)</span>
<span id="cb7-1162"><a href="#cb7-1162" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb7-1163"><a href="#cb7-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1164"><a href="#cb7-1164" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for results</span></span>
<span id="cb7-1165"><a href="#cb7-1165" aria-hidden="true" tabindex="-1"></a>bias_unbiased <span class="op">=</span> []</span>
<span id="cb7-1166"><a href="#cb7-1166" aria-hidden="true" tabindex="-1"></a>bias_mle <span class="op">=</span> []</span>
<span id="cb7-1167"><a href="#cb7-1167" aria-hidden="true" tabindex="-1"></a>variance_unbiased <span class="op">=</span> []</span>
<span id="cb7-1168"><a href="#cb7-1168" aria-hidden="true" tabindex="-1"></a>variance_mle <span class="op">=</span> []</span>
<span id="cb7-1169"><a href="#cb7-1169" aria-hidden="true" tabindex="-1"></a>mse_unbiased <span class="op">=</span> []</span>
<span id="cb7-1170"><a href="#cb7-1170" aria-hidden="true" tabindex="-1"></a>mse_mle <span class="op">=</span> []</span>
<span id="cb7-1171"><a href="#cb7-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1172"><a href="#cb7-1172" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> n_values:</span>
<span id="cb7-1173"><a href="#cb7-1173" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate many samples and compute both estimators</span></span>
<span id="cb7-1174"><a href="#cb7-1174" aria-hidden="true" tabindex="-1"></a>    unbiased_estimates <span class="op">=</span> []</span>
<span id="cb7-1175"><a href="#cb7-1175" aria-hidden="true" tabindex="-1"></a>    mle_estimates <span class="op">=</span> []</span>
<span id="cb7-1176"><a href="#cb7-1176" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1177"><a href="#cb7-1177" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb7-1178"><a href="#cb7-1178" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate sample from N(0, 1)</span></span>
<span id="cb7-1179"><a href="#cb7-1179" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb7-1180"><a href="#cb7-1180" aria-hidden="true" tabindex="-1"></a>        sample_mean <span class="op">=</span> np.mean(sample)</span>
<span id="cb7-1181"><a href="#cb7-1181" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-1182"><a href="#cb7-1182" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Unbiased estimator (n-1 denominator)</span></span>
<span id="cb7-1183"><a href="#cb7-1183" aria-hidden="true" tabindex="-1"></a>        s_squared <span class="op">=</span> np.<span class="bu">sum</span>((sample <span class="op">-</span> sample_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (n <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb7-1184"><a href="#cb7-1184" aria-hidden="true" tabindex="-1"></a>        unbiased_estimates.append(s_squared)</span>
<span id="cb7-1185"><a href="#cb7-1185" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-1186"><a href="#cb7-1186" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MLE (n denominator)</span></span>
<span id="cb7-1187"><a href="#cb7-1187" aria-hidden="true" tabindex="-1"></a>        sigma_hat_squared <span class="op">=</span> np.<span class="bu">sum</span>((sample <span class="op">-</span> sample_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> n</span>
<span id="cb7-1188"><a href="#cb7-1188" aria-hidden="true" tabindex="-1"></a>        mle_estimates.append(sigma_hat_squared)</span>
<span id="cb7-1189"><a href="#cb7-1189" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1190"><a href="#cb7-1190" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate bias, variance, and MSE</span></span>
<span id="cb7-1191"><a href="#cb7-1191" aria-hidden="true" tabindex="-1"></a>    unbiased_estimates <span class="op">=</span> np.array(unbiased_estimates)</span>
<span id="cb7-1192"><a href="#cb7-1192" aria-hidden="true" tabindex="-1"></a>    mle_estimates <span class="op">=</span> np.array(mle_estimates)</span>
<span id="cb7-1193"><a href="#cb7-1193" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1194"><a href="#cb7-1194" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias</span></span>
<span id="cb7-1195"><a href="#cb7-1195" aria-hidden="true" tabindex="-1"></a>    bias_unbiased.append(np.mean(unbiased_estimates) <span class="op">-</span> true_variance)</span>
<span id="cb7-1196"><a href="#cb7-1196" aria-hidden="true" tabindex="-1"></a>    bias_mle.append(np.mean(mle_estimates) <span class="op">-</span> true_variance)</span>
<span id="cb7-1197"><a href="#cb7-1197" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1198"><a href="#cb7-1198" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Variance</span></span>
<span id="cb7-1199"><a href="#cb7-1199" aria-hidden="true" tabindex="-1"></a>    variance_unbiased.append(np.var(unbiased_estimates))</span>
<span id="cb7-1200"><a href="#cb7-1200" aria-hidden="true" tabindex="-1"></a>    variance_mle.append(np.var(mle_estimates))</span>
<span id="cb7-1201"><a href="#cb7-1201" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1202"><a href="#cb7-1202" aria-hidden="true" tabindex="-1"></a>    <span class="co"># MSE</span></span>
<span id="cb7-1203"><a href="#cb7-1203" aria-hidden="true" tabindex="-1"></a>    mse_unbiased.append(np.mean((unbiased_estimates <span class="op">-</span> true_variance)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb7-1204"><a href="#cb7-1204" aria-hidden="true" tabindex="-1"></a>    mse_mle.append(np.mean((mle_estimates <span class="op">-</span> true_variance)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb7-1205"><a href="#cb7-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1206"><a href="#cb7-1206" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plots</span></span>
<span id="cb7-1207"><a href="#cb7-1207" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb7-1208"><a href="#cb7-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1209"><a href="#cb7-1209" aria-hidden="true" tabindex="-1"></a><span class="co"># Bias plot</span></span>
<span id="cb7-1210"><a href="#cb7-1210" aria-hidden="true" tabindex="-1"></a>ax1.plot(n_values, bias_unbiased, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Unbiased (S²)'</span>)</span>
<span id="cb7-1211"><a href="#cb7-1211" aria-hidden="true" tabindex="-1"></a>ax1.plot(n_values, bias_mle, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE (σ̂²)'</span>)</span>
<span id="cb7-1212"><a href="#cb7-1212" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-1213"><a href="#cb7-1213" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Sample size (n)'</span>)</span>
<span id="cb7-1214"><a href="#cb7-1214" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Bias'</span>)</span>
<span id="cb7-1215"><a href="#cb7-1215" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Bias'</span>)</span>
<span id="cb7-1216"><a href="#cb7-1216" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb7-1217"><a href="#cb7-1217" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-1218"><a href="#cb7-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1219"><a href="#cb7-1219" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance plot  </span></span>
<span id="cb7-1220"><a href="#cb7-1220" aria-hidden="true" tabindex="-1"></a>ax2.plot(n_values, variance_unbiased, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Unbiased (S²)'</span>)</span>
<span id="cb7-1221"><a href="#cb7-1221" aria-hidden="true" tabindex="-1"></a>ax2.plot(n_values, variance_mle, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE (σ̂²)'</span>)</span>
<span id="cb7-1222"><a href="#cb7-1222" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Sample size (n)'</span>)</span>
<span id="cb7-1223"><a href="#cb7-1223" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Variance'</span>)</span>
<span id="cb7-1224"><a href="#cb7-1224" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Variance of Estimator'</span>)</span>
<span id="cb7-1225"><a href="#cb7-1225" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb7-1226"><a href="#cb7-1226" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-1227"><a href="#cb7-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1228"><a href="#cb7-1228" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE plot</span></span>
<span id="cb7-1229"><a href="#cb7-1229" aria-hidden="true" tabindex="-1"></a>ax3.plot(n_values, mse_unbiased, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Unbiased (S²)'</span>)</span>
<span id="cb7-1230"><a href="#cb7-1230" aria-hidden="true" tabindex="-1"></a>ax3.plot(n_values, mse_mle, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE (σ̂²)'</span>)</span>
<span id="cb7-1231"><a href="#cb7-1231" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'Sample size (n)'</span>)</span>
<span id="cb7-1232"><a href="#cb7-1232" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'MSE'</span>)</span>
<span id="cb7-1233"><a href="#cb7-1233" aria-hidden="true" tabindex="-1"></a>ax3.set_title(<span class="st">'Mean Squared Error'</span>)</span>
<span id="cb7-1234"><a href="#cb7-1234" aria-hidden="true" tabindex="-1"></a>ax3.legend()</span>
<span id="cb7-1235"><a href="#cb7-1235" aria-hidden="true" tabindex="-1"></a>ax3.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-1236"><a href="#cb7-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1237"><a href="#cb7-1237" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">'Bias-Variance Tradeoff: Variance Estimators'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-1238"><a href="#cb7-1238" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-1239"><a href="#cb7-1239" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-1240"><a href="#cb7-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1241"><a href="#cb7-1241" aria-hidden="true" tabindex="-1"></a><span class="co"># Print numerical comparison for n=10</span></span>
<span id="cb7-1242"><a href="#cb7-1242" aria-hidden="true" tabindex="-1"></a>n_idx <span class="op">=</span> <span class="dv">1</span>  <span class="co"># n=10</span></span>
<span id="cb7-1243"><a href="#cb7-1243" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For n=10:"</span>)</span>
<span id="cb7-1244"><a href="#cb7-1244" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbiased (S²): Bias = </span><span class="sc">{</span>bias_unbiased[n_idx]<span class="sc">:.4f}</span><span class="ss">, Variance = </span><span class="sc">{</span>variance_unbiased[n_idx]<span class="sc">:.4f}</span><span class="ss">, MSE = </span><span class="sc">{</span>mse_unbiased[n_idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-1245"><a href="#cb7-1245" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE (σ̂²):      Bias = </span><span class="sc">{</span>bias_mle[n_idx]<span class="sc">:.4f}</span><span class="ss">, Variance = </span><span class="sc">{</span>variance_mle[n_idx]<span class="sc">:.4f}</span><span class="ss">, MSE = </span><span class="sc">{</span>mse_mle[n_idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-1246"><a href="#cb7-1246" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">The MLE has lower MSE despite being biased!"</span>)</span>
<span id="cb7-1247"><a href="#cb7-1247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-1248"><a href="#cb7-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1249"><a href="#cb7-1249" aria-hidden="true" tabindex="-1"></a>Key insights from the visualization:</span>
<span id="cb7-1250"><a href="#cb7-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1251"><a href="#cb7-1251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The unbiased estimator $S^2$ has zero bias (blue solid line at 0)</span>
<span id="cb7-1252"><a href="#cb7-1252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The MLE $\hat{\sigma}^2$ has negative bias that shrinks as $n$ grows</span>
<span id="cb7-1253"><a href="#cb7-1253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The MLE has lower variance than the unbiased estimator</span>
<span id="cb7-1254"><a href="#cb7-1254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For finite samples, the biased MLE has lower MSE!**</span>
<span id="cb7-1255"><a href="#cb7-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1256"><a href="#cb7-1256" aria-hidden="true" tabindex="-1"></a>This demonstrates a profound principle: the "best" estimator depends on your criterion. Unbiasedness is nice, but minimizing MSE often matters more in practice.</span>
<span id="cb7-1257"><a href="#cb7-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1258"><a href="#cb7-1258" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1259"><a href="#cb7-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1260"><a href="#cb7-1260" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb7-1261"><a href="#cb7-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1262"><a href="#cb7-1262" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb7-1263"><a href="#cb7-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1264"><a href="#cb7-1264" aria-hidden="true" tabindex="-1"></a>We've built a complete framework for understanding randomness and inference:</span>
<span id="cb7-1265"><a href="#cb7-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1266"><a href="#cb7-1266" aria-hidden="true" tabindex="-1"></a>**Inequalities** bound the unknown:</span>
<span id="cb7-1267"><a href="#cb7-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1268"><a href="#cb7-1268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Markov**: $\mathbb{P}(X \geq t) \leq \mathbb{E}(X)/t$ — averages constrain extremes</span>
<span id="cb7-1269"><a href="#cb7-1269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chebyshev**: $\mathbb{P}(|X - \mu| \geq k\sigma) \leq 1/k^2$ — universal bounds using variance</span>
<span id="cb7-1270"><a href="#cb7-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1271"><a href="#cb7-1271" aria-hidden="true" tabindex="-1"></a>**Convergence** describes limiting behavior:</span>
<span id="cb7-1272"><a href="#cb7-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1273"><a href="#cb7-1273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**In probability**: $X_n \xrightarrow{P} X$ — the random variable itself settles down</span>
<span id="cb7-1274"><a href="#cb7-1274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**In distribution**: $X_n \rightsquigarrow X$ — the shape of the distribution stabilizes</span>
<span id="cb7-1275"><a href="#cb7-1275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key relationship**: Convergence in probability implies convergence in distribution</span>
<span id="cb7-1276"><a href="#cb7-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1277"><a href="#cb7-1277" aria-hidden="true" tabindex="-1"></a>**Fundamental theorems** guarantee nice behavior:</span>
<span id="cb7-1278"><a href="#cb7-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1279"><a href="#cb7-1279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Law of Large Numbers**: $\bar{X}_n \xrightarrow{P} \mu$ — sample means converge to population means</span>
<span id="cb7-1280"><a href="#cb7-1280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Central Limit Theorem**: $\sqrt{n}(\bar{X}_n - \mu)/\sigma \rightsquigarrow \mathcal{N}(0,1)$ — sample means are approximately normal</span>
<span id="cb7-1281"><a href="#cb7-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1282"><a href="#cb7-1282" aria-hidden="true" tabindex="-1"></a>**Statistical inference** flips the perspective:</span>
<span id="cb7-1283"><a href="#cb7-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1284"><a href="#cb7-1284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Models**: Parametric (finite-dimensional) vs nonparametric (infinite-dimensional)</span>
<span id="cb7-1285"><a href="#cb7-1285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Estimators**: Functions of data that guess parameters</span>
<span id="cb7-1286"><a href="#cb7-1286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Standard error**: Standard deviation of an estimator's sampling distribution</span>
<span id="cb7-1287"><a href="#cb7-1287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Evaluation criteria**: Bias, variance, consistency, MSE</span>
<span id="cb7-1288"><a href="#cb7-1288" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bias-variance tradeoff**: $\text{MSE} = \text{bias}^2 + \text{variance}$</span>
<span id="cb7-1289"><a href="#cb7-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1290"><a href="#cb7-1290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why These Concepts Matter</span></span>
<span id="cb7-1291"><a href="#cb7-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1292"><a href="#cb7-1292" aria-hidden="true" tabindex="-1"></a>**For Statistical Inference**:</span>
<span id="cb7-1293"><a href="#cb7-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1294"><a href="#cb7-1294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LLN justifies using sample statistics to estimate population parameters</span>
<span id="cb7-1295"><a href="#cb7-1295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CLT enables confidence intervals and hypothesis tests (to be seen in the next chapters)</span>
<span id="cb7-1296"><a href="#cb7-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias-variance tradeoff guides choice of estimators</span>
<span id="cb7-1297"><a href="#cb7-1297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consistency ensures our methods improve with more data</span>
<span id="cb7-1298"><a href="#cb7-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1299"><a href="#cb7-1299" aria-hidden="true" tabindex="-1"></a>**For Machine Learning**:</span>
<span id="cb7-1300"><a href="#cb7-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1301"><a href="#cb7-1301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence concepts analyze iterative algorithms such as stochastic optimization</span>
<span id="cb7-1302"><a href="#cb7-1302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias-variance tradeoff explains overfitting vs underfitting</span>
<span id="cb7-1303"><a href="#cb7-1303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CLT justifies bootstrap and cross-validation, as we will see in the next chapters</span>
<span id="cb7-1304"><a href="#cb7-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1305"><a href="#cb7-1305" aria-hidden="true" tabindex="-1"></a>**For Data Science Practice**:</span>
<span id="cb7-1306"><a href="#cb7-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1307"><a href="#cb7-1307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understanding variability in estimates prevents overconfidence</span>
<span id="cb7-1308"><a href="#cb7-1308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Recognizing when CLT applies (and when it doesn't)</span>
<span id="cb7-1309"><a href="#cb7-1309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Choosing between simple and complex models</span>
<span id="cb7-1310"><a href="#cb7-1310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpreting A/B test results correctly</span>
<span id="cb7-1311"><a href="#cb7-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1312"><a href="#cb7-1312" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb7-1313"><a href="#cb7-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1314"><a href="#cb7-1314" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Confusing convergence types**: </span>
<span id="cb7-1315"><a href="#cb7-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1316"><a href="#cb7-1316" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>"My algorithm converged" -- in what sense?</span>
<span id="cb7-1317"><a href="#cb7-1317" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Convergence in distribution does *not* imply convergence in probability!</span>
<span id="cb7-1318"><a href="#cb7-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1319"><a href="#cb7-1319" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Misapplying the CLT**:</span>
<span id="cb7-1320"><a href="#cb7-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1321"><a href="#cb7-1321" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>CLT is about *sample means*, not individual observations</span>
<span id="cb7-1322"><a href="#cb7-1322" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Need large enough $n$ (depends on skewness)</span>
<span id="cb7-1323"><a href="#cb7-1323" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Doesn't work without finite variance (Cauchy!)</span>
<span id="cb7-1324"><a href="#cb7-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1325"><a href="#cb7-1325" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Overvaluing unbiasedness**:</span>
<span id="cb7-1326"><a href="#cb7-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1327"><a href="#cb7-1327" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Unbiased doesn't mean good (e.g., using just $X_1$)</span>
<span id="cb7-1328"><a href="#cb7-1328" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Biased can be better in statistics (regularization, priors)</span>
<span id="cb7-1329"><a href="#cb7-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1330"><a href="#cb7-1330" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Ignoring assumptions**:</span>
<span id="cb7-1331"><a href="#cb7-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1332"><a href="#cb7-1332" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Independence matters for variance calculations</span>
<span id="cb7-1333"><a href="#cb7-1333" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Finite variance required for CLT</span>
<span id="cb7-1334"><a href="#cb7-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1335"><a href="#cb7-1335" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Misinterpreting bounds**:</span>
<span id="cb7-1336"><a href="#cb7-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1337"><a href="#cb7-1337" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Markov/Chebyshev give worst-case bounds</span>
<span id="cb7-1338"><a href="#cb7-1338" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Often very loose in practice</span>
<span id="cb7-1339"><a href="#cb7-1339" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Tighter bounds exist for specific distributions</span>
<span id="cb7-1340"><a href="#cb7-1340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1341"><a href="#cb7-1341" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb7-1342"><a href="#cb7-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1343"><a href="#cb7-1343" aria-hidden="true" tabindex="-1"></a>This chapter connects fundamental probability theory to practical statistical inference:</span>
<span id="cb7-1344"><a href="#cb7-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1345"><a href="#cb7-1345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**From Previous Chapters**: We've applied Chapter 1's probability framework and Chapter 2's expectation/variance concepts to prove convergence theorems (LLN, CLT) that explain why sample statistics work as estimators</span>
<span id="cb7-1346"><a href="#cb7-1346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next - Chapter 4 (Bootstrap)**: While this chapter gave us theoretical tools for inference (CLT-based confidence intervals), the bootstrap will provide a computational approach that works even when theoretical distributions are intractable</span>
<span id="cb7-1347"><a href="#cb7-1347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistical Modeling (Chapters 5+)**: The bias-variance tradeoff introduced here becomes central to model selection, while MSE serves as our primary tool for comparing estimators in regression and machine learning</span>
<span id="cb7-1348"><a href="#cb7-1348" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Throughout the Course**: The convergence concepts (especially CLT) and inference framework established here underpin virtually every statistical method—from hypothesis testing to Bayesian inference</span>
<span id="cb7-1349"><a href="#cb7-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1350"><a href="#cb7-1350" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb7-1351"><a href="#cb7-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1352"><a href="#cb7-1352" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Applying Chebyshev**: A website's daily visitors have mean 10,000 and standard deviation 2,000. Without assuming any distribution, what can you say about the probability of getting fewer than 4,000 or more than 16,000 visitors?</span>
<span id="cb7-1353"><a href="#cb7-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1354"><a href="#cb7-1354" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**CLT Application**: A casino's slot machine pays out €1 with probability 0.4 and €0 otherwise. If someone plays 400 times, approximate the probability their total winnings exceed €170. </span>
<span id="cb7-1355"><a href="#cb7-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1356"><a href="#cb7-1356" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Comparing Estimators**: Given $X_1, \ldots, X_n \sim \text{Uniform}(0, \theta)$, consider two estimators:</span>
<span id="cb7-1357"><a href="#cb7-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1358"><a href="#cb7-1358" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\hat{\theta}_1 = 2\bar{X}_n$</span>
<span id="cb7-1359"><a href="#cb7-1359" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\hat{\theta}_2 = \frac{n+1}{n} \max<span class="sc">\{</span>X_1, \ldots, X_n<span class="sc">\}</span>$</span>
<span id="cb7-1360"><a href="#cb7-1360" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-1361"><a href="#cb7-1361" aria-hidden="true" tabindex="-1"></a>   Calculate bias and variance for each. Which has lower MSE?</span>
<span id="cb7-1362"><a href="#cb7-1362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1363"><a href="#cb7-1363" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Convergence Concepts**: Let $X_n$ have PMF:</span>
<span id="cb7-1364"><a href="#cb7-1364" aria-hidden="true" tabindex="-1"></a>  $$P(X_n = 0) = 1 - 1/n, \quad P(X_n = n) = 1/n$$</span>
<span id="cb7-1365"><a href="#cb7-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1366"><a href="#cb7-1366" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Does $X_n \xrightarrow{P} 0$?</span>
<span id="cb7-1367"><a href="#cb7-1367" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Does $X_n \rightsquigarrow 0$?</span>
<span id="cb7-1368"><a href="#cb7-1368" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Does $\mathbb{E}(X_n) \to 0$?</span>
<span id="cb7-1369"><a href="#cb7-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1370"><a href="#cb7-1370" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb7-1371"><a href="#cb7-1371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1372"><a href="#cb7-1372" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb7-1373"><a href="#cb7-1373" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb7-1374"><a href="#cb7-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1375"><a href="#cb7-1375" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb7-1376"><a href="#cb7-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1377"><a href="#cb7-1377" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-1378"><a href="#cb7-1378" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-1379"><a href="#cb7-1379" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-1380"><a href="#cb7-1380" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb7-1381"><a href="#cb7-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1382"><a href="#cb7-1382" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability inequalities</span></span>
<span id="cb7-1383"><a href="#cb7-1383" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> markov_bound(mean, t):</span>
<span id="cb7-1384"><a href="#cb7-1384" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Markov inequality bound P(X &gt;= t)"""</span></span>
<span id="cb7-1385"><a href="#cb7-1385" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">min</span>(mean <span class="op">/</span> t, <span class="dv">1</span>) <span class="cf">if</span> t <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb7-1386"><a href="#cb7-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1387"><a href="#cb7-1387" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chebyshev_bound(k):</span>
<span id="cb7-1388"><a href="#cb7-1388" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Chebyshev bound P(|X - μ| &gt;= kσ)"""</span></span>
<span id="cb7-1389"><a href="#cb7-1389" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">min</span>(<span class="dv">1</span> <span class="op">/</span> k<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>) <span class="cf">if</span> k <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb7-1390"><a href="#cb7-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1391"><a href="#cb7-1391" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulating convergence</span></span>
<span id="cb7-1392"><a href="#cb7-1392" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_lln(dist, n_max<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb7-1393"><a href="#cb7-1393" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show Law of Large Numbers"""</span></span>
<span id="cb7-1394"><a href="#cb7-1394" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> dist.rvs(n_max)</span>
<span id="cb7-1395"><a href="#cb7-1395" aria-hidden="true" tabindex="-1"></a>    cumulative_mean <span class="op">=</span> np.cumsum(samples) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_max <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-1396"><a href="#cb7-1396" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cumulative_mean</span>
<span id="cb7-1397"><a href="#cb7-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1398"><a href="#cb7-1398" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_clt(dist, n, n_simulations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb7-1399"><a href="#cb7-1399" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show Central Limit Theorem"""</span></span>
<span id="cb7-1400"><a href="#cb7-1400" aria-hidden="true" tabindex="-1"></a>    sample_means <span class="op">=</span> []</span>
<span id="cb7-1401"><a href="#cb7-1401" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb7-1402"><a href="#cb7-1402" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> dist.rvs(n)</span>
<span id="cb7-1403"><a href="#cb7-1403" aria-hidden="true" tabindex="-1"></a>        sample_means.append(np.mean(sample))</span>
<span id="cb7-1404"><a href="#cb7-1404" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(sample_means)</span>
<span id="cb7-1405"><a href="#cb7-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1406"><a href="#cb7-1406" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimator evaluation</span></span>
<span id="cb7-1407"><a href="#cb7-1407" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_estimator(estimator_func, true_value, n, sample_func, n_simulations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb7-1408"><a href="#cb7-1408" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute bias, variance, and MSE of an estimator via simulation."""</span></span>
<span id="cb7-1409"><a href="#cb7-1409" aria-hidden="true" tabindex="-1"></a>    estimates <span class="op">=</span> []</span>
<span id="cb7-1410"><a href="#cb7-1410" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb7-1411"><a href="#cb7-1411" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> sample_func(n)</span>
<span id="cb7-1412"><a href="#cb7-1412" aria-hidden="true" tabindex="-1"></a>        estimate <span class="op">=</span> estimator_func(sample)</span>
<span id="cb7-1413"><a href="#cb7-1413" aria-hidden="true" tabindex="-1"></a>        estimates.append(estimate)</span>
<span id="cb7-1414"><a href="#cb7-1414" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1415"><a href="#cb7-1415" aria-hidden="true" tabindex="-1"></a>    estimates <span class="op">=</span> np.array(estimates)</span>
<span id="cb7-1416"><a href="#cb7-1416" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> np.mean(estimates) <span class="op">-</span> true_value</span>
<span id="cb7-1417"><a href="#cb7-1417" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> np.var(estimates)</span>
<span id="cb7-1418"><a href="#cb7-1418" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean((estimates <span class="op">-</span> true_value)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-1419"><a href="#cb7-1419" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-1420"><a href="#cb7-1420" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'bias'</span>: bias, <span class="st">'variance'</span>: variance, <span class="st">'mse'</span>: mse}</span>
<span id="cb7-1421"><a href="#cb7-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1422"><a href="#cb7-1422" aria-hidden="true" tabindex="-1"></a><span class="co"># Common distributions for examples</span></span>
<span id="cb7-1423"><a href="#cb7-1423" aria-hidden="true" tabindex="-1"></a>normal_dist <span class="op">=</span> stats.norm(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-1424"><a href="#cb7-1424" aria-hidden="true" tabindex="-1"></a>exp_dist <span class="op">=</span> stats.expon(scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-1425"><a href="#cb7-1425" aria-hidden="true" tabindex="-1"></a>uniform_dist <span class="op">=</span> stats.uniform(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-1426"><a href="#cb7-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1427"><a href="#cb7-1427" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb7-1428"><a href="#cb7-1428" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_mean(x):</span>
<span id="cb7-1429"><a href="#cb7-1429" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(x)</span>
<span id="cb7-1430"><a href="#cb7-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1431"><a href="#cb7-1431" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_variance_unbiased(x):</span>
<span id="cb7-1432"><a href="#cb7-1432" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.var(x, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># n-1 denominator</span></span>
<span id="cb7-1433"><a href="#cb7-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1434"><a href="#cb7-1434" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_variance_mle(x):</span>
<span id="cb7-1435"><a href="#cb7-1435" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.var(x, ddof<span class="op">=</span><span class="dv">0</span>)  <span class="co"># n denominator</span></span>
<span id="cb7-1436"><a href="#cb7-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1437"><a href="#cb7-1437" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Evaluate variance estimators for N(0,1) samples (true variance = 1)</span></span>
<span id="cb7-1438"><a href="#cb7-1438" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_normal_sample(n):</span>
<span id="cb7-1439"><a href="#cb7-1439" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>n)</span>
<span id="cb7-1440"><a href="#cb7-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1441"><a href="#cb7-1441" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"--- Evaluating Variance Estimators (n=10) ---"</span>)</span>
<span id="cb7-1442"><a href="#cb7-1442" aria-hidden="true" tabindex="-1"></a>mle_results <span class="op">=</span> evaluate_estimator(</span>
<span id="cb7-1443"><a href="#cb7-1443" aria-hidden="true" tabindex="-1"></a>    estimator_func<span class="op">=</span>sample_variance_mle,</span>
<span id="cb7-1444"><a href="#cb7-1444" aria-hidden="true" tabindex="-1"></a>    true_value<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb7-1445"><a href="#cb7-1445" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb7-1446"><a href="#cb7-1446" aria-hidden="true" tabindex="-1"></a>    sample_func<span class="op">=</span>generate_normal_sample</span>
<span id="cb7-1447"><a href="#cb7-1447" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-1448"><a href="#cb7-1448" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE Estimator: </span><span class="sc">{</span>mle_results<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-1449"><a href="#cb7-1449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1450"><a href="#cb7-1450" aria-hidden="true" tabindex="-1"></a>unbiased_results <span class="op">=</span> evaluate_estimator(</span>
<span id="cb7-1451"><a href="#cb7-1451" aria-hidden="true" tabindex="-1"></a>    estimator_func<span class="op">=</span>sample_variance_unbiased,</span>
<span id="cb7-1452"><a href="#cb7-1452" aria-hidden="true" tabindex="-1"></a>    true_value<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb7-1453"><a href="#cb7-1453" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb7-1454"><a href="#cb7-1454" aria-hidden="true" tabindex="-1"></a>    sample_func<span class="op">=</span>generate_normal_sample</span>
<span id="cb7-1455"><a href="#cb7-1455" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-1456"><a href="#cb7-1456" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbiased Estimator: </span><span class="sc">{</span>unbiased_results<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-1457"><a href="#cb7-1457" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-1458"><a href="#cb7-1458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1459"><a href="#cb7-1459" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb7-1460"><a href="#cb7-1460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1461"><a href="#cb7-1461" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb7-1462"><a href="#cb7-1462" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-1463"><a href="#cb7-1463" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability inequalities</span></span>
<span id="cb7-1464"><a href="#cb7-1464" aria-hidden="true" tabindex="-1"></a>markov_bound <span class="ot">&lt;-</span> <span class="cf">function</span>(mean_val, t) {</span>
<span id="cb7-1465"><a href="#cb7-1465" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (t <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="dv">1</span>)</span>
<span id="cb7-1466"><a href="#cb7-1466" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min</span>(mean_val <span class="sc">/</span> t, <span class="dv">1</span>)</span>
<span id="cb7-1467"><a href="#cb7-1467" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-1468"><a href="#cb7-1468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1469"><a href="#cb7-1469" aria-hidden="true" tabindex="-1"></a>chebyshev_bound <span class="ot">&lt;-</span> <span class="cf">function</span>(k) {</span>
<span id="cb7-1470"><a href="#cb7-1470" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (k <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="dv">1</span>)</span>
<span id="cb7-1471"><a href="#cb7-1471" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min</span>(<span class="dv">1</span> <span class="sc">/</span> k<span class="sc">^</span><span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb7-1472"><a href="#cb7-1472" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-1473"><a href="#cb7-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1474"><a href="#cb7-1474" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulating convergence</span></span>
<span id="cb7-1475"><a href="#cb7-1475" aria-hidden="true" tabindex="-1"></a>demonstrate_lln <span class="ot">&lt;-</span> <span class="cf">function</span>(dist_func, <span class="at">n_max =</span> <span class="dv">10000</span>, ...) {</span>
<span id="cb7-1476"><a href="#cb7-1476" aria-hidden="true" tabindex="-1"></a>  <span class="co"># dist_func should be a function like rnorm, rexp, etc.</span></span>
<span id="cb7-1477"><a href="#cb7-1477" aria-hidden="true" tabindex="-1"></a>  samples <span class="ot">&lt;-</span> <span class="fu">dist_func</span>(n_max, ...)</span>
<span id="cb7-1478"><a href="#cb7-1478" aria-hidden="true" tabindex="-1"></a>  cumulative_mean <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(samples) <span class="sc">/</span> <span class="fu">seq_len</span>(n_max)</span>
<span id="cb7-1479"><a href="#cb7-1479" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(cumulative_mean)</span>
<span id="cb7-1480"><a href="#cb7-1480" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-1481"><a href="#cb7-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1482"><a href="#cb7-1482" aria-hidden="true" tabindex="-1"></a>demonstrate_clt <span class="ot">&lt;-</span> <span class="cf">function</span>(dist_func, n, <span class="at">n_simulations =</span> <span class="dv">10000</span>, ...) {</span>
<span id="cb7-1483"><a href="#cb7-1483" aria-hidden="true" tabindex="-1"></a>  sample_means <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_simulations, {</span>
<span id="cb7-1484"><a href="#cb7-1484" aria-hidden="true" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">dist_func</span>(n, ...)</span>
<span id="cb7-1485"><a href="#cb7-1485" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(sample)</span>
<span id="cb7-1486"><a href="#cb7-1486" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb7-1487"><a href="#cb7-1487" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(sample_means)</span>
<span id="cb7-1488"><a href="#cb7-1488" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-1489"><a href="#cb7-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1490"><a href="#cb7-1490" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimator evaluation</span></span>
<span id="cb7-1491"><a href="#cb7-1491" aria-hidden="true" tabindex="-1"></a>evaluate_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(estimator_func, true_value, n, </span>
<span id="cb7-1492"><a href="#cb7-1492" aria-hidden="true" tabindex="-1"></a>                              sample_func, <span class="at">n_simulations =</span> <span class="dv">10000</span>) {</span>
<span id="cb7-1493"><a href="#cb7-1493" aria-hidden="true" tabindex="-1"></a>  estimates <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_simulations, {</span>
<span id="cb7-1494"><a href="#cb7-1494" aria-hidden="true" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">sample_func</span>(n)</span>
<span id="cb7-1495"><a href="#cb7-1495" aria-hidden="true" tabindex="-1"></a>    <span class="fu">estimator_func</span>(sample)</span>
<span id="cb7-1496"><a href="#cb7-1496" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb7-1497"><a href="#cb7-1497" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-1498"><a href="#cb7-1498" aria-hidden="true" tabindex="-1"></a>  bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(estimates) <span class="sc">-</span> true_value</span>
<span id="cb7-1499"><a href="#cb7-1499" aria-hidden="true" tabindex="-1"></a>  variance <span class="ot">&lt;-</span> <span class="fu">var</span>(estimates)</span>
<span id="cb7-1500"><a href="#cb7-1500" aria-hidden="true" tabindex="-1"></a>  mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((estimates <span class="sc">-</span> true_value)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb7-1501"><a href="#cb7-1501" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-1502"><a href="#cb7-1502" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">bias =</span> bias, <span class="at">variance =</span> variance, <span class="at">mse =</span> mse)</span>
<span id="cb7-1503"><a href="#cb7-1503" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-1504"><a href="#cb7-1504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1505"><a href="#cb7-1505" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb7-1506"><a href="#cb7-1506" aria-hidden="true" tabindex="-1"></a><span class="co"># LLN for exponential</span></span>
<span id="cb7-1507"><a href="#cb7-1507" aria-hidden="true" tabindex="-1"></a>lln_exp <span class="ot">&lt;-</span> <span class="fu">demonstrate_lln</span>(rexp, <span class="at">n_max =</span> <span class="dv">10000</span>, <span class="at">rate =</span> <span class="dv">1</span>)</span>
<span id="cb7-1508"><a href="#cb7-1508" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lln_exp, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">1.5</span>), <span class="at">main =</span> <span class="st">"LLN for Exponential(1)"</span>)</span>
<span id="cb7-1509"><a href="#cb7-1509" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="co"># True mean is 1</span></span>
<span id="cb7-1510"><a href="#cb7-1510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1511"><a href="#cb7-1511" aria-hidden="true" tabindex="-1"></a><span class="co"># CLT for uniform</span></span>
<span id="cb7-1512"><a href="#cb7-1512" aria-hidden="true" tabindex="-1"></a>clt_unif <span class="ot">&lt;-</span> <span class="fu">demonstrate_clt</span>(runif, <span class="at">n =</span> <span class="dv">30</span>, <span class="at">n_simulations =</span> <span class="dv">10000</span>)</span>
<span id="cb7-1513"><a href="#cb7-1513" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(clt_unif, <span class="at">breaks =</span> <span class="dv">50</span>, <span class="at">probability =</span> <span class="cn">TRUE</span>, <span class="at">main =</span> <span class="st">"CLT for Uniform(0,1)"</span>)</span>
<span id="cb7-1514"><a href="#cb7-1514" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance of U(0,1) is 1/12. SD of sample mean = sqrt(Var(X)/n)</span></span>
<span id="cb7-1515"><a href="#cb7-1515" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>((<span class="dv">1</span><span class="sc">/</span><span class="dv">12</span>)<span class="sc">/</span><span class="dv">30</span>)), <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb7-1516"><a href="#cb7-1516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1517"><a href="#cb7-1517" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare variance estimators</span></span>
<span id="cb7-1518"><a href="#cb7-1518" aria-hidden="true" tabindex="-1"></a>mle_var <span class="ot">&lt;-</span> <span class="cf">function</span>(x) { <span class="fu">var</span>(x) <span class="sc">*</span> (<span class="fu">length</span>(x) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">length</span>(x) }</span>
<span id="cb7-1519"><a href="#cb7-1519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1520"><a href="#cb7-1520" aria-hidden="true" tabindex="-1"></a>compare_var_estimators <span class="ot">&lt;-</span> <span class="cf">function</span>(n) {</span>
<span id="cb7-1521"><a href="#cb7-1521" aria-hidden="true" tabindex="-1"></a>  sample_func <span class="ot">&lt;-</span> <span class="cf">function</span>(k) <span class="fu">rnorm</span>(k, <span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># True variance = 1</span></span>
<span id="cb7-1522"><a href="#cb7-1522" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-1523"><a href="#cb7-1523" aria-hidden="true" tabindex="-1"></a>  unbiased_res <span class="ot">&lt;-</span> <span class="fu">evaluate_estimator</span>(</span>
<span id="cb7-1524"><a href="#cb7-1524" aria-hidden="true" tabindex="-1"></a>    var,  <span class="co"># R's var() is the unbiased (n-1) version</span></span>
<span id="cb7-1525"><a href="#cb7-1525" aria-hidden="true" tabindex="-1"></a>    <span class="at">true_value =</span> <span class="dv">1</span>, <span class="at">n =</span> n, <span class="at">sample_func =</span> sample_func</span>
<span id="cb7-1526"><a href="#cb7-1526" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-1527"><a href="#cb7-1527" aria-hidden="true" tabindex="-1"></a>  mle_res <span class="ot">&lt;-</span> <span class="fu">evaluate_estimator</span>(</span>
<span id="cb7-1528"><a href="#cb7-1528" aria-hidden="true" tabindex="-1"></a>    mle_var,</span>
<span id="cb7-1529"><a href="#cb7-1529" aria-hidden="true" tabindex="-1"></a>    <span class="at">true_value =</span> <span class="dv">1</span>, <span class="at">n =</span> n, <span class="at">sample_func =</span> sample_func</span>
<span id="cb7-1530"><a href="#cb7-1530" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-1531"><a href="#cb7-1531" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">unbiased =</span> unbiased_res, <span class="at">mle =</span> mle_res)</span>
<span id="cb7-1532"><a href="#cb7-1532" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-1533"><a href="#cb7-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1534"><a href="#cb7-1534" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"--- Comparing Variance Estimators (n=10) ---"</span>)</span>
<span id="cb7-1535"><a href="#cb7-1535" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">compare_var_estimators</span>(<span class="dv">10</span>))</span>
<span id="cb7-1536"><a href="#cb7-1536" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-1537"><a href="#cb7-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1538"><a href="#cb7-1538" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1539"><a href="#cb7-1539" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1540"><a href="#cb7-1540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1541"><a href="#cb7-1541" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb7-1542"><a href="#cb7-1542" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-1543"><a href="#cb7-1543" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb7-1544"><a href="#cb7-1544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1545"><a href="#cb7-1545" aria-hidden="true" tabindex="-1"></a>Python and R code examples for this chapter can be found in the HTML version of these notes.</span>
<span id="cb7-1546"><a href="#cb7-1546" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1547"><a href="#cb7-1547" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1548"><a href="#cb7-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1549"><a href="#cb7-1549" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb7-1550"><a href="#cb7-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1551"><a href="#cb7-1551" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-1552"><a href="#cb7-1552" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb7-1553"><a href="#cb7-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1554"><a href="#cb7-1554" aria-hidden="true" tabindex="-1"></a>This table maps sections in these lecture notes to the corresponding sections in @wasserman2013all ("All of Statistics" or AoS).</span>
<span id="cb7-1555"><a href="#cb7-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1556"><a href="#cb7-1556" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding AoS Section(s) |</span>
<span id="cb7-1557"><a href="#cb7-1557" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb7-1558"><a href="#cb7-1558" aria-hidden="true" tabindex="-1"></a>| **Introduction and Motivation** | Expanded material from the slides, contextualizing convergence for machine learning. |</span>
<span id="cb7-1559"><a href="#cb7-1559" aria-hidden="true" tabindex="-1"></a>| **Inequalities: Bounding the Unknown** | |</span>
<span id="cb7-1560"><a href="#cb7-1560" aria-hidden="true" tabindex="-1"></a>| ↳ Markov's Inequality | AoS §4.1 (Theorem 4.1) |</span>
<span id="cb7-1561"><a href="#cb7-1561" aria-hidden="true" tabindex="-1"></a>| ↳ Chebyshev's Inequality | AoS §4.1 (Theorem 4.2) |</span>
<span id="cb7-1562"><a href="#cb7-1562" aria-hidden="true" tabindex="-1"></a>| ↳ Hoeffding's Inequality | AoS §4.1 (Theorems 4.4 &amp; 4.5) |</span>
<span id="cb7-1563"><a href="#cb7-1563" aria-hidden="true" tabindex="-1"></a>| **Convergence of Random Variables** | |</span>
<span id="cb7-1564"><a href="#cb7-1564" aria-hidden="true" tabindex="-1"></a>| ↳ The Need for Probabilistic Convergence | AoS §5.1 |</span>
<span id="cb7-1565"><a href="#cb7-1565" aria-hidden="true" tabindex="-1"></a>| ↳ Convergence in Probability | AoS §5.2 (Definition 5.1) |</span>
<span id="cb7-1566"><a href="#cb7-1566" aria-hidden="true" tabindex="-1"></a>| ↳ Convergence in Distribution | AoS §5.2 (Definition 5.1) |</span>
<span id="cb7-1567"><a href="#cb7-1567" aria-hidden="true" tabindex="-1"></a>| ↳ Comparing Modes of Convergence | AoS §5.2 (Theorem 5.4) |</span>
<span id="cb7-1568"><a href="#cb7-1568" aria-hidden="true" tabindex="-1"></a>| ↳ Properties and Transformations | AoS §5.2 (Theorem 5.5, including Slutsky's Theorem) |</span>
<span id="cb7-1569"><a href="#cb7-1569" aria-hidden="true" tabindex="-1"></a>| **The Two Fundamental Theorems of Statistics** | |</span>
<span id="cb7-1570"><a href="#cb7-1570" aria-hidden="true" tabindex="-1"></a>| ↳ The Law of Large Numbers (LLN) | AoS §5.3 (The Weak Law of Large Numbers, Theorem 5.6) |</span>
<span id="cb7-1571"><a href="#cb7-1571" aria-hidden="true" tabindex="-1"></a>| ↳ The Central Limit Theorem (CLT) | AoS §5.4 (Theorems 5.8 &amp; 5.10) |</span>
<span id="cb7-1572"><a href="#cb7-1572" aria-hidden="true" tabindex="-1"></a>| **The Language of Statistical Inference** | |</span>
<span id="cb7-1573"><a href="#cb7-1573" aria-hidden="true" tabindex="-1"></a>| ↳ From Probability to Inference | AoS §6.1 |</span>
<span id="cb7-1574"><a href="#cb7-1574" aria-hidden="true" tabindex="-1"></a>| ↳ Statistical Models | AoS §6.2 |</span>
<span id="cb7-1575"><a href="#cb7-1575" aria-hidden="true" tabindex="-1"></a>| ↳ Point Estimation | AoS §6.3.1 |</span>
<span id="cb7-1576"><a href="#cb7-1576" aria-hidden="true" tabindex="-1"></a>| ↳ How to Evaluate Estimators | AoS §6.3.1 (covers bias, consistency, MSE) |</span>
<span id="cb7-1577"><a href="#cb7-1577" aria-hidden="true" tabindex="-1"></a>| ↳ The Bias-Variance Tradeoff | AoS §6.3.1 (MSE decomposition, Theorem 6.9) |</span>
<span id="cb7-1578"><a href="#cb7-1578" aria-hidden="true" tabindex="-1"></a>| **Chapter Summary and Connections** | New summary material. |</span>
<span id="cb7-1579"><a href="#cb7-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1580"><a href="#cb7-1580" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-1581"><a href="#cb7-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1582"><a href="#cb7-1582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1583"><a href="#cb7-1583" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Reading</span></span>
<span id="cb7-1584"><a href="#cb7-1584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1585"><a href="#cb7-1585" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistical inference**: Casella &amp; Berger, "Statistical Inference" </span>
<span id="cb7-1586"><a href="#cb7-1586" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Machine learning perspective**: Shalev-Shwartz &amp; Ben-David, "Understanding Machine Learning: From Theory to Algorithms"</span>
<span id="cb7-1587"><a href="#cb7-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1588"><a href="#cb7-1588" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-1589"><a href="#cb7-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1590"><a href="#cb7-1590" aria-hidden="true" tabindex="-1"></a>*Remember: Convergence and inference concepts are the bedrock of statistics. The Law of Large Numbers tells us why sampling works. The Central Limit Theorem tells us how to quantify uncertainty. The bias-variance tradeoff tells us how to choose good estimators. Master these ideas -- they're the key to everything that follows!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>