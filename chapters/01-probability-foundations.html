<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 1&nbsp; Probability Foundations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/02-expectation.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/01-probability-foundations.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#why-do-we-need-statistics" id="toc-why-do-we-need-statistics" class="nav-link" data-scroll-target="#why-do-we-need-statistics"><span class="header-section-number">1.2</span> Why Do We Need Statistics?</a>
  <ul class="collapse">
  <li><a href="#statistics-and-machine-learning-in-data-science" id="toc-statistics-and-machine-learning-in-data-science" class="nav-link" data-scroll-target="#statistics-and-machine-learning-in-data-science"><span class="header-section-number">1.2.1</span> Statistics and Machine Learning in Data Science</a></li>
  <li><a href="#case-study-ibm-watson-health" id="toc-case-study-ibm-watson-health" class="nav-link" data-scroll-target="#case-study-ibm-watson-health"><span class="header-section-number">1.2.2</span> Case Study: IBM Watson Health</a></li>
  <li><a href="#two-cultures-of-statistical-modeling" id="toc-two-cultures-of-statistical-modeling" class="nav-link" data-scroll-target="#two-cultures-of-statistical-modeling"><span class="header-section-number">1.2.3</span> Two Cultures of Statistical Modeling</a></li>
  </ul></li>
  <li><a href="#foundations-of-probability" id="toc-foundations-of-probability" class="nav-link" data-scroll-target="#foundations-of-probability"><span class="header-section-number">1.3</span> Foundations of Probability</a>
  <ul class="collapse">
  <li><a href="#sample-spaces-and-events" id="toc-sample-spaces-and-events" class="nav-link" data-scroll-target="#sample-spaces-and-events"><span class="header-section-number">1.3.1</span> Sample Spaces and Events</a></li>
  <li><a href="#set-operations-for-events" id="toc-set-operations-for-events" class="nav-link" data-scroll-target="#set-operations-for-events"><span class="header-section-number">1.3.2</span> Set Operations for Events</a></li>
  <li><a href="#probability-axioms" id="toc-probability-axioms" class="nav-link" data-scroll-target="#probability-axioms"><span class="header-section-number">1.3.3</span> Probability Axioms</a></li>
  <li><a href="#interpretations-of-probability" id="toc-interpretations-of-probability" class="nav-link" data-scroll-target="#interpretations-of-probability"><span class="header-section-number">1.3.4</span> Interpretations of Probability</a></li>
  <li><a href="#finite-sample-spaces-and-counting" id="toc-finite-sample-spaces-and-counting" class="nav-link" data-scroll-target="#finite-sample-spaces-and-counting"><span class="header-section-number">1.3.5</span> Finite Sample Spaces and Counting</a></li>
  </ul></li>
  <li><a href="#independence-and-conditional-probability" id="toc-independence-and-conditional-probability" class="nav-link" data-scroll-target="#independence-and-conditional-probability"><span class="header-section-number">1.4</span> Independence and Conditional Probability</a>
  <ul class="collapse">
  <li><a href="#independent-events" id="toc-independent-events" class="nav-link" data-scroll-target="#independent-events"><span class="header-section-number">1.4.1</span> Independent Events</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability"><span class="header-section-number">1.4.2</span> Conditional Probability</a></li>
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem"><span class="header-section-number">1.4.3</span> Bayes’ Theorem</a></li>
  <li><a href="#classic-probability-examples" id="toc-classic-probability-examples" class="nav-link" data-scroll-target="#classic-probability-examples"><span class="header-section-number">1.4.4</span> Classic Probability Examples</a></li>
  </ul></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables"><span class="header-section-number">1.5</span> Random Variables</a>
  <ul class="collapse">
  <li><a href="#definition-and-intuition" id="toc-definition-and-intuition" class="nav-link" data-scroll-target="#definition-and-intuition"><span class="header-section-number">1.5.1</span> Definition and Intuition</a></li>
  <li><a href="#cumulative-distribution-functions" id="toc-cumulative-distribution-functions" class="nav-link" data-scroll-target="#cumulative-distribution-functions"><span class="header-section-number">1.5.2</span> Cumulative Distribution Functions</a></li>
  <li><a href="#discrete-random-variables" id="toc-discrete-random-variables" class="nav-link" data-scroll-target="#discrete-random-variables"><span class="header-section-number">1.5.3</span> Discrete Random Variables</a></li>
  <li><a href="#core-discrete-distributions" id="toc-core-discrete-distributions" class="nav-link" data-scroll-target="#core-discrete-distributions"><span class="header-section-number">1.5.4</span> Core Discrete Distributions</a></li>
  <li><a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables"><span class="header-section-number">1.5.5</span> Continuous Random Variables</a></li>
  <li><a href="#core-continuous-distributions" id="toc-core-continuous-distributions" class="nav-link" data-scroll-target="#core-continuous-distributions"><span class="header-section-number">1.5.6</span> Core Continuous Distributions</a></li>
  </ul></li>
  <li><a href="#multivariate-distributions" id="toc-multivariate-distributions" class="nav-link" data-scroll-target="#multivariate-distributions"><span class="header-section-number">1.6</span> Multivariate Distributions</a>
  <ul class="collapse">
  <li><a href="#joint-distributions" id="toc-joint-distributions" class="nav-link" data-scroll-target="#joint-distributions"><span class="header-section-number">1.6.1</span> Joint Distributions</a></li>
  <li><a href="#marginal-distributions" id="toc-marginal-distributions" class="nav-link" data-scroll-target="#marginal-distributions"><span class="header-section-number">1.6.2</span> Marginal Distributions</a></li>
  <li><a href="#independent-random-variables" id="toc-independent-random-variables" class="nav-link" data-scroll-target="#independent-random-variables"><span class="header-section-number">1.6.3</span> Independent Random Variables</a></li>
  <li><a href="#conditional-distributions" id="toc-conditional-distributions" class="nav-link" data-scroll-target="#conditional-distributions"><span class="header-section-number">1.6.4</span> Conditional Distributions</a></li>
  <li><a href="#interactive-exploration-marginal-and-conditional-distributions" id="toc-interactive-exploration-marginal-and-conditional-distributions" class="nav-link" data-scroll-target="#interactive-exploration-marginal-and-conditional-distributions"><span class="header-section-number">1.6.5</span> Interactive Exploration: Marginal and Conditional Distributions</a></li>
  <li><a href="#random-vectors-and-iid-random-variables" id="toc-random-vectors-and-iid-random-variables" class="nav-link" data-scroll-target="#random-vectors-and-iid-random-variables"><span class="header-section-number">1.6.6</span> Random Vectors and IID Random Variables</a></li>
  <li><a href="#important-multivariate-distributions" id="toc-important-multivariate-distributions" class="nav-link" data-scroll-target="#important-multivariate-distributions"><span class="header-section-number">1.6.7</span> Important Multivariate Distributions</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">1.7</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">1.7.1</span> Key Concepts Review</a></li>
  <li><a href="#why-these-concepts-matter" id="toc-why-these-concepts-matter" class="nav-link" data-scroll-target="#why-these-concepts-matter"><span class="header-section-number">1.7.2</span> Why These Concepts Matter</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">1.7.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">1.7.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">1.7.5</span> Self-Test Problems</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">1.7.6</span> Connections to Source Material</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">1.7.7</span> Further Reading</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">1.7.8</span> Python and R Reference</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li>Explain the role of probability in data science and statistical modeling.</li>
<li>Apply fundamental probability concepts, including sample spaces, events, and the axioms of probability.</li>
<li>Calculate probabilities using independence, conditional probability, and Bayes’ theorem.</li>
<li>Distinguish between discrete and continuous random variables and use their distribution functions (PMF, PDF, CDF).</li>
<li>Describe and apply key univariate and multivariate distributions (e.g., Binomial, Normal, Multinomial).</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter covers fundamental probability concepts essential for statistical inference. The material is adapted and expanded from Chapters 1 and 2 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span>, which interested readers are encouraged to consult directly for a more rigorous and comprehensive treatment.</p>
</div>
</div>
</section>
<section id="why-do-we-need-statistics" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="why-do-we-need-statistics"><span class="header-section-number">1.2</span> Why Do We Need Statistics?</h2>
<section id="statistics-and-machine-learning-in-data-science" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="statistics-and-machine-learning-in-data-science"><span class="header-section-number">1.2.1</span> Statistics and Machine Learning in Data Science</h3>
<p>Machine learning (ML) has revolutionized our ability to make predictions. Given enough training data, modern ML models can achieve remarkable accuracy on unseen data that resembles what they’ve been trained on. But there’s a crucial limitation: these models excel when the new data comes from the same distribution as the training data.</p>
<p>How do we move beyond this constraint to make reliable predictions in the real world, where conditions change and data can be messy, incomplete, or collected differently than our training set?</p>
<p>This is where statistics becomes essential. Statistics provides the tools to:</p>
<ul>
<li><strong>Understand principles of data collection</strong>: How was the data gathered? What biases might exist?</li>
<li><strong>Plan data collection strategically</strong>: Design experiments and surveys that yield meaningful insights</li>
<li><strong>Deal with missing data</strong>: Real-world data is rarely complete - we need principled ways to handle gaps</li>
<li><strong>Understand causality in modeling</strong>: Correlation isn’t causation, and confounding variables can mislead us</li>
</ul>
<p>Without these statistical foundations, even the most sophisticated ML models can fail spectacularly when deployed in practice.</p>
</section>
<section id="case-study-ibm-watson-health" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="case-study-ibm-watson-health"><span class="header-section-number">1.2.2</span> Case Study: IBM Watson Health</h3>
<p>The story of IBM Watson Health illustrates why statistical thinking is crucial for real-world AI applications.</p>
<p>In 2011, IBM Watson made headlines by defeating human champions on the quiz show <em>Jeopardy!</em> This victory showcased the power of natural language processing and knowledge retrieval. IBM saw an opportunity: if Watson could master general knowledge, why not train it to be a doctor?</p>
<p>Watson Health launched in 2015 with an ambitious goal: use data from top US hospitals to train an AI system that could diagnose and treat patients anywhere in the world. The vision was compelling - bring world-class medical expertise to underserved areas through AI.</p>
<p>Over the years, IBM:</p>
<ul>
<li>Spent over $4 billion on acquisitions</li>
<li>Employed 7,000 people developing the system<br>
</li>
<li>Partnered with prestigious medical institutions</li>
</ul>
<p>Yet by 2022, IBM sold Watson Health’s data and assets for just $1 billion - a massive loss. What went wrong?</p>
<p>The fundamental issue was <strong>data representativeness</strong>. Watson Health was trained on data from elite US hospitals treating specific patient populations. But this data didn’t represent:</p>
<ul>
<li>Different healthcare systems and practices globally</li>
<li>Diverse patient populations with varying genetics, lifestyles, and environmental factors</li>
<li>Resource constraints in different settings</li>
<li>Variations in how medical data is recorded and coded</li>
</ul>
<p>This failure wasn’t due to inadequate machine learning algorithms - it was a failure to apply statistical thinking about data collection, representation, and generalization. No amount of computational power can overcome fundamentally biased or unrepresentative data.</p>
<p>Read more <a href="https://slate.com/technology/2022/01/ibm-watson-health-failure-artificial-intelligence.html">in this Slate article</a>.</p>
</section>
<section id="two-cultures-of-statistical-modeling" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="two-cultures-of-statistical-modeling"><span class="header-section-number">1.2.3</span> Two Cultures of Statistical Modeling</h3>
<p>In his influential essay <span class="citation" data-cites="breiman2001statistical">Breiman (<a href="../references.html#ref-breiman2001statistical" role="doc-biblioref">2001</a>)</span>, statistician Leo Breiman identified two distinct approaches to statistical modeling, each with different goals and philosophies. These are often cast as the two approaches of <strong>prediction</strong> vs.&nbsp;<strong>explanation</strong>.</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">The Algorithmic Modeling Culture</th>
<th style="text-align: left;">The Data Modeling Culture</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Goal</strong></td>
<td style="text-align: left;">Accurate prediction</td>
<td style="text-align: left;">Understanding mechanisms</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Approach</strong></td>
<td style="text-align: left;">Treat the mapping from inputs to outputs as a black box</td>
<td style="text-align: left;">Specify interpretable models that represent how nature works</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Validation</strong></td>
<td style="text-align: left;">Predictive accuracy on held-out data</td>
<td style="text-align: left;">Statistical tests, confidence intervals, model diagnostics</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Philosophy</strong></td>
<td style="text-align: left;">“It doesn’t matter how it works, as long as it works”</td>
<td style="text-align: left;">“We need to understand which factors matter and why”</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Examples</strong></td>
<td style="text-align: left;">Deep learning, random forests, boosting</td>
<td style="text-align: left;">Linear regression, causal inference, experimental design</td>
</tr>
</tbody>
</table>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255584-920-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-920-1" role="tab" aria-controls="tabset-1757255584-920-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-920-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-920-2" role="tab" aria-controls="tabset-1757255584-920-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-920-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-920-3" role="tab" aria-controls="tabset-1757255584-920-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255584-920-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255584-920-1-tab"><p>Think of these two cultures like different approaches to cooking:</p><p>The <strong>algorithmic approach</strong> is like following a
top-rated recipe - you don’t know why you fold (not stir) the batter or
why ingredients must be room temperature, but following the steps
precisely often produces better results than many trained cooks achieve.
You can pick 5-star recipes and succeed without any cooking
knowledge.</p><p>The <strong>data modeling approach</strong> is like understanding
food science - you know about Maillard reactions, gluten development,
and emulsification. But translating this into a great dish is slow and
complex. You might spend hours calculating optimal ratios only to
produce something inferior to what a simple recipe would have given
you.</p><p>This creates a fundamental tension: The recipe follower often
produces better food faster. The food scientist understands why things
work and with time might produce a breakthrough - but may struggle to
match the empirical success of well-tested recipes. In machine learning,
this same tension exists - a neural network might predict customer
behavior better than any theory-based model, even if we don’t understand
why. Sometimes, letting algorithms find patterns empirically works
better than imposing our theoretical understanding. However,
understanding often gives us an edge to build better algorithms and
generalize to novel scenarios.</p></div><div id="tabset-1757255584-920-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-920-2-tab"><p>Formally, both cultures can be seen as addressing the problem of
characterizing a mapping:
<span class="math display">\[X \rightarrow Y\]</span></p><p>where <span class="math inline">\(X\)</span> represents
<strong>input</strong> features and
<span class="math inline">\(Y\)</span> represents the
<strong>response</strong>.</p><p><strong>Algorithmic approach (example)</strong>: Find a function
<span class="math inline">\(\hat{f}\)</span> that minimizes prediction
error. A common approach is to find the function that minimizes the
<strong>mean squared error (MSE)</strong>,
<span class="math display">\[\text{MSE} = \frac{1}{N} \sum_{n=1}^N (Y_n - \hat{f}(X_n))^2\]</span></p><p>that is make the squared difference between the actual outcome
<span class="math inline">\(Y_n\)</span> and the prediction
<span class="math inline">\(\hat{f}(X_n)\)</span> as small as possible
over the available training data. In practice, we often report the
<strong>root mean squared error (RMSE)</strong> =
<span class="math inline">\(\sqrt{\text{MSE}}\)</span>, which has the
same units as <span class="math inline">\(Y\)</span>. We don’t care
about what the function <span class="math inline">\(\hat{f}\)</span>
looks like, as long as it minimizes this error.</p><p><strong>Data modeling approach (example)</strong>: Build a
mechanistic model
<span class="math inline">\(Y = f(X; \theta) + \epsilon\)</span>
where:</p><ul>
<li><span class="math inline">\(f\)</span> has a specific, interpretable
form</li>
<li><span class="math inline">\(\theta\)</span> are parameters with
scientific, interpretable meaning</li>
<li><span class="math inline">\(\epsilon\)</span> represents random
error</li>
</ul><p>While fitting the model to data often still involves optimizing some
objective, the goal here is to study the best-fitting parameters
<span class="math inline">\(\theta\)</span>, or find the best model
<span class="math inline">\(f\)</span> among a set of competing
hypotheses.</p></div><div id="tabset-1757255584-920-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-920-3-tab"><p>We compare here the two approaches represented by a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a>
(RF) model and a <a href="https://en.wikipedia.org/wiki/Linear_regression">linear
regression</a> model. The former represents a traditional machine
learning approach, while the latter is a staple of statistical
modelling.</p><p>A trained random forest model is harder to interpret, hence falls in
the “algorithmic” camp for the purpose of this example. Conversely, a
fitted linear regression model yields interpretable <em>weights</em>
which directly tell us how the features linearly affect the response, so
it represents the data modeling camp.</p><div id="ffb91b91" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" data-code-fold="false"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparing algorithmic vs data modeling approaches</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load synthetic housing price data with complex patterns</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># File available here: </span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># https://raw.githubusercontent.com/lacerbi/stats-for-ds-website/refs/heads/main/data/housing_prices.csv</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../data/housing_prices.csv'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare features and target</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'size_sqft'</span>, <span class="st">'bedrooms'</span>, <span class="st">'age_years'</span>, <span class="st">'location_score'</span>, </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">'garage_spaces'</span>, <span class="st">'has_pool'</span>, <span class="st">'crime_rate'</span>, <span class="st">'school_rating'</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[features]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'price'</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset: </span><span class="sc">{</span>X<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">:,}</span><span class="ss"> houses with </span><span class="sc">{</span>X<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training on </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">:,}</span><span class="ss"> houses, testing on </span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Average house price: €</span><span class="sc">{</span>y<span class="sc">.</span>mean()<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Price standard deviation: €</span><span class="sc">{</span>y<span class="sc">.</span>std()<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># ALGORITHMIC APPROACH: Random Forest</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>rf_predictions <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>rf_rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, rf_predictions))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== ALGORITHMIC APPROACH (Random Forest) ==="</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Root Mean Squared Error (RMSE): €</span><span class="sc">{</span>rf_rmse<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Importances:"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature, importance <span class="kw">in</span> <span class="bu">zip</span>(features, rf_model.feature_importances_):</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>importance<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset: 5,000 houses with 8 features
Training on 4,000 houses, testing on 1,000

Average house price: €439,750
Price standard deviation: €108,720

=== ALGORITHMIC APPROACH (Random Forest) ===
Root Mean Squared Error (RMSE): €32,608

Feature Importances:
  size_sqft: 0.125
  bedrooms: 0.022
  age_years: 0.022
  location_score: 0.047
  garage_spaces: 0.013
  has_pool: 0.007
  crime_rate: 0.018
  school_rating: 0.746</code></pre>
</div>
</div><div id="de02c957" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" data-code-fold="false"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DATA MODELING APPROACH: Linear Regression</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Add constant term for intercept</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X_train_lm <span class="op">=</span> sm.add_constant(X_train)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>X_test_lm <span class="op">=</span> sm.add_constant(X_test)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear model</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>lm_model <span class="op">=</span> sm.OLS(y_train, X_train_lm)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>lm_results <span class="op">=</span> lm_model.fit()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>lm_predictions <span class="op">=</span> lm_results.predict(X_test_lm)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>lm_rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, lm_predictions))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== DATA MODELING APPROACH (Linear Regression) ==="</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Root Mean Squared Error (RMSE): €</span><span class="sc">{</span>lm_rmse<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Show interpretable coefficients</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Linear Model Coefficients:"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>coef_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: [<span class="st">'Intercept'</span>] <span class="op">+</span> features,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Coefficient'</span>: lm_results.params,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Std Error'</span>: lm_results.bse,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'P-value'</span>: lm_results.pvalues</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>coef_df[<span class="st">'Significant'</span>] <span class="op">=</span> coef_df[<span class="st">'P-value'</span>] <span class="op">&lt;</span> <span class="fl">0.05</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef_df.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== INTERPRETATION ==="</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear model suggests:"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, feature <span class="kw">in</span> <span class="bu">enumerate</span>(features):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    coef <span class="op">=</span> lm_results.params[i<span class="op">+</span><span class="dv">1</span>]  <span class="co"># +1 to skip intercept</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">abs</span>(coef) <span class="op">&gt;</span> <span class="dv">100</span>:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"- Each unit increase in </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: €</span><span class="sc">{</span>coef<span class="sc">:,.0f}</span><span class="ss"> change in price"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">BUT: The model performs (slightly) worse than Random Forest."</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RF RMSE: €</span><span class="sc">{</span>rf_rmse<span class="sc">:,.0f}</span><span class="ss"> vs Linear RMSE: €</span><span class="sc">{</span>lm_rmse<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"That's €</span><span class="sc">{</span>lm_rmse <span class="op">-</span> rf_rmse<span class="sc">:,.0f}</span><span class="ss"> worse prediction error."</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Should we care more about prediction or understanding?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== DATA MODELING APPROACH (Linear Regression) ===
Root Mean Squared Error (RMSE): €34,061

Linear Model Coefficients:
       Feature    Coefficient   Std Error       P-value  Significant
     Intercept -241058.748914 4060.095405  0.000000e+00         True
     size_sqft      72.843101    1.113174  0.000000e+00         True
      bedrooms   17505.976934  561.915079 6.369481e-191         True
     age_years      66.884992   37.769905  7.666128e-02        False
location_score    6326.172511  191.797597 3.372091e-211         True
 garage_spaces   15599.005835  602.273749 7.547388e-137         True
      has_pool   29992.244032 1362.847287 2.110344e-101         True
    crime_rate   -1268.321711  110.805186  7.133961e-30         True
 school_rating   66814.033458  394.636343  0.000000e+00         True

=== INTERPRETATION ===
Linear model suggests:
- Each unit increase in bedrooms: €17,506 change in price
- Each unit increase in location_score: €6,326 change in price
- Each unit increase in garage_spaces: €15,599 change in price
- Each unit increase in has_pool: €29,992 change in price
- Each unit increase in crime_rate: €-1,268 change in price
- Each unit increase in school_rating: €66,814 change in price

BUT: The model performs (slightly) worse than Random Forest.
RF RMSE: €32,608 vs Linear RMSE: €34,061
That's €1,453 worse prediction error.
Should we care more about prediction or understanding?</code></pre>
</div>
</div></div></div></div>
<p>Both approaches have their place in modern data science. The <strong>algorithmic culture</strong> has driven breakthroughs in areas like computer vision and natural language processing, where prediction accuracy is paramount. For example, large language models (LLMs) are massively large deep neural networks (pre)trained with the extremely simple objective of just “predicting the next word”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> – without any attempt at <em>understanding</em> the underlying process.</p>
<p>The <strong>data modeling</strong> culture remains essential for scientific understanding, policy decisions, and any application where we need to know not just <em>what</em> will happen, but <em>why</em>. For LLMs, and in ML more broadly, this aspect is studied by the field of <em>interpretability</em> or “explainable ML” – trying to understand how modern ML models “think” and reach their conclusions.</p>
</section>
</section>
<section id="foundations-of-probability" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="foundations-of-probability"><span class="header-section-number">1.3</span> Foundations of Probability</h2>
<p>Probability provides the mathematical language for quantifying uncertainty. Before we can make statistical inferences or build predictive models, we need a solid foundation in probability theory.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This course is taught internationally, but for Finnish-speaking students, here’s a reference table of key probability terms you may have encountered in your earlier studies:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample space</td>
<td>Perusjoukko, otosavaruus</td>
<td>The set of all possible outcomes</td>
</tr>
<tr class="even">
<td>Event</td>
<td>Tapahtuma</td>
<td>A subset of the sample space</td>
</tr>
<tr class="odd">
<td>Probability distribution</td>
<td>Todennäköisyysjakauma</td>
<td>Assignment of probabilities to events</td>
</tr>
<tr class="even">
<td>Probability measure</td>
<td>Todennäköisyysmitta</td>
<td>Mathematical function P satisfying axioms</td>
</tr>
<tr class="odd">
<td>Independent</td>
<td>Riippumattomat</td>
<td>Events that don’t influence each other</td>
</tr>
<tr class="even">
<td>Conditional probability</td>
<td>Ehdollinen todennäköisyys</td>
<td>Probability given some information</td>
</tr>
<tr class="odd">
<td>Bayes’ Theorem</td>
<td>Bayesin kaava</td>
<td>Formula for updating probabilities</td>
</tr>
<tr class="even">
<td>Random variable</td>
<td>Satunnaismuuttuja</td>
<td>Function mapping outcomes to numbers</td>
</tr>
<tr class="odd">
<td>Cumulative distribution function (CDF)</td>
<td>Kertymäfunktio</td>
<td><span class="math inline">P(X \le x)</span></td>
</tr>
<tr class="even">
<td>Discrete</td>
<td>Diskreetti</td>
<td>Taking countable values</td>
</tr>
<tr class="odd">
<td>Probability mass function (PMF)</td>
<td>Todennäköisyysmassafunktio</td>
<td><span class="math inline">P(X = x)</span> for discrete <span class="math inline">X</span></td>
</tr>
<tr class="even">
<td>Probability density function (PDF)</td>
<td>Tiheysfunktio</td>
<td>Density for continuous variables</td>
</tr>
<tr class="odd">
<td>Quantile function</td>
<td>Kvantiilifunktio</td>
<td>Inverse of CDF</td>
</tr>
<tr class="even">
<td>First quartile</td>
<td>Ensimmäinen kvartiili</td>
<td>25th percentile</td>
</tr>
<tr class="odd">
<td>Median</td>
<td>Mediaani</td>
<td>50th percentile</td>
</tr>
<tr class="even">
<td>Joint density function</td>
<td>Yhteistiheysfunktio</td>
<td>PDF for multiple variables</td>
</tr>
<tr class="odd">
<td>Marginal density</td>
<td>Reunatiheysfunktio</td>
<td>PDF of one variable from joint</td>
</tr>
<tr class="even">
<td>Conditional density</td>
<td>Ehdollinen tiheysfunktio</td>
<td>PDF given another variable’s value</td>
</tr>
<tr class="odd">
<td>Random vector</td>
<td>Satunnaisvektori</td>
<td>Vector of random variables</td>
</tr>
<tr class="even">
<td>Independent and identically distributed (IID)</td>
<td>Riippumattomat ja samoin jakautuneet</td>
<td>Common assumption for data</td>
</tr>
<tr class="odd">
<td>Random sample</td>
<td>Satunnaisotos</td>
<td>IID observations from population</td>
</tr>
<tr class="even">
<td>Frequentist probability</td>
<td>Frekventistinen todennäköisyys</td>
<td>Long-run frequency interpretation</td>
</tr>
<tr class="odd">
<td>Subjective probability</td>
<td>Subjektiivinen todennäköisyys</td>
<td>Degree of belief interpretation</td>
</tr>
</tbody>
</table>
<p><strong>Note</strong>: Some terms have multiple Finnish translations. We report here the most common ones.</p>
</div>
</div>
</div>
<section id="sample-spaces-and-events" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="sample-spaces-and-events"><span class="header-section-number">1.3.1</span> Sample Spaces and Events</h3>
<div class="definition">
<p>The <strong>sample space</strong> <span class="math inline">\Omega</span> is the set of all possible outcomes of an experiment. Individual elements <span class="math inline">\omega \in \Omega</span> are called <strong>sample outcomes</strong>, <strong>realizations</strong>, or just <strong>elements</strong>. Subsets of <span class="math inline">\Omega</span> are called <strong>events</strong>.</p>
</div>
<p><strong>Notation:</strong> <span class="math inline">\omega</span> and <span class="math inline">\Omega</span> are the lowercase (respectively, uppercase) version of the Greek letter <a href="https://en.wikipedia.org/wiki/Omega">omega</a>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Coin flips
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we flip a coin twice, where each outcome can be head (<span class="math inline">H</span>) or tails (<span class="math inline">T</span>), the sample space is: <span class="math display">\Omega = \{HH, HT, TH, TT\}</span></p>
<p>The event “first flip is heads” is <span class="math inline">A = \{HH, HT\}</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Temperature measurement
</div>
</div>
<div class="callout-body-container callout-body">
<p>When measuring temperature, the sample space might be the full set of real numbers: <span class="math display">\Omega = \mathbb{R} = (-\infty, \infty)</span></p>
<p>The event “temperature between 20°C and 25°C” is the interval <span class="math inline">A = [20, 25]</span>.</p>
<p>Note that we often take <span class="math inline">\Omega</span> to be larger than strictly necessary - in this case for example we are including physically impossible values like -1000°C. This is still <em>mathematically</em> valid. As we will see later, we can assign zero probability to impossible events.</p>
<p><strong>Notation:</strong> <span class="math inline">[a, b]</span> denotes the <em>interval</em> between <span class="math inline">a</span> and <span class="math inline">b</span> (included), whereas <span class="math inline">(a, b)</span> is the interval between <span class="math inline">a</span> and <span class="math inline">b</span> (excluded).</p>
</div>
</div>
<p>Sample spaces can be:</p>
<ul>
<li><strong>Finite</strong>: <span class="math inline">\Omega = \{1, 2, 3, 4, 5, 6\}</span> for a die roll</li>
<li><strong>Countably infinite</strong>: <span class="math inline">\Omega = \{1, 2, 3, ...\}</span> for “number of flips until first heads”</li>
<li><strong>Uncountably infinite</strong>: <span class="math inline">\Omega = [0, 1]</span> for “random number between 0 and 1”</li>
</ul>
</section>
<section id="set-operations-for-events" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="set-operations-for-events"><span class="header-section-number">1.3.2</span> Set Operations for Events</h3>
<p>Since events are sets, we can combine them using standard set operations:</p>
<table class="table">
<thead>
<tr class="header">
<th>Operation</th>
<th>Notation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Complement</td>
<td><span class="math inline">A^c</span></td>
<td>“not A” - all outcomes not in A</td>
</tr>
<tr class="even">
<td>Union</td>
<td><span class="math inline">A \cup B</span></td>
<td>“A or B” - outcomes in either A or B (or both)</td>
</tr>
<tr class="odd">
<td>Intersection</td>
<td><span class="math inline">A \cap B</span></td>
<td>“A and B” - outcomes in both A and B</td>
</tr>
<tr class="even">
<td>Difference</td>
<td><span class="math inline">A \setminus B</span></td>
<td>Outcomes in A but not in B</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Disjoint events</strong>: Events <span class="math inline">A</span> and <span class="math inline">B</span> are disjoint (or mutually exclusive) if <span class="math inline">A \cap B = \emptyset</span>. This means they cannot occur simultaneously. For example, in the case of a standard six-sided die roll, let <span class="math inline">A</span> = “rolling an even number” = <span class="math inline">\{2, 4, 6\}</span> and <span class="math inline">B</span> = “rolling a 1” = <span class="math inline">\{1\}</span>. These events are disjoint because you can’t roll both an even number AND a 1 simultaneously.</p>
</div>
</div>
</section>
<section id="probability-axioms" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="probability-axioms"><span class="header-section-number">1.3.3</span> Probability Axioms</h3>
<p>Now that we have defined the space of possible events, we can define the probability of an event. A probability measure must satisfy three fundamental axioms:</p>
<div class="definition">
<p>A function <span class="math inline">\mathbb{P}</span> that assigns a real number <span class="math inline">\mathbb{P}(A)</span> to each event <span class="math inline">A</span> is a <strong>probability measure</strong> if:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <span class="math inline">\mathbb{P}(A) \geq 0</span> for every event <span class="math inline">A</span></li>
<li><strong>Normalization</strong>: <span class="math inline">\mathbb{P}(\Omega) = 1</span></li>
<li><strong>Countable additivity</strong>: If <span class="math inline">A_1, A_2, ...</span> are disjoint events, then: <span class="math display">\mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mathbb{P}(A_i)</span></li>
</ol>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why These Axioms
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>These axioms ensure probability respects intuitive laws:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: The probability of rain tomorrow cannot be negative.</li>
<li><strong>Normalization</strong>: When rolling a six-sided die, the probability of getting one of the faces <span class="math inline">\{1, 2, 3, 4, 5, 6\}</span> is 1: 1 represent the total probability.</li>
<li><strong>Countable additivity</strong>: The probability of rolling a 1 <em>or</em> a 2 on a die is the sum of their individual probabilities, as these events cannot happen together.</li>
</ol>
<p>It turns out that under some assumptions, it can be shown that these axioms are exactly what you would pick if one wants to quantify the concept of “possibility of an event” with a single number – a result known as <a href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox’s theorem</a>.</p>
</div>
</div>
</div>
<p>From these axioms, we can derive many useful properties:</p>
<ul>
<li><span class="math inline">\mathbb{P}(\emptyset) = 0</span> (the impossible event has probability 0)</li>
<li><span class="math inline">\mathbb{P}(A^c) = 1 - \mathbb{P}(A)</span> (complement rule)</li>
<li>If <span class="math inline">A \subset B</span>, then <span class="math inline">\mathbb{P}(A) \leq \mathbb{P}(B)</span> (monotonicity)</li>
<li><span class="math inline">0 \leq \mathbb{P}(A) \leq 1</span> for any event <span class="math inline">A</span></li>
</ul>
<div class="theorem" name="Inclusion-Exclusion">
<p>For any events <span class="math inline">A</span> and <span class="math inline">B</span>: <span class="math display">\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)</span></p>
</div>
<p><strong>Why?</strong> This formula accounts for the “double counting” when we add <span class="math inline">\mathbb{P}(A)</span> and <span class="math inline">\mathbb{P}(B)</span> – the intersection <span class="math inline">A \cap B</span> gets counted twice, so we subtract it once.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes you will see the notation <span class="math inline">\mathbb{P}(A B)</span> to denote <span class="math inline">\mathbb{P}(A \cap B)</span>.</p>
</div>
</div>
</section>
<section id="interpretations-of-probability" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="interpretations-of-probability"><span class="header-section-number">1.3.4</span> Interpretations of Probability</h3>
<p>Probability can be understood from different philosophical perspectives, each leading to the same mathematical framework.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255584-557-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-557-1" role="tab" aria-controls="tabset-1757255584-557-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-557-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-557-2" role="tab" aria-controls="tabset-1757255584-557-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-557-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-557-3" role="tab" aria-controls="tabset-1757255584-557-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255584-557-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255584-557-1-tab"><p>There are two main ways to think about what probability means:</p><p><strong>Frequency interpretation</strong>: Probability is the
long-run proportion of times an event occurs in repeated experiments. If
we flip a fair coin millions of times, we expect heads about 50% of the
time.</p><p><strong>Subjective interpretation</strong>: Probability represents a
degree of belief. When a weather forecaster says “30% chance of rain,”
they’re expressing their confidence based on available information.</p><p>Both interpretations are useful, and both lead to the same
mathematical rules.</p></div><div id="tabset-1757255584-557-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-557-2-tab"><p><strong>Frequentist probability</strong>:
<span class="math display">\[\mathbb{P}(A) = \lim_{n \to \infty} \frac{\text{number of times A occurs in n trials}}{n}\]</span></p><p>This requires the experiment to be repeatable under identical
conditions.</p><p><strong>Subjective/Bayesian probability</strong>:
<span class="math inline">\(\mathbb{P}(A)\)</span> quantifies an agent’s
degree of belief that <span class="math inline">\(A\)</span> is true,
constrained by:</p><ul>
<li>Coherence: beliefs must satisfy the probability axioms</li>
<li>Updating: beliefs change rationally when new information arrives
(via Bayes’ theorem)</li>
</ul></div><div id="tabset-1757255584-557-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-557-3-tab"><p>Let’s simulate the <strong>frequentist interpretation</strong> by
flipping a fair coin many times and tracking how the proportion of heads
converges to the true probability of 0.5. This directly demonstrates the
mathematical definition: probability as the long-run proportion.</p><div id="5c5264fd" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate many coin flips to see frequentist convergence</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n_flips <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> np.random.choice([<span class="st">'H'</span>, <span class="st">'T'</span>], size<span class="op">=</span>n_flips)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate running proportion of heads</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>heads_count <span class="op">=</span> np.cumsum(flips <span class="op">==</span> <span class="st">'H'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>proportions <span class="op">=</span> heads_count <span class="op">/</span> np.arange(<span class="dv">1</span>, n_flips <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot convergence to true probability</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.plot(proportions, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True probability = 0.5'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of flips'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Proportion of heads'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Frequentist Probability: Long-Run Proportion Converges to True Value'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.3</span>, <span class="fl">0.7</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"After </span><span class="sc">{</span>n_flips<span class="sc">:,}</span><span class="ss"> flips:"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Proportion of heads: </span><span class="sc">{</span>proportions[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Deviation from 0.5: </span><span class="sc">{</span><span class="bu">abs</span>(proportions[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> <span class="fl">0.5</span>)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01-probability-foundations_files/figure-html/cell-4-output-1.png" width="612" height="376"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After 10,000 flips:
Proportion of heads: 0.5013
Deviation from 0.5: 0.0013</code></pre>
</div>
</div><p>The <strong>subjective/Bayesian interpretation</strong> involves
updating beliefs based on evidence. We’ll explore this computational
approach in detail when we cover Bayes’ theorem.</p></div></div></div>
</section>
<section id="finite-sample-spaces-and-counting" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="finite-sample-spaces-and-counting"><span class="header-section-number">1.3.5</span> Finite Sample Spaces and Counting</h3>
<p>When <span class="math inline">\Omega</span> is finite and all outcomes are equally likely, probability calculations reduce to counting:</p>
<p><span class="math display">\mathbb{P}(A) = \frac{|A|}{|\Omega|} = \frac{\text{number of outcomes in A}}{\text{total number of outcomes}}</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Rolling two dice
</div>
</div>
<div class="callout-body-container callout-body">
<p>What’s the probability the sum of rolling two six-sided dice equals 7?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\Omega = \{(i,j) : i,j \in \{1,2,3,4,5,6\}\}</span>, so <span class="math inline">|\Omega| = 36</span>.</p>
<p>The event “sum equals 7” is <span class="math inline">A = \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}</span>.</p>
<p>Therefore <span class="math inline">\mathbb{P}(A) = \frac{6}{36} = \frac{1}{6}</span>.</p>
</div>
</div>
</div>
</div>
</div>
<p><strong>Key counting principle - Binomial Coefficient</strong>:</p>
<p>The <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> (read as “<span class="math inline">n</span> choose <span class="math inline">k</span>”) is:</p>
<p><span class="math display">\binom{n}{k} = \frac{n!}{k!(n-k)!}</span></p>
<p>Where <span class="math inline">n!</span> denotes the <a href="https://en.wikipedia.org/wiki/Factorial">factorial</a>, e.g.&nbsp;<span class="math inline">4! = 4 \cdot 3 \cdot 2 \cdot 1 = 24</span>.</p>
<p>The binomial coefficient<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> counts the number of ways to choose <span class="math inline">k</span> objects from <span class="math inline">n</span> objects when order doesn’t matter. For example:</p>
<ul>
<li><span class="math inline">\binom{5}{2} = \frac{5!}{2!3!} = \frac{5 \times 4}{2 \times 1} = 10</span> ways to choose 2 items from 5</li>
<li>Choosing 2 students from a class of 30: <span class="math inline">\binom{30}{2} = 435</span> possible pairs</li>
</ul>
</section>
</section>
<section id="independence-and-conditional-probability" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="independence-and-conditional-probability"><span class="header-section-number">1.4</span> Independence and Conditional Probability</h2>
<section id="independent-events" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="independent-events"><span class="header-section-number">1.4.1</span> Independent Events</h3>
<div class="definition">
<p>Two events <span class="math inline">A</span> and <span class="math inline">B</span> are <strong>independent</strong> if: <span class="math display">\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)</span></p>
<p>We denote this as <span class="math inline">A \perp\!\!\!\perp B</span>. When events are not independent, we write <span class="math inline">A \not\perp\!\!\!\perp B</span>.</p>
</div>
<p>Independence means that knowing whether one event occurred tells us <em>nothing</em> about the other event.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The textbook uses non-standard notation for independence and non-independence. We use the standard notation <span class="math inline">A \perp\!\!\!\perp B</span> (and <span class="math inline">A \not\perp\!\!\!\perp B</span> for dependence), which is widely adopted in probability and statistics literature.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Fair coin tosses
</div>
</div>
<div class="callout-body-container callout-body">
<p>A fair coin is tossed twice. Let <span class="math inline">H_1</span> = “first toss is heads” and <span class="math inline">H_2</span> = “second toss is heads”. Are these two events independent?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><span class="math inline">\mathbb{P}(H_1) = \mathbb{P}(H_2) = \frac{1}{2}</span></li>
<li><span class="math inline">\mathbb{P}(H_1 \cap H_2) = \mathbb{P}(\text{both heads}) = \frac{1}{4}</span></li>
<li>Since <span class="math inline">\frac{1}{4} = \frac{1}{2} \times \frac{1}{2}</span>, the events are independent</li>
</ul>
<p>This matches the intuition – whether we obtain head on the first flip does not tell us anything about the second flip, and vice versa.</p>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Dependent events
</div>
</div>
<div class="callout-body-container callout-body">
<p>Draw two cards from a deck without replacement.</p>
<ul>
<li><span class="math inline">A</span> = “first card is an ace”</li>
<li><span class="math inline">B</span> = “second card is an ace”</li>
</ul>
<p>Are these events independent?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><span class="math inline">\mathbb{P}(A) = \mathbb{P}(B) = \frac{4}{52}</span></li>
<li>However: <span class="math inline">\mathbb{P}(A \cap B) = \frac{4}{52} \times \frac{3}{51} \neq \mathbb{P}(A)\mathbb{P}(B)</span></li>
</ul>
<p>The events are <em>dependent</em> because drawing an ace first changes the probability of drawing an ace second.</p>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Common misconception</strong>: Disjoint events are NOT independent!</p>
<p>If <span class="math inline">A</span> and <span class="math inline">B</span> are disjoint with positive probability, then:</p>
<ul>
<li><span class="math inline">\mathbb{P}(A \cap B) = 0</span> (since they are disjoint, they can’t happen together)</li>
<li><span class="math inline">\mathbb{P}(A)\mathbb{P}(B) &gt; 0</span> (if both have positive probability)</li>
</ul>
<p>So disjoint events are actually maximally dependent - if one occurs, the other definitely doesn’t!</p>
</div>
</div>
</section>
<section id="conditional-probability" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="conditional-probability"><span class="header-section-number">1.4.2</span> Conditional Probability</h3>
<div class="definition">
<p>The <strong>conditional probability</strong> of <span class="math inline">A</span> given <span class="math inline">B</span> is: <span class="math display">\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}</span> provided <span class="math inline">\mathbb{P}(B) &gt; 0</span>.</p>
</div>
<p>Think of <span class="math inline">\mathbb{P}(A|B)</span> as the probability of <span class="math inline">A</span> in the “new universe” where we know <span class="math inline">B</span> has occurred.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Prosecutor’s Fallacy</strong>: Confusing <span class="math inline">\mathbb{P}(A|B)</span> with <span class="math inline">\mathbb{P}(B|A)</span>.</p>
<p>These can be vastly different! For example:</p>
<ul>
<li><span class="math inline">\mathbb{P}(\text{match} | \text{guilty})</span> might be 0.98.</li>
<li><span class="math inline">\mathbb{P}(\text{guilty} | \text{match})</span> might be 0.04.</li>
</ul>
<p>The second depends on the prior probability of guilt and how many innocent people might also match. We will see next how to compute one from the other.</p>
</div>
</div>
</section>
<section id="bayes-theorem" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">1.4.3</span> Bayes’ Theorem</h3>
<p>Sometimes we know <span class="math inline">\mathbb{P}(B|A)</span> but we are really interested in the other way round, <span class="math inline">\mathbb{P}(A|B)</span>.</p>
<p>For example, in the example above, we may know the probability that a test gives a match if the suspect is guilty, <span class="math inline">\mathbb{P}(\text{match} \mid \text{guilty})</span>, but what we really want to know is the probability that the suspect is guilty given that we find a match, <span class="math inline">\mathbb{P}(\text{guilty} \mid \text{match})</span>.</p>
<p>Such “inverse” conditional probabilities can be calculated via <strong>Bayes’ theorem</strong>.</p>
<div class="theorem" name="Bayes' Theorem">
<p>For events <span class="math inline">A</span> and <span class="math inline">B</span> with <span class="math inline">\mathbb{P}(B) &gt; 0</span>: <span class="math display">\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}</span></p>
</div>
<p>Where <span class="math inline">B</span> is some information (evidence) and <span class="math inline">A</span> an hypothesis.</p>
<div class="theorem" name="Law of Total Probability">
<p>If <span class="math inline">A_1, ..., A_k</span> partition <span class="math inline">\Omega</span> (they’re disjoint and cover all possibilities), then: <span class="math display">\mathbb{P}(B) = \sum_{i=1}^k \mathbb{P}(B|A_i)\mathbb{P}(A_i)</span></p>
</div>
<p>Combining these gives the full form of Bayes’ theorem: <span class="math display">\mathbb{P}(A_i|B) = \frac{\mathbb{P}(B|A_i)\mathbb{P}(A_i)}{\sum_j \mathbb{P}(B|A_j)\mathbb{P}(A_j)}</span></p>
<p><strong>Terminology</strong>:</p>
<ul>
<li><span class="math inline">\mathbb{P}(A_i)</span>: Prior probability for hypothesis <span class="math inline">A_i</span> (before seeing evidence <span class="math inline">B</span>), also known as “base rate”.</li>
<li><span class="math inline">\mathbb{P}(A_i|B)</span>: Posterior probability (after seeing evidence <span class="math inline">B</span>).</li>
<li><span class="math inline">\mathbb{P}(B|A_i)</span>: Likelihood of hypothesis <span class="math inline">A_i</span> for fixed evidence <span class="math inline">B</span>.</li>
</ul>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Email spam detection
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Prior: 70% of emails are spam</li>
<li>“Free” appears in 90% of spam emails</li>
<li>“Free” appears in 1% of legitimate emails</li>
</ul>
<p>If an email contains “free”, what’s the probability it’s spam?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">S</span> = “email is spam” and <span class="math inline">F</span> = “email contains ‘free’”.</p>
<p>Given:</p>
<ul>
<li><span class="math inline">\mathbb{P}(S) = 0.7</span></li>
<li><span class="math inline">\mathbb{P}(F|S) = 0.9</span><br>
</li>
<li><span class="math inline">\mathbb{P}(F|S^c) = 0.01</span></li>
</ul>
<p>By Bayes’ theorem: <span class="math display">\mathbb{P}(S|F) = \frac{0.9 \times 0.7}{0.9 \times 0.7 + 0.01 \times 0.3} = \frac{0.63}{0.633} \approx 0.995</span></p>
<p>So an email containing “free” has a 99.5% chance of being spam!</p>
</div>
</div>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255584-361-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-361-1" role="tab" aria-controls="tabset-1757255584-361-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-361-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-361-2" role="tab" aria-controls="tabset-1757255584-361-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-361-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-361-3" role="tab" aria-controls="tabset-1757255584-361-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255584-361-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255584-361-1-tab"><p>Conditional probability answers questions like:</p><ul>
<li>What’s the probability of rain given that it’s cloudy?</li>
<li>What’s the probability a patient has a disease given a positive test
result?</li>
<li>What’s the probability a user clicks an ad given they’re on a mobile
device?</li>
</ul><p>The key insight: additional information changes probabilities.
Knowing that <span class="math inline">\(B\)</span> occurred restricts
our attention to outcomes where <span class="math inline">\(B\)</span>
is true, potentially changing how likely
<span class="math inline">\(A\)</span> becomes.</p><p>Some conditional probabilities are easier to compute than others. For
example, we may know that <strong>if</strong> the patient has a disease,
then the test will return positive with a certain probability. However,
to compute the “inverse” probability (if the test is positive, what’s
the probability of the patient having the disease?) we need Bayes’
theorem.</p></div><div id="tabset-1757255584-361-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-361-2-tab"><p>For fixed <span class="math inline">\(B\)</span> with
<span class="math inline">\(\mathbb{P}(B) &gt; 0\)</span>, the conditional
probability <span class="math inline">\(\mathbb{P}(\cdot|B)\)</span> is
itself a probability measure:</p><ol type="1">
<li><span class="math inline">\(\mathbb{P}(A|B) \geq 0\)</span> for all
<span class="math inline">\(A\)</span></li>
<li><span class="math inline">\(\mathbb{P}(\Omega|B) = 1\)</span></li>
<li>If <span class="math inline">\(A_1, A_2, ...\)</span> are disjoint,
then
<span class="math inline">\(\mathbb{P}(\bigcup_i A_i|B) = \sum_i \mathbb{P}(A_i|B)\)</span></li>
</ol><p>Key relationships:</p><ul>
<li>If <span class="math inline">\(A \perp\!\!\!\perp B\)</span>, then
<span class="math inline">\(\mathbb{P}(A|B) = \mathbb{P}(A)\)</span>
(independence means conditioning doesn’t matter)</li>
<li><span class="math inline">\(\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)\)</span>
(multiplication rule)</li>
</ul><p>Conversely, in general
<span class="math inline">\(\mathbb{P}(A|\cdot)\)</span> (the
likelihood) is <em>not</em> a probability measure.</p></div><div id="tabset-1757255584-361-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-361-3-tab"><p>We will visualize how conditional probability can be
counterintuitive. We’ll simulate a medical test scenario to show how
base rates affect our interpretation of test results.</p><div id="f665cec1" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Medical test scenario</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>p_disease <span class="op">=</span> <span class="fl">0.001</span>                   <span class="co"># 0.1% have the disease (base rate)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>p_pos_given_disease <span class="op">=</span> <span class="fl">0.99</span>          <span class="co"># 99% sensitivity</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>p_neg_given_healthy <span class="op">=</span> <span class="fl">0.99</span>          <span class="co"># 99% specificity</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate probability of positive test</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>p_healthy <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_disease</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>p_pos_given_healthy <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_neg_given_healthy</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>p_positive <span class="op">=</span> p_pos_given_disease <span class="op">*</span> p_disease <span class="op">+</span> p_pos_given_healthy <span class="op">*</span> p_healthy</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Bayes' theorem: P(disease|positive)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>p_disease_given_pos <span class="op">=</span> (p_pos_given_disease <span class="op">*</span> p_disease) <span class="op">/</span> p_positive</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with different base rates</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>base_rates <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">50</span>)  <span class="co"># 0.01% to 10%</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>posterior_probs <span class="op">=</span> []</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> base_rate <span class="kw">in</span> base_rates:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    p_pos <span class="op">=</span> p_pos_given_disease <span class="op">*</span> base_rate <span class="op">+</span> p_pos_given_healthy <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> base_rate)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> (p_pos_given_disease <span class="op">*</span> base_rate) <span class="op">/</span> p_pos</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    posterior_probs.append(posterior)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.semilogx(base_rates <span class="op">*</span> <span class="dv">100</span>, np.array(posterior_probs) <span class="op">*</span> <span class="dv">100</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Current: </span><span class="sc">{</span>p_disease_given_pos<span class="sc">:.1%}</span><span class="ss"> chance'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Disease Prevalence (%)'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(Disease | Positive Test) (%)'</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Impact of Base Rate on Test Interpretation'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">100</span>)  <span class="co"># Set y-axis range from 0 to 100</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>], [<span class="st">'0.01'</span>, <span class="st">'0.1'</span>, <span class="st">'1'</span>, <span class="st">'10'</span>])  <span class="co"># Set custom x-axis ticks</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"With 99% accurate test and 0.1% base rate:"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(disease | positive test) = </span><span class="sc">{</span>p_disease_given_pos<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Surprising: A positive test means only ~9% chance of disease!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01-probability-foundations_files/figure-html/cell-5-output-1.png" width="594" height="449"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>With 99% accurate test and 0.1% base rate:
P(disease | positive test) = 9.0%
Surprising: A positive test means only ~9% chance of disease!</code></pre>
</div>
</div></div></div></div>
</section>
<section id="classic-probability-examples" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="classic-probability-examples"><span class="header-section-number">1.4.4</span> Classic Probability Examples</h3>
<p>Let’s work through some classic examples that illustrate key concepts:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: At least one head in 10 flips
</div>
</div>
<div class="callout-body-container callout-body">
<p>What’s the probability of getting at least one head in 10 coin flips?</p>
<p><em>Hint:</em> Instead of counting all the ways to get 1, 2, …, or 10 heads, use the complement.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\mathbb{P}(\text{at least one head}) = 1 - \mathbb{P}(\text{no heads}) = 1 - \mathbb{P}(\text{all tails})</span></p>
<p>Since flips are independent: <span class="math inline">\mathbb{P}(\text{all tails}) = \left(\frac{1}{2}\right)^{10} = \frac{1}{1024}</span></p>
<p>Therefore: <span class="math inline">\mathbb{P}(\text{at least one head}) = 1 - \frac{1}{1024} \approx 0.999</span></p>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example (advanced): Basketball competition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two players take turns shooting. Player A shoots first with probability 1/3 of scoring. Player B shoots second with probability 1/4. First to score wins. What’s the probability A wins?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A wins if:</p>
<ul>
<li>A scores on first shot: probability 1/3</li>
<li>Both miss, then A scores: <span class="math inline">(2/3)(3/4)(1/3)</span></li>
<li>Both miss twice, then A scores: <span class="math inline">(2/3)(3/4)(2/3)(3/4)(1/3)</span></li>
<li>…</li>
</ul>
<p>This is a <a href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a>: <span class="math display">\mathbb{P}(A \text{ wins}) = \frac{1}{3} \sum_{k=0}^{\infty} \left(\frac{2}{3} \cdot \frac{3}{4}\right)^k = \frac{1}{3} \cdot \frac{1}{1-\frac{1}{2}} = \frac{2}{3}</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="random-variables" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="random-variables"><span class="header-section-number">1.5</span> Random Variables</h2>
<p>So far, we’ve worked with events - subsets of the sample space. But in practice, we usually care about numerical quantities associated with random outcomes. This is where random variables come in.</p>
<section id="definition-and-intuition" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="definition-and-intuition"><span class="header-section-number">1.5.1</span> Definition and Intuition</h3>
<div class="definition">
<p>A <strong>random variable</strong> is a function <span class="math inline">X: \Omega \rightarrow \mathbb{R}</span> that assigns a real number to each outcome in the sample space.</p>
</div>
<p>A random variable is defined by its possible <em>values</em> (real numbers) and their <em>probabilities</em>.</p>
<p>In the case of a <em>discrete</em> random variable, the set of values is discrete (finite or infinite), <span class="math inline">x_1, \ldots</span>, and each value can be assigned a corresponding point probability <span class="math inline">p_1, \ldots</span> with <span class="math inline">0 \le p_i \le 1</span>, <span class="math inline">\sum_{i=1}^\infty p_i = 1</span>.</p>
<p>In the case of a continuous random variable, probabilities are defined by a non-negative probability density function that integrates to 1.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255584-954-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-954-1" role="tab" aria-controls="tabset-1757255584-954-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-954-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-954-2" role="tab" aria-controls="tabset-1757255584-954-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-954-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-954-3" role="tab" aria-controls="tabset-1757255584-954-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255584-954-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255584-954-1-tab"><p>A random variable is just a way to assign numbers to outcomes. Think
of it as a measurement or quantity that depends on chance.</p><p>Examples:</p><ul>
<li>Number of heads in 10 coin flips</li>
<li>Time until next customer arrives</li>
<li>Temperature at noon tomorrow</li>
<li>Stock price at market close</li>
</ul><p>The key insight: once we have numbers, we can do arithmetic,
calculate averages, measure spread, and use all the tools of
mathematics.</p></div><div id="tabset-1757255584-954-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-954-2-tab"><p><em>Warning:</em> The following will likely make sense only if you
have taken an advanced course in probability theory or measure theory.
Feel free to skip it otherwise.</p><p>Formally, <span class="math inline">\(X\)</span> is a measurable
function from <span class="math inline">\((\Omega, \mathcal{F})\)</span>
to <span class="math inline">\((\mathbb{R}, \mathcal{B})\)</span>
where:</p><ul>
<li><span class="math inline">\(\mathcal{F}\)</span> is the
<span class="math inline">\(\sigma\)</span>-algebra of events in
<span class="math inline">\(\Omega\)</span></li>
<li><span class="math inline">\(\mathcal{B}\)</span> is the Borel
<span class="math inline">\(\sigma\)</span>-algebra on
<span class="math inline">\(\mathbb{R}\)</span></li>
</ul><p>Measurability means: for any Borel set
<span class="math inline">\(B \subset \mathbb{R}\)</span>, the pre-image
<span class="math inline">\(X^{-1}(B) = \{\omega : X(\omega) \in B\}\)</span>
is an event in <span class="math inline">\(\mathcal{F}\)</span>.</p><p>This technical condition ensures we can compute probabilities like
<span class="math inline">\(\mathbb{P}(X \in B)\)</span>.</p></div><div id="tabset-1757255584-954-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-954-3-tab"><p>Here we demonstrate how random variables map outcomes to numbers,
allowing us to analyze randomness mathematically. For example, we can
plot a histogram for the realizations over multiple experiments.</p><div id="786d5f5c" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate a random variable: X = number of heads in 10 coin flips</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Single experiment</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> np.random.choice([<span class="st">'H'</span>, <span class="st">'T'</span>], size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.<span class="bu">sum</span>(flips <span class="op">==</span> <span class="st">'H'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Outcomes (single experiment): </span><span class="sc">{</span>flips<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X (number of heads) = </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate many experiments to see the distribution</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>n_sims <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X_values <span class="op">=</span> [np.<span class="bu">sum</span>(np.random.choice([<span class="st">'H'</span>, <span class="st">'T'</span>], size<span class="op">=</span><span class="dv">10</span>) <span class="op">==</span> <span class="st">'H'</span>) </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_sims)]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize distribution</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>counts, bins, _ <span class="op">=</span> plt.hist(X_values, bins<span class="op">=</span>np.arange(<span class="fl">0.5</span>, <span class="fl">11.5</span>, <span class="dv">1</span>), density<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>                          alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">11</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Heads'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Random Variable X: Number of Heads in 10 Flips (</span><span class="sc">{</span>n_sims<span class="sc">}</span><span class="ss"> experiments)'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Average value: </span><span class="sc">{</span>np<span class="sc">.</span>mean(X_values)<span class="sc">:.3f}</span><span class="ss"> (theoretical: 5.0)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Outcomes (single experiment): ['H' 'T' 'H' 'H' 'H' 'T' 'H' 'H' 'H' 'T']
X (number of heads) = 7

Average value: 4.993 (theoretical: 5.0)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="01-probability-foundations_files/figure-html/cell-6-output-2.png" width="618" height="376"></p>
</div>
</div></div></div></div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Coin flips
</div>
</div>
<div class="callout-body-container callout-body">
<p>Within the same sample space we can define multiple distinct random variables.</p>
<p>For example, let <span class="math inline">\Omega = \{HH, HT, TH, TT\}</span> (two flips). Define:</p>
<ul>
<li><span class="math inline">X</span> = number of heads</li>
<li><span class="math inline">Y</span> = 1 if first flip is heads, 0 otherwise</li>
<li><span class="math inline">Z</span> = 1 if flips match, 0 otherwise</li>
</ul>
<p>Then:</p>
<ul>
<li><span class="math inline">X(HH) = 2</span>, <span class="math inline">X(HT) = 1</span>, <span class="math inline">X(TH) = 1</span>, <span class="math inline">X(TT) = 0</span></li>
<li><span class="math inline">Y(HH) = 1</span>, <span class="math inline">Y(HT) = 1</span>, <span class="math inline">Y(TH) = 0</span>, <span class="math inline">Y(TT) = 0</span><br>
</li>
<li><span class="math inline">Z(HH) = 1</span>, <span class="math inline">Z(HT) = 0</span>, <span class="math inline">Z(TH) = 0</span>, <span class="math inline">Z(TT) = 1</span></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Notation convention</strong>:</p>
<ul>
<li>Capital letters <span class="math inline">(X, Y, Z)</span> denote random variables</li>
<li>Lowercase letters <span class="math inline">(x, y, z)</span> denote specific values</li>
<li><span class="math inline">\{X = x\}</span> is the event that <span class="math inline">X</span> takes value <span class="math inline">x</span></li>
</ul>
<p>However, do not expect people to strictly follow this convention beyond mathematical and statistical textbooks. In the real world, you will often see “<span class="math inline">x</span>” used to refer both to a <em>value</em> and to a random variable “<span class="math inline">X</span>” that happens to take value <span class="math inline">x</span>.</p>
</div>
</div>
</section>
<section id="cumulative-distribution-functions" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="cumulative-distribution-functions"><span class="header-section-number">1.5.2</span> Cumulative Distribution Functions</h3>
<p>The Cumulative Distribution Function (CDF) completely characterizes a random variable’s probability distribution.</p>
<div class="definition">
<p>The <strong>cumulative distribution function (CDF)</strong> of a random variable <span class="math inline">X</span> is the function <span class="math inline">F_X(x): \mathbb{R} \rightarrow [0, 1]</span> defined by <span class="math display">F_X(x) = \mathbb{P}(X \leq x)</span> for all <span class="math inline">x \in \mathbb{R}</span>.</p>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Two coin flips
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X</span> = number of heads for two flips of fair coins.</p>
<ul>
<li><span class="math inline">\mathbb{P}(X = 0) = 1/4</span></li>
<li><span class="math inline">\mathbb{P}(X = 1) = 1/2</span></li>
<li><span class="math inline">\mathbb{P}(X = 2) = 1/4</span></li>
</ul>
<p>The CDF is: <span class="math display">F_X(x) = \begin{cases}
0 &amp; \text{if } x &lt; 0 \\
1/4 &amp; \text{if } 0 \leq x &lt; 1 \\
3/4 &amp; \text{if } 1 \leq x &lt; 2 \\
1 &amp; \text{if } x \geq 2
\end{cases}</span></p>
<p>Note: The CDF is defined for ALL real <span class="math inline">x</span>, even though <span class="math inline">X</span> only takes values 0, 1, 2!</p>
<div id="cell-fig-cdf-example" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="6">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the CDF values</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x_jumps <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]  <span class="co"># Points where jumps occur</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>cdf_values <span class="op">=</span> [<span class="fl">0.25</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>]  <span class="co"># CDF values after jumps</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>cdf_values_before <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.75</span>]  <span class="co"># CDF values before jumps</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the step function</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Left segment (x &lt; 0)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>ax.hlines(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each segment</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x_jumps)):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Horizontal line segment</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(x_jumps) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        ax.hlines(cdf_values[i], x_jumps[i], x_jumps[i<span class="op">+</span><span class="dv">1</span>], colors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Last segment extends to the right</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        ax.hlines(cdf_values[i], x_jumps[i], <span class="dv">3</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Open circles (at discontinuities, left endpoints)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        ax.plot(x_jumps[i], cdf_values_before[i], <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                markerfacecolor<span class="op">=</span><span class="st">'white'</span>, markersize<span class="op">=</span><span class="dv">8</span>, markeredgewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filled circles (at jump points, right endpoints)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_jumps[i], cdf_values[i], <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            markerfacecolor<span class="op">=</span><span class="st">'black'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Open circle at x=0, y=0</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="dv">0</span>, <span class="dv">0</span>, <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, markerfacecolor<span class="op">=</span><span class="st">'white'</span>, </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        markersize<span class="op">=</span><span class="dv">8</span>, markeredgewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Set axis properties</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'$F_X(x)$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Set tick marks</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([<span class="fl">0.25</span>, <span class="fl">0.50</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>])</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Add grid</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Add arrows to axes</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">''</span>, xy<span class="op">=</span>(<span class="fl">3.2</span>, <span class="dv">0</span>), xytext<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">0</span>),</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">''</span>, xy<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1.15</span>), xytext<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1.1</span>),</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-cdf-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cdf-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-probability-foundations_files/figure-html/fig-cdf-example-output-1.png" width="661" height="374" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cdf-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Cumulative distribution function (CDF) for the number of heads when flipping a coin twice.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="discrete-random-variables" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="discrete-random-variables"><span class="header-section-number">1.5.3</span> Discrete Random Variables</h3>
<div class="definition">
<p>A random variable <span class="math inline">X</span> is <strong>discrete</strong> if it takes countably many values <span class="math inline">\{x_1, x_2, ...\}</span>. Its <strong>probability mass function (PMF)</strong> (sometimes just <strong>probability function</strong>) is defined as: <span class="math display">f_X(x) = \mathbb{P}(X = x)</span></p>
</div>
<p>Properties of PMFs:</p>
<ul>
<li><span class="math inline">f_X(x) \geq 0</span> for all <span class="math inline">x</span></li>
<li><span class="math inline">\sum_{i} f_X(x_i) = 1</span> (probabilities sum to 1)</li>
<li><span class="math inline">F_X(x) = \sum_{x_i \leq x} f_X(x_i)</span> (CDF is sum of PMF)</li>
</ul>
<div id="cell-fig-pmf-cdf-example" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="7">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PMF for coin flipping example</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>pmf_values <span class="op">=</span> [<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot vertical lines from x-axis to probability values</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, p <span class="kw">in</span> <span class="bu">zip</span>(x_values, pmf_values):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    ax.plot([x, x], [<span class="dv">0</span>, p], <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add filled circles at the top</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, p, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>, markerfacecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Set axis properties</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'$f_X(x)$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Set tick marks</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Add grid</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a horizontal line at y=0 for clarity</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pmf-cdf-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pmf-cdf-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-probability-foundations_files/figure-html/fig-pmf-cdf-example-output-1.png" width="661" height="373" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pmf-cdf-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Probability mass function (PMF) for the number of heads when flipping a coin twice.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="core-discrete-distributions" class="level3" data-number="1.5.4">
<h3 data-number="1.5.4" class="anchored" data-anchor-id="core-discrete-distributions"><span class="header-section-number">1.5.4</span> Core Discrete Distributions</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Notation preview</strong>: We’ll use <span class="math inline">\mathbb{E}[X]</span> to denote the <em>expected value</em> (mean) of a random variable <span class="math inline">X</span>, and <span class="math inline">\text{Var}(X)</span> or <span class="math inline">\sigma^2</span> for its <em>variance</em> (a measure of spread). These concepts will be covered in detail in Chapter 2 of the lecture notes.</p>
</div>
</div>
<section id="bernoulli-distribution" class="level4">
<h4 class="anchored" data-anchor-id="bernoulli-distribution">Bernoulli Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> is the simplest non-trivial random variable – a single binary outcome with probability <span class="math inline">p \in [0, 1]</span> of happening.</p>
<div class="definition">
<p><span class="math inline">X \sim \text{Bernoulli}(p)</span> if: <span class="math display">f_X(x) = \begin{cases}
p &amp; \text{if } x = 1 \\
1-p &amp; \text{if } x = 0 \\
0 &amp; \text{otherwise}
\end{cases}</span></p>
</div>
<p>An outcome of <span class="math inline">X = 1</span> is often referred to as a “hit” or a “success”, while <span class="math inline">X = 0</span> is a “miss” or a “failure”.</p>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Coin flip (heads/tails)
<ul>
<li>If <span class="math inline">p \neq 0.5</span>, this is known as a <em>biased</em> coin (as opposed to a <em>fair</em> coin with <span class="math inline">p = 0.5</span>)</li>
<li>Here what constitutes a “hit” and a “miss” is arbitrary!</li>
</ul></li>
<li>Success/failure of a single trial</li>
<li>Binary classification (spam/not spam)</li>
<li>User clicks/doesn’t click an ad</li>
</ul>
<div id="57991096" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="8">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli distribution PMF</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameter</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.3</span>  <span class="co"># probability of success</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># PMF visualization</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot PMF</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> [<span class="dv">1</span><span class="op">-</span>p, p]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, pmf, width<span class="op">=</span><span class="fl">0.4</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span>[<span class="st">'lightcoral'</span>, <span class="st">'lightblue'</span>], </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>               edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, pi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, pmf)):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    ax.text(xi, pi <span class="op">+</span> <span class="fl">0.02</span>, <span class="ss">f'</span><span class="sc">{</span>pi<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'Failure (0)'</span>, <span class="st">'Success (1)'</span>])</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Bernoulli Distribution PMF (p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = p(1-p) = </span><span class="sc">{</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-9-output-1.png" width="661" height="373" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>E[X] = p = 0.3
Var(X) = p(1-p) = 0.210</code></pre>
</div>
</div>
</section>
<section id="binomial-distribution" class="level4">
<h4 class="anchored" data-anchor-id="binomial-distribution">Binomial Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a> counts the number of successes in a fixed number <span class="math inline">n</span> of independent Bernoulli trials each with probability <span class="math inline">p</span>.</p>
<div class="definition">
<p><span class="math inline">X \sim \text{Binomial}(n, p)</span> if: <span class="math display">f_X(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x = 0, 1, ..., n</span></p>
</div>
<p><strong>Key properties</strong>:</p>
<ul>
<li>Sum of independent Bernoullis: If <span class="math inline">X_i \sim \text{Bernoulli}(p)</span> are independent, then <span class="math inline">\sum_{i=1}^n X_i \sim \text{Binomial}(n, p)</span></li>
<li>Additivity: If <span class="math inline">X \sim \text{Binomial}(n_1, p)</span> and <span class="math inline">Y \sim \text{Binomial}(n_2, p)</span> are independent, then <span class="math inline">X + Y \sim \text{Binomial}(n_1 + n_2, p)</span></li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Number of heads in <span class="math inline">n</span> coin flips</li>
<li>Number of defective items in a batch</li>
<li>Number of customers who make a purchase</li>
<li>Number of successful treatments in a clinical trial</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Independence assumption</strong>: The binomial distribution assumes all trials are independent - each outcome does not affect the probability of subsequent outcomes. This assumption may not hold in practice!</p>
<p>For example, if items are defective because a machine has broken (rather than random variation), then finding one defective item suggests all subsequent items might also be defective. In such cases, the binomial distribution would be inappropriate.</p>
</div>
</div>
<div id="92f19957" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="9">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Binomial distribution visualization</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> <span class="dv">20</span>, <span class="fl">0.3</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">0</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot PMF</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> binom.pmf(x, n, p)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, pmf, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'steelblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight mean</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> n <span class="op">*</span> p</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>ax.axvline(mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Mean = </span><span class="sc">{</span>mean<span class="sc">:.1f}</span><span class="ss">'</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels on significant bars</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, pi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, pmf)):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pi <span class="op">&gt;</span> <span class="fl">0.01</span>:  <span class="co"># Only label visible bars</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        ax.text(xi, pi <span class="op">+</span> <span class="fl">0.003</span>, <span class="ss">f'</span><span class="sc">{</span>pi<span class="sc">:.3f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Number of successes (k)'</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'P(X = k)'</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Binomial Distribution PMF: n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, p=</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = np = </span><span class="sc">{</span>n<span class="op">*</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = np(1-p) = </span><span class="sc">{</span>n<span class="op">*</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"σ = </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(n<span class="op">*</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p))<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-10-output-1.png" width="661" height="468" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>E[X] = np = 6.0
Var(X) = np(1-p) = 4.199999999999999
σ = 2.049</code></pre>
</div>
</div>
</section>
<section id="discrete-uniform-distribution" class="level4">
<h4 class="anchored" data-anchor-id="discrete-uniform-distribution">Discrete Uniform Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution">discrete uniform distribution</a> is another simple discrete distribution - every outcome is equally likely.</p>
<div class="definition">
<p><span class="math inline">X \sim \text{DiscreteUniform}(a, b)</span> if: <span class="math display">f_X(x) = \frac{1}{b-a+1}, \quad x \in \{a, a+1, ..., b\}</span> where <span class="math inline">a</span> and <span class="math inline">b</span> are integers with <span class="math inline">a \leq b</span>.</p>
</div>
<p><strong>Key properties</strong>:</p>
<ul>
<li><span class="math inline">\mathbb{E}[X] = \frac{a+b}{2}</span></li>
<li><span class="math inline">\text{Var}(X) = \frac{(b-a+1)^2 - 1}{12}</span></li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Fair die roll: <span class="math inline">\text{DiscreteUniform}(1, 6)</span></li>
<li><a href="https://en.wikipedia.org/wiki/Dice#Polyhedral_dice">Attack roll</a>: <span class="math inline">\text{DiscreteUniform}(1, 20)</span></li>
<li>Random selection from a finite set</li>
<li>Lottery number selection</li>
</ul>
<div id="05fa21fb" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="10">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete Uniform distribution</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> randint</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: fair die</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="dv">1</span>, <span class="dv">6</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(a, b<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> [<span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a<span class="op">+</span><span class="dv">1</span>)] <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, pmf, width<span class="op">=</span><span class="fl">0.6</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'lightgreen'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, pi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, pmf)):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    ax.text(xi, pi <span class="op">+</span> <span class="fl">0.01</span>, <span class="ss">f'</span><span class="sc">{</span>pi<span class="sc">:.3f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Outcome'</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Discrete Uniform Distribution: Fair Die'</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.3</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = </span><span class="sc">{</span>(a<span class="op">+</span>b)<span class="op">/</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = </span><span class="sc">{</span>((b<span class="op">-</span>a<span class="op">+</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span><span class="dv">12</span><span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-11-output-1.png" width="661" height="372" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>E[X] = 3.5
Var(X) = 2.917</code></pre>
</div>
</div>
</section>
<section id="categorical-distribution" class="level4">
<h4 class="anchored" data-anchor-id="categorical-distribution">Categorical Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical distribution</a> is a generalization of Bernoulli to multiple categories (also called “Generalized Bernoulli” or “Multinoulli”). You can also see it as a generalization of the discrete uniform distribution to a discrete <em>non</em>-uniform distribution.</p>
<div class="definition">
<p><span class="math inline">X \sim \text{Categorical}(p_1, ..., p_k)</span> if: <span class="math display">f_X(x) = p_x, \quad x \in \{1, 2, ..., k\}</span> where <span class="math inline">p_i \geq 0</span> and <span class="math inline">\sum_{i=1}^k p_i = 1</span>.</p>
</div>
<p><strong>Key properties</strong>:</p>
<ul>
<li>One-hot encoding: Often represented as a vector with one 1 and rest 0s</li>
<li>Special case: Categorical with <span class="math inline">k=2</span> is equivalent to Bernoulli</li>
<li>Special case: If all probabilities are equal, it becomes a discrete uniform</li>
<li>Foundation for multinomial distribution (multiple categorical trials)</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Outcome of rolling a (possibly unfair) die</li>
<li>Classification into multiple categories</li>
<li>Language modeling (next-token prediction)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
<li>Customer choice among products</li>
</ul>
<div id="3fa0b45a" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="11">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Categorical distribution</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Customer choice among 5 products</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> [<span class="st">'Product A'</span>, <span class="st">'Product B'</span>, <span class="st">'Product C'</span>, <span class="st">'Product D'</span>, <span class="st">'Product E'</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> [<span class="fl">0.30</span>, <span class="fl">0.25</span>, <span class="fl">0.20</span>, <span class="fl">0.15</span>, <span class="fl">0.10</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(categories))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, probabilities, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span>[<span class="st">'#1f77b4'</span>, <span class="st">'#ff7f0e'</span>, <span class="st">'#2ca02c'</span>, <span class="st">'#d62728'</span>, <span class="st">'#9467bd'</span>],</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>               edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(probabilities):</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    ax.text(i, p <span class="op">+</span> <span class="fl">0.01</span>, <span class="ss">f'</span><span class="sc">{</span>p<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Category'</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Categorical Distribution: Customer Product Choice'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(categories, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.4</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected value for indicator representation (does it make sense here?)</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"If we encode categories as 1, 2, 3, 4, 5:"</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>expected <span class="op">=</span> <span class="bu">sum</span>((i<span class="op">+</span><span class="dv">1</span>) <span class="op">*</span> p <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(probabilities))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = </span><span class="sc">{</span>expected<span class="sc">:.2f}</span><span class="ss"> (does it really make sense here?)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-12-output-1.png" width="661" height="373" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>If we encode categories as 1, 2, 3, 4, 5:
E[X] = 2.50 (does it really make sense here?)</code></pre>
</div>
</div>
</section>
<section id="brief-catalog-other-discrete-distributions" class="level4">
<h4 class="anchored" data-anchor-id="brief-catalog-other-discrete-distributions">Brief Catalog: Other Discrete Distributions</h4>
<p><strong>Poisson(<span class="math inline">\lambda</span>)</strong>: The <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> models count of rare events in fixed intervals:</p>
<ul>
<li>PMF: <span class="math inline">f_X(x) = e^{-\lambda} \frac{\lambda^x}{x!}</span> for <span class="math inline">x = 0, 1, 2, ...</span></li>
<li>Mean = Variance = <span class="math inline">\lambda</span> (<em>lambda</em>)</li>
<li>Use: Email arrivals, typos per page, customer arrivals</li>
<li>Approximates Binomial(<span class="math inline">n,p</span>) when <span class="math inline">n</span> large, <span class="math inline">p</span> small: use <span class="math inline">\lambda = np</span></li>
</ul>
<p><strong>Geometric(<span class="math inline">p</span>)</strong>: The <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a> represents the number of trials until first success:</p>
<ul>
<li>PMF: <span class="math inline">f_X(x) = p(1-p)^{x-1}</span> for <span class="math inline">x = 1, 2, ...</span></li>
<li>Use: Waiting times, number of attempts until success</li>
</ul>
<p><strong>Negative Binomial(<span class="math inline">r, p</span>)</strong>: The <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial</a> represents the number of failures before <span class="math inline">r</span>th success</p>
<ul>
<li>Generalization of geometric distribution</li>
<li>Use: Overdispersed count data, robust alternative to Poisson</li>
</ul>
</section>
</section>
<section id="continuous-random-variables" class="level3" data-number="1.5.5">
<h3 data-number="1.5.5" class="anchored" data-anchor-id="continuous-random-variables"><span class="header-section-number">1.5.5</span> Continuous Random Variables</h3>
<div class="definition">
<p>A random variable <span class="math inline">X</span> is <strong>continuous</strong> if there exists a function <span class="math inline">f_X</span> such that:</p>
<ol type="1">
<li><span class="math inline">f_X(x) \geq 0</span> for all <span class="math inline">x</span></li>
<li><span class="math inline">\int_{-\infty}^{\infty} f_X(x) dx = 1</span></li>
<li>For any <span class="math inline">a &lt; b</span>: <span class="math inline">\mathbb{P}(a &lt; X &lt; b) = \int_a^b f_X(x) dx</span></li>
</ol>
<p>The function <span class="math inline">f_X</span> is called the <strong>probability density function (PDF)</strong>.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Important distinctions from discrete case</strong>:</p>
<ul>
<li><span class="math inline">\mathbb{P}(X = x) = 0</span> for any single point <span class="math inline">x</span>: in a continuum, there is zero probability of picking one specific point</li>
<li>PDF can exceed 1 (it’s a <em>density</em>, not a probability!)</li>
<li>We get probabilities by integrating densities over an interval, not summing</li>
</ul>
</div>
</div>
<p><strong>Relationship between PDF and CDF</strong>:</p>
<ul>
<li><span class="math inline">F_X(x) = \int_{-\infty}^x f_X(t) dt</span></li>
<li><span class="math inline">f_X(x) = F_X'(x)</span> (where the derivative exists)</li>
</ul>
</section>
<section id="core-continuous-distributions" class="level3" data-number="1.5.6">
<h3 data-number="1.5.6" class="anchored" data-anchor-id="core-continuous-distributions"><span class="header-section-number">1.5.6</span> Core Continuous Distributions</h3>
<section id="uniform-distribution" class="level4">
<h4 class="anchored" data-anchor-id="uniform-distribution">Uniform Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">uniform distribution</a> is the continuous analog of “equally likely outcomes.”</p>
<div class="definition">
<p><span class="math inline">X \sim \text{Uniform}(a, b)</span> if: <span class="math display">f_X(x) = \begin{cases}
\frac{1}{b-a} &amp; \text{if } a \leq x \leq b \\
0 &amp; \text{otherwise}
\end{cases}</span></p>
</div>
<p><strong>Properties</strong>:</p>
<ul>
<li>CDF: <span class="math inline">F_X(x) = \frac{x-a}{b-a}</span> for <span class="math inline">a \leq x \leq b</span></li>
<li>Every interval of equal length has equal probability</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Random number generation (<span class="math inline">\text{Uniform}(0,1)</span> is fundamental in computational statistics)</li>
<li>Modeling complete uncertainty within bounds</li>
<li>Arrival times when “any time is equally likely”</li>
</ul>
<div id="84da8bfd" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="12">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Uniform distribution visualization</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> uniform</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Uniform(a=2, b=5)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">1000</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># PDF</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> uniform.pdf(x, loc<span class="op">=</span>a, scale<span class="op">=</span>b<span class="op">-</span>a)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the PDF</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>ax.plot(x, pdf, linewidth<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, label<span class="op">=</span><span class="ss">f'Uniform(</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x, <span class="dv">0</span>, pdf, where<span class="op">=</span>(x <span class="op">&gt;=</span> a) <span class="op">&amp;</span> (x <span class="op">&lt;=</span> b), alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'lightblue'</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the boundaries</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>a, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>b, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Add height label</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>ax.text((a<span class="op">+</span>b)<span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a) <span class="op">+</span> <span class="fl">0.02</span>, <span class="ss">f'height = 1/</span><span class="sc">{</span>b<span class="op">-</span>a<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a)<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density f(x)'</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Uniform Distribution PDF'</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = (a+b)/2 = </span><span class="sc">{</span>(a<span class="op">+</span>b)<span class="op">/</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = (b-a)²/12 = </span><span class="sc">{</span>(b<span class="op">-</span>a)<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">12</span><span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total area under curve = </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a)<span class="sc">}</span><span class="ss"> × </span><span class="sc">{</span>b<span class="op">-</span>a<span class="sc">}</span><span class="ss"> = 1 ✓"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-13-output-1.png" width="661" height="372" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>E[X] = (a+b)/2 = 3.5
Var(X) = (b-a)²/12 = 0.750
Total area under curve = 0.3333333333333333 × 3 = 1 ✓</code></pre>
</div>
</div>
</section>
<section id="normal-gaussian-distribution" class="level4">
<h4 class="anchored" data-anchor-id="normal-gaussian-distribution">Normal (Gaussian) Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> (also called Gaussian distribution) is the most important distribution in statistics, arising from the Central Limit Theorem.</p>
<div class="definition">
<p><span class="math inline">X \sim \text{Normal}(\mu, \sigma^2)</span> or <span class="math inline">\mathcal{N}(\mu, \sigma^2)</span> if: <span class="math display">f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)</span></p>
</div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><span class="math inline">\mu</span> (<em>mu</em>): mean (center of distribution)</li>
<li><span class="math inline">\sigma^2</span> (<em>sigma</em> squared): variance (<span class="math inline">\sigma</span> is standard deviation - controls spread)</li>
</ul>
<p><strong>Key properties</strong>:</p>
<ul>
<li>Symmetric bell curve centered at <span class="math inline">\mu</span></li>
<li>About 68% of probability within <span class="math inline">\mu \pm \sigma</span></li>
<li>About 95% within <span class="math inline">\mu \pm 2\sigma</span><br>
</li>
<li>About 99.7% within <span class="math inline">\mu \pm 3\sigma</span></li>
</ul>
<p><strong>Standard Normal</strong>: <span class="math inline">Z \sim \mathcal{N}(0, 1)</span> has <span class="math inline">\mu = 0</span>, <span class="math inline">\sigma = 1</span></p>
<ul>
<li>Any normal can be standardized: If <span class="math inline">X \sim \mathcal{N}(\mu, \sigma^2)</span>, then <span class="math inline">Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)</span></li>
<li>This allows us to use standard normal tables for any normal distribution</li>
</ul>
<p><strong>Additivity</strong>: If <span class="math inline">X_i \sim \mathcal{N}(\mu_i, \sigma_i^2)</span> are independent, then: <span class="math display">\sum_{i=1}^n X_i \sim \mathcal{N}\left(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma_i^2\right)</span></p>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Measurement errors</li>
<li>Heights, weights, test scores in large populations</li>
<li>Sum of many small independent effects (CLT)</li>
<li>Approximation for many other distributions</li>
</ul>
<div id="29bead47" class="cell" data-fig-height="8" data-fig-width="7" data-execution_count="13">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Normal distribution visualization</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. PDF with different parameters</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1000</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Different means</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mu, color <span class="kw">in</span> <span class="bu">zip</span>([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>], [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>]):</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x, norm.pdf(x, loc<span class="op">=</span>mu), linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'μ=</span><span class="sc">{</span>mu<span class="sc">}</span><span class="ss">, σ=1'</span>, color<span class="op">=</span>color)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Different standard deviations</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sigma, style <span class="kw">in</span> <span class="bu">zip</span>([<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>], [<span class="st">'-'</span>, <span class="st">'--'</span>, <span class="st">':'</span>]):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x, norm.pdf(x, scale<span class="op">=</span>sigma), linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'μ=0, σ=</span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">'</span>, linestyle<span class="op">=</span>style, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Normal Distribution PDF'</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>ax1.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 68-95-99.7 Rule</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1000</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> norm.pdf(x_range, mu, sigma)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_range, y, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill areas for different sigma ranges</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'lightblue'</span>, <span class="st">'lightgreen'</span>, <span class="st">'lightyellow'</span>]</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.8</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>]</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'68% (±1σ)'</span>, <span class="st">'95% (±2σ)'</span>, <span class="st">'99.7% (±3σ)'</span>]</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_sigma, color, alpha, label <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], colors, alphas, labels):</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    x_fill <span class="op">=</span> x_range[np.<span class="bu">abs</span>(x_range <span class="op">-</span> mu) <span class="op">&lt;=</span> n_sigma <span class="op">*</span> sigma]</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    y_fill <span class="op">=</span> norm.pdf(x_fill, mu, sigma)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    ax2.fill_between(x_fill, <span class="dv">0</span>, y_fill, color<span class="op">=</span>color, alpha<span class="op">=</span>alpha, label<span class="op">=</span>label)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Standard Deviations from Mean'</span>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'68-95-99.7 Rule for Standard Normal'</span>)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Key probabilities</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Standard Normal Probabilities:"</span>)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(-1 ≤ Z ≤ 1) = </span><span class="sc">{</span>norm<span class="sc">.</span>cdf(<span class="dv">1</span>) <span class="op">-</span> norm<span class="sc">.</span>cdf(<span class="op">-</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0.68"</span>)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(-2 ≤ Z ≤ 2) = </span><span class="sc">{</span>norm<span class="sc">.</span>cdf(<span class="dv">2</span>) <span class="op">-</span> norm<span class="sc">.</span>cdf(<span class="op">-</span><span class="dv">2</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0.95"</span>)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(-3 ≤ Z ≤ 3) = </span><span class="sc">{</span>norm<span class="sc">.</span>cdf(<span class="dv">3</span>) <span class="op">-</span> norm<span class="sc">.</span>cdf(<span class="op">-</span><span class="dv">3</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0.997"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-14-output-1.png" width="661" height="756" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Standard Normal Probabilities:
P(-1 ≤ Z ≤ 1) = 0.683 ≈ 0.68
P(-2 ≤ Z ≤ 2) = 0.954 ≈ 0.95
P(-3 ≤ Z ≤ 3) = 0.997 ≈ 0.997</code></pre>
</div>
</div>
</section>
<section id="exponential-distribution" class="level4">
<h4 class="anchored" data-anchor-id="exponential-distribution">Exponential Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a> models waiting times between events in a Poisson process.</p>
<div class="definition">
<p><span class="math inline">X \sim \text{Exponential}(\beta)</span> if: <span class="math display">f_X(x) = \frac{1}{\beta} e^{-x/\beta}, \quad x &gt; 0</span></p>
</div>
<p>Here <span class="math inline">\beta</span> (<em>beta</em>) is the scale parameter, the average time between events.</p>
<p>You can also find the exponential distribution parameterized in terms of a <em>rate</em> parameter <span class="math inline">\lambda = \frac{1}{\beta}</span>.</p>
<p><strong>Key properties</strong>:</p>
<ul>
<li><strong>Memoryless</strong>: <span class="math inline">\mathbb{P}(X &gt; s + t | X &gt; s) = \mathbb{P}(X &gt; t)</span>
<ul>
<li>“The future doesn’t depend on how long you’ve already waited”</li>
</ul></li>
<li>Connection to Poisson: If events occur at rate <span class="math inline">\lambda = 1/\beta</span>, time between events is Exponential(<span class="math inline">\beta</span>)</li>
<li>CDF: <span class="math inline">F_X(x) = 1 - e^{-x/\beta}</span> for <span class="math inline">x &gt; 0</span></li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Time between customer arrivals</li>
<li>Lifetime of electronic components<br>
</li>
<li>Time until next earthquake</li>
<li>Duration of phone calls</li>
</ul>
<div id="82813a8b" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="14">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponential distribution visualization</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> expon</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Different β values (mean waiting time)</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>]</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">1000</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> beta, color <span class="kw">in</span> <span class="bu">zip</span>(betas, colors):</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    pdf <span class="op">=</span> expon.pdf(x, scale<span class="op">=</span>beta)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, pdf, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'β = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show mean as vertical line</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    ax.axvline(beta, color<span class="op">=</span>color, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Time (x)'</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Exponential Distribution PDF'</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">6</span>)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">2.1</span>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Example calculations</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For β = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss"> (rate λ = </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>beta<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = β = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = β² = </span><span class="sc">{</span>beta<span class="op">**</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(X &gt; 1) = </span><span class="sc">{</span><span class="dv">1</span> <span class="op">-</span> expon<span class="sc">.</span>cdf(<span class="dv">1</span>, scale<span class="op">=</span>beta)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Median waiting time = </span><span class="sc">{</span>expon<span class="sc">.</span>ppf(<span class="fl">0.5</span>, scale<span class="op">=</span>beta)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-15-output-1.png" width="661" height="468" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>For β = 1 (rate λ = 1.0):
E[X] = β = 1
Var(X) = β² = 1
P(X &gt; 1) = 0.368
Median waiting time = 0.693</code></pre>
</div>
</div>
</section>
<section id="brief-catalog-other-continuous-distributions" class="level4">
<h4 class="anchored" data-anchor-id="brief-catalog-other-continuous-distributions">Brief Catalog: Other Continuous Distributions</h4>
<p><strong>Gamma(<span class="math inline">\alpha, \beta</span>)</strong>: The <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> is a generalization of exponential</p>
<ul>
<li>Sum of independent exponentials</li>
<li>Models waiting time for multiple events</li>
</ul>
<p><strong>Beta(<span class="math inline">\alpha, \beta</span>)</strong>: The <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta distribution</a> models values between 0 and 1</p>
<ul>
<li>Models proportions and probabilities</li>
<li>Conjugate prior for binomial parameter</li>
</ul>
<p><strong>t(<span class="math inline">\nu</span>)</strong>: The <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">t-distribution</a> is a heavy-tailed alternative to normal</p>
<ul>
<li>More probability in tails than normal</li>
<li>Used when variance is unknown/unstable</li>
<li><span class="math inline">\nu = 1</span> gives Cauchy (no finite mean!)</li>
</ul>
<p><strong><span class="math inline">\chi^2(p)</span></strong>: The <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a> is the sum of squared standard normals</p>
<ul>
<li>If <span class="math inline">Z_1, ..., Z_p \sim \mathcal{N}(0,1)</span> independent, then <span class="math inline">\sum Z_i^2 \sim \chi^2(p)</span></li>
<li>Used in hypothesis testing and confidence intervals</li>
</ul>
</section>
</section>
</section>
<section id="multivariate-distributions" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="multivariate-distributions"><span class="header-section-number">1.6</span> Multivariate Distributions</h2>
<p>So far we’ve focused on single random variables. But in practice, we often deal with multiple related variables: height and weight, temperature and humidity, stock prices of different companies. This leads us to multivariate distributions.</p>
<section id="joint-distributions" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="joint-distributions"><span class="header-section-number">1.6.1</span> Joint Distributions</h3>
<div class="definition">
<p>For random variables <span class="math inline">X</span> and <span class="math inline">Y</span>, the <strong>joint distribution</strong> describes their behavior together:</p>
<ul>
<li><strong>Discrete case</strong>: Joint PMF <span class="math inline">f_{X,Y}(x,y) = \mathbb{P}(X = x, Y = y)</span></li>
<li><strong>Continuous case</strong>: Joint PDF <span class="math inline">f_{X,Y}(x,y)</span> where <span class="math display">\mathbb{P}((X,Y) \in A) = \iint_A f_{X,Y}(x,y) \, dx \, dy</span></li>
</ul>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Discrete joint distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Roll two fair six-sided dice. Let <span class="math inline">X</span> = first die, <span class="math inline">Y</span> = second die.</p>
<p>The joint PMF is: <span class="math display">f_{X,Y}(x,y) = \frac{1}{36} \text{ for } x,y \in \{1,2,3,4,5,6\}</span></p>
<p>We can display this as a 6×6 table with each entry equal to 1/36.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Continuous joint distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">(X,Y)</span> be uniformly distributed on the unit disk.</p>
<p><span class="math display">f_{X,Y}(x,y) = \begin{cases}
\frac{1}{\pi} &amp; \text{if } x^2 + y^2 \leq 1 \\
0 &amp; \text{otherwise}
\end{cases}</span></p>
<p>The normalizing constant <span class="math inline">1/\pi</span> makes the total probability equal to 1 (area of unit disk is <span class="math inline">\pi</span>).</p>
</div>
</div>
</section>
<section id="marginal-distributions" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="marginal-distributions"><span class="header-section-number">1.6.2</span> Marginal Distributions</h3>
<p>Given a joint distribution, we can find the distribution of each variable separately.</p>
<div class="definition">
<p>The <strong>marginal distribution</strong> of <span class="math inline">X</span> is obtained by “summing out” or “integrating out” the other variable:</p>
<ul>
<li><strong>Discrete</strong>: <span class="math inline">f_X(x) = \sum_y f_{X,Y}(x,y)</span></li>
<li><strong>Continuous</strong>: <span class="math inline">f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dy</span></li>
</ul>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of marginal distributions as projections: if you have points scattered in 2D, the marginal distribution of X is like looking at their shadows on the X-axis.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Sum of two dice
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X</span> = first die, <span class="math inline">Y</span> = second die, <span class="math inline">S = X + Y</span>.</p>
<p>What is <span class="math inline">\mathbb{P}(S = 7)</span>?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To find <span class="math inline">\mathbb{P}(S = 7)</span>, we sum over all ways to get 7:</p>
<ul>
<li><span class="math inline">(1,6)</span>, <span class="math inline">(2,5)</span>, <span class="math inline">(3,4)</span>, <span class="math inline">(4,3)</span>, <span class="math inline">(5,2)</span>, <span class="math inline">(6,1)</span></li>
</ul>
<p>So <span class="math inline">\mathbb{P}(S = 7) = 6 \times \frac{1}{36} = \frac{1}{6}</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="independent-random-variables" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="independent-random-variables"><span class="header-section-number">1.6.3</span> Independent Random Variables</h3>
<div class="definition">
<p>Random variables <span class="math inline">X</span> and <span class="math inline">Y</span> are <strong>independent</strong> if: <span class="math display">f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y)</span> for all <span class="math inline">x, y</span>.</p>
</div>
<p>This means the joint distribution factors into the product of marginals - knowing the value of one variable tells us nothing about the other.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Independent coin flips
</div>
</div>
<div class="callout-body-container callout-body">
<p>Flip two fair coins. Let <span class="math inline">X</span> = 1 if first is heads, 0 otherwise. Same for <span class="math inline">Y</span> with second coin.</p>
<p>Joint distribution:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Y = 0</th>
<th>Y = 1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>X = 0</td>
<td>1/4</td>
<td>1/4</td>
</tr>
<tr class="even">
<td>X = 1</td>
<td>1/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
<p>Since each entry equals the product of marginal probabilities (e.g., <span class="math inline">\frac{1}{4} = \frac{1}{2} \times \frac{1}{2}</span>), <span class="math inline">X</span> and <span class="math inline">Y</span> are independent.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Common mistake</strong>: Assuming uncorrelated means independent.</p>
<p>Independence implies zero correlation, but zero correlation does NOT imply independence! We’ll see counterexamples when we study correlation in Chapter 3.</p>
</div>
</div>
</section>
<section id="conditional-distributions" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="conditional-distributions"><span class="header-section-number">1.6.4</span> Conditional Distributions</h3>
<div class="definition">
<p>The <strong>conditional distribution</strong> of <span class="math inline">X</span> given <span class="math inline">Y = y</span> is:</p>
<ul>
<li><strong>Discrete</strong>: <span class="math inline">f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}</span> if <span class="math inline">f_Y(y) &gt; 0</span></li>
<li><strong>Continuous</strong>: Same formula, interpreted as densities</li>
</ul>
</div>
<p>This tells us how <span class="math inline">X</span> behaves when we know <span class="math inline">Y = y</span>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Quality control
</div>
</div>
<div class="callout-body-container callout-body">
<p>A factory produces items on two machines. Let:</p>
<ul>
<li><span class="math inline">X</span> = quality score (0-100)</li>
<li><span class="math inline">Y</span> = machine (1 or 2)</li>
</ul>
<p>Suppose Machine 1 produces 60% of items with quality <span class="math inline">\sim \mathcal{N}(80, 25)</span>, and Machine 2 produces 40% with quality <span class="math inline">\sim \mathcal{N}(70, 100)</span>.</p>
<p>If we observe a quality score of 75, which machine likely produced it? This requires the conditional distribution <span class="math inline">\mathbb{P}(Y|X=75)</span>.</p>
</div>
</div>
</section>
<section id="interactive-exploration-marginal-and-conditional-distributions" class="level3" data-number="1.6.5">
<h3 data-number="1.6.5" class="anchored" data-anchor-id="interactive-exploration-marginal-and-conditional-distributions"><span class="header-section-number">1.6.5</span> Interactive Exploration: Marginal and Conditional Distributions</h3>
<p>Let’s explore how marginal and conditional distributions relate to a joint distribution using an interactive visualization.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Use the sliders to change the <span class="math inline">x</span> and <span class="math inline">y</span> values</li>
<li>Check the boxes to switch between marginal distributions (e.g., <span class="math inline">f_X(x)</span>) and conditional distributions (e.g., <span class="math inline">f_{X|Y}(x|y)</span>)</li>
<li>When showing conditional distributions, red dashed lines appear on the joint distribution showing where we’re conditioning</li>
<li>The visualization uses the simpler shorthand notation <span class="math inline">p(x)</span> for <span class="math inline">f_X(x)</span> and <span class="math inline">p(x|y)</span> for <span class="math inline">f_{X|Y}(x|y)</span> (and analogous formulas for other pdfs)</li>
</ul>
<div class="cell">
<details class="code-fold hidden">
<summary>Show code</summary>
<div class="sourceCode cell-code hidden" id="cb17" data-startfrom="1684" data-source-offset="-49"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 1683;"><span id="cb17-1684"><a href="#cb17-1684" aria-hidden="true" tabindex="-1"></a>d3 <span class="op">=</span> <span class="pp">require</span>(<span class="st">"d3@7"</span>)</span>
<span id="cb17-1685"><a href="#cb17-1685" aria-hidden="true" tabindex="-1"></a>htl <span class="op">=</span> <span class="pp">require</span>(<span class="st">"htl"</span>)</span>
<span id="cb17-1686"><a href="#cb17-1686" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> { bivariateDemo } <span class="im">from</span> <span class="st">"../js/bivariate-demo.js"</span></span>
<span id="cb17-1687"><a href="#cb17-1687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1688"><a href="#cb17-1688" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize the demo</span></span>
<span id="cb17-1689"><a href="#cb17-1689" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> <span class="fu">bivariateDemo</span>(d3)</span>
<span id="cb17-1690"><a href="#cb17-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1691"><a href="#cb17-1691" aria-hidden="true" tabindex="-1"></a><span class="co">// Define interactive controls</span></span>
<span id="cb17-1692"><a href="#cb17-1692" aria-hidden="true" tabindex="-1"></a>viewof x_value <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"x value"</span>})</span>
<span id="cb17-1693"><a href="#cb17-1693" aria-hidden="true" tabindex="-1"></a>viewof y_value <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">2</span><span class="op">,</span> <span class="dv">4</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"y value"</span>})</span>
<span id="cb17-1694"><a href="#cb17-1694" aria-hidden="true" tabindex="-1"></a>viewof show_conditionals <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">checkbox</span>([<span class="st">"p(x|y)"</span><span class="op">,</span> <span class="st">"p(y|x)"</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> []<span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Show conditionals"</span>})</span>
<span id="cb17-1695"><a href="#cb17-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1696"><a href="#cb17-1696" aria-hidden="true" tabindex="-1"></a><span class="co">// This block will be the output of the cell.</span></span>
<span id="cb17-1697"><a href="#cb17-1697" aria-hidden="true" tabindex="-1"></a><span class="co">// It lays out ONLY the plot, but it will still react to the controls above.</span></span>
<span id="cb17-1698"><a href="#cb17-1698" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb17-1699"><a href="#cb17-1699" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> plot <span class="op">=</span> demo<span class="op">.</span><span class="fu">createVisualization</span>(</span>
<span id="cb17-1700"><a href="#cb17-1700" aria-hidden="true" tabindex="-1"></a>    x_value<span class="op">,</span></span>
<span id="cb17-1701"><a href="#cb17-1701" aria-hidden="true" tabindex="-1"></a>    y_value<span class="op">,</span></span>
<span id="cb17-1702"><a href="#cb17-1702" aria-hidden="true" tabindex="-1"></a>    show_conditionals<span class="op">.</span><span class="fu">includes</span>(<span class="st">"p(x|y)"</span>)<span class="op">,</span></span>
<span id="cb17-1703"><a href="#cb17-1703" aria-hidden="true" tabindex="-1"></a>    show_conditionals<span class="op">.</span><span class="fu">includes</span>(<span class="st">"p(y|x)"</span>)</span>
<span id="cb17-1704"><a href="#cb17-1704" aria-hidden="true" tabindex="-1"></a>  )<span class="op">;</span></span>
<span id="cb17-1705"><a href="#cb17-1705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1706"><a href="#cb17-1706" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Return ONLY the plot element. The controls will be hidden but still work.</span></span>
<span id="cb17-1707"><a href="#cb17-1707" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> plot<span class="op">;</span></span>
<span id="cb17-1708"><a href="#cb17-1708" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p><strong>Key insights:</strong></p>
<ul>
<li><strong>Marginal distributions</strong> show the overall distribution of one variable, ignoring the other</li>
<li><strong>Conditional distributions</strong> show how one variable is distributed when we fix the other at a specific value</li>
<li>The shape of conditional distributions changes as we move the conditioning value</li>
<li>This demonstrates how knowing one variable’s value provides information about the other when they’re not independent</li>
</ul>
</section>
<section id="random-vectors-and-iid-random-variables" class="level3" data-number="1.6.6">
<h3 data-number="1.6.6" class="anchored" data-anchor-id="random-vectors-and-iid-random-variables"><span class="header-section-number">1.6.6</span> Random Vectors and IID Random Variables</h3>
<div class="definition">
<p>A <strong>random vector</strong> is a vector <span class="math inline">\mathbf{X} = (X_1, X_2, ..., X_n)^T</span> where each component <span class="math inline">X_i</span> is a random variable. The joint behavior of all components is characterized by their joint distribution.</p>
</div>
<p>Random vectors allow us to study multiple random quantities together, which leads us to an important special case.</p>
<p><strong>IID Random Variables:</strong></p>
<div class="definition">
<p>Random variables <span class="math inline">X_1, ..., X_n</span> are <strong>independent and identically distributed (IID)</strong> if:</p>
<ol type="1">
<li>They are mutually independent</li>
<li>They all have the same distribution</li>
</ol>
<p>We write: <span class="math inline">X_1, ..., X_n \stackrel{iid}{\sim} F</span>.</p>
<p>If <span class="math inline">F</span> has density <span class="math inline">f</span> we also write <span class="math inline">X_1, ..., X_n \stackrel{iid}{\sim} f</span>.</p>
<p><span class="math inline">X_1, ..., X_n</span> is a <strong>random sample of size <span class="math inline">n</span></strong> from <span class="math inline">F</span> (or <span class="math inline">f</span>, respectively).</p>
</div>
<p>IID assumptions are fundamental in statistics:</p>
<ul>
<li><strong>Random sampling</strong>: Each observation comes from the same population</li>
<li><strong>No interference</strong>: One observation doesn’t affect others</li>
<li><strong>Stable conditions</strong>: The underlying distribution doesn’t change</li>
</ul>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Customer arrivals
</div>
</div>
<div class="callout-body-container callout-body">
<p>Times between customer arrivals at a stable business might be IID Exponential(<span class="math inline">\beta</span>).</p>
<p><strong>Not IID</strong>:</p>
<ul>
<li>Stock prices (today’s price depends on yesterday’s)</li>
<li>Temperature readings (temporal correlation)</li>
<li>Survey responses from same household (likely correlated)</li>
</ul>
</div>
</div>
</section>
<section id="important-multivariate-distributions" class="level3" data-number="1.6.7">
<h3 data-number="1.6.7" class="anchored" data-anchor-id="important-multivariate-distributions"><span class="header-section-number">1.6.7</span> Important Multivariate Distributions</h3>
<section id="multinomial-distribution" class="level4">
<h4 class="anchored" data-anchor-id="multinomial-distribution">Multinomial Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a> is a generalization of binomial to multiple categories.</p>
<div class="definition">
<p>If we have <span class="math inline">k</span> categories with probabilities <span class="math inline">p_1, ..., p_k</span> (summing to 1), and we observe <span class="math inline">n</span> independent trials, then the counts <span class="math inline">(X_1, ..., X_k)</span> follow a <strong>Multinomial</strong> distribution:</p>
<p><span class="math display">f(x_1, ..., x_k) = \frac{n!}{x_1! \cdots x_k!} p_1^{x_1} \cdots p_k^{x_k}</span></p>
<p>where <span class="math inline">\sum x_i = n</span>.</p>
</div>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Dice rolls (6 categories for a standard die – or <span class="math inline">k</span> for <span class="math inline">k</span>-sided dice)</li>
<li>Survey responses (multiple choice)</li>
<li>Document word counts</li>
<li>Genetic allele frequencies</li>
</ul>
</section>
<section id="multivariate-normal-distribution" class="level4">
<h4 class="anchored" data-anchor-id="multivariate-normal-distribution">Multivariate Normal Distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal distribution</a> is the multivariate generalization of the normal distribution.</p>
<div class="definition">
<p>A random vector <span class="math inline">\mathbf{X} = (X_1, ..., X_k)^T</span> has a <strong>multivariate normal</strong> distribution, written <span class="math inline">\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span>, if:</p>
<p><span class="math display">f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\boldsymbol{\mu}</span> is the mean vector</li>
<li><span class="math inline">\boldsymbol{\Sigma}</span> is the covariance matrix (symmetric, positive definite)</li>
</ul>
</div>
<p><strong>Key properties</strong>:</p>
<ul>
<li>Marginals are normal: If <span class="math inline">\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span>, then <span class="math inline">X_i \sim \mathcal{N}(\mu_i, \Sigma_{ii})</span></li>
<li>Linear combinations are normal: <span class="math inline">\mathbf{a}^T\mathbf{X} \sim \mathcal{N}(\mathbf{a}^T\boldsymbol{\mu}, \mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a})</span></li>
<li>Conditional distributions are normal (with formulas for conditional mean and variance)</li>
</ul>
<p><strong>Special case - Bivariate normal</strong>: For two variables with correlation <span class="math inline">\rho</span>: <span class="math display">\boldsymbol{\Sigma} = \begin{pmatrix} \sigma_1^2 &amp; \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 &amp; \sigma_2^2 \end{pmatrix}</span></p>
<p>The correlation <span class="math inline">\rho</span> controls the relationship:</p>
<ul>
<li><span class="math inline">\rho = 0</span>: independent (for normal variables, uncorrelated = independent!)</li>
<li><span class="math inline">\rho &gt; 0</span>: positive relationship</li>
<li><span class="math inline">\rho &lt; 0</span>: negative relationship</li>
</ul>
<div id="23933f9c" class="cell" data-fig-height="6" data-fig-width="7" data-execution_count="15">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bivariate normal distribution visualization</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Bivariate normal with correlation</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">0.7</span>], [<span class="fl">0.7</span>, <span class="dv">1</span>]]  <span class="co"># correlation = 0.7</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> np.dstack((X, Y))</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate PDF</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> multivariate_normal(mean, cov)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> rv.pdf(pos)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Contour plot</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>ax.clabel(contour, inline<span class="op">=</span><span class="va">True</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>contourf <span class="op">=</span> ax.contourf(X, Y, Z, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>fig.colorbar(contourf, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'Density'</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add marginal indicators</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Y'</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Bivariate Normal Distribution (ρ = 0.7)'</span>)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Example calculations</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bivariate Normal with ρ = 0.7:"</span>)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = Var(Y) = 1"</span>)</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cov(X,Y) = ρ·σ_X·σ_Y = 0.7"</span>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"If we observe Y=1, then:"</span>)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  E[X|Y=1] = ρ·(Y-μ_Y) = 0.7"</span>)</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X|Y=1) = (1-ρ²) = </span><span class="sc">{</span><span class="dv">1</span> <span class="op">-</span> <span class="fl">0.7</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01-probability-foundations_files/figure-html/cell-16-output-1.png" width="589" height="497" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Bivariate Normal with ρ = 0.7:
Var(X) = Var(Y) = 1
Cov(X,Y) = ρ·σ_X·σ_Y = 0.7
If we observe Y=1, then:
  E[X|Y=1] = ρ·(Y-μ_Y) = 0.7
  Var(X|Y=1) = (1-ρ²) = 0.51</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multivariate normal distribution is central to many statistical methods. We will return to it in more detail in Chapter 2 when we discuss expectations, covariances, and the properties of linear combinations of random variables.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-40-contents" aria-controls="callout-40" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Transformations of Random Variables
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-40" class="callout-40-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We often define variables that are <a href="https://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables">transformations</a> <span class="math inline">g(\cdot)</span> of other random variables. Assuming we know the distribution of <span class="math inline">X</span> or <span class="math inline">(X, Y)</span>, how do we find the distribution of <span class="math inline">Y = g(X)</span> or <span class="math inline">(U,V) = g(X,Y)</span>?</p>
<p><strong>Method 1: CDF technique</strong></p>
<ol type="1">
<li>Find the CDF: <span class="math inline">F_Y(y) = \mathbb{P}(Y \leq y) = \mathbb{P}(g(X) \leq y)</span></li>
<li>Differentiate to get PDF: <span class="math inline">f_Y(y) = F_Y'(y)</span></li>
</ol>
<p><strong>Method 2: Jacobian method (for bijective transformations)</strong></p>
<p>If <span class="math inline">(U,V) = g(X,Y)</span> is one-to-one with inverse <span class="math inline">(X,Y) = h(U,V)</span>, then: <span class="math display">f_{U,V}(u,v) = f_{X,Y}(h(u,v)) \cdot |J|</span></p>
<p>where <span class="math inline">J</span> is the Jacobian determinant: <span class="math display">J = \det\begin{pmatrix} \frac{\partial x}{\partial u} &amp; \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} &amp; \frac{\partial y}{\partial v} \end{pmatrix}</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Box-Muller transform
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can generate Gaussian (normal) distributed random numbers starting from uniform.</p>
<p>If <span class="math inline">U_1, U_2 \sim \text{Uniform}(0,1)</span> independently, then:</p>
<ul>
<li><span class="math inline">X = \sqrt{-2\ln U_1} \cos(2\pi U_2)</span></li>
<li><span class="math inline">Y = \sqrt{-2\ln U_1} \sin(2\pi U_2)</span></li>
</ul>
<p>are independent <span class="math inline">\mathcal{N}(0,1)</span> random variables!</p>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">1.7</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">1.7.1</span> Key Concepts Review</h3>
<p>We’ve built up probability theory from its foundations:</p>
<ol type="1">
<li><strong>Sample spaces and events</strong> provide the basic framework for describing uncertainty</li>
<li><strong>Probability axioms</strong> give us consistent rules for quantifying uncertainty</li>
<li><strong>Independence and conditioning</strong> let us model relationships between events</li>
<li><strong>Random variables</strong> connect probability to numerical quantities we can analyze</li>
<li><strong>Distributions</strong> characterize the behavior of random variables</li>
<li><strong>Multivariate distributions</strong> handle multiple related random quantities</li>
</ol>
</section>
<section id="why-these-concepts-matter" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" class="anchored" data-anchor-id="why-these-concepts-matter"><span class="header-section-number">1.7.2</span> Why These Concepts Matter</h3>
<p><strong>For Statistical Inference</strong>:</p>
<ul>
<li>Random variables let us model data mathematically</li>
<li>Distributions provide templates for common patterns</li>
<li>Independence assumptions simplify analysis</li>
<li>Conditioning lets us update beliefs with data</li>
</ul>
<p><strong>For Machine Learning</strong>:</p>
<ul>
<li>Probability distributions model uncertainty in predictions</li>
<li>Bayes’ theorem enables Bayesian ML methods</li>
<li>Multivariate distributions handle high-dimensional data</li>
<li>Independence assumptions make computation tractable</li>
</ul>
<p><strong>For Data Science Practice</strong>:</p>
<ul>
<li>Understanding distributions helps choose appropriate methods</li>
<li>Recognizing dependence prevents incorrect analyses</li>
<li>Conditional probability quantifies relationships in data</li>
<li>Simulation using these distributions validates methods</li>
</ul>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="1.7.3">
<h3 data-number="1.7.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">1.7.3</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><strong>Confusing <span class="math inline">\mathbb{P}(A|B)</span> with <span class="math inline">\mathbb{P}(B|A)</span></strong> - These can be vastly different!</li>
<li><strong>Assuming independence without justification</strong> - Real-world variables are often dependent</li>
<li><strong>Misinterpreting PDFs as probabilities</strong> - PDFs are densities, not probabilities</li>
<li><strong>Forgetting <span class="math inline">\mathbb{P}(X = x) = 0</span> for continuous variables</strong> - Use intervals for continuous RVs</li>
<li><strong>Thinking disjoint means independent</strong> - Disjoint events are maximally dependent!</li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="1.7.4">
<h3 data-number="1.7.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">1.7.4</span> Chapter Connections</h3>
<p>The probability foundations from this chapter provide the mathematical language for all of statistics:</p>
<ul>
<li><strong>Next - Chapter 2 (Expectation)</strong>: Building on our introduction to random variables, we’ll explore expectation as a fundamental tool for summarizing distributions, including variance and the powerful linearity of expectation property</li>
<li><strong>Chapter 3 (Convergence &amp; Inference)</strong>: Using the probability framework and IID concept from this chapter, we’ll prove the Law of Large Numbers and Central Limit Theorem—the theoretical foundations that justify using samples to learn about populations</li>
<li><strong>Chapter 4 (Bootstrap)</strong>: Apply our understanding of empirical distributions to develop computational methods for quantifying uncertainty, providing a modern alternative to traditional parametric approaches</li>
</ul>
</section>
<section id="self-test-problems" class="level3" data-number="1.7.5">
<h3 data-number="1.7.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">1.7.5</span> Self-Test Problems</h3>
<p>Try to answer these questions after reading these lecture notes.</p>
<ol type="1">
<li><p><strong>Bayes in action</strong>: A test for a disease has 95% sensitivity (true positive rate) and 98% specificity (true negative rate). If 0.1% of the population has the disease, what’s the probability someone with a positive test actually has the disease?</p></li>
<li><p><strong>Distribution identification</strong>: Times between earthquakes in a region average 50 days. What distribution would you use to model the time until the next earthquake? Why?</p></li>
<li><p><strong>Independence check</strong>: You roll two dice. Let A = “sum is even” and B = “first die shows 3”. Are A and B independent?</p></li>
<li><p><strong>Conditional expectation preview</strong>: In a factory, Machine 1 makes 70% of products with defect rate 2%. Machine 2 makes 30% with defect rate 5%. If a product is defective, what’s the probability it came from Machine 1?</p></li>
</ol>
</section>
<section id="connections-to-source-material" class="level3" data-number="1.7.6">
<h3 data-number="1.7.6" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">1.7.6</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-41-contents" aria-controls="callout-41" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-41" class="callout-41-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This table maps sections in these lecture notes to the corresponding sections in <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> (“All of Statistics” or AoS).</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding AoS Section(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Why Do We Need Statistics?</strong></td>
<td style="text-align: left;">Expanded material from the slides, contextualizing statistics for data science.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Foundations of Probability</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Sample Spaces and Events</td>
<td style="text-align: left;">AoS §1.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Probability Axioms</td>
<td style="text-align: left;">AoS §1.3 (Definition 1.5)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Interpretations of Probability</td>
<td style="text-align: left;">AoS §1.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Finite Sample Spaces &amp; Counting</td>
<td style="text-align: left;">AoS §1.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Independence and Conditional Probability</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Independent Events</td>
<td style="text-align: left;">AoS §1.5 (Definition 1.9)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Conditional Probability</td>
<td style="text-align: left;">AoS §1.6 (Definition 1.12)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Bayes’ Theorem &amp; Law of Total Probability</td>
<td style="text-align: left;">AoS §1.7 (Theorems 1.16, 1.17)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Random Variables</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Definition and Intuition</td>
<td style="text-align: left;">AoS §2.1 (Definition 2.1)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ CDF, PMF, and PDF</td>
<td style="text-align: left;">AoS §2.2 (Definitions 2.5, 2.9, 2.11)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Core Discrete Distributions</td>
<td style="text-align: left;">AoS §2.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Core Continuous Distributions</td>
<td style="text-align: left;">AoS §2.4</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Multivariate Distributions</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Joint Distributions</td>
<td style="text-align: left;">AoS §2.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Marginal Distributions</td>
<td style="text-align: left;">AoS §2.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Independent Random Variables</td>
<td style="text-align: left;">AoS §2.7 (Definition 2.29)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Conditional Distributions</td>
<td style="text-align: left;">AoS §2.8 (Definitions 2.35, 2.36)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Random Vectors and IID Samples</td>
<td style="text-align: left;">AoS §2.9 (Definition 2.41)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Important Multivariate Distributions</td>
<td style="text-align: left;">AoS §2.10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Transformations of Random Variables</td>
<td style="text-align: left;">AoS §2.11, §2.12</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Chapter Summary and Connections</strong></td>
<td style="text-align: left;">New summary material.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-reading" class="level3" data-number="1.7.7">
<h3 data-number="1.7.7" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">1.7.7</span> Further Reading</h3>
<ul>
<li><strong>Probability Theory</strong>: Ross, “A First Course in Probability” - accessible introduction</li>
<li><strong>Mathematical Statistics</strong>: Casella &amp; Berger, “Statistical Inference” - rigorous treatment</li>
<li><strong>Bayesian Perspective</strong>: Gelman et al., “Bayesian Data Analysis” - modern Bayesian view</li>
<li><strong>Computational Approach</strong>: Blitzstein &amp; Hwang, “Introduction to Probability” - simulation-based</li>
</ul>
</section>
<section id="python-and-r-reference" class="level3" data-number="1.7.8">
<h3 data-number="1.7.8" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">1.7.8</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255584-894-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-894-1" role="tab" aria-controls="tabset-1757255584-894-1" aria-selected="true" href="">Python Code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255584-894-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255584-894-2" role="tab" aria-controls="tabset-1757255584-894-2" aria-selected="false" href="">R Code</a></li></ul><div class="tab-content"><div id="tabset-1757255584-894-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255584-894-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability distributions in Python</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete distributions</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>stats.binom.pmf(x, n<span class="op">=</span>n, p<span class="op">=</span>p)           <span class="co"># Binomial PMF</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>stats.binom.cdf(x, n<span class="op">=</span>n, p<span class="op">=</span>p)           <span class="co"># Binomial CDF</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>stats.binom.rvs(n<span class="op">=</span>n, p<span class="op">=</span>p, size<span class="op">=</span>size)   <span class="co"># Generate random binomial</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>stats.poisson.pmf(x, mu<span class="op">=</span>lam)           <span class="co"># Poisson PMF</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>stats.poisson.cdf(x, mu<span class="op">=</span>lam)           <span class="co"># Poisson CDF</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>stats.poisson.rvs(mu<span class="op">=</span>lam, size<span class="op">=</span>size)   <span class="co"># Generate random Poisson</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous distributions  </span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>stats.norm.pdf(x, loc<span class="op">=</span>mean, scale<span class="op">=</span>sd)   <span class="co"># Normal PDF</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>stats.norm.cdf(x, loc<span class="op">=</span>mean, scale<span class="op">=</span>sd)   <span class="co"># Normal CDF</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>stats.norm.rvs(loc<span class="op">=</span>mean, scale<span class="op">=</span>sd, size<span class="op">=</span>size) <span class="co"># Generate random normal</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>stats.expon.pdf(x, scale<span class="op">=</span>beta)          <span class="co"># Exponential PDF</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>stats.expon.cdf(x, scale<span class="op">=</span>beta)          <span class="co"># Exponential CDF</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>stats.expon.rvs(scale<span class="op">=</span>beta, size<span class="op">=</span>size)  <span class="co"># Generate random exponential</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Multivariate normal</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>stats.multivariate_normal.rvs(mean, cov, size<span class="op">=</span>size)  <span class="co"># Generate</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>stats.multivariate_normal.pdf(x, mean, cov)          <span class="co"># Density</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div data-__quarto_custom="true" data-__quarto_custom_type="Callout" data-__quarto_custom_context="Block" data-__quarto_custom_id="45">
<div data-__quarto_custom_scaffold="true">

</div>
<div data-__quarto_custom_scaffold="true">
<p><strong>Note on <code>lambda</code> parameter</strong>: In the Python
code, we used <code>lam</code> instead of <code>lambda</code>
(<span class="math inline">\(\lambda\)</span>) for the Poisson
distribution parameter because <code>lambda</code> is a reserved keyword
in Python (used for anonymous functions). Using <code>lam</code> (or
<code>lamb</code>) in Python is a common convention to avoid syntax
errors.</p>
</div>
</div></div><div id="tabset-1757255584-894-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255584-894-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability distributions in R</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete distributions</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(x, <span class="at">size=</span>n, <span class="at">prob=</span>p)     <span class="co"># Binomial PMF</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(x, <span class="at">size=</span>n, <span class="at">prob=</span>p)     <span class="co"># Binomial CDF</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span>(n, size, prob)          <span class="co"># Generate random binomial</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(x, lambda)               <span class="co"># Poisson PMF</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(x, lambda)               <span class="co"># Poisson CDF  </span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rpois</span>(n, lambda)               <span class="co"># Generate random Poisson</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous distributions</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(x, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)         <span class="co"># Normal PDF</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(x, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)         <span class="co"># Normal CDF</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(n, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)         <span class="co"># Generate random normal</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">dexp</span>(x, <span class="at">rate=</span><span class="dv">1</span><span class="sc">/</span>beta)           <span class="co"># Exponential PDF</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">pexp</span>(x, <span class="at">rate=</span><span class="dv">1</span><span class="sc">/</span>beta)           <span class="co"># Exponential CDF</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">rexp</span>(n, <span class="at">rate=</span><span class="dv">1</span><span class="sc">/</span>beta)           <span class="co"># Generate random exponential</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Multivariate normal</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu">rmvnorm</span>(n, mean, sigma)        <span class="co"># Generate multivariate normal</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">dmvnorm</span>(x, mean, sigma)        <span class="co"># Multivariate normal density</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<hr>
<p><em>Remember: Probability is the language of uncertainty. Master this language, and you’ll be able to express and analyze uncertainty in any domain.</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-breiman2001statistical" class="csl-entry" role="listitem">
Breiman, Leo. 2001. <span>“Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).”</span> <em>Statistical Science</em> 16 (3): 199–231.
</div>
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>More correctly, LLMs predict <em>tokens</em>, which are parts of words and other characters.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The name “binomial” comes from its appearance in the binomial theorem: <span class="math inline">(x+y)^n = \sum_{k=0}^{n} \binom{n}{k} x^k y^{n-k}</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In modern LLMs, the categorical distribution is over tokens (parts of words), not full words. The token vocabulary can be huge - tens of thousands of different tokens like “a”, “aba”, “add”, etc. GPT models typically use vocabularies of 50,000-100,000 tokens.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6IlxuLy8gSW1wb3J0IEQzLCBodGwsIGFuZCBvdXIgdmlzdWFsaXphdGlvbiBtb2R1bGVcbmQzID0gcmVxdWlyZShcImQzQDdcIilcbmh0bCA9IHJlcXVpcmUoXCJodGxcIilcbmltcG9ydCB7IGJpdmFyaWF0ZURlbW8gfSBmcm9tIFwiLi4vanMvYml2YXJpYXRlLWRlbW8uanNcIlxuXG4vLyBJbml0aWFsaXplIHRoZSBkZW1vXG5kZW1vID0gYml2YXJpYXRlRGVtbyhkMylcblxuLy8gRGVmaW5lIGludGVyYWN0aXZlIGNvbnRyb2xzXG52aWV3b2YgeF92YWx1ZSA9IElucHV0cy5yYW5nZShbLTIsIDJdLCB7c3RlcDogMC4xLCB2YWx1ZTogMCwgbGFiZWw6IFwieCB2YWx1ZVwifSlcbnZpZXdvZiB5X3ZhbHVlID0gSW5wdXRzLnJhbmdlKFstMiwgNF0sIHtzdGVwOiAwLjEsIHZhbHVlOiAxLCBsYWJlbDogXCJ5IHZhbHVlXCJ9KVxudmlld29mIHNob3dfY29uZGl0aW9uYWxzID0gSW5wdXRzLmNoZWNrYm94KFtcInAoeHx5KVwiLCBcInAoeXx4KVwiXSwge3ZhbHVlOiBbXSwgbGFiZWw6IFwiU2hvdyBjb25kaXRpb25hbHNcIn0pXG5cbi8vIFRoaXMgYmxvY2sgd2lsbCBiZSB0aGUgb3V0cHV0IG9mIHRoZSBjZWxsLlxuLy8gSXQgbGF5cyBvdXQgT05MWSB0aGUgcGxvdCwgYnV0IGl0IHdpbGwgc3RpbGwgcmVhY3QgdG8gdGhlIGNvbnRyb2xzIGFib3ZlLlxue1xuICBjb25zdCBwbG90ID0gZGVtby5jcmVhdGVWaXN1YWxpemF0aW9uKFxuICAgIHhfdmFsdWUsXG4gICAgeV92YWx1ZSxcbiAgICBzaG93X2NvbmRpdGlvbmFscy5pbmNsdWRlcyhcInAoeHx5KVwiKSxcbiAgICBzaG93X2NvbmRpdGlvbmFscy5pbmNsdWRlcyhcInAoeXx4KVwiKVxuICApO1xuXG4gIC8vIFJldHVybiBPTkxZIHRoZSBwbG90IGVsZW1lbnQuIFRoZSBjb250cm9scyB3aWxsIGJlIGhpZGRlbiBidXQgc3RpbGwgd29yay5cbiAgcmV0dXJuIHBsb3Q7XG59XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ3hfdmFsdWUnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgneV92YWx1ZScpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdzaG93X2NvbmRpdGlvbmFscycpIn1dfQ==
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../chapters";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/02-expectation.html" class="pagination-link" aria-label="Expectation">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Probability Foundations</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain the role of probability in data science and statistical modeling.</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply fundamental probability concepts, including sample spaces, events, and the axioms of probability.</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate probabilities using independence, conditional probability, and Bayes' theorem.</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Distinguish between discrete and continuous random variables and use their distribution functions (PMF, PDF, CDF).</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Describe and apply key univariate and multivariate distributions (e.g., Binomial, Normal, Multinomial).</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>This chapter covers fundamental probability concepts essential for statistical inference. The material is adapted and expanded from Chapters 1 and 2 of @wasserman2013all, which interested readers are encouraged to consult directly for a more rigorous and comprehensive treatment.</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Do We Need Statistics?</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistics and Machine Learning in Data Science</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>Machine learning (ML) has revolutionized our ability to make predictions. Given enough training data, modern ML models can achieve remarkable accuracy on unseen data that resembles what they've been trained on. But there's a crucial limitation: these models excel when the new data comes from the same distribution as the training data.</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>How do we move beyond this constraint to make reliable predictions in the real world, where conditions change and data can be messy, incomplete, or collected differently than our training set?</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>This is where statistics becomes essential. Statistics provides the tools to:</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Understand principles of data collection**: How was the data gathered? What biases might exist? </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Plan data collection strategically**: Design experiments and surveys that yield meaningful insights</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deal with missing data**: Real-world data is rarely complete - we need principled ways to handle gaps</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Understand causality in modeling**: Correlation isn't causation, and confounding variables can mislead us</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>Without these statistical foundations, even the most sophisticated ML models can fail spectacularly when deployed in practice.</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### Case Study: IBM Watson Health</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>The story of IBM Watson Health illustrates why statistical thinking is crucial for real-world AI applications.</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>In 2011, IBM Watson made headlines by defeating human champions on the quiz show *Jeopardy!* This victory showcased the power of natural language processing and knowledge retrieval. IBM saw an opportunity: if Watson could master general knowledge, why not train it to be a doctor?</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>Watson Health launched in 2015 with an ambitious goal: use data from top US hospitals to train an AI system that could diagnose and treat patients anywhere in the world. The vision was compelling - bring world-class medical expertise to underserved areas through AI.</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>Over the years, IBM:</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Spent over $4 billion on acquisitions</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Employed 7,000 people developing the system  </span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Partnered with prestigious medical institutions</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>Yet by 2022, IBM sold Watson Health's data and assets for just $1 billion - a massive loss. What went wrong?</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>The fundamental issue was **data representativeness**. Watson Health was trained on data from elite US hospitals treating specific patient populations. But this data didn't represent:</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Different healthcare systems and practices globally</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Diverse patient populations with varying genetics, lifestyles, and environmental factors</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Resource constraints in different settings</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variations in how medical data is recorded and coded</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>This failure wasn't due to inadequate machine learning algorithms - it was a failure to apply statistical thinking about data collection, representation, and generalization. No amount of computational power can overcome fundamentally biased or unrepresentative data.</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>Read more <span class="co">[</span><span class="ot">in this Slate article</span><span class="co">](https://slate.com/technology/2022/01/ibm-watson-health-failure-artificial-intelligence.html)</span>.</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Two Cultures of Statistical Modeling</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>In his influential essay @breiman2001statistical, statistician Leo Breiman identified two distinct approaches to statistical modeling, each with different goals and philosophies. These are often cast as the two approaches of **prediction** vs. **explanation**.</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>| Feature | The Algorithmic Modeling Culture | The Data Modeling Culture |</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>| :--- | :--- | :--- |</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>| **Goal** | Accurate prediction | Understanding mechanisms |</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>| **Approach** | Treat the mapping from inputs to outputs as a black box | Specify interpretable models that represent how nature works |</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>| **Validation** | Predictive accuracy on held-out data | Statistical tests, confidence intervals, model diagnostics |</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>| **Philosophy** | "It doesn't matter how it works, as long as it works" | "We need to understand which factors matter and why" |</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>| **Examples** | Deep learning, random forests, boosting | Linear regression, causal inference, experimental design |</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>Think of these two cultures like different approaches to cooking:</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>The **algorithmic approach** is like following a top-rated recipe - you don't know why you fold (not stir) the batter or why ingredients must be room temperature, but following the steps precisely often produces better results than many trained cooks achieve. You can pick 5-star recipes and succeed without any cooking knowledge.</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>The **data modeling approach** is like understanding food science - you know about Maillard reactions, gluten development, and emulsification. But translating this into a great dish is slow and complex. You might spend hours calculating optimal ratios only to produce something inferior to what a simple recipe would have given you.</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>This creates a fundamental tension: The recipe follower often produces better food faster. The food scientist understands why things work and with time might produce a breakthrough - but may struggle to match the empirical success of well-tested recipes. In machine learning, this same tension exists - a neural network might predict customer behavior better than any theory-based model, even if we don't understand why. Sometimes, letting algorithms find patterns empirically works better than imposing our theoretical understanding. However, understanding often gives us an edge to build better algorithms and generalize to novel scenarios.</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>Formally, both cultures can be seen as addressing the problem of characterizing a mapping:</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>$$X \rightarrow Y$$</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>where $X$ represents **input** features and $Y$ represents the **response**.</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>**Algorithmic approach (example)**: Find a function $\hat{f}$ that minimizes prediction error. A common approach is to find the function that minimizes the **mean squared error (MSE)**,</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a>$$\text{MSE} = \frac{1}{N} \sum_{n=1}^N (Y_n - \hat{f}(X_n))^2$$</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>that is make the squared difference between the actual outcome $Y_n$ and the prediction $\hat{f}(X_n)$ as small as possible over the available training data. In practice, we often report the **root mean squared error (RMSE)** = $\sqrt{\text{MSE}}$, which has the same units as $Y$. We don't care about what the function $\hat{f}$ looks like, as long as it minimizes this error.</span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>**Data modeling approach (example)**: Build a mechanistic model $Y = f(X; \theta) + \epsilon$ where:</span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f$ has a specific, interpretable form</span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta$ are parameters with scientific, interpretable meaning</span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\epsilon$ represents random error</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a>While fitting the model to data often still involves optimizing some objective, the goal here is to study the best-fitting parameters $\theta$, or find the best model $f$ among a set of competing hypotheses.</span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>We compare here the two approaches represented by a <span class="co">[</span><span class="ot">random forest</span><span class="co">](https://en.wikipedia.org/wiki/Random_forest)</span> (RF) model and a <span class="co">[</span><span class="ot">linear regression</span><span class="co">](https://en.wikipedia.org/wiki/Linear_regression)</span> model.</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>The former represents a traditional machine learning approach, while the latter is a staple of statistical modelling.</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>A trained random forest model is harder to interpret, hence falls in the "algorithmic" camp for the purpose of this example. Conversely, a fitted linear regression model yields interpretable *weights* which directly tell us how the features linearly affect the response, so it represents the data modeling camp.</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparing algorithmic vs data modeling approaches</span></span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a><span class="co"># Load synthetic housing price data with complex patterns</span></span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a><span class="co"># File available here: </span></span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a><span class="co"># https://raw.githubusercontent.com/lacerbi/stats-for-ds-website/refs/heads/main/data/housing_prices.csv</span></span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../data/housing_prices.csv'</span>)</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare features and target</span></span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'size_sqft'</span>, <span class="st">'bedrooms'</span>, <span class="st">'age_years'</span>, <span class="st">'location_score'</span>, </span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>            <span class="st">'garage_spaces'</span>, <span class="st">'has_pool'</span>, <span class="st">'crime_rate'</span>, <span class="st">'school_rating'</span>]</span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[features]</span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'price'</span>]</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset: </span><span class="sc">{</span>X<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">:,}</span><span class="ss"> houses with </span><span class="sc">{</span>X<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features"</span>)</span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training on </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">:,}</span><span class="ss"> houses, testing on </span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Average house price: €</span><span class="sc">{</span>y<span class="sc">.</span>mean()<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Price standard deviation: €</span><span class="sc">{</span>y<span class="sc">.</span>std()<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a><span class="co"># ALGORITHMIC APPROACH: Random Forest</span></span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a>rf_predictions <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a>rf_rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, rf_predictions))</span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== ALGORITHMIC APPROACH (Random Forest) ==="</span>)</span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Root Mean Squared Error (RMSE): €</span><span class="sc">{</span>rf_rmse<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Importances:"</span>)</span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature, importance <span class="kw">in</span> <span class="bu">zip</span>(features, rf_model.feature_importances_):</span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>importance<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a><span class="co"># DATA MODELING APPROACH: Linear Regression</span></span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Add constant term for intercept</span></span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a>X_train_lm <span class="op">=</span> sm.add_constant(X_train)</span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a>X_test_lm <span class="op">=</span> sm.add_constant(X_test)</span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear model</span></span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a>lm_model <span class="op">=</span> sm.OLS(y_train, X_train_lm)</span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a>lm_results <span class="op">=</span> lm_model.fit()</span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a>lm_predictions <span class="op">=</span> lm_results.predict(X_test_lm)</span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a>lm_rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, lm_predictions))</span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== DATA MODELING APPROACH (Linear Regression) ==="</span>)</span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Root Mean Squared Error (RMSE): €</span><span class="sc">{</span>lm_rmse<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a><span class="co"># Show interpretable coefficients</span></span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Linear Model Coefficients:"</span>)</span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a>coef_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: [<span class="st">'Intercept'</span>] <span class="op">+</span> features,</span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Coefficient'</span>: lm_results.params,</span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Std Error'</span>: lm_results.bse,</span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a>    <span class="st">'P-value'</span>: lm_results.pvalues</span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a>coef_df[<span class="st">'Significant'</span>] <span class="op">=</span> coef_df[<span class="st">'P-value'</span>] <span class="op">&lt;</span> <span class="fl">0.05</span></span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef_df.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== INTERPRETATION ==="</span>)</span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear model suggests:"</span>)</span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, feature <span class="kw">in</span> <span class="bu">enumerate</span>(features):</span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a>    coef <span class="op">=</span> lm_results.params[i<span class="op">+</span><span class="dv">1</span>]  <span class="co"># +1 to skip intercept</span></span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">abs</span>(coef) <span class="op">&gt;</span> <span class="dv">100</span>:</span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"- Each unit increase in </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: €</span><span class="sc">{</span>coef<span class="sc">:,.0f}</span><span class="ss"> change in price"</span>)</span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">BUT: The model performs (slightly) worse than Random Forest."</span>)</span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RF RMSE: €</span><span class="sc">{</span>rf_rmse<span class="sc">:,.0f}</span><span class="ss"> vs Linear RMSE: €</span><span class="sc">{</span>lm_rmse<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"That's €</span><span class="sc">{</span>lm_rmse <span class="op">-</span> rf_rmse<span class="sc">:,.0f}</span><span class="ss"> worse prediction error."</span>)</span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Should we care more about prediction or understanding?"</span>)</span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a>Both approaches have their place in modern data science. The **algorithmic culture** has driven breakthroughs in areas like computer vision and natural language processing, where prediction accuracy is paramount. For example, large language models (LLMs) are massively large deep neural networks (pre)trained with the extremely simple objective of just "predicting the next word"^<span class="co">[</span><span class="ot">More correctly, LLMs predict *tokens*, which are parts of words and other characters.</span><span class="co">]</span> -- without any attempt at *understanding* the underlying process.</span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a>The **data modeling** culture remains essential for scientific understanding, policy decisions, and any application where we need to know not just *what* will happen, but *why*. For LLMs, and in ML more broadly, this aspect is studied by the field of *interpretability* or "explainable ML" -- trying to understand how modern ML models "think" and reach their conclusions.</span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a><span class="fu">## Foundations of Probability</span></span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>Probability provides the mathematical language for quantifying uncertainty. Before we can make statistical inferences or build predictive models, we need a solid foundation in probability theory.</span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a>This course is taught internationally, but for Finnish-speaking students, here's a reference table of key probability terms you may have encountered in your earlier studies:</span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a>| Sample space | Perusjoukko, otosavaruus | The set of all possible outcomes |</span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a>| Event | Tapahtuma | A subset of the sample space |</span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a>| Probability distribution | Todennäköisyysjakauma | Assignment of probabilities to events |</span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>| Probability measure | Todennäköisyysmitta | Mathematical function P satisfying axioms |</span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a>| Independent | Riippumattomat | Events that don't influence each other |</span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a>| Conditional probability | Ehdollinen todennäköisyys | Probability given some information |</span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a>| Bayes' Theorem | Bayesin kaava | Formula for updating probabilities |</span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a>| Random variable | Satunnaismuuttuja | Function mapping outcomes to numbers |</span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a>| Cumulative distribution function (CDF) | Kertymäfunktio | $P(X \le x)$ |</span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a>| Discrete | Diskreetti | Taking countable values |</span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a>| Probability mass function (PMF) | Todennäköisyysmassafunktio | $P(X = x)$ for discrete $X$ |</span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a>| Probability density function (PDF) | Tiheysfunktio | Density for continuous variables |</span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a>| Quantile function | Kvantiilifunktio | Inverse of CDF |</span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a>| First quartile | Ensimmäinen kvartiili | 25th percentile |</span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a>| Median | Mediaani | 50th percentile |</span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a>| Joint density function | Yhteistiheysfunktio | PDF for multiple variables |</span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a>| Marginal density | Reunatiheysfunktio | PDF of one variable from joint |</span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a>| Conditional density | Ehdollinen tiheysfunktio | PDF given another variable's value |</span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>| Random vector | Satunnaisvektori | Vector of random variables |</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a>| Independent and identically distributed (IID) | Riippumattomat ja samoin jakautuneet | Common assumption for data |</span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a>| Random sample | Satunnaisotos | IID observations from population |</span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a>| Frequentist probability | Frekventistinen todennäköisyys | Long-run frequency interpretation |</span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a>| Subjective probability | Subjektiivinen todennäköisyys | Degree of belief interpretation |</span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a>**Note**: Some terms have multiple Finnish translations. We report here the most common ones.</span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sample Spaces and Events</span></span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a>The **sample space** $\Omega$ is the set of all possible outcomes of an experiment. Individual elements $\omega \in \Omega$ are called **sample outcomes**, **realizations**, or just **elements**. Subsets of $\Omega$ are called **events**.</span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a>**Notation:** $\omega$ and $\Omega$ are the lowercase (respectively, uppercase) version of the Greek letter <span class="co">[</span><span class="ot">omega</span><span class="co">](https://en.wikipedia.org/wiki/Omega)</span>.</span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Coin flips</span></span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a>If we flip a coin twice, where each outcome can be head ($H$) or tails ($T$), the sample space is:</span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a>$$\Omega = <span class="sc">\{</span>HH, HT, TH, TT<span class="sc">\}</span>$$</span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a>The event "first flip is heads" is $A = <span class="sc">\{</span>HH, HT<span class="sc">\}</span>$.</span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Temperature measurement</span></span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a>When measuring temperature, the sample space might be the full set of real numbers:</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a>$$\Omega = \mathbb{R} = (-\infty, \infty)$$</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a>The event "temperature between 20°C and 25°C" is the interval $A = <span class="co">[</span><span class="ot">20, 25</span><span class="co">]</span>$.</span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a>Note that we often take $\Omega$ to be larger than strictly necessary - in this case for example we are including physically impossible values like -1000°C. This is still *mathematically* valid. As we will see later, we can assign zero probability to impossible events.</span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a>**Notation:** $<span class="co">[</span><span class="ot">a, b</span><span class="co">]</span>$ denotes the *interval* between $a$ and $b$ (included), whereas $(a, b)$ is the interval between $a$ and $b$ (excluded).</span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a>Sample spaces can be:</span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Finite**: $\Omega = <span class="sc">\{</span>1, 2, 3, 4, 5, 6<span class="sc">\}</span>$ for a die roll</span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Countably infinite**: $\Omega = <span class="sc">\{</span>1, 2, 3, ...<span class="sc">\}</span>$ for "number of flips until first heads"</span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Uncountably infinite**: $\Omega = <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ for "random number between 0 and 1"</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a><span class="fu">### Set Operations for Events</span></span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a>Since events are sets, we can combine them using standard set operations:</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>| Operation | Notation | Meaning |</span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a>|-----------|----------|---------|</span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a>| Complement | $A^c$ | "not A" - all outcomes not in A |</span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a>| Union | $A \cup B$ | "A or B" - outcomes in either A or B (or both) |</span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a>| Intersection | $A \cap B$ | "A and B" - outcomes in both A and B |</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a>| Difference | $A \setminus B$ | Outcomes in A but not in B |</span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a>**Disjoint events**: Events $A$ and $B$ are disjoint (or mutually exclusive) if $A \cap B = \emptyset$. This means they cannot occur simultaneously. For example, in the case of a standard six-sided die roll, let $A$ = "rolling an even number" = $<span class="sc">\{</span>2, 4, 6<span class="sc">\}</span>$ and $B$ = "rolling a 1" = $<span class="sc">\{</span>1<span class="sc">\}</span>$. These events are disjoint because you can't roll both an even number AND a 1 simultaneously.</span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a><span class="fu">### Probability Axioms</span></span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-303"><a href="#cb20-303" aria-hidden="true" tabindex="-1"></a>Now that we have defined the space of possible events, we can define the probability of an event.</span>
<span id="cb20-304"><a href="#cb20-304" aria-hidden="true" tabindex="-1"></a>A probability measure must satisfy three fundamental axioms:</span>
<span id="cb20-305"><a href="#cb20-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-306"><a href="#cb20-306" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-307"><a href="#cb20-307" aria-hidden="true" tabindex="-1"></a>A function $\mathbb{P}$ that assigns a real number $\mathbb{P}(A)$ to each event $A$ is a **probability measure** if:</span>
<span id="cb20-308"><a href="#cb20-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-309"><a href="#cb20-309" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Non-negativity**: $\mathbb{P}(A) \geq 0$ for every event $A$</span>
<span id="cb20-310"><a href="#cb20-310" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Normalization**: $\mathbb{P}(\Omega) = 1$ </span>
<span id="cb20-311"><a href="#cb20-311" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Countable additivity**: If $A_1, A_2, ...$ are disjoint events, then:</span>
<span id="cb20-312"><a href="#cb20-312" aria-hidden="true" tabindex="-1"></a>   $$\mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mathbb{P}(A_i)$$</span>
<span id="cb20-313"><a href="#cb20-313" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-314"><a href="#cb20-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-315"><a href="#cb20-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-316"><a href="#cb20-316" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-317"><a href="#cb20-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-318"><a href="#cb20-318" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why These Axioms</span></span>
<span id="cb20-319"><a href="#cb20-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-320"><a href="#cb20-320" aria-hidden="true" tabindex="-1"></a>These axioms ensure probability respects intuitive laws:</span>
<span id="cb20-321"><a href="#cb20-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-322"><a href="#cb20-322" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Non-negativity**: The probability of rain tomorrow cannot be negative.</span>
<span id="cb20-323"><a href="#cb20-323" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Normalization**: When rolling a six-sided die, the probability of getting one of the faces $<span class="sc">\{</span>1, 2, 3, 4, 5, 6<span class="sc">\}</span>$ is 1: 1 represent the total probability.</span>
<span id="cb20-324"><a href="#cb20-324" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Countable additivity**: The probability of rolling a 1 *or* a 2 on a die is the sum of their individual probabilities, as these events cannot happen together.</span>
<span id="cb20-325"><a href="#cb20-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-326"><a href="#cb20-326" aria-hidden="true" tabindex="-1"></a>It turns out that under some assumptions, it can be shown that these axioms are exactly what you would pick if one wants to quantify the concept of "possibility of an event" with a single number -- a result known as <span class="co">[</span><span class="ot">Cox's theorem</span><span class="co">](https://en.wikipedia.org/wiki/Cox%27s_theorem)</span>.</span>
<span id="cb20-327"><a href="#cb20-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-328"><a href="#cb20-328" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-329"><a href="#cb20-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-330"><a href="#cb20-330" aria-hidden="true" tabindex="-1"></a>From these axioms, we can derive many useful properties:</span>
<span id="cb20-331"><a href="#cb20-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-332"><a href="#cb20-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(\emptyset) = 0$ (the impossible event has probability 0)</span>
<span id="cb20-333"><a href="#cb20-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A^c) = 1 - \mathbb{P}(A)$ (complement rule)</span>
<span id="cb20-334"><a href="#cb20-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $A \subset B$, then $\mathbb{P}(A) \leq \mathbb{P}(B)$ (monotonicity)</span>
<span id="cb20-335"><a href="#cb20-335" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$0 \leq \mathbb{P}(A) \leq 1$ for any event $A$</span>
<span id="cb20-336"><a href="#cb20-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-337"><a href="#cb20-337" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Inclusion-Exclusion"}</span>
<span id="cb20-338"><a href="#cb20-338" aria-hidden="true" tabindex="-1"></a>For any events $A$ and $B$:</span>
<span id="cb20-339"><a href="#cb20-339" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$$</span>
<span id="cb20-340"><a href="#cb20-340" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-341"><a href="#cb20-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-342"><a href="#cb20-342" aria-hidden="true" tabindex="-1"></a>**Why?** This formula accounts for the "double counting" when we add $\mathbb{P}(A)$ and $\mathbb{P}(B)$ -- the intersection $A \cap B$ gets counted twice, so we subtract it once.</span>
<span id="cb20-343"><a href="#cb20-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-344"><a href="#cb20-344" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-345"><a href="#cb20-345" aria-hidden="true" tabindex="-1"></a>Sometimes you will see the notation $\mathbb{P}(A B)$ to denote $\mathbb{P}(A \cap B)$.</span>
<span id="cb20-346"><a href="#cb20-346" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-347"><a href="#cb20-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-348"><a href="#cb20-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-349"><a href="#cb20-349" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpretations of Probability</span></span>
<span id="cb20-350"><a href="#cb20-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-351"><a href="#cb20-351" aria-hidden="true" tabindex="-1"></a>Probability can be understood from different philosophical perspectives, each leading to the same mathematical framework.</span>
<span id="cb20-352"><a href="#cb20-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-353"><a href="#cb20-353" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb20-354"><a href="#cb20-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-355"><a href="#cb20-355" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb20-356"><a href="#cb20-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-357"><a href="#cb20-357" aria-hidden="true" tabindex="-1"></a>There are two main ways to think about what probability means:</span>
<span id="cb20-358"><a href="#cb20-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-359"><a href="#cb20-359" aria-hidden="true" tabindex="-1"></a>**Frequency interpretation**: Probability is the long-run proportion of times an event occurs in repeated experiments. If we flip a fair coin millions of times, we expect heads about 50% of the time.</span>
<span id="cb20-360"><a href="#cb20-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-361"><a href="#cb20-361" aria-hidden="true" tabindex="-1"></a>**Subjective interpretation**: Probability represents a degree of belief. When a weather forecaster says "30% chance of rain," they're expressing their confidence based on available information.</span>
<span id="cb20-362"><a href="#cb20-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-363"><a href="#cb20-363" aria-hidden="true" tabindex="-1"></a>Both interpretations are useful, and both lead to the same mathematical rules.</span>
<span id="cb20-364"><a href="#cb20-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-365"><a href="#cb20-365" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb20-366"><a href="#cb20-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-367"><a href="#cb20-367" aria-hidden="true" tabindex="-1"></a>**Frequentist probability**: </span>
<span id="cb20-368"><a href="#cb20-368" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A) = \lim_{n \to \infty} \frac{\text{number of times A occurs in n trials}}{n}$$</span>
<span id="cb20-369"><a href="#cb20-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-370"><a href="#cb20-370" aria-hidden="true" tabindex="-1"></a>This requires the experiment to be repeatable under identical conditions.</span>
<span id="cb20-371"><a href="#cb20-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-372"><a href="#cb20-372" aria-hidden="true" tabindex="-1"></a>**Subjective/Bayesian probability**: </span>
<span id="cb20-373"><a href="#cb20-373" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(A)$ quantifies an agent's degree of belief that $A$ is true, constrained by:</span>
<span id="cb20-374"><a href="#cb20-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-375"><a href="#cb20-375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coherence: beliefs must satisfy the probability axioms</span>
<span id="cb20-376"><a href="#cb20-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Updating: beliefs change rationally when new information arrives (via Bayes' theorem)</span>
<span id="cb20-377"><a href="#cb20-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-378"><a href="#cb20-378" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb20-379"><a href="#cb20-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-380"><a href="#cb20-380" aria-hidden="true" tabindex="-1"></a>Let's simulate the **frequentist interpretation** by flipping a fair coin many times and tracking how the proportion of heads converges to the true probability of 0.5. This directly demonstrates the mathematical definition: probability as the long-run proportion.</span>
<span id="cb20-381"><a href="#cb20-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-384"><a href="#cb20-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-385"><a href="#cb20-385" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-386"><a href="#cb20-386" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-387"><a href="#cb20-387" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-388"><a href="#cb20-388" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-389"><a href="#cb20-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-390"><a href="#cb20-390" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate many coin flips to see frequentist convergence</span></span>
<span id="cb20-391"><a href="#cb20-391" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb20-392"><a href="#cb20-392" aria-hidden="true" tabindex="-1"></a>n_flips <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb20-393"><a href="#cb20-393" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> np.random.choice([<span class="st">'H'</span>, <span class="st">'T'</span>], size<span class="op">=</span>n_flips)</span>
<span id="cb20-394"><a href="#cb20-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-395"><a href="#cb20-395" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate running proportion of heads</span></span>
<span id="cb20-396"><a href="#cb20-396" aria-hidden="true" tabindex="-1"></a>heads_count <span class="op">=</span> np.cumsum(flips <span class="op">==</span> <span class="st">'H'</span>)</span>
<span id="cb20-397"><a href="#cb20-397" aria-hidden="true" tabindex="-1"></a>proportions <span class="op">=</span> heads_count <span class="op">/</span> np.arange(<span class="dv">1</span>, n_flips <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb20-398"><a href="#cb20-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-399"><a href="#cb20-399" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot convergence to true probability</span></span>
<span id="cb20-400"><a href="#cb20-400" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-401"><a href="#cb20-401" aria-hidden="true" tabindex="-1"></a>plt.plot(proportions, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb20-402"><a href="#cb20-402" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True probability = 0.5'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-403"><a href="#cb20-403" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of flips'</span>)</span>
<span id="cb20-404"><a href="#cb20-404" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Proportion of heads'</span>)</span>
<span id="cb20-405"><a href="#cb20-405" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Frequentist Probability: Long-Run Proportion Converges to True Value'</span>)</span>
<span id="cb20-406"><a href="#cb20-406" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-407"><a href="#cb20-407" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-408"><a href="#cb20-408" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.3</span>, <span class="fl">0.7</span>)</span>
<span id="cb20-409"><a href="#cb20-409" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-410"><a href="#cb20-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-411"><a href="#cb20-411" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary</span></span>
<span id="cb20-412"><a href="#cb20-412" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"After </span><span class="sc">{</span>n_flips<span class="sc">:,}</span><span class="ss"> flips:"</span>)</span>
<span id="cb20-413"><a href="#cb20-413" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Proportion of heads: </span><span class="sc">{</span>proportions[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-414"><a href="#cb20-414" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Deviation from 0.5: </span><span class="sc">{</span><span class="bu">abs</span>(proportions[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> <span class="fl">0.5</span>)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-415"><a href="#cb20-415" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-416"><a href="#cb20-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-417"><a href="#cb20-417" aria-hidden="true" tabindex="-1"></a>The **subjective/Bayesian interpretation** involves updating beliefs based on evidence. We'll explore this computational approach in detail when we cover Bayes' theorem.</span>
<span id="cb20-418"><a href="#cb20-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-419"><a href="#cb20-419" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-420"><a href="#cb20-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-421"><a href="#cb20-421" aria-hidden="true" tabindex="-1"></a><span class="fu">### Finite Sample Spaces and Counting</span></span>
<span id="cb20-422"><a href="#cb20-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-423"><a href="#cb20-423" aria-hidden="true" tabindex="-1"></a>When $\Omega$ is finite and all outcomes are equally likely, probability calculations reduce to counting:</span>
<span id="cb20-424"><a href="#cb20-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-425"><a href="#cb20-425" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A) = \frac{|A|}{|\Omega|} = \frac{\text{number of outcomes in A}}{\text{total number of outcomes}}$$</span>
<span id="cb20-426"><a href="#cb20-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-427"><a href="#cb20-427" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-428"><a href="#cb20-428" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Rolling two dice</span></span>
<span id="cb20-429"><a href="#cb20-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-430"><a href="#cb20-430" aria-hidden="true" tabindex="-1"></a>What's the probability the sum of rolling two six-sided dice equals 7?</span>
<span id="cb20-431"><a href="#cb20-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-432"><a href="#cb20-432" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-433"><a href="#cb20-433" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-434"><a href="#cb20-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-435"><a href="#cb20-435" aria-hidden="true" tabindex="-1"></a>$\Omega = <span class="sc">\{</span>(i,j) : i,j \in <span class="sc">\{</span>1,2,3,4,5,6<span class="sc">\}\}</span>$, so $|\Omega| = 36$.</span>
<span id="cb20-436"><a href="#cb20-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-437"><a href="#cb20-437" aria-hidden="true" tabindex="-1"></a>The event "sum equals 7" is $A = <span class="sc">\{</span>(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)<span class="sc">\}</span>$.</span>
<span id="cb20-438"><a href="#cb20-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-439"><a href="#cb20-439" aria-hidden="true" tabindex="-1"></a>Therefore $\mathbb{P}(A) = \frac{6}{36} = \frac{1}{6}$.</span>
<span id="cb20-440"><a href="#cb20-440" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-441"><a href="#cb20-441" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-442"><a href="#cb20-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-443"><a href="#cb20-443" aria-hidden="true" tabindex="-1"></a>**Key counting principle - Binomial Coefficient**: </span>
<span id="cb20-444"><a href="#cb20-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-445"><a href="#cb20-445" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">binomial coefficient</span><span class="co">](https://en.wikipedia.org/wiki/Binomial_coefficient)</span> (read as "$n$ choose $k$") is:</span>
<span id="cb20-446"><a href="#cb20-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-447"><a href="#cb20-447" aria-hidden="true" tabindex="-1"></a>$$\binom{n}{k} = \frac{n!}{k!(n-k)!}$$</span>
<span id="cb20-448"><a href="#cb20-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-449"><a href="#cb20-449" aria-hidden="true" tabindex="-1"></a>Where $n!$ denotes the <span class="co">[</span><span class="ot">factorial</span><span class="co">](https://en.wikipedia.org/wiki/Factorial)</span>, e.g. $4! = 4 \cdot 3 \cdot 2 \cdot 1 = 24$.</span>
<span id="cb20-450"><a href="#cb20-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-451"><a href="#cb20-451" aria-hidden="true" tabindex="-1"></a>The binomial coefficient^<span class="co">[</span><span class="ot">The name "binomial" comes from its appearance in the binomial theorem: $(x+y)^n = \sum_{k=0}^{n} \binom{n}{k} x^k y^{n-k}$.</span><span class="co">]</span> counts the number of ways to choose $k$ objects from $n$ objects when order doesn't matter. For example:</span>
<span id="cb20-452"><a href="#cb20-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-453"><a href="#cb20-453" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\binom{5}{2} = \frac{5!}{2!3!} = \frac{5 \times 4}{2 \times 1} = 10$ ways to choose 2 items from 5</span>
<span id="cb20-454"><a href="#cb20-454" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Choosing 2 students from a class of 30: $\binom{30}{2} = 435$ possible pairs</span>
<span id="cb20-455"><a href="#cb20-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-456"><a href="#cb20-456" aria-hidden="true" tabindex="-1"></a><span class="fu">## Independence and Conditional Probability</span></span>
<span id="cb20-457"><a href="#cb20-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-458"><a href="#cb20-458" aria-hidden="true" tabindex="-1"></a><span class="fu">### Independent Events</span></span>
<span id="cb20-459"><a href="#cb20-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-460"><a href="#cb20-460" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-461"><a href="#cb20-461" aria-hidden="true" tabindex="-1"></a>Two events $A$ and $B$ are **independent** if:</span>
<span id="cb20-462"><a href="#cb20-462" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$$</span>
<span id="cb20-463"><a href="#cb20-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-464"><a href="#cb20-464" aria-hidden="true" tabindex="-1"></a>We denote this as $A \perp<span class="sc">\!\!\!</span>\perp B$. When events are not independent, we write $A \not\perp<span class="sc">\!\!\!</span>\perp B$.</span>
<span id="cb20-465"><a href="#cb20-465" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-466"><a href="#cb20-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-467"><a href="#cb20-467" aria-hidden="true" tabindex="-1"></a>Independence means that knowing whether one event occurred tells us *nothing* about the other event.</span>
<span id="cb20-468"><a href="#cb20-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-469"><a href="#cb20-469" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-470"><a href="#cb20-470" aria-hidden="true" tabindex="-1"></a>The textbook uses non-standard notation for independence and non-independence. We use the standard notation $A \perp<span class="sc">\!\!\!</span>\perp B$ (and $A \not\perp<span class="sc">\!\!\!</span>\perp B$ for dependence), which is widely adopted in probability and statistics literature.</span>
<span id="cb20-471"><a href="#cb20-471" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-472"><a href="#cb20-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-473"><a href="#cb20-473" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-474"><a href="#cb20-474" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Fair coin tosses</span></span>
<span id="cb20-475"><a href="#cb20-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-476"><a href="#cb20-476" aria-hidden="true" tabindex="-1"></a>A fair coin is tossed twice. Let $H_1$ = "first toss is heads" and $H_2$ = "second toss is heads". Are these two events independent?</span>
<span id="cb20-477"><a href="#cb20-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-478"><a href="#cb20-478" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-479"><a href="#cb20-479" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-480"><a href="#cb20-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-481"><a href="#cb20-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(H_1) = \mathbb{P}(H_2) = \frac{1}{2}$</span>
<span id="cb20-482"><a href="#cb20-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(H_1 \cap H_2) = \mathbb{P}(\text{both heads}) = \frac{1}{4}$</span>
<span id="cb20-483"><a href="#cb20-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Since $\frac{1}{4} = \frac{1}{2} \times \frac{1}{2}$, the events are independent</span>
<span id="cb20-484"><a href="#cb20-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-485"><a href="#cb20-485" aria-hidden="true" tabindex="-1"></a>This matches the intuition -- whether we obtain head on the first flip does not tell us anything about the second flip, and vice versa.</span>
<span id="cb20-486"><a href="#cb20-486" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-487"><a href="#cb20-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-488"><a href="#cb20-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-489"><a href="#cb20-489" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-490"><a href="#cb20-490" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Dependent events</span></span>
<span id="cb20-491"><a href="#cb20-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-492"><a href="#cb20-492" aria-hidden="true" tabindex="-1"></a>Draw two cards from a deck without replacement.</span>
<span id="cb20-493"><a href="#cb20-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-494"><a href="#cb20-494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$A$ = "first card is an ace" </span>
<span id="cb20-495"><a href="#cb20-495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$B$ = "second card is an ace"</span>
<span id="cb20-496"><a href="#cb20-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-497"><a href="#cb20-497" aria-hidden="true" tabindex="-1"></a>Are these events independent?</span>
<span id="cb20-498"><a href="#cb20-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-499"><a href="#cb20-499" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-500"><a href="#cb20-500" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-501"><a href="#cb20-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-502"><a href="#cb20-502" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A) = \mathbb{P}(B) = \frac{4}{52}$</span>
<span id="cb20-503"><a href="#cb20-503" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>However: $\mathbb{P}(A \cap B) = \frac{4}{52} \times \frac{3}{51} \neq \mathbb{P}(A)\mathbb{P}(B)$</span>
<span id="cb20-504"><a href="#cb20-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-505"><a href="#cb20-505" aria-hidden="true" tabindex="-1"></a>The events are *dependent* because drawing an ace first changes the probability of drawing an ace second.</span>
<span id="cb20-506"><a href="#cb20-506" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-507"><a href="#cb20-507" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-508"><a href="#cb20-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-509"><a href="#cb20-509" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb20-510"><a href="#cb20-510" aria-hidden="true" tabindex="-1"></a>**Common misconception**: Disjoint events are NOT independent!</span>
<span id="cb20-511"><a href="#cb20-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-512"><a href="#cb20-512" aria-hidden="true" tabindex="-1"></a>If $A$ and $B$ are disjoint with positive probability, then:</span>
<span id="cb20-513"><a href="#cb20-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-514"><a href="#cb20-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A \cap B) = 0$ (since they are disjoint, they can't happen together)</span>
<span id="cb20-515"><a href="#cb20-515" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A)\mathbb{P}(B) &gt; 0$ (if both have positive probability)</span>
<span id="cb20-516"><a href="#cb20-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-517"><a href="#cb20-517" aria-hidden="true" tabindex="-1"></a>So disjoint events are actually maximally dependent - if one occurs, the other definitely doesn't!</span>
<span id="cb20-518"><a href="#cb20-518" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-519"><a href="#cb20-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-520"><a href="#cb20-520" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditional Probability</span></span>
<span id="cb20-521"><a href="#cb20-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-522"><a href="#cb20-522" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-523"><a href="#cb20-523" aria-hidden="true" tabindex="-1"></a>The **conditional probability** of $A$ given $B$ is:</span>
<span id="cb20-524"><a href="#cb20-524" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$$</span>
<span id="cb20-525"><a href="#cb20-525" aria-hidden="true" tabindex="-1"></a>provided $\mathbb{P}(B) &gt; 0$.</span>
<span id="cb20-526"><a href="#cb20-526" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-527"><a href="#cb20-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-528"><a href="#cb20-528" aria-hidden="true" tabindex="-1"></a>Think of $\mathbb{P}(A|B)$ as the probability of $A$ in the "new universe" where we know $B$ has occurred.</span>
<span id="cb20-529"><a href="#cb20-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-530"><a href="#cb20-530" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution}</span>
<span id="cb20-531"><a href="#cb20-531" aria-hidden="true" tabindex="-1"></a>**Prosecutor's Fallacy**: Confusing $\mathbb{P}(A|B)$ with $\mathbb{P}(B|A)$.</span>
<span id="cb20-532"><a href="#cb20-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-533"><a href="#cb20-533" aria-hidden="true" tabindex="-1"></a>These can be vastly different! For example:</span>
<span id="cb20-534"><a href="#cb20-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-535"><a href="#cb20-535" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(\text{match} | \text{guilty})$ might be 0.98.</span>
<span id="cb20-536"><a href="#cb20-536" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(\text{guilty} | \text{match})$ might be 0.04.</span>
<span id="cb20-537"><a href="#cb20-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-538"><a href="#cb20-538" aria-hidden="true" tabindex="-1"></a>The second depends on the prior probability of guilt and how many innocent people might also match.</span>
<span id="cb20-539"><a href="#cb20-539" aria-hidden="true" tabindex="-1"></a>We will see next how to compute one from the other.</span>
<span id="cb20-540"><a href="#cb20-540" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-541"><a href="#cb20-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-542"><a href="#cb20-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-543"><a href="#cb20-543" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayes' Theorem</span></span>
<span id="cb20-544"><a href="#cb20-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-545"><a href="#cb20-545" aria-hidden="true" tabindex="-1"></a>Sometimes we know $\mathbb{P}(B|A)$ but we are really interested in the other way round, $\mathbb{P}(A|B)$.</span>
<span id="cb20-546"><a href="#cb20-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-547"><a href="#cb20-547" aria-hidden="true" tabindex="-1"></a>For example, in the example above, we may know the probability that a test gives a match if the suspect is guilty, $\mathbb{P}(\text{match} \mid \text{guilty})$, but what we really want to know is the probability that the suspect is guilty given that we find a match, $\mathbb{P}(\text{guilty} \mid \text{match})$.</span>
<span id="cb20-548"><a href="#cb20-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-549"><a href="#cb20-549" aria-hidden="true" tabindex="-1"></a>Such "inverse" conditional probabilities can be calculated via **Bayes' theorem**.</span>
<span id="cb20-550"><a href="#cb20-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-551"><a href="#cb20-551" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Bayes' Theorem"}</span>
<span id="cb20-552"><a href="#cb20-552" aria-hidden="true" tabindex="-1"></a>For events $A$ and $B$ with $\mathbb{P}(B) &gt; 0$:</span>
<span id="cb20-553"><a href="#cb20-553" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}$$</span>
<span id="cb20-554"><a href="#cb20-554" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-555"><a href="#cb20-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-556"><a href="#cb20-556" aria-hidden="true" tabindex="-1"></a>Where $B$ is some information (evidence) and $A$ an hypothesis.</span>
<span id="cb20-557"><a href="#cb20-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-558"><a href="#cb20-558" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Law of Total Probability"}</span>
<span id="cb20-559"><a href="#cb20-559" aria-hidden="true" tabindex="-1"></a>If $A_1, ..., A_k$ partition $\Omega$ (they're disjoint and cover all possibilities), then:</span>
<span id="cb20-560"><a href="#cb20-560" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(B) = \sum_{i=1}^k \mathbb{P}(B|A_i)\mathbb{P}(A_i)$$</span>
<span id="cb20-561"><a href="#cb20-561" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-562"><a href="#cb20-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-563"><a href="#cb20-563" aria-hidden="true" tabindex="-1"></a>Combining these gives the full form of Bayes' theorem:</span>
<span id="cb20-564"><a href="#cb20-564" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A_i|B) = \frac{\mathbb{P}(B|A_i)\mathbb{P}(A_i)}{\sum_j \mathbb{P}(B|A_j)\mathbb{P}(A_j)}$$</span>
<span id="cb20-565"><a href="#cb20-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-566"><a href="#cb20-566" aria-hidden="true" tabindex="-1"></a>**Terminology**:</span>
<span id="cb20-567"><a href="#cb20-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-568"><a href="#cb20-568" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A_i)$: Prior probability for hypothesis $A_i$ (before seeing evidence $B$), also known as "base rate".</span>
<span id="cb20-569"><a href="#cb20-569" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A_i|B)$: Posterior probability (after seeing evidence $B$).</span>
<span id="cb20-570"><a href="#cb20-570" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(B|A_i)$: Likelihood of hypothesis $A_i$ for fixed evidence $B$.</span>
<span id="cb20-571"><a href="#cb20-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-572"><a href="#cb20-572" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-573"><a href="#cb20-573" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Email spam detection</span></span>
<span id="cb20-574"><a href="#cb20-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-575"><a href="#cb20-575" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prior: 70% of emails are spam</span>
<span id="cb20-576"><a href="#cb20-576" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Free" appears in 90% of spam emails</span>
<span id="cb20-577"><a href="#cb20-577" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Free" appears in 1% of legitimate emails</span>
<span id="cb20-578"><a href="#cb20-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-579"><a href="#cb20-579" aria-hidden="true" tabindex="-1"></a>If an email contains "free", what's the probability it's spam?</span>
<span id="cb20-580"><a href="#cb20-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-581"><a href="#cb20-581" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-582"><a href="#cb20-582" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-583"><a href="#cb20-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-584"><a href="#cb20-584" aria-hidden="true" tabindex="-1"></a>Let $S$ = "email is spam" and $F$ = "email contains 'free'".</span>
<span id="cb20-585"><a href="#cb20-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-586"><a href="#cb20-586" aria-hidden="true" tabindex="-1"></a>Given:</span>
<span id="cb20-587"><a href="#cb20-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-588"><a href="#cb20-588" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(S) = 0.7$</span>
<span id="cb20-589"><a href="#cb20-589" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(F|S) = 0.9$  </span>
<span id="cb20-590"><a href="#cb20-590" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(F|S^c) = 0.01$</span>
<span id="cb20-591"><a href="#cb20-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-592"><a href="#cb20-592" aria-hidden="true" tabindex="-1"></a>By Bayes' theorem:</span>
<span id="cb20-593"><a href="#cb20-593" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(S|F) = \frac{0.9 \times 0.7}{0.9 \times 0.7 + 0.01 \times 0.3} = \frac{0.63}{0.633} \approx 0.995$$</span>
<span id="cb20-594"><a href="#cb20-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-595"><a href="#cb20-595" aria-hidden="true" tabindex="-1"></a>So an email containing "free" has a 99.5% chance of being spam!</span>
<span id="cb20-596"><a href="#cb20-596" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-597"><a href="#cb20-597" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-598"><a href="#cb20-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-599"><a href="#cb20-599" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb20-600"><a href="#cb20-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-601"><a href="#cb20-601" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb20-602"><a href="#cb20-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-603"><a href="#cb20-603" aria-hidden="true" tabindex="-1"></a>Conditional probability answers questions like:</span>
<span id="cb20-604"><a href="#cb20-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-605"><a href="#cb20-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's the probability of rain given that it's cloudy?</span>
<span id="cb20-606"><a href="#cb20-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's the probability a patient has a disease given a positive test result?</span>
<span id="cb20-607"><a href="#cb20-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's the probability a user clicks an ad given they're on a mobile device?</span>
<span id="cb20-608"><a href="#cb20-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-609"><a href="#cb20-609" aria-hidden="true" tabindex="-1"></a>The key insight: additional information changes probabilities. Knowing that $B$ occurred restricts our attention to outcomes where $B$ is true, potentially changing how likely $A$ becomes.</span>
<span id="cb20-610"><a href="#cb20-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-611"><a href="#cb20-611" aria-hidden="true" tabindex="-1"></a>Some conditional probabilities are easier to compute than others. For example, we may know that **if** the patient has a disease, then the test will return positive with a certain probability.</span>
<span id="cb20-612"><a href="#cb20-612" aria-hidden="true" tabindex="-1"></a>However, to compute the "inverse" probability (if the test is positive, what's the probability of the patient having the disease?) we need Bayes' theorem.</span>
<span id="cb20-613"><a href="#cb20-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-614"><a href="#cb20-614" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb20-615"><a href="#cb20-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-616"><a href="#cb20-616" aria-hidden="true" tabindex="-1"></a>For fixed $B$ with $\mathbb{P}(B) &gt; 0$, the conditional probability $\mathbb{P}(\cdot|B)$ is itself a probability measure:</span>
<span id="cb20-617"><a href="#cb20-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-618"><a href="#cb20-618" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbb{P}(A|B) \geq 0$ for all $A$</span>
<span id="cb20-619"><a href="#cb20-619" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\mathbb{P}(\Omega|B) = 1$</span>
<span id="cb20-620"><a href="#cb20-620" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If $A_1, A_2, ...$ are disjoint, then $\mathbb{P}(\bigcup_i A_i|B) = \sum_i \mathbb{P}(A_i|B)$</span>
<span id="cb20-621"><a href="#cb20-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-622"><a href="#cb20-622" aria-hidden="true" tabindex="-1"></a>Key relationships:</span>
<span id="cb20-623"><a href="#cb20-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-624"><a href="#cb20-624" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $A \perp<span class="sc">\!\!\!</span>\perp B$, then $\mathbb{P}(A|B) = \mathbb{P}(A)$ (independence means conditioning doesn't matter)</span>
<span id="cb20-625"><a href="#cb20-625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)$ (multiplication rule)</span>
<span id="cb20-626"><a href="#cb20-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-627"><a href="#cb20-627" aria-hidden="true" tabindex="-1"></a>Conversely, in general $\mathbb{P}(A|\cdot)$ (the likelihood) is *not* a probability measure.</span>
<span id="cb20-628"><a href="#cb20-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-629"><a href="#cb20-629" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb20-630"><a href="#cb20-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-631"><a href="#cb20-631" aria-hidden="true" tabindex="-1"></a>We will visualize how conditional probability can be counterintuitive. We'll simulate a medical test scenario to show how base rates affect our interpretation of test results.</span>
<span id="cb20-632"><a href="#cb20-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-635"><a href="#cb20-635" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-636"><a href="#cb20-636" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-637"><a href="#cb20-637" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb20-638"><a href="#cb20-638" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-639"><a href="#cb20-639" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-640"><a href="#cb20-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-641"><a href="#cb20-641" aria-hidden="true" tabindex="-1"></a><span class="co"># Medical test scenario</span></span>
<span id="cb20-642"><a href="#cb20-642" aria-hidden="true" tabindex="-1"></a>p_disease <span class="op">=</span> <span class="fl">0.001</span>                   <span class="co"># 0.1% have the disease (base rate)</span></span>
<span id="cb20-643"><a href="#cb20-643" aria-hidden="true" tabindex="-1"></a>p_pos_given_disease <span class="op">=</span> <span class="fl">0.99</span>          <span class="co"># 99% sensitivity</span></span>
<span id="cb20-644"><a href="#cb20-644" aria-hidden="true" tabindex="-1"></a>p_neg_given_healthy <span class="op">=</span> <span class="fl">0.99</span>          <span class="co"># 99% specificity</span></span>
<span id="cb20-645"><a href="#cb20-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-646"><a href="#cb20-646" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate probability of positive test</span></span>
<span id="cb20-647"><a href="#cb20-647" aria-hidden="true" tabindex="-1"></a>p_healthy <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_disease</span>
<span id="cb20-648"><a href="#cb20-648" aria-hidden="true" tabindex="-1"></a>p_pos_given_healthy <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_neg_given_healthy</span>
<span id="cb20-649"><a href="#cb20-649" aria-hidden="true" tabindex="-1"></a>p_positive <span class="op">=</span> p_pos_given_disease <span class="op">*</span> p_disease <span class="op">+</span> p_pos_given_healthy <span class="op">*</span> p_healthy</span>
<span id="cb20-650"><a href="#cb20-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-651"><a href="#cb20-651" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Bayes' theorem: P(disease|positive)</span></span>
<span id="cb20-652"><a href="#cb20-652" aria-hidden="true" tabindex="-1"></a>p_disease_given_pos <span class="op">=</span> (p_pos_given_disease <span class="op">*</span> p_disease) <span class="op">/</span> p_positive</span>
<span id="cb20-653"><a href="#cb20-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-654"><a href="#cb20-654" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with different base rates</span></span>
<span id="cb20-655"><a href="#cb20-655" aria-hidden="true" tabindex="-1"></a>base_rates <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">50</span>)  <span class="co"># 0.01% to 10%</span></span>
<span id="cb20-656"><a href="#cb20-656" aria-hidden="true" tabindex="-1"></a>posterior_probs <span class="op">=</span> []</span>
<span id="cb20-657"><a href="#cb20-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-658"><a href="#cb20-658" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> base_rate <span class="kw">in</span> base_rates:</span>
<span id="cb20-659"><a href="#cb20-659" aria-hidden="true" tabindex="-1"></a>    p_pos <span class="op">=</span> p_pos_given_disease <span class="op">*</span> base_rate <span class="op">+</span> p_pos_given_healthy <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> base_rate)</span>
<span id="cb20-660"><a href="#cb20-660" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> (p_pos_given_disease <span class="op">*</span> base_rate) <span class="op">/</span> p_pos</span>
<span id="cb20-661"><a href="#cb20-661" aria-hidden="true" tabindex="-1"></a>    posterior_probs.append(posterior)</span>
<span id="cb20-662"><a href="#cb20-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-663"><a href="#cb20-663" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb20-664"><a href="#cb20-664" aria-hidden="true" tabindex="-1"></a>plt.semilogx(base_rates <span class="op">*</span> <span class="dv">100</span>, np.array(posterior_probs) <span class="op">*</span> <span class="dv">100</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb20-665"><a href="#cb20-665" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Current: </span><span class="sc">{</span>p_disease_given_pos<span class="sc">:.1%}</span><span class="ss"> chance'</span>)</span>
<span id="cb20-666"><a href="#cb20-666" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Disease Prevalence (%)'</span>)</span>
<span id="cb20-667"><a href="#cb20-667" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(Disease | Positive Test) (%)'</span>)</span>
<span id="cb20-668"><a href="#cb20-668" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Impact of Base Rate on Test Interpretation'</span>)</span>
<span id="cb20-669"><a href="#cb20-669" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">100</span>)  <span class="co"># Set y-axis range from 0 to 100</span></span>
<span id="cb20-670"><a href="#cb20-670" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>], [<span class="st">'0.01'</span>, <span class="st">'0.1'</span>, <span class="st">'1'</span>, <span class="st">'10'</span>])  <span class="co"># Set custom x-axis ticks</span></span>
<span id="cb20-671"><a href="#cb20-671" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-672"><a href="#cb20-672" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-673"><a href="#cb20-673" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-674"><a href="#cb20-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-675"><a href="#cb20-675" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"With 99% accurate test and 0.1% base rate:"</span>)</span>
<span id="cb20-676"><a href="#cb20-676" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(disease | positive test) = </span><span class="sc">{</span>p_disease_given_pos<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb20-677"><a href="#cb20-677" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Surprising: A positive test means only ~9% chance of disease!"</span>)</span>
<span id="cb20-678"><a href="#cb20-678" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-679"><a href="#cb20-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-680"><a href="#cb20-680" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-681"><a href="#cb20-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-682"><a href="#cb20-682" aria-hidden="true" tabindex="-1"></a><span class="fu">### Classic Probability Examples</span></span>
<span id="cb20-683"><a href="#cb20-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-684"><a href="#cb20-684" aria-hidden="true" tabindex="-1"></a>Let's work through some classic examples that illustrate key concepts:</span>
<span id="cb20-685"><a href="#cb20-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-686"><a href="#cb20-686" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-687"><a href="#cb20-687" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: At least one head in 10 flips</span></span>
<span id="cb20-688"><a href="#cb20-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-689"><a href="#cb20-689" aria-hidden="true" tabindex="-1"></a>What's the probability of getting at least one head in 10 coin flips?</span>
<span id="cb20-690"><a href="#cb20-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-691"><a href="#cb20-691" aria-hidden="true" tabindex="-1"></a>*Hint:* Instead of counting all the ways to get 1, 2, ..., or 10 heads, use the complement.</span>
<span id="cb20-692"><a href="#cb20-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-693"><a href="#cb20-693" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-694"><a href="#cb20-694" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-695"><a href="#cb20-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-696"><a href="#cb20-696" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(\text{at least one head}) = 1 - \mathbb{P}(\text{no heads}) = 1 - \mathbb{P}(\text{all tails})$</span>
<span id="cb20-697"><a href="#cb20-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-698"><a href="#cb20-698" aria-hidden="true" tabindex="-1"></a>Since flips are independent:</span>
<span id="cb20-699"><a href="#cb20-699" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(\text{all tails}) = \left(\frac{1}{2}\right)^{10} = \frac{1}{1024}$</span>
<span id="cb20-700"><a href="#cb20-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-701"><a href="#cb20-701" aria-hidden="true" tabindex="-1"></a>Therefore: $\mathbb{P}(\text{at least one head}) = 1 - \frac{1}{1024} \approx 0.999$</span>
<span id="cb20-702"><a href="#cb20-702" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-703"><a href="#cb20-703" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-704"><a href="#cb20-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-705"><a href="#cb20-705" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-706"><a href="#cb20-706" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example (advanced): Basketball competition</span></span>
<span id="cb20-707"><a href="#cb20-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-708"><a href="#cb20-708" aria-hidden="true" tabindex="-1"></a>Two players take turns shooting. Player A shoots first with probability 1/3 of scoring. Player B shoots second with probability 1/4. First to score wins. What's the probability A wins?</span>
<span id="cb20-709"><a href="#cb20-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-710"><a href="#cb20-710" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-711"><a href="#cb20-711" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-712"><a href="#cb20-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-713"><a href="#cb20-713" aria-hidden="true" tabindex="-1"></a>A wins if:</span>
<span id="cb20-714"><a href="#cb20-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-715"><a href="#cb20-715" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A scores on first shot: probability 1/3</span>
<span id="cb20-716"><a href="#cb20-716" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Both miss, then A scores: $(2/3)(3/4)(1/3)$</span>
<span id="cb20-717"><a href="#cb20-717" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Both miss twice, then A scores: $(2/3)(3/4)(2/3)(3/4)(1/3)$</span>
<span id="cb20-718"><a href="#cb20-718" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>...</span>
<span id="cb20-719"><a href="#cb20-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-720"><a href="#cb20-720" aria-hidden="true" tabindex="-1"></a>This is a <span class="co">[</span><span class="ot">geometric series</span><span class="co">](https://en.wikipedia.org/wiki/Geometric_series)</span>:</span>
<span id="cb20-721"><a href="#cb20-721" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(A \text{ wins}) = \frac{1}{3} \sum_{k=0}^{\infty} \left(\frac{2}{3} \cdot \frac{3}{4}\right)^k = \frac{1}{3} \cdot \frac{1}{1-\frac{1}{2}} = \frac{2}{3}$$</span>
<span id="cb20-722"><a href="#cb20-722" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-723"><a href="#cb20-723" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-724"><a href="#cb20-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-725"><a href="#cb20-725" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Variables</span></span>
<span id="cb20-726"><a href="#cb20-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-727"><a href="#cb20-727" aria-hidden="true" tabindex="-1"></a>So far, we've worked with events - subsets of the sample space. But in practice, we usually care about numerical quantities associated with random outcomes. This is where random variables come in.</span>
<span id="cb20-728"><a href="#cb20-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-729"><a href="#cb20-729" aria-hidden="true" tabindex="-1"></a><span class="fu">### Definition and Intuition</span></span>
<span id="cb20-730"><a href="#cb20-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-731"><a href="#cb20-731" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-732"><a href="#cb20-732" aria-hidden="true" tabindex="-1"></a>A **random variable** is a function $X: \Omega \rightarrow \mathbb{R}$ that assigns a real number to each outcome in the sample space.</span>
<span id="cb20-733"><a href="#cb20-733" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-734"><a href="#cb20-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-735"><a href="#cb20-735" aria-hidden="true" tabindex="-1"></a>A random variable is defined by its possible *values* (real numbers) and their *probabilities*.</span>
<span id="cb20-736"><a href="#cb20-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-737"><a href="#cb20-737" aria-hidden="true" tabindex="-1"></a>In the case of a *discrete* random variable, the set of values is discrete (finite or infinite), $x_1, \ldots$, and each value can be assigned a corresponding point probability $p_1, \ldots$ with $0 \le p_i \le 1$, $\sum_{i=1}^\infty p_i = 1$.</span>
<span id="cb20-738"><a href="#cb20-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-739"><a href="#cb20-739" aria-hidden="true" tabindex="-1"></a>In the case of a continuous random variable, probabilities are defined by a non-negative probability density function that integrates to 1.</span>
<span id="cb20-740"><a href="#cb20-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-741"><a href="#cb20-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-742"><a href="#cb20-742" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb20-743"><a href="#cb20-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-744"><a href="#cb20-744" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb20-745"><a href="#cb20-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-746"><a href="#cb20-746" aria-hidden="true" tabindex="-1"></a>A random variable is just a way to assign numbers to outcomes. Think of it as a measurement or quantity that depends on chance.</span>
<span id="cb20-747"><a href="#cb20-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-748"><a href="#cb20-748" aria-hidden="true" tabindex="-1"></a>Examples:</span>
<span id="cb20-749"><a href="#cb20-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-750"><a href="#cb20-750" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Number of heads in 10 coin flips</span>
<span id="cb20-751"><a href="#cb20-751" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time until next customer arrives</span>
<span id="cb20-752"><a href="#cb20-752" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temperature at noon tomorrow</span>
<span id="cb20-753"><a href="#cb20-753" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stock price at market close</span>
<span id="cb20-754"><a href="#cb20-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-755"><a href="#cb20-755" aria-hidden="true" tabindex="-1"></a>The key insight: once we have numbers, we can do arithmetic, calculate averages, measure spread, and use all the tools of mathematics.</span>
<span id="cb20-756"><a href="#cb20-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-757"><a href="#cb20-757" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb20-758"><a href="#cb20-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-759"><a href="#cb20-759" aria-hidden="true" tabindex="-1"></a>*Warning:* The following will likely make sense only if you have taken an advanced course in probability theory or measure theory. Feel free to skip it otherwise.</span>
<span id="cb20-760"><a href="#cb20-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-761"><a href="#cb20-761" aria-hidden="true" tabindex="-1"></a>Formally, $X$ is a measurable function from $(\Omega, \mathcal{F})$ to $(\mathbb{R}, \mathcal{B})$ where:</span>
<span id="cb20-762"><a href="#cb20-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-763"><a href="#cb20-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathcal{F}$ is the $\sigma$-algebra of events in $\Omega$</span>
<span id="cb20-764"><a href="#cb20-764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathcal{B}$ is the Borel $\sigma$-algebra on $\mathbb{R}$</span>
<span id="cb20-765"><a href="#cb20-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-766"><a href="#cb20-766" aria-hidden="true" tabindex="-1"></a>Measurability means: for any Borel set $B \subset \mathbb{R}$, the pre-image $X^{-1}(B) = <span class="sc">\{</span>\omega : X(\omega) \in B<span class="sc">\}</span>$ is an event in $\mathcal{F}$.</span>
<span id="cb20-767"><a href="#cb20-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-768"><a href="#cb20-768" aria-hidden="true" tabindex="-1"></a>This technical condition ensures we can compute probabilities like $\mathbb{P}(X \in B)$.</span>
<span id="cb20-769"><a href="#cb20-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-770"><a href="#cb20-770" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb20-771"><a href="#cb20-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-772"><a href="#cb20-772" aria-hidden="true" tabindex="-1"></a>Here we demonstrate how random variables map outcomes to numbers, allowing us to analyze randomness mathematically.</span>
<span id="cb20-773"><a href="#cb20-773" aria-hidden="true" tabindex="-1"></a>For example, we can plot a histogram for the realizations over multiple experiments.</span>
<span id="cb20-774"><a href="#cb20-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-777"><a href="#cb20-777" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-778"><a href="#cb20-778" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-779"><a href="#cb20-779" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-780"><a href="#cb20-780" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-781"><a href="#cb20-781" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-782"><a href="#cb20-782" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb20-783"><a href="#cb20-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-784"><a href="#cb20-784" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate a random variable: X = number of heads in 10 coin flips</span></span>
<span id="cb20-785"><a href="#cb20-785" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb20-786"><a href="#cb20-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-787"><a href="#cb20-787" aria-hidden="true" tabindex="-1"></a><span class="co"># Single experiment</span></span>
<span id="cb20-788"><a href="#cb20-788" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> np.random.choice([<span class="st">'H'</span>, <span class="st">'T'</span>], size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb20-789"><a href="#cb20-789" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.<span class="bu">sum</span>(flips <span class="op">==</span> <span class="st">'H'</span>)</span>
<span id="cb20-790"><a href="#cb20-790" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Outcomes (single experiment): </span><span class="sc">{</span>flips<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-791"><a href="#cb20-791" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X (number of heads) = </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-792"><a href="#cb20-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-793"><a href="#cb20-793" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate many experiments to see the distribution</span></span>
<span id="cb20-794"><a href="#cb20-794" aria-hidden="true" tabindex="-1"></a>n_sims <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb20-795"><a href="#cb20-795" aria-hidden="true" tabindex="-1"></a>X_values <span class="op">=</span> [np.<span class="bu">sum</span>(np.random.choice([<span class="st">'H'</span>, <span class="st">'T'</span>], size<span class="op">=</span><span class="dv">10</span>) <span class="op">==</span> <span class="st">'H'</span>) </span>
<span id="cb20-796"><a href="#cb20-796" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_sims)]</span>
<span id="cb20-797"><a href="#cb20-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-798"><a href="#cb20-798" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize distribution</span></span>
<span id="cb20-799"><a href="#cb20-799" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-800"><a href="#cb20-800" aria-hidden="true" tabindex="-1"></a>counts, bins, _ <span class="op">=</span> plt.hist(X_values, bins<span class="op">=</span>np.arange(<span class="fl">0.5</span>, <span class="fl">11.5</span>, <span class="dv">1</span>), density<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb20-801"><a href="#cb20-801" aria-hidden="true" tabindex="-1"></a>                          alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb20-802"><a href="#cb20-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-803"><a href="#cb20-803" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">11</span>)</span>
<span id="cb20-804"><a href="#cb20-804" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Heads'</span>)</span>
<span id="cb20-805"><a href="#cb20-805" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb20-806"><a href="#cb20-806" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Random Variable X: Number of Heads in 10 Flips (</span><span class="sc">{</span>n_sims<span class="sc">}</span><span class="ss"> experiments)'</span>)</span>
<span id="cb20-807"><a href="#cb20-807" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-808"><a href="#cb20-808" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-809"><a href="#cb20-809" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-810"><a href="#cb20-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-811"><a href="#cb20-811" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Average value: </span><span class="sc">{</span>np<span class="sc">.</span>mean(X_values)<span class="sc">:.3f}</span><span class="ss"> (theoretical: 5.0)"</span>)</span>
<span id="cb20-812"><a href="#cb20-812" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-813"><a href="#cb20-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-814"><a href="#cb20-814" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-815"><a href="#cb20-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-816"><a href="#cb20-816" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-817"><a href="#cb20-817" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Coin flips</span></span>
<span id="cb20-818"><a href="#cb20-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-819"><a href="#cb20-819" aria-hidden="true" tabindex="-1"></a>Within the same sample space we can define multiple distinct random variables.</span>
<span id="cb20-820"><a href="#cb20-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-821"><a href="#cb20-821" aria-hidden="true" tabindex="-1"></a>For example, let $\Omega = <span class="sc">\{</span>HH, HT, TH, TT<span class="sc">\}</span>$ (two flips). Define:</span>
<span id="cb20-822"><a href="#cb20-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-823"><a href="#cb20-823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X$ = number of heads</span>
<span id="cb20-824"><a href="#cb20-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Y$ = 1 if first flip is heads, 0 otherwise</span>
<span id="cb20-825"><a href="#cb20-825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Z$ = 1 if flips match, 0 otherwise</span>
<span id="cb20-826"><a href="#cb20-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-827"><a href="#cb20-827" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb20-828"><a href="#cb20-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-829"><a href="#cb20-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X(HH) = 2$, $X(HT) = 1$, $X(TH) = 1$, $X(TT) = 0$</span>
<span id="cb20-830"><a href="#cb20-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Y(HH) = 1$, $Y(HT) = 1$, $Y(TH) = 0$, $Y(TT) = 0$  </span>
<span id="cb20-831"><a href="#cb20-831" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Z(HH) = 1$, $Z(HT) = 0$, $Z(TH) = 0$, $Z(TT) = 1$</span>
<span id="cb20-832"><a href="#cb20-832" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-833"><a href="#cb20-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-834"><a href="#cb20-834" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb20-835"><a href="#cb20-835" aria-hidden="true" tabindex="-1"></a>**Notation convention**: </span>
<span id="cb20-836"><a href="#cb20-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-837"><a href="#cb20-837" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Capital letters $(X, Y, Z)$ denote random variables</span>
<span id="cb20-838"><a href="#cb20-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lowercase letters $(x, y, z)$ denote specific values</span>
<span id="cb20-839"><a href="#cb20-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$<span class="sc">\{</span>X = x<span class="sc">\}</span>$ is the event that $X$ takes value $x$</span>
<span id="cb20-840"><a href="#cb20-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-841"><a href="#cb20-841" aria-hidden="true" tabindex="-1"></a>However, do not expect people to strictly follow this convention beyond mathematical and statistical textbooks. </span>
<span id="cb20-842"><a href="#cb20-842" aria-hidden="true" tabindex="-1"></a>In the real world, you will often see "$x$" used to refer both to a *value* and to a random variable "$X$" that happens to take value $x$.</span>
<span id="cb20-843"><a href="#cb20-843" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-844"><a href="#cb20-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-845"><a href="#cb20-845" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cumulative Distribution Functions</span></span>
<span id="cb20-846"><a href="#cb20-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-847"><a href="#cb20-847" aria-hidden="true" tabindex="-1"></a>The Cumulative Distribution Function (CDF) completely characterizes a random variable's probability distribution.</span>
<span id="cb20-848"><a href="#cb20-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-849"><a href="#cb20-849" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-850"><a href="#cb20-850" aria-hidden="true" tabindex="-1"></a>The **cumulative distribution function (CDF)** of a random variable $X$ is the function $F_X(x): \mathbb{R} \rightarrow <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ defined by</span>
<span id="cb20-851"><a href="#cb20-851" aria-hidden="true" tabindex="-1"></a>$$F_X(x) = \mathbb{P}(X \leq x)$$</span>
<span id="cb20-852"><a href="#cb20-852" aria-hidden="true" tabindex="-1"></a>for all $x \in \mathbb{R}$.</span>
<span id="cb20-853"><a href="#cb20-853" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-854"><a href="#cb20-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-855"><a href="#cb20-855" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-856"><a href="#cb20-856" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Two coin flips</span></span>
<span id="cb20-857"><a href="#cb20-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-858"><a href="#cb20-858" aria-hidden="true" tabindex="-1"></a>Let $X$ = number of heads for two flips of fair coins.</span>
<span id="cb20-859"><a href="#cb20-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-860"><a href="#cb20-860" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(X = 0) = 1/4$</span>
<span id="cb20-861"><a href="#cb20-861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(X = 1) = 1/2$</span>
<span id="cb20-862"><a href="#cb20-862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(X = 2) = 1/4$</span>
<span id="cb20-863"><a href="#cb20-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-864"><a href="#cb20-864" aria-hidden="true" tabindex="-1"></a>The CDF is:</span>
<span id="cb20-865"><a href="#cb20-865" aria-hidden="true" tabindex="-1"></a>$$F_X(x) = \begin{cases}</span>
<span id="cb20-866"><a href="#cb20-866" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{if } x &lt; 0 <span class="sc">\\</span></span>
<span id="cb20-867"><a href="#cb20-867" aria-hidden="true" tabindex="-1"></a>1/4 &amp; \text{if } 0 \leq x &lt; 1 <span class="sc">\\</span></span>
<span id="cb20-868"><a href="#cb20-868" aria-hidden="true" tabindex="-1"></a>3/4 &amp; \text{if } 1 \leq x &lt; 2 <span class="sc">\\</span></span>
<span id="cb20-869"><a href="#cb20-869" aria-hidden="true" tabindex="-1"></a>1 &amp; \text{if } x \geq 2</span>
<span id="cb20-870"><a href="#cb20-870" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb20-871"><a href="#cb20-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-872"><a href="#cb20-872" aria-hidden="true" tabindex="-1"></a>Note: The CDF is defined for ALL real $x$, even though $X$ only takes values 0, 1, 2!</span>
<span id="cb20-873"><a href="#cb20-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-876"><a href="#cb20-876" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-877"><a href="#cb20-877" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cdf-example</span></span>
<span id="cb20-878"><a href="#cb20-878" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Cumulative distribution function (CDF) for the number of heads when flipping a coin twice."</span></span>
<span id="cb20-879"><a href="#cb20-879" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-880"><a href="#cb20-880" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-881"><a href="#cb20-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-882"><a href="#cb20-882" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-883"><a href="#cb20-883" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-884"><a href="#cb20-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-885"><a href="#cb20-885" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the CDF values</span></span>
<span id="cb20-886"><a href="#cb20-886" aria-hidden="true" tabindex="-1"></a>x_jumps <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]  <span class="co"># Points where jumps occur</span></span>
<span id="cb20-887"><a href="#cb20-887" aria-hidden="true" tabindex="-1"></a>cdf_values <span class="op">=</span> [<span class="fl">0.25</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>]  <span class="co"># CDF values after jumps</span></span>
<span id="cb20-888"><a href="#cb20-888" aria-hidden="true" tabindex="-1"></a>cdf_values_before <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.75</span>]  <span class="co"># CDF values before jumps</span></span>
<span id="cb20-889"><a href="#cb20-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-890"><a href="#cb20-890" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-891"><a href="#cb20-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-892"><a href="#cb20-892" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the step function</span></span>
<span id="cb20-893"><a href="#cb20-893" aria-hidden="true" tabindex="-1"></a><span class="co"># Left segment (x &lt; 0)</span></span>
<span id="cb20-894"><a href="#cb20-894" aria-hidden="true" tabindex="-1"></a>ax.hlines(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-895"><a href="#cb20-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-896"><a href="#cb20-896" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each segment</span></span>
<span id="cb20-897"><a href="#cb20-897" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x_jumps)):</span>
<span id="cb20-898"><a href="#cb20-898" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Horizontal line segment</span></span>
<span id="cb20-899"><a href="#cb20-899" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(x_jumps) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb20-900"><a href="#cb20-900" aria-hidden="true" tabindex="-1"></a>        ax.hlines(cdf_values[i], x_jumps[i], x_jumps[i<span class="op">+</span><span class="dv">1</span>], colors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-901"><a href="#cb20-901" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-902"><a href="#cb20-902" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Last segment extends to the right</span></span>
<span id="cb20-903"><a href="#cb20-903" aria-hidden="true" tabindex="-1"></a>        ax.hlines(cdf_values[i], x_jumps[i], <span class="dv">3</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-904"><a href="#cb20-904" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-905"><a href="#cb20-905" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Open circles (at discontinuities, left endpoints)</span></span>
<span id="cb20-906"><a href="#cb20-906" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb20-907"><a href="#cb20-907" aria-hidden="true" tabindex="-1"></a>        ax.plot(x_jumps[i], cdf_values_before[i], <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb20-908"><a href="#cb20-908" aria-hidden="true" tabindex="-1"></a>                markerfacecolor<span class="op">=</span><span class="st">'white'</span>, markersize<span class="op">=</span><span class="dv">8</span>, markeredgewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-909"><a href="#cb20-909" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-910"><a href="#cb20-910" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filled circles (at jump points, right endpoints)</span></span>
<span id="cb20-911"><a href="#cb20-911" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_jumps[i], cdf_values[i], <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb20-912"><a href="#cb20-912" aria-hidden="true" tabindex="-1"></a>            markerfacecolor<span class="op">=</span><span class="st">'black'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb20-913"><a href="#cb20-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-914"><a href="#cb20-914" aria-hidden="true" tabindex="-1"></a><span class="co"># Open circle at x=0, y=0</span></span>
<span id="cb20-915"><a href="#cb20-915" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="dv">0</span>, <span class="dv">0</span>, <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, markerfacecolor<span class="op">=</span><span class="st">'white'</span>, </span>
<span id="cb20-916"><a href="#cb20-916" aria-hidden="true" tabindex="-1"></a>        markersize<span class="op">=</span><span class="dv">8</span>, markeredgewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-917"><a href="#cb20-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-918"><a href="#cb20-918" aria-hidden="true" tabindex="-1"></a><span class="co"># Set axis properties</span></span>
<span id="cb20-919"><a href="#cb20-919" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-920"><a href="#cb20-920" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'$F_X(x)$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-921"><a href="#cb20-921" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb20-922"><a href="#cb20-922" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>)</span>
<span id="cb20-923"><a href="#cb20-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-924"><a href="#cb20-924" aria-hidden="true" tabindex="-1"></a><span class="co"># Set tick marks</span></span>
<span id="cb20-925"><a href="#cb20-925" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb20-926"><a href="#cb20-926" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([<span class="fl">0.25</span>, <span class="fl">0.50</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>])</span>
<span id="cb20-927"><a href="#cb20-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-928"><a href="#cb20-928" aria-hidden="true" tabindex="-1"></a><span class="co"># Add grid</span></span>
<span id="cb20-929"><a href="#cb20-929" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-930"><a href="#cb20-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-931"><a href="#cb20-931" aria-hidden="true" tabindex="-1"></a><span class="co"># Add arrows to axes</span></span>
<span id="cb20-932"><a href="#cb20-932" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">''</span>, xy<span class="op">=</span>(<span class="fl">3.2</span>, <span class="dv">0</span>), xytext<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">0</span>),</span>
<span id="cb20-933"><a href="#cb20-933" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb20-934"><a href="#cb20-934" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">''</span>, xy<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1.15</span>), xytext<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1.1</span>),</span>
<span id="cb20-935"><a href="#cb20-935" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb20-936"><a href="#cb20-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-937"><a href="#cb20-937" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-938"><a href="#cb20-938" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-939"><a href="#cb20-939" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-940"><a href="#cb20-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-941"><a href="#cb20-941" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-942"><a href="#cb20-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-943"><a href="#cb20-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-944"><a href="#cb20-944" aria-hidden="true" tabindex="-1"></a><span class="fu">### Discrete Random Variables</span></span>
<span id="cb20-945"><a href="#cb20-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-946"><a href="#cb20-946" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-947"><a href="#cb20-947" aria-hidden="true" tabindex="-1"></a>A random variable $X$ is **discrete** if it takes countably many values $\{x_1, x_2, ...\}$. Its **probability mass function (PMF)** (sometimes just **probability function**) is defined as:</span>
<span id="cb20-948"><a href="#cb20-948" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \mathbb{P}(X = x)$$</span>
<span id="cb20-949"><a href="#cb20-949" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-950"><a href="#cb20-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-951"><a href="#cb20-951" aria-hidden="true" tabindex="-1"></a>Properties of PMFs:</span>
<span id="cb20-952"><a href="#cb20-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-953"><a href="#cb20-953" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f_X(x) \geq 0$ for all $x$</span>
<span id="cb20-954"><a href="#cb20-954" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sum_{i} f_X(x_i) = 1$ (probabilities sum to 1)</span>
<span id="cb20-955"><a href="#cb20-955" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$F_X(x) = \sum_{x_i \leq x} f_X(x_i)$ (CDF is sum of PMF)</span>
<span id="cb20-956"><a href="#cb20-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-959"><a href="#cb20-959" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-960"><a href="#cb20-960" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pmf-cdf-example</span></span>
<span id="cb20-961"><a href="#cb20-961" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Probability mass function (PMF) for the number of heads when flipping a coin twice."</span></span>
<span id="cb20-962"><a href="#cb20-962" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-963"><a href="#cb20-963" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-964"><a href="#cb20-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-965"><a href="#cb20-965" aria-hidden="true" tabindex="-1"></a><span class="co"># PMF for coin flipping example</span></span>
<span id="cb20-966"><a href="#cb20-966" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb20-967"><a href="#cb20-967" aria-hidden="true" tabindex="-1"></a>pmf_values <span class="op">=</span> [<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>]</span>
<span id="cb20-968"><a href="#cb20-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-969"><a href="#cb20-969" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-970"><a href="#cb20-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-971"><a href="#cb20-971" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot vertical lines from x-axis to probability values</span></span>
<span id="cb20-972"><a href="#cb20-972" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, p <span class="kw">in</span> <span class="bu">zip</span>(x_values, pmf_values):</span>
<span id="cb20-973"><a href="#cb20-973" aria-hidden="true" tabindex="-1"></a>    ax.plot([x, x], [<span class="dv">0</span>, p], <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-974"><a href="#cb20-974" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add filled circles at the top</span></span>
<span id="cb20-975"><a href="#cb20-975" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, p, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>, markerfacecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb20-976"><a href="#cb20-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-977"><a href="#cb20-977" aria-hidden="true" tabindex="-1"></a><span class="co"># Set axis properties</span></span>
<span id="cb20-978"><a href="#cb20-978" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-979"><a href="#cb20-979" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'$f_X(x)$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-980"><a href="#cb20-980" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span>)</span>
<span id="cb20-981"><a href="#cb20-981" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb20-982"><a href="#cb20-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-983"><a href="#cb20-983" aria-hidden="true" tabindex="-1"></a><span class="co"># Set tick marks</span></span>
<span id="cb20-984"><a href="#cb20-984" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb20-985"><a href="#cb20-985" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>])</span>
<span id="cb20-986"><a href="#cb20-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-987"><a href="#cb20-987" aria-hidden="true" tabindex="-1"></a><span class="co"># Add grid</span></span>
<span id="cb20-988"><a href="#cb20-988" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb20-989"><a href="#cb20-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-990"><a href="#cb20-990" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a horizontal line at y=0 for clarity</span></span>
<span id="cb20-991"><a href="#cb20-991" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb20-992"><a href="#cb20-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-993"><a href="#cb20-993" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-994"><a href="#cb20-994" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-995"><a href="#cb20-995" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-996"><a href="#cb20-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-997"><a href="#cb20-997" aria-hidden="true" tabindex="-1"></a><span class="fu">### Core Discrete Distributions</span></span>
<span id="cb20-998"><a href="#cb20-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-999"><a href="#cb20-999" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-1000"><a href="#cb20-1000" aria-hidden="true" tabindex="-1"></a>**Notation preview**: We'll use $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>$ to denote the *expected value* (mean) of a random variable $X$, and $\text{Var}(X)$ or $\sigma^2$ for its *variance* (a measure of spread). These concepts will be covered in detail in Chapter 2 of the lecture notes.</span>
<span id="cb20-1001"><a href="#cb20-1001" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1002"><a href="#cb20-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1003"><a href="#cb20-1003" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bernoulli Distribution</span></span>
<span id="cb20-1004"><a href="#cb20-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1005"><a href="#cb20-1005" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">Bernoulli distribution</span><span class="co">](https://en.wikipedia.org/wiki/Bernoulli_distribution)</span> is the simplest non-trivial random variable -- a single binary outcome with probability $p \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ of happening.</span>
<span id="cb20-1006"><a href="#cb20-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1007"><a href="#cb20-1007" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1008"><a href="#cb20-1008" aria-hidden="true" tabindex="-1"></a>$X \sim \text{Bernoulli}(p)$ if:</span>
<span id="cb20-1009"><a href="#cb20-1009" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \begin{cases}</span>
<span id="cb20-1010"><a href="#cb20-1010" aria-hidden="true" tabindex="-1"></a>p &amp; \text{if } x = 1 <span class="sc">\\</span></span>
<span id="cb20-1011"><a href="#cb20-1011" aria-hidden="true" tabindex="-1"></a>1-p &amp; \text{if } x = 0 <span class="sc">\\</span></span>
<span id="cb20-1012"><a href="#cb20-1012" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{otherwise}</span>
<span id="cb20-1013"><a href="#cb20-1013" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb20-1014"><a href="#cb20-1014" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1015"><a href="#cb20-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1016"><a href="#cb20-1016" aria-hidden="true" tabindex="-1"></a>An outcome of $X = 1$ is often referred to as a "hit" or a "success", while $X = 0$ is a "miss" or a "failure".</span>
<span id="cb20-1017"><a href="#cb20-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1018"><a href="#cb20-1018" aria-hidden="true" tabindex="-1"></a>**Use cases**: </span>
<span id="cb20-1019"><a href="#cb20-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1020"><a href="#cb20-1020" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coin flip (heads/tails) </span>
<span id="cb20-1021"><a href="#cb20-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>If $p \neq 0.5$, this is known as a *biased* coin (as opposed to a *fair* coin with $p = 0.5$)</span>
<span id="cb20-1022"><a href="#cb20-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Here what constitutes a "hit" and a "miss" is arbitrary!</span>
<span id="cb20-1023"><a href="#cb20-1023" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Success/failure of a single trial</span>
<span id="cb20-1024"><a href="#cb20-1024" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary classification (spam/not spam)</span>
<span id="cb20-1025"><a href="#cb20-1025" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>User clicks/doesn't click an ad</span>
<span id="cb20-1026"><a href="#cb20-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1029"><a href="#cb20-1029" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1030"><a href="#cb20-1030" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1031"><a href="#cb20-1031" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-1032"><a href="#cb20-1032" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli distribution PMF</span></span>
<span id="cb20-1033"><a href="#cb20-1033" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1034"><a href="#cb20-1034" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1035"><a href="#cb20-1035" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli</span>
<span id="cb20-1036"><a href="#cb20-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1037"><a href="#cb20-1037" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameter</span></span>
<span id="cb20-1038"><a href="#cb20-1038" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.3</span>  <span class="co"># probability of success</span></span>
<span id="cb20-1039"><a href="#cb20-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1040"><a href="#cb20-1040" aria-hidden="true" tabindex="-1"></a><span class="co"># PMF visualization</span></span>
<span id="cb20-1041"><a href="#cb20-1041" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-1042"><a href="#cb20-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1043"><a href="#cb20-1043" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot PMF</span></span>
<span id="cb20-1044"><a href="#cb20-1044" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb20-1045"><a href="#cb20-1045" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> [<span class="dv">1</span><span class="op">-</span>p, p]</span>
<span id="cb20-1046"><a href="#cb20-1046" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, pmf, width<span class="op">=</span><span class="fl">0.4</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span>[<span class="st">'lightcoral'</span>, <span class="st">'lightblue'</span>], </span>
<span id="cb20-1047"><a href="#cb20-1047" aria-hidden="true" tabindex="-1"></a>               edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-1048"><a href="#cb20-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1049"><a href="#cb20-1049" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels</span></span>
<span id="cb20-1050"><a href="#cb20-1050" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, pi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, pmf)):</span>
<span id="cb20-1051"><a href="#cb20-1051" aria-hidden="true" tabindex="-1"></a>    ax.text(xi, pi <span class="op">+</span> <span class="fl">0.02</span>, <span class="ss">f'</span><span class="sc">{</span>pi<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-1052"><a href="#cb20-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1053"><a href="#cb20-1053" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb20-1054"><a href="#cb20-1054" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'Failure (0)'</span>, <span class="st">'Success (1)'</span>])</span>
<span id="cb20-1055"><a href="#cb20-1055" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb20-1056"><a href="#cb20-1056" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Bernoulli Distribution PMF (p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb20-1057"><a href="#cb20-1057" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb20-1058"><a href="#cb20-1058" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb20-1059"><a href="#cb20-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1060"><a href="#cb20-1060" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1061"><a href="#cb20-1061" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1062"><a href="#cb20-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1063"><a href="#cb20-1063" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1064"><a href="#cb20-1064" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = p(1-p) = </span><span class="sc">{</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-1065"><a href="#cb20-1065" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1066"><a href="#cb20-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1067"><a href="#cb20-1067" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Binomial Distribution</span></span>
<span id="cb20-1068"><a href="#cb20-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1069"><a href="#cb20-1069" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">binomial distribution</span><span class="co">](https://en.wikipedia.org/wiki/Binomial_distribution)</span> counts the number of successes in a fixed number $n$ of independent Bernoulli trials each with probability $p$.</span>
<span id="cb20-1070"><a href="#cb20-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1071"><a href="#cb20-1071" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1072"><a href="#cb20-1072" aria-hidden="true" tabindex="-1"></a>$X \sim \text{Binomial}(n, p)$ if:</span>
<span id="cb20-1073"><a href="#cb20-1073" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x = 0, 1, ..., n$$</span>
<span id="cb20-1074"><a href="#cb20-1074" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1075"><a href="#cb20-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1076"><a href="#cb20-1076" aria-hidden="true" tabindex="-1"></a>**Key properties**:</span>
<span id="cb20-1077"><a href="#cb20-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1078"><a href="#cb20-1078" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sum of independent Bernoullis: If $X_i \sim \text{Bernoulli}(p)$ are independent, then $\sum_{i=1}^n X_i \sim \text{Binomial}(n, p)$</span>
<span id="cb20-1079"><a href="#cb20-1079" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Additivity: If $X \sim \text{Binomial}(n_1, p)$ and $Y \sim \text{Binomial}(n_2, p)$ are independent, then $X + Y \sim \text{Binomial}(n_1 + n_2, p)$</span>
<span id="cb20-1080"><a href="#cb20-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1081"><a href="#cb20-1081" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1082"><a href="#cb20-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1083"><a href="#cb20-1083" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Number of heads in $n$ coin flips</span>
<span id="cb20-1084"><a href="#cb20-1084" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Number of defective items in a batch</span>
<span id="cb20-1085"><a href="#cb20-1085" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Number of customers who make a purchase</span>
<span id="cb20-1086"><a href="#cb20-1086" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Number of successful treatments in a clinical trial</span>
<span id="cb20-1087"><a href="#cb20-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1088"><a href="#cb20-1088" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb20-1089"><a href="#cb20-1089" aria-hidden="true" tabindex="-1"></a>**Independence assumption**: The binomial distribution assumes all trials are independent - each outcome does not affect the probability of subsequent outcomes. This assumption may not hold in practice! </span>
<span id="cb20-1090"><a href="#cb20-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1091"><a href="#cb20-1091" aria-hidden="true" tabindex="-1"></a>For example, if items are defective because a machine has broken (rather than random variation), then finding one defective item suggests all subsequent items might also be defective. In such cases, the binomial distribution would be inappropriate.</span>
<span id="cb20-1092"><a href="#cb20-1092" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1093"><a href="#cb20-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1096"><a href="#cb20-1096" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1097"><a href="#cb20-1097" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1098"><a href="#cb20-1098" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb20-1099"><a href="#cb20-1099" aria-hidden="true" tabindex="-1"></a><span class="co"># Binomial distribution visualization</span></span>
<span id="cb20-1100"><a href="#cb20-1100" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1101"><a href="#cb20-1101" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1102"><a href="#cb20-1102" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb20-1103"><a href="#cb20-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1104"><a href="#cb20-1104" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb20-1105"><a href="#cb20-1105" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> <span class="dv">20</span>, <span class="fl">0.3</span></span>
<span id="cb20-1106"><a href="#cb20-1106" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">0</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb20-1107"><a href="#cb20-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1108"><a href="#cb20-1108" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure</span></span>
<span id="cb20-1109"><a href="#cb20-1109" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb20-1110"><a href="#cb20-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1111"><a href="#cb20-1111" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot PMF</span></span>
<span id="cb20-1112"><a href="#cb20-1112" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> binom.pmf(x, n, p)</span>
<span id="cb20-1113"><a href="#cb20-1113" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, pmf, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'steelblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb20-1114"><a href="#cb20-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1115"><a href="#cb20-1115" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight mean</span></span>
<span id="cb20-1116"><a href="#cb20-1116" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> n <span class="op">*</span> p</span>
<span id="cb20-1117"><a href="#cb20-1117" aria-hidden="true" tabindex="-1"></a>ax.axvline(mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Mean = </span><span class="sc">{</span>mean<span class="sc">:.1f}</span><span class="ss">'</span>)</span>
<span id="cb20-1118"><a href="#cb20-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1119"><a href="#cb20-1119" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels on significant bars</span></span>
<span id="cb20-1120"><a href="#cb20-1120" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, pi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, pmf)):</span>
<span id="cb20-1121"><a href="#cb20-1121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pi <span class="op">&gt;</span> <span class="fl">0.01</span>:  <span class="co"># Only label visible bars</span></span>
<span id="cb20-1122"><a href="#cb20-1122" aria-hidden="true" tabindex="-1"></a>        ax.text(xi, pi <span class="op">+</span> <span class="fl">0.003</span>, <span class="ss">f'</span><span class="sc">{</span>pi<span class="sc">:.3f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb20-1123"><a href="#cb20-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1124"><a href="#cb20-1124" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Number of successes (k)'</span>)</span>
<span id="cb20-1125"><a href="#cb20-1125" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'P(X = k)'</span>)</span>
<span id="cb20-1126"><a href="#cb20-1126" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Binomial Distribution PMF: n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, p=</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-1127"><a href="#cb20-1127" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb20-1128"><a href="#cb20-1128" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb20-1129"><a href="#cb20-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1130"><a href="#cb20-1130" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1131"><a href="#cb20-1131" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1132"><a href="#cb20-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1133"><a href="#cb20-1133" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = np = </span><span class="sc">{</span>n<span class="op">*</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1134"><a href="#cb20-1134" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = np(1-p) = </span><span class="sc">{</span>n<span class="op">*</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1135"><a href="#cb20-1135" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"σ = </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(n<span class="op">*</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-1136"><a href="#cb20-1136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1137"><a href="#cb20-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1138"><a href="#cb20-1138" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Discrete Uniform Distribution</span></span>
<span id="cb20-1139"><a href="#cb20-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1140"><a href="#cb20-1140" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">discrete uniform distribution</span><span class="co">](https://en.wikipedia.org/wiki/Discrete_uniform_distribution)</span> is another simple discrete distribution - every outcome is equally likely.</span>
<span id="cb20-1141"><a href="#cb20-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1142"><a href="#cb20-1142" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1143"><a href="#cb20-1143" aria-hidden="true" tabindex="-1"></a>$X \sim \text{DiscreteUniform}(a, b)$ if:</span>
<span id="cb20-1144"><a href="#cb20-1144" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \frac{1}{b-a+1}, \quad x \in <span class="sc">\{</span>a, a+1, ..., b<span class="sc">\}</span>$$</span>
<span id="cb20-1145"><a href="#cb20-1145" aria-hidden="true" tabindex="-1"></a>where $a$ and $b$ are integers with $a \leq b$.</span>
<span id="cb20-1146"><a href="#cb20-1146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1147"><a href="#cb20-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1148"><a href="#cb20-1148" aria-hidden="true" tabindex="-1"></a>**Key properties**:</span>
<span id="cb20-1149"><a href="#cb20-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1150"><a href="#cb20-1150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{a+b}{2}$</span>
<span id="cb20-1151"><a href="#cb20-1151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(X) = \frac{(b-a+1)^2 - 1}{12}$</span>
<span id="cb20-1152"><a href="#cb20-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1153"><a href="#cb20-1153" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1154"><a href="#cb20-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1155"><a href="#cb20-1155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fair die roll: $\text{DiscreteUniform}(1, 6)$</span>
<span id="cb20-1156"><a href="#cb20-1156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Attack roll</span><span class="co">](https://en.wikipedia.org/wiki/Dice#Polyhedral_dice)</span>: $\text{DiscreteUniform}(1, 20)$</span>
<span id="cb20-1157"><a href="#cb20-1157" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random selection from a finite set</span>
<span id="cb20-1158"><a href="#cb20-1158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lottery number selection</span>
<span id="cb20-1159"><a href="#cb20-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1162"><a href="#cb20-1162" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1163"><a href="#cb20-1163" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1164"><a href="#cb20-1164" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-1165"><a href="#cb20-1165" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete Uniform distribution</span></span>
<span id="cb20-1166"><a href="#cb20-1166" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1167"><a href="#cb20-1167" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1168"><a href="#cb20-1168" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> randint</span>
<span id="cb20-1169"><a href="#cb20-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1170"><a href="#cb20-1170" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: fair die</span></span>
<span id="cb20-1171"><a href="#cb20-1171" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="dv">1</span>, <span class="dv">6</span></span>
<span id="cb20-1172"><a href="#cb20-1172" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(a, b<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb20-1173"><a href="#cb20-1173" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> [<span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a<span class="op">+</span><span class="dv">1</span>)] <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb20-1174"><a href="#cb20-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1175"><a href="#cb20-1175" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-1176"><a href="#cb20-1176" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, pmf, width<span class="op">=</span><span class="fl">0.6</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'lightgreen'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-1177"><a href="#cb20-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1178"><a href="#cb20-1178" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels</span></span>
<span id="cb20-1179"><a href="#cb20-1179" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, pi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, pmf)):</span>
<span id="cb20-1180"><a href="#cb20-1180" aria-hidden="true" tabindex="-1"></a>    ax.text(xi, pi <span class="op">+</span> <span class="fl">0.01</span>, <span class="ss">f'</span><span class="sc">{</span>pi<span class="sc">:.3f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb20-1181"><a href="#cb20-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1182"><a href="#cb20-1182" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Outcome'</span>)</span>
<span id="cb20-1183"><a href="#cb20-1183" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb20-1184"><a href="#cb20-1184" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Discrete Uniform Distribution: Fair Die'</span>)</span>
<span id="cb20-1185"><a href="#cb20-1185" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.3</span>)</span>
<span id="cb20-1186"><a href="#cb20-1186" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb20-1187"><a href="#cb20-1187" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb20-1188"><a href="#cb20-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1189"><a href="#cb20-1189" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1190"><a href="#cb20-1190" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1191"><a href="#cb20-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1192"><a href="#cb20-1192" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = </span><span class="sc">{</span>(a<span class="op">+</span>b)<span class="op">/</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1193"><a href="#cb20-1193" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = </span><span class="sc">{</span>((b<span class="op">-</span>a<span class="op">+</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span><span class="dv">12</span><span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-1194"><a href="#cb20-1194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1195"><a href="#cb20-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1196"><a href="#cb20-1196" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Categorical Distribution</span></span>
<span id="cb20-1197"><a href="#cb20-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1198"><a href="#cb20-1198" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">categorical distribution</span><span class="co">](https://en.wikipedia.org/wiki/Categorical_distribution)</span> is a generalization of Bernoulli to multiple categories (also called "Generalized Bernoulli" or "Multinoulli").</span>
<span id="cb20-1199"><a href="#cb20-1199" aria-hidden="true" tabindex="-1"></a>You can also see it as a generalization of the discrete uniform distribution to a discrete *non*-uniform distribution.</span>
<span id="cb20-1200"><a href="#cb20-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1201"><a href="#cb20-1201" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1202"><a href="#cb20-1202" aria-hidden="true" tabindex="-1"></a>$X \sim \text{Categorical}(p_1, ..., p_k)$ if:</span>
<span id="cb20-1203"><a href="#cb20-1203" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = p_x, \quad x \in <span class="sc">\{</span>1, 2, ..., k<span class="sc">\}</span>$$</span>
<span id="cb20-1204"><a href="#cb20-1204" aria-hidden="true" tabindex="-1"></a>where $p_i \geq 0$ and $\sum_{i=1}^k p_i = 1$.</span>
<span id="cb20-1205"><a href="#cb20-1205" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1206"><a href="#cb20-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1207"><a href="#cb20-1207" aria-hidden="true" tabindex="-1"></a>**Key properties**:</span>
<span id="cb20-1208"><a href="#cb20-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1209"><a href="#cb20-1209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One-hot encoding: Often represented as a vector with one 1 and rest 0s</span>
<span id="cb20-1210"><a href="#cb20-1210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Special case: Categorical with $k=2$ is equivalent to Bernoulli</span>
<span id="cb20-1211"><a href="#cb20-1211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Special case: If all probabilities are equal, it becomes a discrete uniform</span>
<span id="cb20-1212"><a href="#cb20-1212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Foundation for multinomial distribution (multiple categorical trials)</span>
<span id="cb20-1213"><a href="#cb20-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1214"><a href="#cb20-1214" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1215"><a href="#cb20-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1216"><a href="#cb20-1216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Outcome of rolling a (possibly unfair) die</span>
<span id="cb20-1217"><a href="#cb20-1217" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Classification into multiple categories</span>
<span id="cb20-1218"><a href="#cb20-1218" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Language modeling (next-token prediction)^<span class="co">[</span><span class="ot">In modern LLMs, the categorical distribution is over tokens (parts of words), not full words. The token vocabulary can be huge - tens of thousands of different tokens like "a", "aba", "add", etc. GPT models typically use vocabularies of 50,000-100,000 tokens.</span><span class="co">]</span></span>
<span id="cb20-1219"><a href="#cb20-1219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Customer choice among products</span>
<span id="cb20-1220"><a href="#cb20-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1223"><a href="#cb20-1223" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1224"><a href="#cb20-1224" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1225"><a href="#cb20-1225" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-1226"><a href="#cb20-1226" aria-hidden="true" tabindex="-1"></a><span class="co"># Categorical distribution</span></span>
<span id="cb20-1227"><a href="#cb20-1227" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1228"><a href="#cb20-1228" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1229"><a href="#cb20-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1230"><a href="#cb20-1230" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Customer choice among 5 products</span></span>
<span id="cb20-1231"><a href="#cb20-1231" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> [<span class="st">'Product A'</span>, <span class="st">'Product B'</span>, <span class="st">'Product C'</span>, <span class="st">'Product D'</span>, <span class="st">'Product E'</span>]</span>
<span id="cb20-1232"><a href="#cb20-1232" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> [<span class="fl">0.30</span>, <span class="fl">0.25</span>, <span class="fl">0.20</span>, <span class="fl">0.15</span>, <span class="fl">0.10</span>]</span>
<span id="cb20-1233"><a href="#cb20-1233" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(categories))</span>
<span id="cb20-1234"><a href="#cb20-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1235"><a href="#cb20-1235" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-1236"><a href="#cb20-1236" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x, probabilities, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span>[<span class="st">'#1f77b4'</span>, <span class="st">'#ff7f0e'</span>, <span class="st">'#2ca02c'</span>, <span class="st">'#d62728'</span>, <span class="st">'#9467bd'</span>],</span>
<span id="cb20-1237"><a href="#cb20-1237" aria-hidden="true" tabindex="-1"></a>               edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-1238"><a href="#cb20-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1239"><a href="#cb20-1239" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels</span></span>
<span id="cb20-1240"><a href="#cb20-1240" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(probabilities):</span>
<span id="cb20-1241"><a href="#cb20-1241" aria-hidden="true" tabindex="-1"></a>    ax.text(i, p <span class="op">+</span> <span class="fl">0.01</span>, <span class="ss">f'</span><span class="sc">{</span>p<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb20-1242"><a href="#cb20-1242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1243"><a href="#cb20-1243" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Category'</span>)</span>
<span id="cb20-1244"><a href="#cb20-1244" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb20-1245"><a href="#cb20-1245" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Categorical Distribution: Customer Product Choice'</span>)</span>
<span id="cb20-1246"><a href="#cb20-1246" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb20-1247"><a href="#cb20-1247" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(categories, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb20-1248"><a href="#cb20-1248" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.4</span>)</span>
<span id="cb20-1249"><a href="#cb20-1249" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb20-1250"><a href="#cb20-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1251"><a href="#cb20-1251" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1252"><a href="#cb20-1252" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1253"><a href="#cb20-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1254"><a href="#cb20-1254" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected value for indicator representation (does it make sense here?)</span></span>
<span id="cb20-1255"><a href="#cb20-1255" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"If we encode categories as 1, 2, 3, 4, 5:"</span>)</span>
<span id="cb20-1256"><a href="#cb20-1256" aria-hidden="true" tabindex="-1"></a>expected <span class="op">=</span> <span class="bu">sum</span>((i<span class="op">+</span><span class="dv">1</span>) <span class="op">*</span> p <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(probabilities))</span>
<span id="cb20-1257"><a href="#cb20-1257" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = </span><span class="sc">{</span>expected<span class="sc">:.2f}</span><span class="ss"> (does it really make sense here?)"</span>)</span>
<span id="cb20-1258"><a href="#cb20-1258" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1259"><a href="#cb20-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1260"><a href="#cb20-1260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Brief Catalog: Other Discrete Distributions</span></span>
<span id="cb20-1261"><a href="#cb20-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1262"><a href="#cb20-1262" aria-hidden="true" tabindex="-1"></a>**Poisson($\lambda$)**: The <span class="co">[</span><span class="ot">Poisson distribution</span><span class="co">](https://en.wikipedia.org/wiki/Poisson_distribution)</span> models count of rare events in fixed intervals:</span>
<span id="cb20-1263"><a href="#cb20-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1264"><a href="#cb20-1264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PMF: $f_X(x) = e^{-\lambda} \frac{\lambda^x}{x!}$ for $x = 0, 1, 2, ...$</span>
<span id="cb20-1265"><a href="#cb20-1265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mean = Variance = $\lambda$ (*lambda*)</span>
<span id="cb20-1266"><a href="#cb20-1266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use: Email arrivals, typos per page, customer arrivals</span>
<span id="cb20-1267"><a href="#cb20-1267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Approximates Binomial($n,p$) when $n$ large, $p$ small: use $\lambda = np$</span>
<span id="cb20-1268"><a href="#cb20-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1269"><a href="#cb20-1269" aria-hidden="true" tabindex="-1"></a>**Geometric($p$)**: The <span class="co">[</span><span class="ot">geometric distribution</span><span class="co">](https://en.wikipedia.org/wiki/Geometric_distribution)</span> represents the number of trials until first success:</span>
<span id="cb20-1270"><a href="#cb20-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1271"><a href="#cb20-1271" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PMF: $f_X(x) = p(1-p)^{x-1}$ for $x = 1, 2, ...$</span>
<span id="cb20-1272"><a href="#cb20-1272" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use: Waiting times, number of attempts until success</span>
<span id="cb20-1273"><a href="#cb20-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1274"><a href="#cb20-1274" aria-hidden="true" tabindex="-1"></a>**Negative Binomial($r, p$)**: The <span class="co">[</span><span class="ot">negative binomial</span><span class="co">](https://en.wikipedia.org/wiki/Negative_binomial_distribution)</span> represents the number of failures before $r$th success</span>
<span id="cb20-1275"><a href="#cb20-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1276"><a href="#cb20-1276" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generalization of geometric distribution</span>
<span id="cb20-1277"><a href="#cb20-1277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use: Overdispersed count data, robust alternative to Poisson</span>
<span id="cb20-1278"><a href="#cb20-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1279"><a href="#cb20-1279" aria-hidden="true" tabindex="-1"></a><span class="fu">### Continuous Random Variables</span></span>
<span id="cb20-1280"><a href="#cb20-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1281"><a href="#cb20-1281" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1282"><a href="#cb20-1282" aria-hidden="true" tabindex="-1"></a>A random variable $X$ is **continuous** if there exists a function $f_X$ such that:</span>
<span id="cb20-1283"><a href="#cb20-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1284"><a href="#cb20-1284" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$f_X(x) \geq 0$ for all $x$</span>
<span id="cb20-1285"><a href="#cb20-1285" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\int_{-\infty}^{\infty} f_X(x) dx = 1$</span>
<span id="cb20-1286"><a href="#cb20-1286" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>For any $a &lt; b$: $\mathbb{P}(a &lt; X &lt; b) = \int_a^b f_X(x) dx$</span>
<span id="cb20-1287"><a href="#cb20-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1288"><a href="#cb20-1288" aria-hidden="true" tabindex="-1"></a>The function $f_X$ is called the **probability density function (PDF)**.</span>
<span id="cb20-1289"><a href="#cb20-1289" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1290"><a href="#cb20-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1291"><a href="#cb20-1291" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb20-1292"><a href="#cb20-1292" aria-hidden="true" tabindex="-1"></a>**Important distinctions from discrete case**:</span>
<span id="cb20-1293"><a href="#cb20-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1294"><a href="#cb20-1294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}(X = x) = 0$ for any single point $x$: in a continuum, there is zero probability of picking one specific point</span>
<span id="cb20-1295"><a href="#cb20-1295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PDF can exceed 1 (it's a *density*, not a probability!)</span>
<span id="cb20-1296"><a href="#cb20-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We get probabilities by integrating densities over an interval, not summing</span>
<span id="cb20-1297"><a href="#cb20-1297" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1298"><a href="#cb20-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1299"><a href="#cb20-1299" aria-hidden="true" tabindex="-1"></a>**Relationship between PDF and CDF**:</span>
<span id="cb20-1300"><a href="#cb20-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1301"><a href="#cb20-1301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$F_X(x) = \int_{-\infty}^x f_X(t) dt$</span>
<span id="cb20-1302"><a href="#cb20-1302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f_X(x) = F_X'(x)$ (where the derivative exists)</span>
<span id="cb20-1303"><a href="#cb20-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1304"><a href="#cb20-1304" aria-hidden="true" tabindex="-1"></a><span class="fu">### Core Continuous Distributions</span></span>
<span id="cb20-1305"><a href="#cb20-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1306"><a href="#cb20-1306" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Uniform Distribution</span></span>
<span id="cb20-1307"><a href="#cb20-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1308"><a href="#cb20-1308" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">uniform distribution</span><span class="co">](https://en.wikipedia.org/wiki/Continuous_uniform_distribution)</span> is the continuous analog of "equally likely outcomes."</span>
<span id="cb20-1309"><a href="#cb20-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1310"><a href="#cb20-1310" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1311"><a href="#cb20-1311" aria-hidden="true" tabindex="-1"></a>$X \sim \text{Uniform}(a, b)$ if:</span>
<span id="cb20-1312"><a href="#cb20-1312" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \begin{cases}</span>
<span id="cb20-1313"><a href="#cb20-1313" aria-hidden="true" tabindex="-1"></a>\frac{1}{b-a} &amp; \text{if } a \leq x \leq b <span class="sc">\\</span></span>
<span id="cb20-1314"><a href="#cb20-1314" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{otherwise}</span>
<span id="cb20-1315"><a href="#cb20-1315" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb20-1316"><a href="#cb20-1316" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1317"><a href="#cb20-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1318"><a href="#cb20-1318" aria-hidden="true" tabindex="-1"></a>**Properties**:</span>
<span id="cb20-1319"><a href="#cb20-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1320"><a href="#cb20-1320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CDF: $F_X(x) = \frac{x-a}{b-a}$ for $a \leq x \leq b$</span>
<span id="cb20-1321"><a href="#cb20-1321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Every interval of equal length has equal probability</span>
<span id="cb20-1322"><a href="#cb20-1322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1323"><a href="#cb20-1323" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1324"><a href="#cb20-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1325"><a href="#cb20-1325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random number generation ($\text{Uniform}(0,1)$ is fundamental in computational statistics)</span>
<span id="cb20-1326"><a href="#cb20-1326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modeling complete uncertainty within bounds</span>
<span id="cb20-1327"><a href="#cb20-1327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Arrival times when "any time is equally likely"</span>
<span id="cb20-1328"><a href="#cb20-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1331"><a href="#cb20-1331" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1332"><a href="#cb20-1332" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1333"><a href="#cb20-1333" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb20-1334"><a href="#cb20-1334" aria-hidden="true" tabindex="-1"></a><span class="co"># Uniform distribution visualization</span></span>
<span id="cb20-1335"><a href="#cb20-1335" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1336"><a href="#cb20-1336" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1337"><a href="#cb20-1337" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> uniform</span>
<span id="cb20-1338"><a href="#cb20-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1339"><a href="#cb20-1339" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Uniform(a=2, b=5)</span></span>
<span id="cb20-1340"><a href="#cb20-1340" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span></span>
<span id="cb20-1341"><a href="#cb20-1341" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">1000</span>)</span>
<span id="cb20-1342"><a href="#cb20-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1343"><a href="#cb20-1343" aria-hidden="true" tabindex="-1"></a><span class="co"># PDF</span></span>
<span id="cb20-1344"><a href="#cb20-1344" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> uniform.pdf(x, loc<span class="op">=</span>a, scale<span class="op">=</span>b<span class="op">-</span>a)</span>
<span id="cb20-1345"><a href="#cb20-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1346"><a href="#cb20-1346" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb20-1347"><a href="#cb20-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1348"><a href="#cb20-1348" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the PDF</span></span>
<span id="cb20-1349"><a href="#cb20-1349" aria-hidden="true" tabindex="-1"></a>ax.plot(x, pdf, linewidth<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, label<span class="op">=</span><span class="ss">f'Uniform(</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb20-1350"><a href="#cb20-1350" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x, <span class="dv">0</span>, pdf, where<span class="op">=</span>(x <span class="op">&gt;=</span> a) <span class="op">&amp;</span> (x <span class="op">&lt;=</span> b), alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'lightblue'</span>)</span>
<span id="cb20-1351"><a href="#cb20-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1352"><a href="#cb20-1352" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the boundaries</span></span>
<span id="cb20-1353"><a href="#cb20-1353" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>a, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb20-1354"><a href="#cb20-1354" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>b, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb20-1355"><a href="#cb20-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1356"><a href="#cb20-1356" aria-hidden="true" tabindex="-1"></a><span class="co"># Add height label</span></span>
<span id="cb20-1357"><a href="#cb20-1357" aria-hidden="true" tabindex="-1"></a>ax.text((a<span class="op">+</span>b)<span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a) <span class="op">+</span> <span class="fl">0.02</span>, <span class="ss">f'height = 1/</span><span class="sc">{</span>b<span class="op">-</span>a<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a)<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb20-1358"><a href="#cb20-1358" aria-hidden="true" tabindex="-1"></a>        ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb20-1359"><a href="#cb20-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1360"><a href="#cb20-1360" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb20-1361"><a href="#cb20-1361" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density f(x)'</span>)</span>
<span id="cb20-1362"><a href="#cb20-1362" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Uniform Distribution PDF'</span>)</span>
<span id="cb20-1363"><a href="#cb20-1363" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb20-1364"><a href="#cb20-1364" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-1365"><a href="#cb20-1365" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb20-1366"><a href="#cb20-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1367"><a href="#cb20-1367" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1368"><a href="#cb20-1368" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1369"><a href="#cb20-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1370"><a href="#cb20-1370" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = (a+b)/2 = </span><span class="sc">{</span>(a<span class="op">+</span>b)<span class="op">/</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1371"><a href="#cb20-1371" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = (b-a)²/12 = </span><span class="sc">{</span>(b<span class="op">-</span>a)<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">12</span><span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-1372"><a href="#cb20-1372" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total area under curve = </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>(b<span class="op">-</span>a)<span class="sc">}</span><span class="ss"> × </span><span class="sc">{</span>b<span class="op">-</span>a<span class="sc">}</span><span class="ss"> = 1 ✓"</span>)</span>
<span id="cb20-1373"><a href="#cb20-1373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1374"><a href="#cb20-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1375"><a href="#cb20-1375" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Normal (Gaussian) Distribution</span></span>
<span id="cb20-1376"><a href="#cb20-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1377"><a href="#cb20-1377" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">normal distribution</span><span class="co">](https://en.wikipedia.org/wiki/Normal_distribution)</span> (also called Gaussian distribution) is the most important distribution in statistics, arising from the Central Limit Theorem.</span>
<span id="cb20-1378"><a href="#cb20-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1379"><a href="#cb20-1379" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1380"><a href="#cb20-1380" aria-hidden="true" tabindex="-1"></a>$X \sim \text{Normal}(\mu, \sigma^2)$ or $\mathcal{N}(\mu, \sigma^2)$ if:</span>
<span id="cb20-1381"><a href="#cb20-1381" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$</span>
<span id="cb20-1382"><a href="#cb20-1382" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1383"><a href="#cb20-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1384"><a href="#cb20-1384" aria-hidden="true" tabindex="-1"></a>**Parameters**:</span>
<span id="cb20-1385"><a href="#cb20-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1386"><a href="#cb20-1386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu$ (*mu*): mean (center of distribution)</span>
<span id="cb20-1387"><a href="#cb20-1387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma^2$ (*sigma* squared): variance ($\sigma$ is standard deviation - controls spread)</span>
<span id="cb20-1388"><a href="#cb20-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1389"><a href="#cb20-1389" aria-hidden="true" tabindex="-1"></a>**Key properties**:</span>
<span id="cb20-1390"><a href="#cb20-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1391"><a href="#cb20-1391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Symmetric bell curve centered at $\mu$</span>
<span id="cb20-1392"><a href="#cb20-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>About 68% of probability within $\mu \pm \sigma$</span>
<span id="cb20-1393"><a href="#cb20-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>About 95% within $\mu \pm 2\sigma$  </span>
<span id="cb20-1394"><a href="#cb20-1394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>About 99.7% within $\mu \pm 3\sigma$</span>
<span id="cb20-1395"><a href="#cb20-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1396"><a href="#cb20-1396" aria-hidden="true" tabindex="-1"></a>**Standard Normal**: $Z \sim \mathcal{N}(0, 1)$ has $\mu = 0$, $\sigma = 1$</span>
<span id="cb20-1397"><a href="#cb20-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1398"><a href="#cb20-1398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Any normal can be standardized: If $X \sim \mathcal{N}(\mu, \sigma^2)$, then $Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)$</span>
<span id="cb20-1399"><a href="#cb20-1399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This allows us to use standard normal tables for any normal distribution</span>
<span id="cb20-1400"><a href="#cb20-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1401"><a href="#cb20-1401" aria-hidden="true" tabindex="-1"></a>**Additivity**: If $X_i \sim \mathcal{N}(\mu_i, \sigma_i^2)$ are independent, then:</span>
<span id="cb20-1402"><a href="#cb20-1402" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^n X_i \sim \mathcal{N}\left(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma_i^2\right)$$</span>
<span id="cb20-1403"><a href="#cb20-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1404"><a href="#cb20-1404" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1405"><a href="#cb20-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1406"><a href="#cb20-1406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measurement errors</span>
<span id="cb20-1407"><a href="#cb20-1407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Heights, weights, test scores in large populations</span>
<span id="cb20-1408"><a href="#cb20-1408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sum of many small independent effects (CLT)</span>
<span id="cb20-1409"><a href="#cb20-1409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Approximation for many other distributions</span>
<span id="cb20-1410"><a href="#cb20-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1413"><a href="#cb20-1413" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1414"><a href="#cb20-1414" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1415"><a href="#cb20-1415" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb20-1416"><a href="#cb20-1416" aria-hidden="true" tabindex="-1"></a><span class="co"># Normal distribution visualization</span></span>
<span id="cb20-1417"><a href="#cb20-1417" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1418"><a href="#cb20-1418" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1419"><a href="#cb20-1419" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb20-1420"><a href="#cb20-1420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1421"><a href="#cb20-1421" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb20-1422"><a href="#cb20-1422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1423"><a href="#cb20-1423" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. PDF with different parameters</span></span>
<span id="cb20-1424"><a href="#cb20-1424" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1000</span>)</span>
<span id="cb20-1425"><a href="#cb20-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1426"><a href="#cb20-1426" aria-hidden="true" tabindex="-1"></a><span class="co"># Different means</span></span>
<span id="cb20-1427"><a href="#cb20-1427" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mu, color <span class="kw">in</span> <span class="bu">zip</span>([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>], [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>]):</span>
<span id="cb20-1428"><a href="#cb20-1428" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x, norm.pdf(x, loc<span class="op">=</span>mu), linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb20-1429"><a href="#cb20-1429" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'μ=</span><span class="sc">{</span>mu<span class="sc">}</span><span class="ss">, σ=1'</span>, color<span class="op">=</span>color)</span>
<span id="cb20-1430"><a href="#cb20-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1431"><a href="#cb20-1431" aria-hidden="true" tabindex="-1"></a><span class="co"># Different standard deviations</span></span>
<span id="cb20-1432"><a href="#cb20-1432" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sigma, style <span class="kw">in</span> <span class="bu">zip</span>([<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>], [<span class="st">'-'</span>, <span class="st">'--'</span>, <span class="st">':'</span>]):</span>
<span id="cb20-1433"><a href="#cb20-1433" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x, norm.pdf(x, scale<span class="op">=</span>sigma), linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb20-1434"><a href="#cb20-1434" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="ss">f'μ=0, σ=</span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">'</span>, linestyle<span class="op">=</span>style, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb20-1435"><a href="#cb20-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1436"><a href="#cb20-1436" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb20-1437"><a href="#cb20-1437" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb20-1438"><a href="#cb20-1438" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Normal Distribution PDF'</span>)</span>
<span id="cb20-1439"><a href="#cb20-1439" aria-hidden="true" tabindex="-1"></a>ax1.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb20-1440"><a href="#cb20-1440" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-1441"><a href="#cb20-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1442"><a href="#cb20-1442" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 68-95-99.7 Rule</span></span>
<span id="cb20-1443"><a href="#cb20-1443" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb20-1444"><a href="#cb20-1444" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1000</span>)</span>
<span id="cb20-1445"><a href="#cb20-1445" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> norm.pdf(x_range, mu, sigma)</span>
<span id="cb20-1446"><a href="#cb20-1446" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_range, y, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-1447"><a href="#cb20-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1448"><a href="#cb20-1448" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill areas for different sigma ranges</span></span>
<span id="cb20-1449"><a href="#cb20-1449" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'lightblue'</span>, <span class="st">'lightgreen'</span>, <span class="st">'lightyellow'</span>]</span>
<span id="cb20-1450"><a href="#cb20-1450" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.8</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>]</span>
<span id="cb20-1451"><a href="#cb20-1451" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'68% (±1σ)'</span>, <span class="st">'95% (±2σ)'</span>, <span class="st">'99.7% (±3σ)'</span>]</span>
<span id="cb20-1452"><a href="#cb20-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1453"><a href="#cb20-1453" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_sigma, color, alpha, label <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], colors, alphas, labels):</span>
<span id="cb20-1454"><a href="#cb20-1454" aria-hidden="true" tabindex="-1"></a>    x_fill <span class="op">=</span> x_range[np.<span class="bu">abs</span>(x_range <span class="op">-</span> mu) <span class="op">&lt;=</span> n_sigma <span class="op">*</span> sigma]</span>
<span id="cb20-1455"><a href="#cb20-1455" aria-hidden="true" tabindex="-1"></a>    y_fill <span class="op">=</span> norm.pdf(x_fill, mu, sigma)</span>
<span id="cb20-1456"><a href="#cb20-1456" aria-hidden="true" tabindex="-1"></a>    ax2.fill_between(x_fill, <span class="dv">0</span>, y_fill, color<span class="op">=</span>color, alpha<span class="op">=</span>alpha, label<span class="op">=</span>label)</span>
<span id="cb20-1457"><a href="#cb20-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1458"><a href="#cb20-1458" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Standard Deviations from Mean'</span>)</span>
<span id="cb20-1459"><a href="#cb20-1459" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb20-1460"><a href="#cb20-1460" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'68-95-99.7 Rule for Standard Normal'</span>)</span>
<span id="cb20-1461"><a href="#cb20-1461" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb20-1462"><a href="#cb20-1462" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-1463"><a href="#cb20-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1464"><a href="#cb20-1464" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1465"><a href="#cb20-1465" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1466"><a href="#cb20-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1467"><a href="#cb20-1467" aria-hidden="true" tabindex="-1"></a><span class="co"># Key probabilities</span></span>
<span id="cb20-1468"><a href="#cb20-1468" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Standard Normal Probabilities:"</span>)</span>
<span id="cb20-1469"><a href="#cb20-1469" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(-1 ≤ Z ≤ 1) = </span><span class="sc">{</span>norm<span class="sc">.</span>cdf(<span class="dv">1</span>) <span class="op">-</span> norm<span class="sc">.</span>cdf(<span class="op">-</span><span class="dv">1</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0.68"</span>)</span>
<span id="cb20-1470"><a href="#cb20-1470" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(-2 ≤ Z ≤ 2) = </span><span class="sc">{</span>norm<span class="sc">.</span>cdf(<span class="dv">2</span>) <span class="op">-</span> norm<span class="sc">.</span>cdf(<span class="op">-</span><span class="dv">2</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0.95"</span>)</span>
<span id="cb20-1471"><a href="#cb20-1471" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(-3 ≤ Z ≤ 3) = </span><span class="sc">{</span>norm<span class="sc">.</span>cdf(<span class="dv">3</span>) <span class="op">-</span> norm<span class="sc">.</span>cdf(<span class="op">-</span><span class="dv">3</span>)<span class="sc">:.3f}</span><span class="ss"> ≈ 0.997"</span>)</span>
<span id="cb20-1472"><a href="#cb20-1472" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1473"><a href="#cb20-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1474"><a href="#cb20-1474" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exponential Distribution</span></span>
<span id="cb20-1475"><a href="#cb20-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1476"><a href="#cb20-1476" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">exponential distribution</span><span class="co">](https://en.wikipedia.org/wiki/Exponential_distribution)</span> models waiting times between events in a Poisson process.</span>
<span id="cb20-1477"><a href="#cb20-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1478"><a href="#cb20-1478" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1479"><a href="#cb20-1479" aria-hidden="true" tabindex="-1"></a>$X \sim \text{Exponential}(\beta)$ if:</span>
<span id="cb20-1480"><a href="#cb20-1480" aria-hidden="true" tabindex="-1"></a>$$f_X(x) = \frac{1}{\beta} e^{-x/\beta}, \quad x &gt; 0$$</span>
<span id="cb20-1481"><a href="#cb20-1481" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1482"><a href="#cb20-1482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1483"><a href="#cb20-1483" aria-hidden="true" tabindex="-1"></a>Here $\beta$ (*beta*) is the scale parameter, the average time between events.</span>
<span id="cb20-1484"><a href="#cb20-1484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1485"><a href="#cb20-1485" aria-hidden="true" tabindex="-1"></a>You can also find the exponential distribution parameterized in terms of a *rate* parameter $\lambda = \frac{1}{\beta}$.</span>
<span id="cb20-1486"><a href="#cb20-1486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1487"><a href="#cb20-1487" aria-hidden="true" tabindex="-1"></a>**Key properties**:</span>
<span id="cb20-1488"><a href="#cb20-1488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1489"><a href="#cb20-1489" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Memoryless**: $\mathbb{P}(X &gt; s + t | X &gt; s) = \mathbb{P}(X &gt; t)$</span>
<span id="cb20-1490"><a href="#cb20-1490" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>"The future doesn't depend on how long you've already waited"</span>
<span id="cb20-1491"><a href="#cb20-1491" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Connection to Poisson: If events occur at rate $\lambda = 1/\beta$, time between events is Exponential($\beta$)</span>
<span id="cb20-1492"><a href="#cb20-1492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CDF: $F_X(x) = 1 - e^{-x/\beta}$ for $x &gt; 0$</span>
<span id="cb20-1493"><a href="#cb20-1493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1494"><a href="#cb20-1494" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1495"><a href="#cb20-1495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1496"><a href="#cb20-1496" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time between customer arrivals</span>
<span id="cb20-1497"><a href="#cb20-1497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lifetime of electronic components  </span>
<span id="cb20-1498"><a href="#cb20-1498" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time until next earthquake</span>
<span id="cb20-1499"><a href="#cb20-1499" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Duration of phone calls</span>
<span id="cb20-1500"><a href="#cb20-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1503"><a href="#cb20-1503" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1504"><a href="#cb20-1504" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1505"><a href="#cb20-1505" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb20-1506"><a href="#cb20-1506" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponential distribution visualization</span></span>
<span id="cb20-1507"><a href="#cb20-1507" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1508"><a href="#cb20-1508" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1509"><a href="#cb20-1509" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> expon</span>
<span id="cb20-1510"><a href="#cb20-1510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1511"><a href="#cb20-1511" aria-hidden="true" tabindex="-1"></a><span class="co"># Different β values (mean waiting time)</span></span>
<span id="cb20-1512"><a href="#cb20-1512" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb20-1513"><a href="#cb20-1513" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>]</span>
<span id="cb20-1514"><a href="#cb20-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1515"><a href="#cb20-1515" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb20-1516"><a href="#cb20-1516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1517"><a href="#cb20-1517" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">1000</span>)</span>
<span id="cb20-1518"><a href="#cb20-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1519"><a href="#cb20-1519" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> beta, color <span class="kw">in</span> <span class="bu">zip</span>(betas, colors):</span>
<span id="cb20-1520"><a href="#cb20-1520" aria-hidden="true" tabindex="-1"></a>    pdf <span class="op">=</span> expon.pdf(x, scale<span class="op">=</span>beta)</span>
<span id="cb20-1521"><a href="#cb20-1521" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, pdf, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'β = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-1522"><a href="#cb20-1522" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-1523"><a href="#cb20-1523" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show mean as vertical line</span></span>
<span id="cb20-1524"><a href="#cb20-1524" aria-hidden="true" tabindex="-1"></a>    ax.axvline(beta, color<span class="op">=</span>color, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-1525"><a href="#cb20-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1526"><a href="#cb20-1526" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Time (x)'</span>)</span>
<span id="cb20-1527"><a href="#cb20-1527" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb20-1528"><a href="#cb20-1528" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Exponential Distribution PDF'</span>)</span>
<span id="cb20-1529"><a href="#cb20-1529" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb20-1530"><a href="#cb20-1530" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-1531"><a href="#cb20-1531" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">6</span>)</span>
<span id="cb20-1532"><a href="#cb20-1532" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">2.1</span>)</span>
<span id="cb20-1533"><a href="#cb20-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1534"><a href="#cb20-1534" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-1535"><a href="#cb20-1535" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1536"><a href="#cb20-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1537"><a href="#cb20-1537" aria-hidden="true" tabindex="-1"></a><span class="co"># Example calculations</span></span>
<span id="cb20-1538"><a href="#cb20-1538" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-1539"><a href="#cb20-1539" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For β = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss"> (rate λ = </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>beta<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb20-1540"><a href="#cb20-1540" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"E[X] = β = </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1541"><a href="#cb20-1541" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = β² = </span><span class="sc">{</span>beta<span class="op">**</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-1542"><a href="#cb20-1542" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(X &gt; 1) = </span><span class="sc">{</span><span class="dv">1</span> <span class="op">-</span> expon<span class="sc">.</span>cdf(<span class="dv">1</span>, scale<span class="op">=</span>beta)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-1543"><a href="#cb20-1543" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Median waiting time = </span><span class="sc">{</span>expon<span class="sc">.</span>ppf(<span class="fl">0.5</span>, scale<span class="op">=</span>beta)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-1544"><a href="#cb20-1544" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1545"><a href="#cb20-1545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1546"><a href="#cb20-1546" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Brief Catalog: Other Continuous Distributions</span></span>
<span id="cb20-1547"><a href="#cb20-1547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1548"><a href="#cb20-1548" aria-hidden="true" tabindex="-1"></a>**Gamma($\alpha, \beta$)**: The <span class="co">[</span><span class="ot">gamma distribution</span><span class="co">](https://en.wikipedia.org/wiki/Gamma_distribution)</span> is a generalization of exponential</span>
<span id="cb20-1549"><a href="#cb20-1549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1550"><a href="#cb20-1550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sum of independent exponentials</span>
<span id="cb20-1551"><a href="#cb20-1551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models waiting time for multiple events</span>
<span id="cb20-1552"><a href="#cb20-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1553"><a href="#cb20-1553" aria-hidden="true" tabindex="-1"></a>**Beta($\alpha, \beta$)**: The <span class="co">[</span><span class="ot">beta distribution</span><span class="co">](https://en.wikipedia.org/wiki/Beta_distribution)</span> models values between 0 and 1</span>
<span id="cb20-1554"><a href="#cb20-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1555"><a href="#cb20-1555" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models proportions and probabilities</span>
<span id="cb20-1556"><a href="#cb20-1556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conjugate prior for binomial parameter</span>
<span id="cb20-1557"><a href="#cb20-1557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1558"><a href="#cb20-1558" aria-hidden="true" tabindex="-1"></a>**t($\nu$)**: The <span class="co">[</span><span class="ot">t-distribution</span><span class="co">](https://en.wikipedia.org/wiki/Student%27s_t-distribution)</span> is a heavy-tailed alternative to normal</span>
<span id="cb20-1559"><a href="#cb20-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1560"><a href="#cb20-1560" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More probability in tails than normal</span>
<span id="cb20-1561"><a href="#cb20-1561" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Used when variance is unknown/unstable</span>
<span id="cb20-1562"><a href="#cb20-1562" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu = 1$ gives Cauchy (no finite mean!)</span>
<span id="cb20-1563"><a href="#cb20-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1564"><a href="#cb20-1564" aria-hidden="true" tabindex="-1"></a>**$\chi^2(p)$**: The <span class="co">[</span><span class="ot">chi-squared distribution</span><span class="co">](https://en.wikipedia.org/wiki/Chi-squared_distribution)</span> is the sum of squared standard normals</span>
<span id="cb20-1565"><a href="#cb20-1565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1566"><a href="#cb20-1566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $Z_1, ..., Z_p \sim \mathcal{N}(0,1)$ independent, then $\sum Z_i^2 \sim \chi^2(p)$</span>
<span id="cb20-1567"><a href="#cb20-1567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Used in hypothesis testing and confidence intervals</span>
<span id="cb20-1568"><a href="#cb20-1568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1569"><a href="#cb20-1569" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multivariate Distributions</span></span>
<span id="cb20-1570"><a href="#cb20-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1571"><a href="#cb20-1571" aria-hidden="true" tabindex="-1"></a>So far we've focused on single random variables. But in practice, we often deal with multiple related variables: height and weight, temperature and humidity, stock prices of different companies. This leads us to multivariate distributions.</span>
<span id="cb20-1572"><a href="#cb20-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1573"><a href="#cb20-1573" aria-hidden="true" tabindex="-1"></a><span class="fu">### Joint Distributions</span></span>
<span id="cb20-1574"><a href="#cb20-1574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1575"><a href="#cb20-1575" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1576"><a href="#cb20-1576" aria-hidden="true" tabindex="-1"></a>For random variables $X$ and $Y$, the **joint distribution** describes their behavior together:</span>
<span id="cb20-1577"><a href="#cb20-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1578"><a href="#cb20-1578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Discrete case**: Joint PMF $f_{X,Y}(x,y) = \mathbb{P}(X = x, Y = y)$</span>
<span id="cb20-1579"><a href="#cb20-1579" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Continuous case**: Joint PDF $f_{X,Y}(x,y)$ where </span>
<span id="cb20-1580"><a href="#cb20-1580" aria-hidden="true" tabindex="-1"></a>  $$\mathbb{P}((X,Y) \in A) = \iint_A f_{X,Y}(x,y) \, dx \, dy$$</span>
<span id="cb20-1581"><a href="#cb20-1581" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1582"><a href="#cb20-1582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1583"><a href="#cb20-1583" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1584"><a href="#cb20-1584" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Discrete joint distribution</span></span>
<span id="cb20-1585"><a href="#cb20-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1586"><a href="#cb20-1586" aria-hidden="true" tabindex="-1"></a>Roll two fair six-sided dice. Let $X$ = first die, $Y$ = second die.</span>
<span id="cb20-1587"><a href="#cb20-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1588"><a href="#cb20-1588" aria-hidden="true" tabindex="-1"></a>The joint PMF is:</span>
<span id="cb20-1589"><a href="#cb20-1589" aria-hidden="true" tabindex="-1"></a>$$f_{X,Y}(x,y) = \frac{1}{36} \text{ for } x,y \in <span class="sc">\{</span>1,2,3,4,5,6<span class="sc">\}</span>$$</span>
<span id="cb20-1590"><a href="#cb20-1590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1591"><a href="#cb20-1591" aria-hidden="true" tabindex="-1"></a>We can display this as a 6×6 table with each entry equal to 1/36.</span>
<span id="cb20-1592"><a href="#cb20-1592" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1593"><a href="#cb20-1593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1594"><a href="#cb20-1594" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1595"><a href="#cb20-1595" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Continuous joint distribution</span></span>
<span id="cb20-1596"><a href="#cb20-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1597"><a href="#cb20-1597" aria-hidden="true" tabindex="-1"></a>Let $(X,Y)$ be uniformly distributed on the unit disk.</span>
<span id="cb20-1598"><a href="#cb20-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1599"><a href="#cb20-1599" aria-hidden="true" tabindex="-1"></a>$$f_{X,Y}(x,y) = \begin{cases}</span>
<span id="cb20-1600"><a href="#cb20-1600" aria-hidden="true" tabindex="-1"></a>\frac{1}{\pi} &amp; \text{if } x^2 + y^2 \leq 1 <span class="sc">\\</span></span>
<span id="cb20-1601"><a href="#cb20-1601" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{otherwise}</span>
<span id="cb20-1602"><a href="#cb20-1602" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb20-1603"><a href="#cb20-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1604"><a href="#cb20-1604" aria-hidden="true" tabindex="-1"></a>The normalizing constant $1/\pi$ makes the total probability equal to 1 (area of unit disk is $\pi$).</span>
<span id="cb20-1605"><a href="#cb20-1605" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1606"><a href="#cb20-1606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1607"><a href="#cb20-1607" aria-hidden="true" tabindex="-1"></a><span class="fu">### Marginal Distributions</span></span>
<span id="cb20-1608"><a href="#cb20-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1609"><a href="#cb20-1609" aria-hidden="true" tabindex="-1"></a>Given a joint distribution, we can find the distribution of each variable separately.</span>
<span id="cb20-1610"><a href="#cb20-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1611"><a href="#cb20-1611" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1612"><a href="#cb20-1612" aria-hidden="true" tabindex="-1"></a>The **marginal distribution** of $X$ is obtained by "summing out" or "integrating out" the other variable:</span>
<span id="cb20-1613"><a href="#cb20-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1614"><a href="#cb20-1614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Discrete**: $f_X(x) = \sum_y f_{X,Y}(x,y)$</span>
<span id="cb20-1615"><a href="#cb20-1615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Continuous**: $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dy$</span>
<span id="cb20-1616"><a href="#cb20-1616" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1617"><a href="#cb20-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1618"><a href="#cb20-1618" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb20-1619"><a href="#cb20-1619" aria-hidden="true" tabindex="-1"></a>Think of marginal distributions as projections: if you have points scattered in 2D, the marginal distribution of X is like looking at their shadows on the X-axis.</span>
<span id="cb20-1620"><a href="#cb20-1620" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1621"><a href="#cb20-1621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1622"><a href="#cb20-1622" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1623"><a href="#cb20-1623" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Sum of two dice</span></span>
<span id="cb20-1624"><a href="#cb20-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1625"><a href="#cb20-1625" aria-hidden="true" tabindex="-1"></a>Let $X$ = first die, $Y$ = second die, $S = X + Y$.</span>
<span id="cb20-1626"><a href="#cb20-1626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1627"><a href="#cb20-1627" aria-hidden="true" tabindex="-1"></a>What is $\mathbb{P}(S = 7)$?</span>
<span id="cb20-1628"><a href="#cb20-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1629"><a href="#cb20-1629" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-1630"><a href="#cb20-1630" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb20-1631"><a href="#cb20-1631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1632"><a href="#cb20-1632" aria-hidden="true" tabindex="-1"></a>To find $\mathbb{P}(S = 7)$, we sum over all ways to get 7:</span>
<span id="cb20-1633"><a href="#cb20-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1634"><a href="#cb20-1634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$(1,6)$, $(2,5)$, $(3,4)$, $(4,3)$, $(5,2)$, $(6,1)$</span>
<span id="cb20-1635"><a href="#cb20-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1636"><a href="#cb20-1636" aria-hidden="true" tabindex="-1"></a>So $\mathbb{P}(S = 7) = 6 \times \frac{1}{36} = \frac{1}{6}$</span>
<span id="cb20-1637"><a href="#cb20-1637" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1638"><a href="#cb20-1638" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1639"><a href="#cb20-1639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1640"><a href="#cb20-1640" aria-hidden="true" tabindex="-1"></a><span class="fu">### Independent Random Variables</span></span>
<span id="cb20-1641"><a href="#cb20-1641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1642"><a href="#cb20-1642" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1643"><a href="#cb20-1643" aria-hidden="true" tabindex="-1"></a>Random variables $X$ and $Y$ are **independent** if:</span>
<span id="cb20-1644"><a href="#cb20-1644" aria-hidden="true" tabindex="-1"></a>$$f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y)$$</span>
<span id="cb20-1645"><a href="#cb20-1645" aria-hidden="true" tabindex="-1"></a>for all $x, y$.</span>
<span id="cb20-1646"><a href="#cb20-1646" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1647"><a href="#cb20-1647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1648"><a href="#cb20-1648" aria-hidden="true" tabindex="-1"></a>This means the joint distribution factors into the product of marginals - knowing the value of one variable tells us nothing about the other.</span>
<span id="cb20-1649"><a href="#cb20-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1650"><a href="#cb20-1650" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1651"><a href="#cb20-1651" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Independent coin flips</span></span>
<span id="cb20-1652"><a href="#cb20-1652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1653"><a href="#cb20-1653" aria-hidden="true" tabindex="-1"></a>Flip two fair coins. Let $X$ = 1 if first is heads, 0 otherwise. Same for $Y$ with second coin.</span>
<span id="cb20-1654"><a href="#cb20-1654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1655"><a href="#cb20-1655" aria-hidden="true" tabindex="-1"></a>Joint distribution:</span>
<span id="cb20-1656"><a href="#cb20-1656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1657"><a href="#cb20-1657" aria-hidden="true" tabindex="-1"></a>| | Y = 0 | Y = 1 |</span>
<span id="cb20-1658"><a href="#cb20-1658" aria-hidden="true" tabindex="-1"></a>|---|-------|-------|</span>
<span id="cb20-1659"><a href="#cb20-1659" aria-hidden="true" tabindex="-1"></a>| X = 0 | 1/4 | 1/4 |</span>
<span id="cb20-1660"><a href="#cb20-1660" aria-hidden="true" tabindex="-1"></a>| X = 1 | 1/4 | 1/4 |</span>
<span id="cb20-1661"><a href="#cb20-1661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1662"><a href="#cb20-1662" aria-hidden="true" tabindex="-1"></a>Since each entry equals the product of marginal probabilities (e.g., $\frac{1}{4} = \frac{1}{2} \times \frac{1}{2}$), $X$ and $Y$ are independent.</span>
<span id="cb20-1663"><a href="#cb20-1663" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1664"><a href="#cb20-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1665"><a href="#cb20-1665" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb20-1666"><a href="#cb20-1666" aria-hidden="true" tabindex="-1"></a>**Common mistake**: Assuming uncorrelated means independent. </span>
<span id="cb20-1667"><a href="#cb20-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1668"><a href="#cb20-1668" aria-hidden="true" tabindex="-1"></a>Independence implies zero correlation, but zero correlation does NOT imply independence! We'll see counterexamples when we study correlation in Chapter 3.</span>
<span id="cb20-1669"><a href="#cb20-1669" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1670"><a href="#cb20-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1671"><a href="#cb20-1671" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditional Distributions</span></span>
<span id="cb20-1672"><a href="#cb20-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1673"><a href="#cb20-1673" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1674"><a href="#cb20-1674" aria-hidden="true" tabindex="-1"></a>The **conditional distribution** of $X$ given $Y = y$ is:</span>
<span id="cb20-1675"><a href="#cb20-1675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1676"><a href="#cb20-1676" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Discrete**: $f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}$ if $f_Y(y) &gt; 0$</span>
<span id="cb20-1677"><a href="#cb20-1677" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Continuous**: Same formula, interpreted as densities</span>
<span id="cb20-1678"><a href="#cb20-1678" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1679"><a href="#cb20-1679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1680"><a href="#cb20-1680" aria-hidden="true" tabindex="-1"></a>This tells us how $X$ behaves when we know $Y = y$.</span>
<span id="cb20-1681"><a href="#cb20-1681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1682"><a href="#cb20-1682" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1683"><a href="#cb20-1683" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Quality control</span></span>
<span id="cb20-1684"><a href="#cb20-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1685"><a href="#cb20-1685" aria-hidden="true" tabindex="-1"></a>A factory produces items on two machines. Let:</span>
<span id="cb20-1686"><a href="#cb20-1686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1687"><a href="#cb20-1687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X$ = quality score (0-100)</span>
<span id="cb20-1688"><a href="#cb20-1688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Y$ = machine (1 or 2)</span>
<span id="cb20-1689"><a href="#cb20-1689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1690"><a href="#cb20-1690" aria-hidden="true" tabindex="-1"></a>Suppose Machine 1 produces 60% of items with quality $\sim \mathcal{N}(80, 25)$, and Machine 2 produces 40% with quality $\sim \mathcal{N}(70, 100)$.</span>
<span id="cb20-1691"><a href="#cb20-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1692"><a href="#cb20-1692" aria-hidden="true" tabindex="-1"></a>If we observe a quality score of 75, which machine likely produced it? This requires the conditional distribution $\mathbb{P}(Y|X=75)$.</span>
<span id="cb20-1693"><a href="#cb20-1693" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1694"><a href="#cb20-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1695"><a href="#cb20-1695" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interactive Exploration: Marginal and Conditional Distributions</span></span>
<span id="cb20-1696"><a href="#cb20-1696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1697"><a href="#cb20-1697" aria-hidden="true" tabindex="-1"></a>Let's explore how marginal and conditional distributions relate to a joint distribution using an interactive visualization.</span>
<span id="cb20-1698"><a href="#cb20-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1699"><a href="#cb20-1699" aria-hidden="true" tabindex="-1"></a>**Instructions:**</span>
<span id="cb20-1700"><a href="#cb20-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1701"><a href="#cb20-1701" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use the sliders to change the $x$ and $y$ values</span>
<span id="cb20-1702"><a href="#cb20-1702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Check the boxes to switch between marginal distributions (e.g., $f_X(x)$) and conditional distributions (e.g., $f_{X|Y}(x|y)$)</span>
<span id="cb20-1703"><a href="#cb20-1703" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When showing conditional distributions, red dashed lines appear on the joint distribution showing where we're conditioning</span>
<span id="cb20-1704"><a href="#cb20-1704" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The visualization uses the simpler shorthand notation $p(x)$ for $f_X(x)$ and $p(x|y)$ for $f_{X|Y}(x|y)$ (and analogous formulas for other pdfs)</span>
<span id="cb20-1705"><a href="#cb20-1705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1706"><a href="#cb20-1706" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb20-1707"><a href="#cb20-1707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1710"><a href="#cb20-1710" aria-hidden="true" tabindex="-1"></a><span class="in">```{ojs}</span></span>
<span id="cb20-1711"><a href="#cb20-1711" aria-hidden="true" tabindex="-1"></a><span class="in">//| echo: false</span></span>
<span id="cb20-1712"><a href="#cb20-1712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1713"><a href="#cb20-1713" aria-hidden="true" tabindex="-1"></a><span class="in">// Import D3, htl, and our visualization module</span></span>
<span id="cb20-1714"><a href="#cb20-1714" aria-hidden="true" tabindex="-1"></a><span class="in">d3 = require("d3@7")</span></span>
<span id="cb20-1715"><a href="#cb20-1715" aria-hidden="true" tabindex="-1"></a><span class="in">htl = require("htl")</span></span>
<span id="cb20-1716"><a href="#cb20-1716" aria-hidden="true" tabindex="-1"></a><span class="in">import { bivariateDemo } from "../js/bivariate-demo.js"</span></span>
<span id="cb20-1717"><a href="#cb20-1717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1718"><a href="#cb20-1718" aria-hidden="true" tabindex="-1"></a><span class="in">// Initialize the demo</span></span>
<span id="cb20-1719"><a href="#cb20-1719" aria-hidden="true" tabindex="-1"></a><span class="in">demo = bivariateDemo(d3)</span></span>
<span id="cb20-1720"><a href="#cb20-1720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1721"><a href="#cb20-1721" aria-hidden="true" tabindex="-1"></a><span class="in">// Define interactive controls</span></span>
<span id="cb20-1722"><a href="#cb20-1722" aria-hidden="true" tabindex="-1"></a><span class="in">viewof x_value = Inputs.range([-2, 2], {step: 0.1, value: 0, label: "x value"})</span></span>
<span id="cb20-1723"><a href="#cb20-1723" aria-hidden="true" tabindex="-1"></a><span class="in">viewof y_value = Inputs.range([-2, 4], {step: 0.1, value: 1, label: "y value"})</span></span>
<span id="cb20-1724"><a href="#cb20-1724" aria-hidden="true" tabindex="-1"></a><span class="in">viewof show_conditionals = Inputs.checkbox(["p(x|y)", "p(y|x)"], {value: [], label: "Show conditionals"})</span></span>
<span id="cb20-1725"><a href="#cb20-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1726"><a href="#cb20-1726" aria-hidden="true" tabindex="-1"></a><span class="in">// This block will be the output of the cell.</span></span>
<span id="cb20-1727"><a href="#cb20-1727" aria-hidden="true" tabindex="-1"></a><span class="in">// It lays out ONLY the plot, but it will still react to the controls above.</span></span>
<span id="cb20-1728"><a href="#cb20-1728" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb20-1729"><a href="#cb20-1729" aria-hidden="true" tabindex="-1"></a><span class="in">  const plot = demo.createVisualization(</span></span>
<span id="cb20-1730"><a href="#cb20-1730" aria-hidden="true" tabindex="-1"></a><span class="in">    x_value,</span></span>
<span id="cb20-1731"><a href="#cb20-1731" aria-hidden="true" tabindex="-1"></a><span class="in">    y_value,</span></span>
<span id="cb20-1732"><a href="#cb20-1732" aria-hidden="true" tabindex="-1"></a><span class="in">    show_conditionals.includes("p(x|y)"),</span></span>
<span id="cb20-1733"><a href="#cb20-1733" aria-hidden="true" tabindex="-1"></a><span class="in">    show_conditionals.includes("p(y|x)")</span></span>
<span id="cb20-1734"><a href="#cb20-1734" aria-hidden="true" tabindex="-1"></a><span class="in">  );</span></span>
<span id="cb20-1735"><a href="#cb20-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1736"><a href="#cb20-1736" aria-hidden="true" tabindex="-1"></a><span class="in">  // Return ONLY the plot element. The controls will be hidden but still work.</span></span>
<span id="cb20-1737"><a href="#cb20-1737" aria-hidden="true" tabindex="-1"></a><span class="in">  return plot;</span></span>
<span id="cb20-1738"><a href="#cb20-1738" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb20-1739"><a href="#cb20-1739" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1740"><a href="#cb20-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1741"><a href="#cb20-1741" aria-hidden="true" tabindex="-1"></a>**Key insights:**</span>
<span id="cb20-1742"><a href="#cb20-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1743"><a href="#cb20-1743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Marginal distributions** show the overall distribution of one variable, ignoring the other</span>
<span id="cb20-1744"><a href="#cb20-1744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Conditional distributions** show how one variable is distributed when we fix the other at a specific value</span>
<span id="cb20-1745"><a href="#cb20-1745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The shape of conditional distributions changes as we move the conditioning value</span>
<span id="cb20-1746"><a href="#cb20-1746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This demonstrates how knowing one variable's value provides information about the other when they're not independent</span>
<span id="cb20-1747"><a href="#cb20-1747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1748"><a href="#cb20-1748" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1749"><a href="#cb20-1749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1750"><a href="#cb20-1750" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb20-1751"><a href="#cb20-1751" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb20-1752"><a href="#cb20-1752" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interactive Visualization Available Online</span></span>
<span id="cb20-1753"><a href="#cb20-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1754"><a href="#cb20-1754" aria-hidden="true" tabindex="-1"></a>An interactive visualization of marginal and conditional distributions is available in the web version of these notes. This demo allows you to:</span>
<span id="cb20-1755"><a href="#cb20-1755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1756"><a href="#cb20-1756" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adjust $x$ and $y$ values with sliders</span>
<span id="cb20-1757"><a href="#cb20-1757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Toggle between marginal and conditional distributions</span>
<span id="cb20-1758"><a href="#cb20-1758" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>See how conditional distributions change as you vary the conditioning value</span>
<span id="cb20-1759"><a href="#cb20-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1760"><a href="#cb20-1760" aria-hidden="true" tabindex="-1"></a>Visit the online version to explore this interactive demonstration.</span>
<span id="cb20-1761"><a href="#cb20-1761" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1762"><a href="#cb20-1762" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1763"><a href="#cb20-1763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1764"><a href="#cb20-1764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1765"><a href="#cb20-1765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1766"><a href="#cb20-1766" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Vectors and IID Random Variables</span></span>
<span id="cb20-1767"><a href="#cb20-1767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1768"><a href="#cb20-1768" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1769"><a href="#cb20-1769" aria-hidden="true" tabindex="-1"></a>A **random vector** is a vector $\mathbf{X} = (X_1, X_2, ..., X_n)^T$ where each component $X_i$ is a random variable. The joint behavior of all components is characterized by their joint distribution.</span>
<span id="cb20-1770"><a href="#cb20-1770" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1771"><a href="#cb20-1771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1772"><a href="#cb20-1772" aria-hidden="true" tabindex="-1"></a>Random vectors allow us to study multiple random quantities together, which leads us to an important special case.</span>
<span id="cb20-1773"><a href="#cb20-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1774"><a href="#cb20-1774" aria-hidden="true" tabindex="-1"></a>**IID Random Variables:**</span>
<span id="cb20-1775"><a href="#cb20-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1776"><a href="#cb20-1776" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1777"><a href="#cb20-1777" aria-hidden="true" tabindex="-1"></a>Random variables $X_1, ..., X_n$ are **independent and identically distributed (IID)** if:</span>
<span id="cb20-1778"><a href="#cb20-1778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1779"><a href="#cb20-1779" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>They are mutually independent</span>
<span id="cb20-1780"><a href="#cb20-1780" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>They all have the same distribution</span>
<span id="cb20-1781"><a href="#cb20-1781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1782"><a href="#cb20-1782" aria-hidden="true" tabindex="-1"></a>We write: $X_1, ..., X_n \stackrel{iid}{\sim} F$.</span>
<span id="cb20-1783"><a href="#cb20-1783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1784"><a href="#cb20-1784" aria-hidden="true" tabindex="-1"></a>If $F$ has density $f$ we also write $X_1, ..., X_n \stackrel{iid}{\sim} f$.</span>
<span id="cb20-1785"><a href="#cb20-1785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1786"><a href="#cb20-1786" aria-hidden="true" tabindex="-1"></a>$X_1, ..., X_n$ is a **random sample of size $n$** from $F$ (or $f$, respectively).</span>
<span id="cb20-1787"><a href="#cb20-1787" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1788"><a href="#cb20-1788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1789"><a href="#cb20-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1790"><a href="#cb20-1790" aria-hidden="true" tabindex="-1"></a>IID assumptions are fundamental in statistics:</span>
<span id="cb20-1791"><a href="#cb20-1791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1792"><a href="#cb20-1792" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random sampling**: Each observation comes from the same population</span>
<span id="cb20-1793"><a href="#cb20-1793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**No interference**: One observation doesn't affect others</span>
<span id="cb20-1794"><a href="#cb20-1794" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stable conditions**: The underlying distribution doesn't change</span>
<span id="cb20-1795"><a href="#cb20-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1796"><a href="#cb20-1796" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1797"><a href="#cb20-1797" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Customer arrivals</span></span>
<span id="cb20-1798"><a href="#cb20-1798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1799"><a href="#cb20-1799" aria-hidden="true" tabindex="-1"></a>Times between customer arrivals at a stable business might be IID Exponential($\beta$).</span>
<span id="cb20-1800"><a href="#cb20-1800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1801"><a href="#cb20-1801" aria-hidden="true" tabindex="-1"></a>**Not IID**: </span>
<span id="cb20-1802"><a href="#cb20-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1803"><a href="#cb20-1803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stock prices (today's price depends on yesterday's)</span>
<span id="cb20-1804"><a href="#cb20-1804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temperature readings (temporal correlation)</span>
<span id="cb20-1805"><a href="#cb20-1805" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Survey responses from same household (likely correlated)</span>
<span id="cb20-1806"><a href="#cb20-1806" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1807"><a href="#cb20-1807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1808"><a href="#cb20-1808" aria-hidden="true" tabindex="-1"></a><span class="fu">### Important Multivariate Distributions</span></span>
<span id="cb20-1809"><a href="#cb20-1809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1810"><a href="#cb20-1810" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multinomial Distribution</span></span>
<span id="cb20-1811"><a href="#cb20-1811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1812"><a href="#cb20-1812" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">multinomial distribution</span><span class="co">](https://en.wikipedia.org/wiki/Multinomial_distribution)</span> is a generalization of binomial to multiple categories.</span>
<span id="cb20-1813"><a href="#cb20-1813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1814"><a href="#cb20-1814" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1815"><a href="#cb20-1815" aria-hidden="true" tabindex="-1"></a>If we have $k$ categories with probabilities $p_1, ..., p_k$ (summing to 1), and we observe $n$ independent trials, then the counts $(X_1, ..., X_k)$ follow a **Multinomial** distribution:</span>
<span id="cb20-1816"><a href="#cb20-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1817"><a href="#cb20-1817" aria-hidden="true" tabindex="-1"></a>$$f(x_1, ..., x_k) = \frac{n!}{x_1! \cdots x_k!} p_1^{x_1} \cdots p_k^{x_k}$$</span>
<span id="cb20-1818"><a href="#cb20-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1819"><a href="#cb20-1819" aria-hidden="true" tabindex="-1"></a>where $\sum x_i = n$.</span>
<span id="cb20-1820"><a href="#cb20-1820" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1821"><a href="#cb20-1821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1822"><a href="#cb20-1822" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb20-1823"><a href="#cb20-1823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1824"><a href="#cb20-1824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dice rolls (6 categories for a standard die -- or $k$ for $k$-sided dice)</span>
<span id="cb20-1825"><a href="#cb20-1825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Survey responses (multiple choice)</span>
<span id="cb20-1826"><a href="#cb20-1826" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Document word counts</span>
<span id="cb20-1827"><a href="#cb20-1827" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Genetic allele frequencies</span>
<span id="cb20-1828"><a href="#cb20-1828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1829"><a href="#cb20-1829" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multivariate Normal Distribution</span></span>
<span id="cb20-1830"><a href="#cb20-1830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1831"><a href="#cb20-1831" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">multivariate normal distribution</span><span class="co">](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)</span> is the multivariate generalization of the normal distribution.</span>
<span id="cb20-1832"><a href="#cb20-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1833"><a href="#cb20-1833" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb20-1834"><a href="#cb20-1834" aria-hidden="true" tabindex="-1"></a>A random vector $\mathbf{X} = (X_1, ..., X_k)^T$ has a **multivariate normal** distribution, written $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, if:</span>
<span id="cb20-1835"><a href="#cb20-1835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1836"><a href="#cb20-1836" aria-hidden="true" tabindex="-1"></a>$$f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$</span>
<span id="cb20-1837"><a href="#cb20-1837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1838"><a href="#cb20-1838" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb20-1839"><a href="#cb20-1839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1840"><a href="#cb20-1840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{\mu}$ is the mean vector</span>
<span id="cb20-1841"><a href="#cb20-1841" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{\Sigma}$ is the covariance matrix (symmetric, positive definite)</span>
<span id="cb20-1842"><a href="#cb20-1842" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1843"><a href="#cb20-1843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1844"><a href="#cb20-1844" aria-hidden="true" tabindex="-1"></a>**Key properties**:</span>
<span id="cb20-1845"><a href="#cb20-1845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1846"><a href="#cb20-1846" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Marginals are normal: If $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, then $X_i \sim \mathcal{N}(\mu_i, \Sigma_{ii})$</span>
<span id="cb20-1847"><a href="#cb20-1847" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear combinations are normal: $\mathbf{a}^T\mathbf{X} \sim \mathcal{N}(\mathbf{a}^T\boldsymbol{\mu}, \mathbf{a}^T\boldsymbol{\Sigma}\mathbf{a})$</span>
<span id="cb20-1848"><a href="#cb20-1848" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conditional distributions are normal (with formulas for conditional mean and variance)</span>
<span id="cb20-1849"><a href="#cb20-1849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1850"><a href="#cb20-1850" aria-hidden="true" tabindex="-1"></a>**Special case - Bivariate normal**: For two variables with correlation $\rho$:</span>
<span id="cb20-1851"><a href="#cb20-1851" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol{\Sigma} = \begin{pmatrix} \sigma_1^2 &amp; \rho\sigma_1\sigma_2 <span class="sc">\\</span> \rho\sigma_1\sigma_2 &amp; \sigma_2^2 \end{pmatrix}$$</span>
<span id="cb20-1852"><a href="#cb20-1852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1853"><a href="#cb20-1853" aria-hidden="true" tabindex="-1"></a>The correlation $\rho$ controls the relationship:</span>
<span id="cb20-1854"><a href="#cb20-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1855"><a href="#cb20-1855" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\rho = 0$: independent (for normal variables, uncorrelated = independent!)</span>
<span id="cb20-1856"><a href="#cb20-1856" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\rho &gt; 0$: positive relationship</span>
<span id="cb20-1857"><a href="#cb20-1857" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\rho &lt; 0$: negative relationship</span>
<span id="cb20-1858"><a href="#cb20-1858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1859"><a href="#cb20-1859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1862"><a href="#cb20-1862" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-1863"><a href="#cb20-1863" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb20-1864"><a href="#cb20-1864" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb20-1865"><a href="#cb20-1865" aria-hidden="true" tabindex="-1"></a><span class="co"># Bivariate normal distribution visualization</span></span>
<span id="cb20-1866"><a href="#cb20-1866" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-1867"><a href="#cb20-1867" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-1868"><a href="#cb20-1868" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb20-1869"><a href="#cb20-1869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1870"><a href="#cb20-1870" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure</span></span>
<span id="cb20-1871"><a href="#cb20-1871" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb20-1872"><a href="#cb20-1872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1873"><a href="#cb20-1873" aria-hidden="true" tabindex="-1"></a><span class="co"># Bivariate normal with correlation</span></span>
<span id="cb20-1874"><a href="#cb20-1874" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb20-1875"><a href="#cb20-1875" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">0.7</span>], [<span class="fl">0.7</span>, <span class="dv">1</span>]]  <span class="co"># correlation = 0.7</span></span>
<span id="cb20-1876"><a href="#cb20-1876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1877"><a href="#cb20-1877" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb20-1878"><a href="#cb20-1878" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb20-1879"><a href="#cb20-1879" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb20-1880"><a href="#cb20-1880" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb20-1881"><a href="#cb20-1881" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> np.dstack((X, Y))</span>
<span id="cb20-1882"><a href="#cb20-1882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1883"><a href="#cb20-1883" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate PDF</span></span>
<span id="cb20-1884"><a href="#cb20-1884" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> multivariate_normal(mean, cov)</span>
<span id="cb20-1885"><a href="#cb20-1885" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> rv.pdf(pos)</span>
<span id="cb20-1886"><a href="#cb20-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1887"><a href="#cb20-1887" aria-hidden="true" tabindex="-1"></a><span class="co"># Contour plot</span></span>
<span id="cb20-1888"><a href="#cb20-1888" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb20-1889"><a href="#cb20-1889" aria-hidden="true" tabindex="-1"></a>ax.clabel(contour, inline<span class="op">=</span><span class="va">True</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb20-1890"><a href="#cb20-1890" aria-hidden="true" tabindex="-1"></a>contourf <span class="op">=</span> ax.contourf(X, Y, Z, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb20-1891"><a href="#cb20-1891" aria-hidden="true" tabindex="-1"></a>fig.colorbar(contourf, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'Density'</span>)</span>
<span id="cb20-1892"><a href="#cb20-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1893"><a href="#cb20-1893" aria-hidden="true" tabindex="-1"></a><span class="co"># Add marginal indicators</span></span>
<span id="cb20-1894"><a href="#cb20-1894" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-1895"><a href="#cb20-1895" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-1896"><a href="#cb20-1896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1897"><a href="#cb20-1897" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb20-1898"><a href="#cb20-1898" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Y'</span>)</span>
<span id="cb20-1899"><a href="#cb20-1899" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Bivariate Normal Distribution (ρ = 0.7)'</span>)</span>
<span id="cb20-1900"><a href="#cb20-1900" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb20-1901"><a href="#cb20-1901" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-1902"><a href="#cb20-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1903"><a href="#cb20-1903" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-1904"><a href="#cb20-1904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1905"><a href="#cb20-1905" aria-hidden="true" tabindex="-1"></a><span class="co"># Example calculations</span></span>
<span id="cb20-1906"><a href="#cb20-1906" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bivariate Normal with ρ = 0.7:"</span>)</span>
<span id="cb20-1907"><a href="#cb20-1907" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Var(X) = Var(Y) = 1"</span>)</span>
<span id="cb20-1908"><a href="#cb20-1908" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cov(X,Y) = ρ·σ_X·σ_Y = 0.7"</span>)</span>
<span id="cb20-1909"><a href="#cb20-1909" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"If we observe Y=1, then:"</span>)</span>
<span id="cb20-1910"><a href="#cb20-1910" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  E[X|Y=1] = ρ·(Y-μ_Y) = 0.7"</span>)</span>
<span id="cb20-1911"><a href="#cb20-1911" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Var(X|Y=1) = (1-ρ²) = </span><span class="sc">{</span><span class="dv">1</span> <span class="op">-</span> <span class="fl">0.7</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb20-1912"><a href="#cb20-1912" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1913"><a href="#cb20-1913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1914"><a href="#cb20-1914" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-1915"><a href="#cb20-1915" aria-hidden="true" tabindex="-1"></a>The multivariate normal distribution is central to many statistical methods. We will return to it in more detail in Chapter 2 when we discuss expectations, covariances, and the properties of linear combinations of random variables.</span>
<span id="cb20-1916"><a href="#cb20-1916" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1917"><a href="#cb20-1917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1918"><a href="#cb20-1918" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-1919"><a href="#cb20-1919" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Transformations of Random Variables</span></span>
<span id="cb20-1920"><a href="#cb20-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1921"><a href="#cb20-1921" aria-hidden="true" tabindex="-1"></a>We often define variables that are <span class="co">[</span><span class="ot">transformations</span><span class="co">](https://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables)</span> $g(\cdot)$ of other random variables.</span>
<span id="cb20-1922"><a href="#cb20-1922" aria-hidden="true" tabindex="-1"></a>Assuming we know the distribution of $X$ or $(X, Y)$, how do we find the distribution of $Y = g(X)$ or $(U,V) = g(X,Y)$?</span>
<span id="cb20-1923"><a href="#cb20-1923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1924"><a href="#cb20-1924" aria-hidden="true" tabindex="-1"></a>**Method 1: CDF technique**</span>
<span id="cb20-1925"><a href="#cb20-1925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1926"><a href="#cb20-1926" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Find the CDF: $F_Y(y) = \mathbb{P}(Y \leq y) = \mathbb{P}(g(X) \leq y)$</span>
<span id="cb20-1927"><a href="#cb20-1927" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Differentiate to get PDF: $f_Y(y) = F_Y'(y)$</span>
<span id="cb20-1928"><a href="#cb20-1928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1929"><a href="#cb20-1929" aria-hidden="true" tabindex="-1"></a>**Method 2: Jacobian method (for bijective transformations)**</span>
<span id="cb20-1930"><a href="#cb20-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1931"><a href="#cb20-1931" aria-hidden="true" tabindex="-1"></a>If $(U,V) = g(X,Y)$ is one-to-one with inverse $(X,Y) = h(U,V)$, then:</span>
<span id="cb20-1932"><a href="#cb20-1932" aria-hidden="true" tabindex="-1"></a>$$f_{U,V}(u,v) = f_{X,Y}(h(u,v)) \cdot |J|$$</span>
<span id="cb20-1933"><a href="#cb20-1933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1934"><a href="#cb20-1934" aria-hidden="true" tabindex="-1"></a>where $J$ is the Jacobian determinant:</span>
<span id="cb20-1935"><a href="#cb20-1935" aria-hidden="true" tabindex="-1"></a>$$J = \det\begin{pmatrix} \frac{\partial x}{\partial u} &amp; \frac{\partial x}{\partial v} <span class="sc">\\</span> \frac{\partial y}{\partial u} &amp; \frac{\partial y}{\partial v} \end{pmatrix}$$</span>
<span id="cb20-1936"><a href="#cb20-1936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1937"><a href="#cb20-1937" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb20-1938"><a href="#cb20-1938" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Box-Muller transform</span></span>
<span id="cb20-1939"><a href="#cb20-1939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1940"><a href="#cb20-1940" aria-hidden="true" tabindex="-1"></a>We can generate Gaussian (normal) distributed random numbers starting from uniform.</span>
<span id="cb20-1941"><a href="#cb20-1941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1942"><a href="#cb20-1942" aria-hidden="true" tabindex="-1"></a>If $U_1, U_2 \sim \text{Uniform}(0,1)$ independently, then:</span>
<span id="cb20-1943"><a href="#cb20-1943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1944"><a href="#cb20-1944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X = \sqrt{-2\ln U_1} \cos(2\pi U_2)$</span>
<span id="cb20-1945"><a href="#cb20-1945" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Y = \sqrt{-2\ln U_1} \sin(2\pi U_2)$</span>
<span id="cb20-1946"><a href="#cb20-1946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1947"><a href="#cb20-1947" aria-hidden="true" tabindex="-1"></a>are independent $\mathcal{N}(0,1)$ random variables!</span>
<span id="cb20-1948"><a href="#cb20-1948" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1949"><a href="#cb20-1949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1950"><a href="#cb20-1950" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1951"><a href="#cb20-1951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1952"><a href="#cb20-1952" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb20-1953"><a href="#cb20-1953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1954"><a href="#cb20-1954" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb20-1955"><a href="#cb20-1955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1956"><a href="#cb20-1956" aria-hidden="true" tabindex="-1"></a>We've built up probability theory from its foundations:</span>
<span id="cb20-1957"><a href="#cb20-1957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1958"><a href="#cb20-1958" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Sample spaces and events** provide the basic framework for describing uncertainty</span>
<span id="cb20-1959"><a href="#cb20-1959" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Probability axioms** give us consistent rules for quantifying uncertainty</span>
<span id="cb20-1960"><a href="#cb20-1960" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Independence and conditioning** let us model relationships between events</span>
<span id="cb20-1961"><a href="#cb20-1961" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Random variables** connect probability to numerical quantities we can analyze</span>
<span id="cb20-1962"><a href="#cb20-1962" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Distributions** characterize the behavior of random variables</span>
<span id="cb20-1963"><a href="#cb20-1963" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Multivariate distributions** handle multiple related random quantities</span>
<span id="cb20-1964"><a href="#cb20-1964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1965"><a href="#cb20-1965" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why These Concepts Matter</span></span>
<span id="cb20-1966"><a href="#cb20-1966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1967"><a href="#cb20-1967" aria-hidden="true" tabindex="-1"></a>**For Statistical Inference**:</span>
<span id="cb20-1968"><a href="#cb20-1968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1969"><a href="#cb20-1969" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random variables let us model data mathematically</span>
<span id="cb20-1970"><a href="#cb20-1970" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Distributions provide templates for common patterns</span>
<span id="cb20-1971"><a href="#cb20-1971" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Independence assumptions simplify analysis</span>
<span id="cb20-1972"><a href="#cb20-1972" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conditioning lets us update beliefs with data</span>
<span id="cb20-1973"><a href="#cb20-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1974"><a href="#cb20-1974" aria-hidden="true" tabindex="-1"></a>**For Machine Learning**:</span>
<span id="cb20-1975"><a href="#cb20-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1976"><a href="#cb20-1976" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Probability distributions model uncertainty in predictions</span>
<span id="cb20-1977"><a href="#cb20-1977" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayes' theorem enables Bayesian ML methods</span>
<span id="cb20-1978"><a href="#cb20-1978" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multivariate distributions handle high-dimensional data</span>
<span id="cb20-1979"><a href="#cb20-1979" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Independence assumptions make computation tractable</span>
<span id="cb20-1980"><a href="#cb20-1980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1981"><a href="#cb20-1981" aria-hidden="true" tabindex="-1"></a>**For Data Science Practice**:</span>
<span id="cb20-1982"><a href="#cb20-1982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1983"><a href="#cb20-1983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understanding distributions helps choose appropriate methods</span>
<span id="cb20-1984"><a href="#cb20-1984" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Recognizing dependence prevents incorrect analyses</span>
<span id="cb20-1985"><a href="#cb20-1985" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conditional probability quantifies relationships in data</span>
<span id="cb20-1986"><a href="#cb20-1986" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simulation using these distributions validates methods</span>
<span id="cb20-1987"><a href="#cb20-1987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1988"><a href="#cb20-1988" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb20-1989"><a href="#cb20-1989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1990"><a href="#cb20-1990" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Confusing $\mathbb{P}(A|B)$ with $\mathbb{P}(B|A)$** - These can be vastly different!</span>
<span id="cb20-1991"><a href="#cb20-1991" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Assuming independence without justification** - Real-world variables are often dependent</span>
<span id="cb20-1992"><a href="#cb20-1992" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Misinterpreting PDFs as probabilities** - PDFs are densities, not probabilities</span>
<span id="cb20-1993"><a href="#cb20-1993" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Forgetting $\mathbb{P}(X = x) = 0$ for continuous variables** - Use intervals for continuous RVs</span>
<span id="cb20-1994"><a href="#cb20-1994" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Thinking disjoint means independent** - Disjoint events are maximally dependent!</span>
<span id="cb20-1995"><a href="#cb20-1995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1996"><a href="#cb20-1996" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb20-1997"><a href="#cb20-1997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1998"><a href="#cb20-1998" aria-hidden="true" tabindex="-1"></a>The probability foundations from this chapter provide the mathematical language for all of statistics:</span>
<span id="cb20-1999"><a href="#cb20-1999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2000"><a href="#cb20-2000" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next - Chapter 2 (Expectation)**: Building on our introduction to random variables, we'll explore expectation as a fundamental tool for summarizing distributions, including variance and the powerful linearity of expectation property</span>
<span id="cb20-2001"><a href="#cb20-2001" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chapter 3 (Convergence &amp; Inference)**: Using the probability framework and IID concept from this chapter, we'll prove the Law of Large Numbers and Central Limit Theorem—the theoretical foundations that justify using samples to learn about populations</span>
<span id="cb20-2002"><a href="#cb20-2002" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chapter 4 (Bootstrap)**: Apply our understanding of empirical distributions to develop computational methods for quantifying uncertainty, providing a modern alternative to traditional parametric approaches</span>
<span id="cb20-2003"><a href="#cb20-2003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2004"><a href="#cb20-2004" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb20-2005"><a href="#cb20-2005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2006"><a href="#cb20-2006" aria-hidden="true" tabindex="-1"></a>Try to answer these questions after reading these lecture notes.</span>
<span id="cb20-2007"><a href="#cb20-2007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2008"><a href="#cb20-2008" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bayes in action**: A test for a disease has 95% sensitivity (true positive rate) and 98% specificity (true negative rate). If 0.1% of the population has the disease, what's the probability someone with a positive test actually has the disease?</span>
<span id="cb20-2009"><a href="#cb20-2009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2010"><a href="#cb20-2010" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Distribution identification**: Times between earthquakes in a region average 50 days. What distribution would you use to model the time until the next earthquake? Why?</span>
<span id="cb20-2011"><a href="#cb20-2011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2012"><a href="#cb20-2012" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Independence check**: You roll two dice. Let A = "sum is even" and B = "first die shows 3". Are A and B independent?</span>
<span id="cb20-2013"><a href="#cb20-2013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2014"><a href="#cb20-2014" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Conditional expectation preview**: In a factory, Machine 1 makes 70% of products with defect rate 2%. Machine 2 makes 30% with defect rate 5%. If a product is defective, what's the probability it came from Machine 1?</span>
<span id="cb20-2015"><a href="#cb20-2015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2016"><a href="#cb20-2016" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb20-2017"><a href="#cb20-2017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2018"><a href="#cb20-2018" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-2019"><a href="#cb20-2019" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb20-2020"><a href="#cb20-2020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2021"><a href="#cb20-2021" aria-hidden="true" tabindex="-1"></a>This table maps sections in these lecture notes to the corresponding sections in @wasserman2013all ("All of Statistics" or AoS).</span>
<span id="cb20-2022"><a href="#cb20-2022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2023"><a href="#cb20-2023" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding AoS Section(s) |</span>
<span id="cb20-2024"><a href="#cb20-2024" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb20-2025"><a href="#cb20-2025" aria-hidden="true" tabindex="-1"></a>| **Why Do We Need Statistics?** | Expanded material from the slides, contextualizing statistics for data science. |</span>
<span id="cb20-2026"><a href="#cb20-2026" aria-hidden="true" tabindex="-1"></a>| **Foundations of Probability** | |</span>
<span id="cb20-2027"><a href="#cb20-2027" aria-hidden="true" tabindex="-1"></a>| ↳ Sample Spaces and Events | AoS §1.2 |</span>
<span id="cb20-2028"><a href="#cb20-2028" aria-hidden="true" tabindex="-1"></a>| ↳ Probability Axioms | AoS §1.3 (Definition 1.5) |</span>
<span id="cb20-2029"><a href="#cb20-2029" aria-hidden="true" tabindex="-1"></a>| ↳ Interpretations of Probability | AoS §1.3 |</span>
<span id="cb20-2030"><a href="#cb20-2030" aria-hidden="true" tabindex="-1"></a>| ↳ Finite Sample Spaces &amp; Counting | AoS §1.4 |</span>
<span id="cb20-2031"><a href="#cb20-2031" aria-hidden="true" tabindex="-1"></a>| **Independence and Conditional Probability** | |</span>
<span id="cb20-2032"><a href="#cb20-2032" aria-hidden="true" tabindex="-1"></a>| ↳ Independent Events | AoS §1.5 (Definition 1.9) |</span>
<span id="cb20-2033"><a href="#cb20-2033" aria-hidden="true" tabindex="-1"></a>| ↳ Conditional Probability | AoS §1.6 (Definition 1.12) |</span>
<span id="cb20-2034"><a href="#cb20-2034" aria-hidden="true" tabindex="-1"></a>| ↳ Bayes' Theorem &amp; Law of Total Probability | AoS §1.7 (Theorems 1.16, 1.17) |</span>
<span id="cb20-2035"><a href="#cb20-2035" aria-hidden="true" tabindex="-1"></a>| **Random Variables** | |</span>
<span id="cb20-2036"><a href="#cb20-2036" aria-hidden="true" tabindex="-1"></a>| ↳ Definition and Intuition | AoS §2.1 (Definition 2.1) |</span>
<span id="cb20-2037"><a href="#cb20-2037" aria-hidden="true" tabindex="-1"></a>| ↳ CDF, PMF, and PDF | AoS §2.2 (Definitions 2.5, 2.9, 2.11) |</span>
<span id="cb20-2038"><a href="#cb20-2038" aria-hidden="true" tabindex="-1"></a>| ↳ Core Discrete Distributions | AoS §2.3 |</span>
<span id="cb20-2039"><a href="#cb20-2039" aria-hidden="true" tabindex="-1"></a>| ↳ Core Continuous Distributions | AoS §2.4 |</span>
<span id="cb20-2040"><a href="#cb20-2040" aria-hidden="true" tabindex="-1"></a>| **Multivariate Distributions** | |</span>
<span id="cb20-2041"><a href="#cb20-2041" aria-hidden="true" tabindex="-1"></a>| ↳ Joint Distributions | AoS §2.5 |</span>
<span id="cb20-2042"><a href="#cb20-2042" aria-hidden="true" tabindex="-1"></a>| ↳ Marginal Distributions | AoS §2.6 |</span>
<span id="cb20-2043"><a href="#cb20-2043" aria-hidden="true" tabindex="-1"></a>| ↳ Independent Random Variables | AoS §2.7 (Definition 2.29) |</span>
<span id="cb20-2044"><a href="#cb20-2044" aria-hidden="true" tabindex="-1"></a>| ↳ Conditional Distributions | AoS §2.8 (Definitions 2.35, 2.36) |</span>
<span id="cb20-2045"><a href="#cb20-2045" aria-hidden="true" tabindex="-1"></a>| ↳ Random Vectors and IID Samples | AoS §2.9 (Definition 2.41) |</span>
<span id="cb20-2046"><a href="#cb20-2046" aria-hidden="true" tabindex="-1"></a>| ↳ Important Multivariate Distributions | AoS §2.10 |</span>
<span id="cb20-2047"><a href="#cb20-2047" aria-hidden="true" tabindex="-1"></a>| ↳ Transformations of Random Variables | AoS §2.11, §2.12 |</span>
<span id="cb20-2048"><a href="#cb20-2048" aria-hidden="true" tabindex="-1"></a>| **Chapter Summary and Connections** | New summary material. |</span>
<span id="cb20-2049"><a href="#cb20-2049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2050"><a href="#cb20-2050" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-2051"><a href="#cb20-2051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2052"><a href="#cb20-2052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2053"><a href="#cb20-2053" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Reading</span></span>
<span id="cb20-2054"><a href="#cb20-2054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2055"><a href="#cb20-2055" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Probability Theory**: Ross, "A First Course in Probability" - accessible introduction</span>
<span id="cb20-2056"><a href="#cb20-2056" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mathematical Statistics**: Casella &amp; Berger, "Statistical Inference" - rigorous treatment</span>
<span id="cb20-2057"><a href="#cb20-2057" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bayesian Perspective**: Gelman et al., "Bayesian Data Analysis" - modern Bayesian view</span>
<span id="cb20-2058"><a href="#cb20-2058" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Computational Approach**: Blitzstein &amp; Hwang, "Introduction to Probability" - simulation-based</span>
<span id="cb20-2059"><a href="#cb20-2059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2060"><a href="#cb20-2060" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb20-2061"><a href="#cb20-2061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2062"><a href="#cb20-2062" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb20-2063"><a href="#cb20-2063" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb20-2064"><a href="#cb20-2064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2065"><a href="#cb20-2065" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python Code</span></span>
<span id="cb20-2066"><a href="#cb20-2066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2067"><a href="#cb20-2067" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-2068"><a href="#cb20-2068" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability distributions in Python</span></span>
<span id="cb20-2069"><a href="#cb20-2069" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb20-2070"><a href="#cb20-2070" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2071"><a href="#cb20-2071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2072"><a href="#cb20-2072" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete distributions</span></span>
<span id="cb20-2073"><a href="#cb20-2073" aria-hidden="true" tabindex="-1"></a>stats.binom.pmf(x, n<span class="op">=</span>n, p<span class="op">=</span>p)           <span class="co"># Binomial PMF</span></span>
<span id="cb20-2074"><a href="#cb20-2074" aria-hidden="true" tabindex="-1"></a>stats.binom.cdf(x, n<span class="op">=</span>n, p<span class="op">=</span>p)           <span class="co"># Binomial CDF</span></span>
<span id="cb20-2075"><a href="#cb20-2075" aria-hidden="true" tabindex="-1"></a>stats.binom.rvs(n<span class="op">=</span>n, p<span class="op">=</span>p, size<span class="op">=</span>size)   <span class="co"># Generate random binomial</span></span>
<span id="cb20-2076"><a href="#cb20-2076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2077"><a href="#cb20-2077" aria-hidden="true" tabindex="-1"></a>stats.poisson.pmf(x, mu<span class="op">=</span>lam)           <span class="co"># Poisson PMF</span></span>
<span id="cb20-2078"><a href="#cb20-2078" aria-hidden="true" tabindex="-1"></a>stats.poisson.cdf(x, mu<span class="op">=</span>lam)           <span class="co"># Poisson CDF</span></span>
<span id="cb20-2079"><a href="#cb20-2079" aria-hidden="true" tabindex="-1"></a>stats.poisson.rvs(mu<span class="op">=</span>lam, size<span class="op">=</span>size)   <span class="co"># Generate random Poisson</span></span>
<span id="cb20-2080"><a href="#cb20-2080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2081"><a href="#cb20-2081" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous distributions  </span></span>
<span id="cb20-2082"><a href="#cb20-2082" aria-hidden="true" tabindex="-1"></a>stats.norm.pdf(x, loc<span class="op">=</span>mean, scale<span class="op">=</span>sd)   <span class="co"># Normal PDF</span></span>
<span id="cb20-2083"><a href="#cb20-2083" aria-hidden="true" tabindex="-1"></a>stats.norm.cdf(x, loc<span class="op">=</span>mean, scale<span class="op">=</span>sd)   <span class="co"># Normal CDF</span></span>
<span id="cb20-2084"><a href="#cb20-2084" aria-hidden="true" tabindex="-1"></a>stats.norm.rvs(loc<span class="op">=</span>mean, scale<span class="op">=</span>sd, size<span class="op">=</span>size) <span class="co"># Generate random normal</span></span>
<span id="cb20-2085"><a href="#cb20-2085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2086"><a href="#cb20-2086" aria-hidden="true" tabindex="-1"></a>stats.expon.pdf(x, scale<span class="op">=</span>beta)          <span class="co"># Exponential PDF</span></span>
<span id="cb20-2087"><a href="#cb20-2087" aria-hidden="true" tabindex="-1"></a>stats.expon.cdf(x, scale<span class="op">=</span>beta)          <span class="co"># Exponential CDF</span></span>
<span id="cb20-2088"><a href="#cb20-2088" aria-hidden="true" tabindex="-1"></a>stats.expon.rvs(scale<span class="op">=</span>beta, size<span class="op">=</span>size)  <span class="co"># Generate random exponential</span></span>
<span id="cb20-2089"><a href="#cb20-2089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2090"><a href="#cb20-2090" aria-hidden="true" tabindex="-1"></a><span class="co"># Multivariate normal</span></span>
<span id="cb20-2091"><a href="#cb20-2091" aria-hidden="true" tabindex="-1"></a>stats.multivariate_normal.rvs(mean, cov, size<span class="op">=</span>size)  <span class="co"># Generate</span></span>
<span id="cb20-2092"><a href="#cb20-2092" aria-hidden="true" tabindex="-1"></a>stats.multivariate_normal.pdf(x, mean, cov)          <span class="co"># Density</span></span>
<span id="cb20-2093"><a href="#cb20-2093" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-2094"><a href="#cb20-2094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2095"><a href="#cb20-2095" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb20-2096"><a href="#cb20-2096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2097"><a href="#cb20-2097" aria-hidden="true" tabindex="-1"></a>**Note on `lambda` parameter**: In the Python code, we used <span class="in">`lam`</span> instead of <span class="in">`lambda`</span> ($\lambda$) for the Poisson distribution parameter because <span class="in">`lambda`</span> is a reserved keyword in Python (used for anonymous functions). Using <span class="in">`lam`</span> (or <span class="in">`lamb`</span>) in Python is a common convention to avoid syntax errors.</span>
<span id="cb20-2098"><a href="#cb20-2098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2099"><a href="#cb20-2099" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-2100"><a href="#cb20-2100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2101"><a href="#cb20-2101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2102"><a href="#cb20-2102" aria-hidden="true" tabindex="-1"></a><span class="fu">## R Code</span></span>
<span id="cb20-2103"><a href="#cb20-2103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2104"><a href="#cb20-2104" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb20-2105"><a href="#cb20-2105" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability distributions in R</span></span>
<span id="cb20-2106"><a href="#cb20-2106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2107"><a href="#cb20-2107" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete distributions</span></span>
<span id="cb20-2108"><a href="#cb20-2108" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(x, <span class="at">size=</span>n, <span class="at">prob=</span>p)     <span class="co"># Binomial PMF</span></span>
<span id="cb20-2109"><a href="#cb20-2109" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(x, <span class="at">size=</span>n, <span class="at">prob=</span>p)     <span class="co"># Binomial CDF</span></span>
<span id="cb20-2110"><a href="#cb20-2110" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span>(n, size, prob)          <span class="co"># Generate random binomial</span></span>
<span id="cb20-2111"><a href="#cb20-2111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2112"><a href="#cb20-2112" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(x, lambda)               <span class="co"># Poisson PMF</span></span>
<span id="cb20-2113"><a href="#cb20-2113" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(x, lambda)               <span class="co"># Poisson CDF  </span></span>
<span id="cb20-2114"><a href="#cb20-2114" aria-hidden="true" tabindex="-1"></a><span class="fu">rpois</span>(n, lambda)               <span class="co"># Generate random Poisson</span></span>
<span id="cb20-2115"><a href="#cb20-2115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2116"><a href="#cb20-2116" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous distributions</span></span>
<span id="cb20-2117"><a href="#cb20-2117" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(x, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)         <span class="co"># Normal PDF</span></span>
<span id="cb20-2118"><a href="#cb20-2118" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(x, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)         <span class="co"># Normal CDF</span></span>
<span id="cb20-2119"><a href="#cb20-2119" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(n, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)         <span class="co"># Generate random normal</span></span>
<span id="cb20-2120"><a href="#cb20-2120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2121"><a href="#cb20-2121" aria-hidden="true" tabindex="-1"></a><span class="fu">dexp</span>(x, <span class="at">rate=</span><span class="dv">1</span><span class="sc">/</span>beta)           <span class="co"># Exponential PDF</span></span>
<span id="cb20-2122"><a href="#cb20-2122" aria-hidden="true" tabindex="-1"></a><span class="fu">pexp</span>(x, <span class="at">rate=</span><span class="dv">1</span><span class="sc">/</span>beta)           <span class="co"># Exponential CDF</span></span>
<span id="cb20-2123"><a href="#cb20-2123" aria-hidden="true" tabindex="-1"></a><span class="fu">rexp</span>(n, <span class="at">rate=</span><span class="dv">1</span><span class="sc">/</span>beta)           <span class="co"># Generate random exponential</span></span>
<span id="cb20-2124"><a href="#cb20-2124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2125"><a href="#cb20-2125" aria-hidden="true" tabindex="-1"></a><span class="co"># Multivariate normal</span></span>
<span id="cb20-2126"><a href="#cb20-2126" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb20-2127"><a href="#cb20-2127" aria-hidden="true" tabindex="-1"></a><span class="fu">rmvnorm</span>(n, mean, sigma)        <span class="co"># Generate multivariate normal</span></span>
<span id="cb20-2128"><a href="#cb20-2128" aria-hidden="true" tabindex="-1"></a><span class="fu">dmvnorm</span>(x, mean, sigma)        <span class="co"># Multivariate normal density</span></span>
<span id="cb20-2129"><a href="#cb20-2129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-2130"><a href="#cb20-2130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2131"><a href="#cb20-2131" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-2132"><a href="#cb20-2132" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-2133"><a href="#cb20-2133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2134"><a href="#cb20-2134" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb20-2135"><a href="#cb20-2135" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb20-2136"><a href="#cb20-2136" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb20-2137"><a href="#cb20-2137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2138"><a href="#cb20-2138" aria-hidden="true" tabindex="-1"></a>Python and R code examples for this chapter can be found in the HTML version of these notes.</span>
<span id="cb20-2139"><a href="#cb20-2139" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-2140"><a href="#cb20-2140" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-2141"><a href="#cb20-2141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2142"><a href="#cb20-2142" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-2143"><a href="#cb20-2143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2144"><a href="#cb20-2144" aria-hidden="true" tabindex="-1"></a>*Remember: Probability is the language of uncertainty. Master this language, and you'll be able to express and analyze uncertainty in any domain.*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>