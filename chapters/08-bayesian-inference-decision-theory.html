<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 8&nbsp; Bayesian Inference and Statistical Decision Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/09-linear-logistic-regression.html" rel="next">
<link href="../chapters/07-hypothesis-testing.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/08-bayesian-inference-decision-theory.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">8.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-a-different-way-of-thinking" id="toc-introduction-a-different-way-of-thinking" class="nav-link" data-scroll-target="#introduction-a-different-way-of-thinking"><span class="header-section-number">8.2</span> Introduction: A Different Way of Thinking</a>
  <ul class="collapse">
  <li><a href="#the-search-for-air-france-flight-447" id="toc-the-search-for-air-france-flight-447" class="nav-link" data-scroll-target="#the-search-for-air-france-flight-447"><span class="header-section-number">8.2.1</span> The Search for Air France Flight 447</a></li>
  <li><a href="#the-two-philosophies-of-statistics" id="toc-the-two-philosophies-of-statistics" class="nav-link" data-scroll-target="#the-two-philosophies-of-statistics"><span class="header-section-number">8.2.2</span> The Two Philosophies of Statistics</a></li>
  <li><a href="#this-chapters-goal" id="toc-this-chapters-goal" class="nav-link" data-scroll-target="#this-chapters-goal"><span class="header-section-number">8.2.3</span> This Chapter’s Goal</a></li>
  </ul></li>
  <li><a href="#the-bayesian-method-updating-beliefs-with-data" id="toc-the-bayesian-method-updating-beliefs-with-data" class="nav-link" data-scroll-target="#the-bayesian-method-updating-beliefs-with-data"><span class="header-section-number">8.3</span> The Bayesian Method: Updating Beliefs with Data</a>
  <ul class="collapse">
  <li><a href="#the-engine-bayes-theorem-for-inference" id="toc-the-engine-bayes-theorem-for-inference" class="nav-link" data-scroll-target="#the-engine-bayes-theorem-for-inference"><span class="header-section-number">8.3.1</span> The Engine: Bayes’ Theorem for Inference</a></li>
  <li><a href="#summarizing-the-posterior" id="toc-summarizing-the-posterior" class="nav-link" data-scroll-target="#summarizing-the-posterior"><span class="header-section-number">8.3.2</span> Summarizing the Posterior</a></li>
  </ul></li>
  <li><a href="#bayesian-inference-in-action" id="toc-bayesian-inference-in-action" class="nav-link" data-scroll-target="#bayesian-inference-in-action"><span class="header-section-number">8.4</span> Bayesian Inference in Action</a>
  <ul class="collapse">
  <li><a href="#conjugate-models-and-conjugate-priors" id="toc-conjugate-models-and-conjugate-priors" class="nav-link" data-scroll-target="#conjugate-models-and-conjugate-priors"><span class="header-section-number">8.4.1</span> Conjugate Models and Conjugate Priors</a></li>
  <li><a href="#the-art-and-science-of-choosing-priors" id="toc-the-art-and-science-of-choosing-priors" class="nav-link" data-scroll-target="#the-art-and-science-of-choosing-priors"><span class="header-section-number">8.4.2</span> The Art and Science of Choosing Priors</a></li>
  <li><a href="#implementing-bayesian-inference" id="toc-implementing-bayesian-inference" class="nav-link" data-scroll-target="#implementing-bayesian-inference"><span class="header-section-number">8.4.3</span> Implementing Bayesian Inference</a></li>
  </ul></li>
  <li><a href="#statistical-decision-theory-a-framework-for-best" id="toc-statistical-decision-theory-a-framework-for-best" class="nav-link" data-scroll-target="#statistical-decision-theory-a-framework-for-best"><span class="header-section-number">8.5</span> Statistical Decision Theory: A Framework for “Best”</a>
  <ul class="collapse">
  <li><a href="#the-ingredients-loss-and-risk" id="toc-the-ingredients-loss-and-risk" class="nav-link" data-scroll-target="#the-ingredients-loss-and-risk"><span class="header-section-number">8.5.1</span> The Ingredients: Loss and Risk</a></li>
  <li><a href="#the-challenge-of-comparing-risk-functions" id="toc-the-challenge-of-comparing-risk-functions" class="nav-link" data-scroll-target="#the-challenge-of-comparing-risk-functions"><span class="header-section-number">8.5.2</span> The Challenge of Comparing Risk Functions</a></li>
  </ul></li>
  <li><a href="#optimal-estimators-bayes-and-minimax-rules" id="toc-optimal-estimators-bayes-and-minimax-rules" class="nav-link" data-scroll-target="#optimal-estimators-bayes-and-minimax-rules"><span class="header-section-number">8.6</span> Optimal Estimators: Bayes and Minimax Rules</a>
  <ul class="collapse">
  <li><a href="#the-bayesian-approach-minimizing-average-risk" id="toc-the-bayesian-approach-minimizing-average-risk" class="nav-link" data-scroll-target="#the-bayesian-approach-minimizing-average-risk"><span class="header-section-number">8.6.1</span> The Bayesian Approach: Minimizing Average Risk</a></li>
  <li><a href="#the-frequentist-approach-minimizing-worst-case-risk" id="toc-the-frequentist-approach-minimizing-worst-case-risk" class="nav-link" data-scroll-target="#the-frequentist-approach-minimizing-worst-case-risk"><span class="header-section-number">8.6.2</span> The Frequentist Approach: Minimizing Worst-Case Risk</a></li>
  </ul></li>
  <li><a href="#admissibility-ruling-out-bad-estimators" id="toc-admissibility-ruling-out-bad-estimators" class="nav-link" data-scroll-target="#admissibility-ruling-out-bad-estimators"><span class="header-section-number">8.7</span> Admissibility: Ruling Out Bad Estimators</a>
  <ul class="collapse">
  <li><a href="#defining-admissibility" id="toc-defining-admissibility" class="nav-link" data-scroll-target="#defining-admissibility"><span class="header-section-number">8.7.1</span> Defining Admissibility</a></li>
  <li><a href="#key-properties-and-connections" id="toc-key-properties-and-connections" class="nav-link" data-scroll-target="#key-properties-and-connections"><span class="header-section-number">8.7.2</span> Key Properties and Connections</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">8.8</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">8.8.1</span> Key Concepts Review</a></li>
  <li><a href="#the-big-picture" id="toc-the-big-picture" class="nav-link" data-scroll-target="#the-big-picture"><span class="header-section-number">8.8.2</span> The Big Picture</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">8.8.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">8.8.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">8.8.5</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">8.8.6</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">8.8.7</span> Connections to Source Material</a></li>
  <li><a href="#further-materials" id="toc-further-materials" class="nav-link" data-scroll-target="#further-materials"><span class="header-section-number">8.8.8</span> Further Materials</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">8.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li><strong>Apply Bayes’ theorem to compute posterior distributions</strong> from prior and likelihood, and interpret credible intervals vs.&nbsp;confidence intervals.</li>
<li><strong>Work with conjugate models</strong> (Beta-Bernoulli, Normal-Normal) to derive posteriors and understand how data and prior beliefs combine.</li>
<li><strong>Choose appropriate priors</strong> and explain their impact on inference, particularly as sample size increases.</li>
<li><strong>Use decision theory to compare estimators</strong> via loss functions and risk, understanding why we need scalar summaries (Bayes risk, maximum risk).</li>
<li><strong>Identify optimal estimators</strong> by connecting posterior summaries to Bayes estimators, finding minimax estimators via constant risk, and determining admissibility.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter introduces two deeply connected topics: Bayesian inference, which provides a principled way to update beliefs with data, and statistical decision theory, which gives us a formal framework for comparing any statistical procedure. The material is adapted from Chapters 11 and 12 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> and supplemented with modern perspectives and computational examples.</p>
</div>
</div>
</section>
<section id="introduction-a-different-way-of-thinking" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="introduction-a-different-way-of-thinking"><span class="header-section-number">8.2</span> Introduction: A Different Way of Thinking</h2>
<section id="the-search-for-air-france-flight-447" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="the-search-for-air-france-flight-447"><span class="header-section-number">8.2.1</span> The Search for Air France Flight 447</h3>
<p>On June 1, 2009, Air France Flight 447 vanished over the Atlantic Ocean. The Airbus A330, carrying 228 people, disappeared from radar while flying from Rio de Janeiro to Paris, leaving behind only automated messages indicating system failures. What followed was one of the most challenging search operations in aviation history – and ultimately, a powerful demonstration of how Bayesian inference succeeds by integrating multiple sources of uncertain information. This remarkable story is documented in detail in <span class="citation" data-cites="stone2014search">Stone et al. (<a href="../references.html#ref-stone2014search" role="doc-biblioref">2014</a>)</span>, from which the figures and search details in this section are taken.</p>
<div class="columns">
<div class="column" style="width:55%;">
<p>Modern airliners transmit their position every 10 minutes via satellite. When AF447’s transmissions stopped at 2:14 AM, it created a circular search area with a 40 nautical mile (74 km) radius – still covering over 5,000 square nautical miles of ocean.</p>
<p>The depth in this region reaches 14,000 feet, with underwater mountains and valleys making detection extremely challenging. The flight data and cockpit voice recorders, crucial for understanding what happened, emit acoustic beacons that function for about 40 days and have a detection range of about 2,000 meters.</p>
<p>This wasn’t just a search problem – it was a problem of combining uncertain, conflicting information from multiple sources.</p>
</div><div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/Stone2014_fig1.png" class="img-fluid figure-img"></p>
<figcaption>The intended flight path of AF447 and the 40 NM radius circle centered on the last known position (LKP). The circle represents the maximum distance the aircraft could have traveled after its last transmission. Figure from <span class="citation" data-cites="stone2014search">Stone et al. (<a href="../references.html#ref-stone2014search" role="doc-biblioref">2014</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<section id="the-initial-search-efforts" class="level4">
<h4 class="anchored" data-anchor-id="the-initial-search-efforts">The Initial Search Efforts</h4>
<p>Throughout 2009 and 2010, search teams employed sophisticated statistical models including oceanographic drift analysis and confidence regions. However, each search operation focused on a single line of evidence rather than integrating all available information. These efforts, while extensive and expensive, all failed to locate the wreckage:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255605-228-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-228-1" role="tab" aria-controls="tabset-1757255605-228-1" aria-selected="true" href="" aria-current="page">Surface Search (June 2009)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-228-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-228-2" role="tab" aria-controls="tabset-1757255605-228-2" aria-selected="false" href="">Acoustic Search (June 2009)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-228-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-228-3" role="tab" aria-controls="tabset-1757255605-228-3" aria-selected="false" href="">Active Sonar (August 2009)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-228-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-228-4" role="tab" aria-controls="tabset-1757255605-228-4" aria-selected="false" href="">Confidence Region (April-May 2010)</a></li></ul><div class="tab-content"><div id="tabset-1757255605-228-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255605-228-1-tab"><p>The first phase focused on finding floating debris. After six days,
search aircraft spotted debris and bodies approximately 38 NM north of
the last known position. Scientists then used reverse drift modeling
(working backwards from where debris was found, using ocean current data
to estimate where it originated) to predict where the wreckage might
be.</p><p><strong>Result</strong>: No wreckage found in the predicted
areas.</p></div><div id="tabset-1757255605-228-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-228-2-tab"><p>Teams deployed sensitive hydrophones to listen for the flight
recorders’ acoustic beacons. They concentrated along the intended flight
path, reasoning the aircraft was likely on course when it crashed.</p><figure class="figure">
<img src="../images/Stone2014_fig5.png" style="width:80.0%" alt="The vertical and horizontal search lines showing the passive acoustic search paths for the flight recorder beacons. The circles show the 20 and 40 NM radius from the last known position. Figure from Stone et al.&nbsp;(2014)." class="figure-img">
<figcaption aria-hidden="true">The vertical and horizontal search lines
showing the passive acoustic search paths for the flight recorder
beacons. The circles show the 20 and 40 NM radius from the last known
position. Figure from Stone et al.&nbsp;(2014).</figcaption>
</figure><p><strong>Result</strong>: No signals detected. The search assumed the
beacons were functioning – a reasonable but ultimately incorrect
assumption.</p></div><div id="tabset-1757255605-228-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-228-3-tab"><p>A limited side-scan sonar search was conducted south of the last
known position in areas not covered in June.</p><figure class="figure">
<img src="../images/Stone2014_fig6.png" style="width:80.0%" alt="Regions searched by active side-looking sonar. The small rectangle shows the limited August 2009 coverage, while the larger areas show April-May 2010 coverage. Figure from Stone et al.&nbsp;(2014)." class="figure-img">
<figcaption aria-hidden="true">Regions searched by active side-looking
sonar. The small rectangle shows the limited August 2009 coverage, while
the larger areas show April-May 2010 coverage. Figure from Stone et
al.&nbsp;(2014).</figcaption>
</figure><p><strong>Result</strong>: No wreckage found.</p></div><div id="tabset-1757255605-228-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-228-4-tab"><p>Scientists computed a 95% confidence region by reverse drift modeling
from where bodies and debris were recovered. By simulating ocean
currents backwards in time, they estimated where the crash most likely
occurred, producing a search zone north and west of the last known
position.</p><figure class="figure">
<img src="../images/Stone2014_fig2.png" style="width:80.0%" alt="The 95% confidence zone recommended for the 2010 search, located north and west of the LKP, based on reverse drift modeling. Figure from Stone et al.&nbsp;(2014)." class="figure-img">
<figcaption aria-hidden="true">The 95% confidence zone recommended for
the 2010 search, located north and west of the LKP, based on reverse
drift modeling. Figure from Stone et al.&nbsp;(2014).</figcaption>
</figure><p><strong>Result</strong>: No wreckage found. The confidence region,
while statistically valid, relied heavily on ocean current models and
didn’t integrate other sources of evidence like historical crash
locations or search effectiveness.</p></div></div></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why the Initial Approaches Failed
</div>
</div>
<div class="callout-body-container callout-body">
<p>Each search used valid and sophisticated statistical reasoning but treated evidence in isolation:</p>
<ul>
<li>Drift models didn’t account for prior crash locations</li>
<li>Passive acoustic searches couldn’t distinguish between beacon failure and absence of wreckage</li>
<li>Search patterns didn’t incorporate the probability of missing the wreckage</li>
<li>No unified framework was used to combine these different sources of uncertainty</li>
</ul>
</div>
</div>
</section>
<section id="the-bayesian-strategy" class="level4">
<h4 class="anchored" data-anchor-id="the-bayesian-strategy">The Bayesian Strategy</h4>
<p>In July 2010, after four unsuccessful search operations, the French aviation authority (BEA) assembled a new team of statisticians to design a search strategy for 2011. This team took a fundamentally different approach: instead of treating each piece of evidence separately, they used Bayesian inference to combine <em>all</em> sources of information into a single probability distribution.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>The Bayesian Framework</strong></p>
<p>The team constructed a posterior probability distribution for the wreckage location by combining:</p>
<ol type="1">
<li><p><strong>Prior Distribution</strong>: Historical data showed that aircraft are usually found close to their last known position. This gave higher prior probability to areas near the center of the circle.</p></li>
<li><p><strong>Drift Model Likelihood</strong>: Bodies found north of the LKP implied certain starting positions were more likely than others – but with significant uncertainty.</p></li>
<li><p><strong>Search Effectiveness</strong>: Previous searches weren’t perfect. The team modeled the probability of missing the wreckage in searched areas, particularly accounting for terrain difficulty.</p></li>
<li><p><strong>Beacon Failure Possibility</strong>: The lack of acoustic signals could mean either the wreckage wasn’t in searched areas OR the beacons had failed. Bayesian analysis could incorporate both possibilities.</p></li>
</ol>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/Stone2014_fig3.png" class="img-fluid figure-img"></p>
<figcaption>Reverse drift distribution showing the probability density of potential crash locations based on where bodies and debris were found. This was one key input to the Bayesian analysis. Figure from <span class="citation" data-cites="stone2014search">Stone et al. (<a href="../references.html#ref-stone2014search" role="doc-biblioref">2014</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Technical Detail: Computing the Posterior
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The posterior distribution was computed using:</p>
<p><span class="math display">P(\text{location} | \text{all evidence}) \propto P(\text{all evidence} | \text{location}) \times P(\text{location})</span></p>
<p>Where the evidence included:</p>
<ul>
<li>Negative search results (no detection in searched areas)</li>
<li>Positive drift data (bodies found at specific locations)</li>
<li>Timing constraints (time between crash and debris discovery)</li>
</ul>
<p>The likelihood <span class="math inline">P(\text{all evidence} | \text{location})</span> was itself a product of multiple conditional probabilities, each capturing different aspects of the search problem. Monte Carlo methods were used to integrate over unknown parameters like ocean current variations and detection probabilities.</p>
</div>
</div>
</div>
</section>
<section id="the-breakthrough" class="level4">
<h4 class="anchored" data-anchor-id="the-breakthrough">The Breakthrough</h4>
<p>The Bayesian analysis produced a surprising result: the highest probability areas were very close to the last known position. Although these areas had been covered by passive acoustic searches in 2009, the active sonar efforts in 2009-2010 had focused elsewhere based on drift models.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Bayesian approach revealed that multiple pieces of weak evidence all pointed to the same conclusion:</p>
<ul>
<li>Historical data suggested searching near the LKP</li>
<li>Debris drift models had high uncertainty and conflicting predictions</li>
<li>The failure to find wreckage in extensively searched areas increased relative probability elsewhere</li>
<li>Beacon failure was historically more likely than initially assumed</li>
</ul>
<p>No single piece of evidence was conclusive, but together they pointed strongly to areas near the last known position.</p>
</div>
</div>
</section>
<section id="discovery-and-vindication" class="level4">
<h4 class="anchored" data-anchor-id="discovery-and-vindication">Discovery and Vindication</h4>
<p>The new search began in 2011, focusing on the high-probability areas identified by the Bayesian analysis. After just one week of searching, on April 3, 2011, the wreckage was found at a depth of approximately 14,000 feet, very close to the last known position.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/Stone2014_fig8.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Posterior distribution from the Bayesian analysis, showing the actual wreck location marked. The dark area near the center shows the highest probability zone, which correctly identified the area where the wreckage was ultimately found. Figure from <span class="citation" data-cites="stone2014search">Stone et al. (<a href="../references.html#ref-stone2014search" role="doc-biblioref">2014</a>)</span>.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Bayesian Methods Succeeded
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Bayesian approach succeeded where the initial methods failed for three fundamental reasons:</p>
<ol type="1">
<li><p><strong>Coherent Information Integration</strong>: While the initial searches treated each piece of evidence separately, Bayesian inference combined them into a single, coherent picture.</p></li>
<li><p><strong>Uncertainty Quantification</strong>: The approach explicitly modeled multiple sources of uncertainty – from ocean currents to sensor reliability – rather than assuming point estimates were correct.</p></li>
<li><p><strong>Prior Knowledge Utilization</strong>: Historical data about crash locations provided valuable information that pure data-driven approaches ignored.</p></li>
</ol>
<p>This case demonstrates the power of Bayesian thinking: when faced with multiple sources of imperfect information, Bayesian methods provide the mathematical framework to combine them optimally.</p>
</div>
</div>
</section>
</section>
<section id="the-two-philosophies-of-statistics" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="the-two-philosophies-of-statistics"><span class="header-section-number">8.2.2</span> The Two Philosophies of Statistics</h3>
<p>In the world of statistical inference, there are two major philosophical schools of thought about probability, parameters, and how we should make inferences from data. These aren’t just abstract philosophical debates – they lead to fundamentally different methods, interpretations, and answers. Understanding both perspectives is crucial for modern data scientists.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255605-228-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-228-1" role="tab" aria-controls="tabset-1757255605-228-1" aria-selected="true" href="">Frequentist View</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-228-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-228-2" role="tab" aria-controls="tabset-1757255605-228-2" aria-selected="false" href="">Bayesian View</a></li></ul><div class="tab-content"><div id="tabset-1757255605-228-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255605-228-1-tab"><p>We’ve been working primarily within the frequentist framework
throughout this course. Let’s formalize its key principles:</p><p><strong>F1. Probability as Frequency</strong>: Probability refers to
limiting relative frequencies in repeated experiments. Probabilities are
objective properties of the real world. When we say a coin has
probability 0.5 of landing heads, we mean that in an infinite sequence
of flips, exactly half would be heads.</p><p><strong>F2. Fixed Parameters</strong>: Parameters are fixed, unknown
constants. They are not random variables. Because they don’t vary, we
cannot make probability statements about them. We can’t say “there’s a
95% probability that <span class="math inline">\(\mu\)</span> is between
2 and 4” – either it is or it isn’t.</p><p><strong>F3. Long-Run Performance</strong>: Statistical methods should
have well-defined long-run frequency properties. A 95% confidence
interval should trap the true parameter in 95% of repeated experiments.
This is a statement about the procedure, not about any particular
interval.</p><p><strong>F4. Point-Conditioned Prediction</strong>: Predictions are
typically conditioned on a single parameter value, often an estimate
like the MLE. We predict future data assuming our estimate is
correct.</p><p><strong>F5. Separate Theories</strong>: There’s no single,
overarching theory unifying all aspects of frequentist inference.
Estimation theory, hypothesis testing, and prediction each have their
own frameworks and optimality criteria.</p></div><div id="tabset-1757255605-228-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-228-2-tab"><p>The Bayesian approach starts from fundamentally different
assumptions:</p><p><strong>B1. Probability as Belief</strong>: Probability describes
degree of belief or confidence. Probabilities can be subjective and
represent our uncertainty about anything – including fixed events. We
can meaningfully say “I’m 70% confident it rained in Paris on January 1,
1850” even though this is a fixed historical fact.</p><p><strong>B2. Probabilistic Parameters</strong>: We can make
probability statements about parameters, treating our uncertainty about
them as something to be described by a probability distribution. Even
though <span class="math inline">\(\theta\)</span> is fixed, our
knowledge about it is uncertain, and we quantify this uncertainty with
probabilities.</p><p><strong>B3. Inference as Belief Updating</strong>: The core of
inference is updating our beliefs about parameters by producing a
posterior probability distribution after observing data. This posterior
encapsulates everything we know about the parameter.</p><p><strong>B4. Averaged Prediction</strong>: Predictions are made by
averaging over all parameter values, weighted by their posterior
probability. Instead of picking one “best” parameter value, we consider
all plausible values.</p><p><strong>B5. Unified Theory</strong>: The framework has a strong,
unified theoretical foundation based on the rules of probability. Bayes’
theorem provides a single coherent approach to all inference
problems.</p></div></div></div>
</section>
<section id="this-chapters-goal" class="level3" data-number="8.2.3">
<h3 data-number="8.2.3" class="anchored" data-anchor-id="this-chapters-goal"><span class="header-section-number">8.2.3</span> This Chapter’s Goal</h3>
<p>We will explore two deeply connected topics:</p>
<ol type="1">
<li><p><strong>Bayesian Inference</strong>: The machinery for updating our beliefs about parameters using data. We’ll see how prior knowledge combines with observed data to produce posterior distributions.</p></li>
<li><p><strong>Statistical Decision Theory</strong>: A formal framework for choosing the “best” estimator under any paradigm. This theory, which applies to both frequentist and Bayesian methods, gives us a rigorous way to compare different statistical procedures.</p></li>
</ol>
<p>These topics are connected because Bayesian inference naturally leads to optimal estimators under decision theory, while decision theory helps us understand when and why Bayesian methods work well.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here’s a reference table of key terms in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bayesian inference</td>
<td>Bayesiläinen päättely</td>
<td>Main inferential framework</td>
</tr>
<tr class="even">
<td>Prior distribution</td>
<td>Priorijakauma</td>
<td>Beliefs before seeing data</td>
</tr>
<tr class="odd">
<td>Posterior distribution</td>
<td>Posteriorijakauma</td>
<td>Updated beliefs after data</td>
</tr>
<tr class="even">
<td>Likelihood</td>
<td>Uskottavuus</td>
<td>Probability of data given parameters</td>
</tr>
<tr class="odd">
<td>Credible interval</td>
<td>Uskottavuusväli</td>
<td>Bayesian confidence interval</td>
</tr>
<tr class="even">
<td>Loss function</td>
<td>Tappiofunktio</td>
<td>Measure of estimation error</td>
</tr>
<tr class="odd">
<td>Risk</td>
<td>Riski</td>
<td>Expected loss</td>
</tr>
<tr class="even">
<td>Bayes estimator</td>
<td>Bayes-estimaattori</td>
<td>Minimizes Bayes risk</td>
</tr>
<tr class="odd">
<td>Minimax estimator</td>
<td>Minimax-estimaattori</td>
<td>Minimizes maximum risk</td>
</tr>
<tr class="even">
<td>Admissible</td>
<td>Käypä, kelvollinen</td>
<td>Cannot be uniformly improved</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="the-bayesian-method-updating-beliefs-with-data" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="the-bayesian-method-updating-beliefs-with-data"><span class="header-section-number">8.3</span> The Bayesian Method: Updating Beliefs with Data</h2>
<section id="the-engine-bayes-theorem-for-inference" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="the-engine-bayes-theorem-for-inference"><span class="header-section-number">8.3.1</span> The Engine: Bayes’ Theorem for Inference</h3>
<p>The Bayesian method centers on a fundamental question: <strong>how do we make predictions about unknown quantities</strong> when we have uncertain knowledge about the parameters that govern them?</p>
<p>Consider predicting some unknown quantity <span class="math inline">x^*</span> (which could be future data, or properties of the parameter itself) when we have:</p>
<ul>
<li>A model with unknown parameter <span class="math inline">\theta</span></li>
<li>Observed data <span class="math inline">x^n = (x_1, \ldots, x_n)</span> that provides information about <span class="math inline">\theta</span></li>
</ul>
<p>Using the rules of probability, we can write: <span class="math display">f(x^* | x^n) = \int f(x^* | \theta, x^n) f(\theta | x^n) d\theta</span></p>
<p>If <span class="math inline">x^*</span> depends on the data only through <span class="math inline">\theta</span> (a common assumption), this simplifies to: <span class="math display">f(x^* | x^n) = \int f(x^* | \theta) f(\theta | x^n) d\theta</span></p>
<p>This equation reveals the key insight: <strong>to make predictions, we need the posterior distribution</strong> <span class="math inline">f(\theta | x^n)</span>. The posterior tells us which parameter values are plausible given the data, and we average our predictions over all these plausible values.</p>
<div class="definition">
<p><strong>The Components of Bayesian Inference</strong></p>
<p>To compute the posterior distribution <span class="math inline">f(\theta | x^n)</span>, we need:</p>
<ul>
<li><strong>Prior Distribution <span class="math inline">f(\theta)</span></strong>: What we believe about <span class="math inline">\theta</span> <em>before</em> seeing the data. This encodes our initial knowledge or assumptions. We will see later how the prior is chosen.</li>
<li><strong>Likelihood <span class="math inline">f(x^n | \theta)</span> or <span class="math inline">\mathcal{L}_n(\theta)</span></strong>: The probability of observing our data given different parameter values. This is the same likelihood function used in maximum likelihood estimation.</li>
<li><strong>Posterior Distribution <span class="math inline">f(\theta | x^n)</span></strong>: Our updated belief about <span class="math inline">\theta</span> <em>after</em> seeing the data, obtained via Bayes’ theorem.</li>
</ul>
</div>
<div class="theorem" name="Bayes' Theorem for Distributions">
<p>The posterior distribution is computed as: <span class="math display"> f(\theta | x^n) = \frac{f(x^n | \theta) f(\theta)}{\int f(x^n | \theta) f(\theta) d\theta} </span></p>
<p>The denominator <span class="math inline">\int f(x^n | \theta) f(\theta) d\theta</span> is called the <strong>marginal likelihood</strong> or <strong>evidence</strong>. It’s a normalizing constant that ensures the posterior integrates to 1.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</div>
<p>We often do not specifically care about the normalizing constant, and write: <span class="math display"> f(\theta | x^n) \propto f(x^n | \theta) f(\theta) </span></p>
<p>denoting that the posterior is <strong>proportional</strong> to Likelihood times Prior.</p>
<p>When the observations <span class="math inline">X_1, \ldots, X_n</span> are IID given <span class="math inline">\theta</span>, the likelihood factorizes: <span class="math display"> f(\theta | x^n) \propto \mathcal{L}_n(\theta) f(\theta) = \left[\prod_{i=1}^n f(x_i | \theta)\right] f(\theta) </span></p>
<p>This product structure is what allows evidence to accumulate across independent observations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Do We Care About the Posterior?
</div>
</div>
<div class="callout-body-container callout-body">
<p>The posterior distribution serves two distinct purposes:</p>
<p><strong>1. Direct Parameter Inference</strong>: Sometimes the parameters themselves are what we want to know:</p>
<ul>
<li>What’s the true efficacy of a vaccine?</li>
<li>What’s the rate of climate change?</li>
<li>What’s a manufacturing process’s defect rate?</li>
</ul>
<p>Here, we examine the posterior directly to understand the parameter values.</p>
<p><strong>2. Prediction</strong>: Other times, parameters are just a means to predict future observations:</p>
<ul>
<li>Estimating weather model parameters to forecast tomorrow’s conditions</li>
<li>Learning user preferences to recommend movies</li>
<li>Estimating volatility to predict financial risk</li>
</ul>
<p>For prediction, we integrate over the posterior, incorporating parameter uncertainty into our forecasts rather than conditioning on a single estimate.</p>
</div>
</div>
</section>
<section id="summarizing-the-posterior" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="summarizing-the-posterior"><span class="header-section-number">8.3.2</span> Summarizing the Posterior</h3>
<p>The posterior distribution <span class="math inline">f(\theta | x^n)</span> contains all our knowledge about <span class="math inline">\theta</span> after seeing the data. It’s the complete Bayesian answer to an inference problem. However, we often need to summarize this distribution with a single point – a <strong>point estimate</strong> – for communication or decision-making.</p>
<p><strong>Point Estimates</strong>:</p>
<ul>
<li><p><strong>Posterior Mean</strong>: <span class="math inline">\bar{\theta}_n = \mathbb{E}[\theta | x^n] = \int \theta f(\theta | x^n) d\theta</span></p>
<p>The center of our posterior beliefs, weighting all possible values by their posterior probability.</p></li>
<li><p><strong>Posterior Median</strong>: The value <span class="math inline">\theta_m</span> such that <span class="math inline">\mathbb{P}(\theta \le \theta_m | x^n) = 0.5</span></p>
<p>The value that splits the posterior distribution in half.</p></li>
<li><p><strong>Posterior Mode (MAP)</strong>: <span class="math inline">\hat{\theta}_{MAP} = \arg\max_{\theta} f(\theta | x^n)</span></p>
<p>The most probable value according to the posterior. MAP stands for “Maximum A Posteriori.”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p></li>
</ul>
<p><strong>Interval Estimates</strong>:</p>
<ul>
<li><p><strong>Credible Interval</strong>: A <span class="math inline">(1-\alpha)</span> credible interval<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> is a range <span class="math inline">(a, b)</span> such that: <span class="math display">\mathbb{P}(a &lt; \theta &lt; b | x^n) = 1-\alpha</span></p>
<p>Typically computed as an equal-tailed interval by finding <span class="math inline">a</span> and <span class="math inline">b</span> where <span class="math inline">\int_{-\infty}^a f(\theta | x^n) d\theta = \int_b^{\infty} f(\theta | x^n) d\theta = \alpha/2</span>.</p></li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Credible vs.&nbsp;Confidence Intervals
</div>
</div>
<div class="callout-body-container callout-body">
<p>A crucial distinction:</p>
<ul>
<li><strong>Credible interval</strong> (Bayesian): “Given the data, there’s a 95% probability that <span class="math inline">\theta</span> lies in this interval.”</li>
<li><strong>Confidence interval</strong> (Frequentist): “This procedure produces intervals that trap the true <span class="math inline">\theta</span> in 95% of repeated experiments.”</li>
</ul>
<p>The credible interval makes a direct probability statement about the parameter, which is what most people incorrectly think confidence intervals do!</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255605-758-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-758-1" role="tab" aria-controls="tabset-1757255605-758-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-758-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-758-2" role="tab" aria-controls="tabset-1757255605-758-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-758-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-758-3" role="tab" aria-controls="tabset-1757255605-758-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255605-758-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255605-758-1-tab"><p>Imagine you’re trying to estimate the average height in a population.
You take a sample and compute an interval.</p><p><strong>Confidence Interval (Frequentist)</strong>: “If I repeated
this sampling procedure 100 times, about 95 of those intervals would
contain the true average height.” It’s a statement about the reliability
of the <em>method</em>, not about any specific interval. Once computed,
the true value is either in it or not – there’s no probability
involved.</p><p><strong>Credible Interval (Bayesian)</strong>: “Based on the data I
observed and my prior knowledge, I’m 95% confident the true average
height is in this interval.” It’s a direct probability statement about
where the parameter lies, given what we’ve learned.</p><p>The confidence interval is like a fishing net manufacturer’s
guarantee: “95% of our nets catch fish.” The credible interval is like a
weather forecast: “95% chance of rain tomorrow.” One describes a
long-run property of a procedure; the other describes belief about a
specific unknown.</p></div><div id="tabset-1757255605-758-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-758-2-tab"><p>Let <span class="math inline">\(\theta\)</span> be the parameter and
<span class="math inline">\(X^n\)</span> the observed data.</p><p><strong>Confidence Interval</strong>: Find functions
<span class="math inline">\(L(X^n)\)</span> and
<span class="math inline">\(U(X^n)\)</span> such that:
<span class="math display">\[\mathbb{P}_\theta(L(X^n) \leq \theta \leq U(X^n)) = 1-\alpha \text{ for all } \theta\]</span></p><p>The probability is over the random data
<span class="math inline">\(X^n\)</span>, with
<span class="math inline">\(\theta\)</span> fixed. Different data gives
different intervals.</p><p><strong>Credible Interval</strong>: Find constants
<span class="math inline">\(a\)</span> and
<span class="math inline">\(b\)</span> such that:
<span class="math display">\[\int_a^b f(\theta|X^n) d\theta = 1-\alpha\]</span></p><p>The probability is over the parameter
<span class="math inline">\(\theta\)</span> given fixed, observed data
<span class="math inline">\(X^n\)</span>. The interval quantifies our
posterior uncertainty about
<span class="math inline">\(\theta\)</span>.</p><p>Key difference: In confidence intervals, data is random and parameter
is fixed. In credible intervals, data is fixed (observed) and parameter
is treated as random (uncertain).</p></div><div id="tabset-1757255605-758-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-758-3-tab"><p>Let’s simulate both types of intervals to see their fundamental
difference. We’ll generate many datasets to show the frequentist
coverage property, then compute a single credible interval to show the
Bayesian probability statement:</p><div id="f3eaceea" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the difference between confidence and credible intervals</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>true_std <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate many datasets to show confidence interval behavior</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>confidence_intervals <span class="op">=</span> []</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a dataset</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.random.normal(true_mean, true_std, n)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    sample_se <span class="op">=</span> true_std <span class="op">/</span> np.sqrt(n)  <span class="co"># Known variance case</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 95% Confidence interval</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    ci_lower <span class="op">=</span> sample_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> sample_se</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    ci_upper <span class="op">=</span> sample_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> sample_se</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    confidence_intervals.append((ci_lower, ci_upper))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Count how many contain the true parameter</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>coverage <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> (l, u) <span class="kw">in</span> confidence_intervals <span class="cf">if</span> l <span class="op">&lt;=</span> true_mean <span class="op">&lt;=</span> u)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Confidence Interval Coverage: </span><span class="sc">{</span>coverage<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_simulations<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>coverage<span class="op">/</span>n_simulations<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This demonstrates the frequentist guarantee: ~95</span><span class="sc">% c</span><span class="st">overage in repeated sampling"</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize all 100 confidence intervals</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Top panel: Show all confidence intervals</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>ax1.axhline(true_mean, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, label<span class="op">=</span><span class="st">'True mean'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot all intervals, colored by whether they contain the true mean</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    l, u <span class="op">=</span> confidence_intervals[i]</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    contains_true <span class="op">=</span> l <span class="op">&lt;=</span> true_mean <span class="op">&lt;=</span> u</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> <span class="st">'#0173B2'</span> <span class="cf">if</span> contains_true <span class="cf">else</span> <span class="st">'#DE8F05'</span>  <span class="co"># Blue vs Orange (high contrast, colorblind safe)</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use thinner lines and transparency for better visualization</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    ax1.plot([i, i], [l, u], color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="fl">0.8</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Small dots for interval centers</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    ax1.plot(i, (l<span class="op">+</span>u)<span class="op">/</span><span class="dv">2</span>, <span class="st">'.'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Add summary statistics</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>n_containing <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> (l,u) <span class="kw">in</span> confidence_intervals <span class="cf">if</span> l <span class="op">&lt;=</span> true_mean <span class="op">&lt;=</span> u)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Dataset number'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="ss">f'All </span><span class="sc">{</span>n_simulations<span class="sc">}</span><span class="ss"> Confidence Intervals</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>             <span class="ss">f'</span><span class="sc">{</span>n_containing<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_simulations<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>n_containing<span class="op">/</span>n_simulations<span class="sc">:.1%}</span><span class="ss">) contain true mean • '</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>             <span class="ss">f'Blue = contains true mean, Orange = misses'</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="dv">1</span>, n_simulations)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Now show a single Bayesian credible interval</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>single_dataset <span class="op">=</span> np.random.normal(true_mean, true_std, n)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(single_dataset)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="co"># With a Normal prior N(0, 10) and known variance</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>prior_mean, prior_var <span class="op">=</span> <span class="dv">0</span>, <span class="dv">10</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>posterior_var <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span><span class="op">/</span>prior_var <span class="op">+</span> n<span class="op">/</span>true_std<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>posterior_mean <span class="op">=</span> posterior_var <span class="op">*</span> (prior_mean<span class="op">/</span>prior_var <span class="op">+</span> n<span class="op">*</span>sample_mean<span class="op">/</span>true_std<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>posterior_std <span class="op">=</span> np.sqrt(posterior_var)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% Credible interval</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>cred_lower <span class="op">=</span> posterior_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>cred_upper <span class="op">=</span> posterior_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom panel: Show posterior distribution</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(posterior_mean <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>posterior_std, posterior_mean <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>posterior_std, <span class="dv">200</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>posterior_density <span class="op">=</span> stats.norm.pdf(x_range, posterior_mean, posterior_std)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_range, posterior_density, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Posterior'</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>ax2.fill_between(x_range, posterior_density, </span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>                 where<span class="op">=</span>(x_range <span class="op">&gt;=</span> cred_lower) <span class="op">&amp;</span> (x_range <span class="op">&lt;=</span> cred_upper),</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'95% Credible Interval'</span>)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>ax2.axvline(true_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True mean'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>ax2.axvline(sample_mean, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, label<span class="op">=</span><span class="st">'Sample mean'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Posterior density'</span>)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="ss">f'Posterior Distribution for One Dataset</span><span class="ch">\n</span><span class="ss">P(</span><span class="sc">{</span>cred_lower<span class="sc">:.2f}</span><span class="ss"> &lt; θ &lt; </span><span class="sc">{</span>cred_upper<span class="sc">:.2f}</span><span class="ss"> | data) = 0.95'</span>)</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">For this specific dataset:"</span>)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Sample mean: </span><span class="sc">{</span>sample_mean<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  95% Credible Interval: [</span><span class="sc">{</span>cred_lower<span class="sc">:.2f}</span><span class="ss">, </span><span class="sc">{</span>cred_upper<span class="sc">:.2f}</span><span class="ss">]"</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  This is a direct probability statement about the parameter!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confidence Interval Coverage: 97/100 = 97.00%
This demonstrates the frequentist guarantee: ~95% coverage in repeated sampling

For this specific dataset:
  Sample mean: 4.72
  95% Credible Interval: [4.35, 5.06]
  This is a direct probability statement about the parameter!</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="08-bayesian-inference-decision-theory_files/figure-html/cell-2-output-2.png"></p>
</div>
</div><p><strong>Key Takeaway</strong>: Confidence intervals achieve 95%
coverage <em>across many experiments</em> (a procedure property), while
credible intervals give 95% probability <em>for this specific
dataset</em> (a parameter property). Same numbers, fundamentally
different meanings.</p></div></div></div>
</section>
</section>
<section id="bayesian-inference-in-action" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="bayesian-inference-in-action"><span class="header-section-number">8.4</span> Bayesian Inference in Action</h2>
<section id="conjugate-models-and-conjugate-priors" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="conjugate-models-and-conjugate-priors"><span class="header-section-number">8.4.1</span> Conjugate Models and Conjugate Priors</h3>
<p>In principle, Bayesian inference requires us to compute integrals to normalize the posterior distribution. In practice, these integrals are often intractable. However, for certain combinations of priors and likelihoods, the posterior has a nice closed form. These special cases are called <strong>conjugate models</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is a Conjugate Prior?
</div>
</div>
<div class="callout-body-container callout-body">
<p>A prior is <strong>conjugate</strong> to a likelihood if the resulting posterior distribution is in the same family as the prior. This means:</p>
<ul>
<li>If the prior is Beta, the posterior is also Beta</li>
<li>If the prior is Normal, the posterior is also Normal</li>
</ul>
<p>Conjugacy provides a convenient analytical shortcut, though modern computational methods have reduced its importance.</p>
</div>
</div>
<section id="example-the-bernoulli-beta-model" class="level4">
<h4 class="anchored" data-anchor-id="example-the-bernoulli-beta-model">Example: The Bernoulli-Beta Model</h4>
<p>Consider the fundamental problem of estimating a probability from binary data. Let <span class="math inline">X_i \sim \text{Bernoulli}(p)</span> for <span class="math inline">i = 1, ..., n</span>, where we observe <span class="math inline">s</span> successes out of <span class="math inline">n</span> trials.</p>
<p><strong>Starting with a Uniform Prior:</strong></p>
<p>Since <span class="math inline">p</span> is a probability, it must lie in <span class="math inline">[0,1]</span>. If we have no prior information, a natural choice is the uniform prior: <span class="math inline">f(p) = 1</span> for <span class="math inline">p \in [0,1]</span>.</p>
<p><strong>Likelihood</strong>: With <span class="math inline">s</span> successes in <span class="math inline">n</span> trials, the likelihood is: <span class="math display">\mathcal{L}_n(p) \propto p^s(1-p)^{n-s}</span></p>
<p><strong>Posterior calculation</strong>: <span class="math display">f(p | x^n) \propto f(p) \times \mathcal{L}_n(p) = 1 \times p^s(1-p)^{n-s} = p^{(s+1)-1}(1-p)^{(n-s+1)-1}</span></p>
<p>This has the form of a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>! Specifically, if we match the parameters: <span class="math display">p | x^n \sim \text{Beta}(s+1, n-s+1)</span></p>
<p>The mean of <span class="math inline">\text{Beta}(\alpha, \beta)</span> is <span class="math inline">\alpha / (\alpha + \beta)</span>, so the posterior mean here is <span class="math inline">\bar{p} = \frac{s+1}{n+2}</span>, which can be written as: <span class="math display">\bar{p} = \frac{n}{n+2} \cdot \frac{s}{n} + \frac{2}{n+2} \cdot \frac{1}{2}</span></p>
<p>This is a weighted average of the MLE <span class="math inline">\hat{p} = s/n</span> and the prior mean <span class="math inline">1/2</span>, with the data getting more weight as <span class="math inline">n</span> increases.</p>
<p><strong>The General Beta Prior:</strong></p>
<p>The uniform prior is actually a special case of the Beta distribution. In general, if we use a <span class="math inline">\text{Beta}(\alpha, \beta)</span> prior: <span class="math display">f(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} p^{\alpha-1}(1-p)^{\beta-1}</span></p>
<p>Then the posterior is: <span class="math display">p | x^n \sim \text{Beta}(\alpha + s, \beta + n - s)</span></p>
<p><strong>Key insights:</strong></p>
<ul>
<li>The Beta distribution is <strong>conjugate</strong> to the Bernoulli likelihood - the posterior stays in the Beta family</li>
<li>The parameters <span class="math inline">\alpha</span> and <span class="math inline">\beta</span> act as “pseudo-counts”: <span class="math inline">\alpha</span> prior successes, <span class="math inline">\beta</span> prior failures</li>
<li>The uniform prior is <span class="math inline">\text{Beta}(1, 1)</span> - one pseudo-success and one pseudo-failure</li>
<li>The posterior mean <span class="math inline">\bar{p} = \frac{\alpha + s}{\alpha + \beta + n}</span> combines prior pseudo-counts with observed counts</li>
<li>As <span class="math inline">n \to \infty</span>, the data dominates and the prior’s influence vanishes</li>
</ul>
<p>Let’s visualize how the posterior evolves with data:</p>
<div id="c03b9673" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="2">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the figure</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Top panel: Effect of sample size</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>p_true <span class="op">=</span> <span class="fl">0.7</span>  <span class="co"># True probability</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>alpha_prior, beta_prior <span class="op">=</span> <span class="dv">1</span>, <span class="dv">1</span>  <span class="co"># Uniform prior</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Different sample sizes</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">200</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'gray'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'red'</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>p_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">200</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed once outside the loop for reproducible results</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(sample_sizes, colors):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Just the prior</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> stats.beta.pdf(p_range, alpha_prior, beta_prior)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="st">'Prior'</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate data</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> np.random.binomial(n, p_true)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior parameters</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        alpha_post <span class="op">=</span> alpha_prior <span class="op">+</span> s</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        beta_post <span class="op">=</span> beta_prior <span class="op">+</span> n <span class="op">-</span> s</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> stats.beta.pdf(p_range, alpha_post, beta_post)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="ss">f'n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, s=</span><span class="sc">{</span>s<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    ax1.plot(p_range, y, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>label)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>ax1.axvline(p_true, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True p'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Posterior Becomes More Concentrated with More Data'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom panel: Effect of different priors</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">10</span>  <span class="co"># 50% success rate in data</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>priors <span class="op">=</span> [</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">1</span>, <span class="st">'Uniform: Beta(1,1)'</span>),</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="dv">10</span>, <span class="st">'Informative: Beta(10,10)'</span>),</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">10</span>, <span class="st">'Skeptical: Beta(1,10)'</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (alpha, beta, label) <span class="kw">in</span> priors:</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    prior_y <span class="op">=</span> stats.beta.pdf(p_range, alpha, beta)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    ax2.plot(p_range, prior_y, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    alpha_post <span class="op">=</span> alpha <span class="op">+</span> s</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    beta_post <span class="op">=</span> beta <span class="op">+</span> n <span class="op">-</span> s</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    post_y <span class="op">=</span> stats.beta.pdf(p_range, alpha_post, beta_post)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    ax2.plot(p_range, post_y, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>label)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>ax2.axvline(<span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="ss">f'Different Priors, Same Data (n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, s=</span><span class="sc">{</span>s<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-bayesian-inference-decision-theory_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The plots illustrate two key principles:</p>
<ol type="1">
<li><strong>Top panel</strong>: As we collect more data, the posterior becomes increasingly concentrated around the true value, regardless of the prior.</li>
<li><strong>Bottom panel</strong>: Different priors lead to different posteriors (but this effect diminishes with larger sample sizes).</li>
</ol>
</section>
<section id="example-the-normal-normal-model" class="level4">
<h4 class="anchored" data-anchor-id="example-the-normal-normal-model">Example: The Normal-Normal Model</h4>
<p>Now consider estimating the mean of a Normal distribution with known variance. Let <span class="math inline">X_i \sim \mathcal{N}(\theta, \sigma^2)</span> where <span class="math inline">\sigma^2</span> is known (this assumption simplifies the math and is commonly used in introductory examples).</p>
<p><strong>Prior</strong>: <span class="math inline">\theta \sim \mathcal{N}(\theta_0, \sigma_0^2)</span></p>
<p><strong>Likelihood</strong>: For IID data, the sufficient statistic is the sample mean <span class="math inline">\bar{x}</span>, and: <span class="math display">\bar{x} | \theta \sim \mathcal{N}(\theta, \sigma^2/n)</span></p>
<div class="theorem" name="Posterior for Normal-Normal Model">
<p>Given a likelihood <span class="math inline">X_i | \theta \sim \mathcal{N}(\theta, \sigma^2)</span> (known <span class="math inline">\sigma^2</span>) and a prior <span class="math inline">\theta \sim \mathcal{N}(\theta_0, \sigma_0^2)</span>, the posterior is:</p>
<p><span class="math display">\theta | x^n \sim \mathcal{N}(\theta_*, \sigma_*^2)</span></p>
<p>where:</p>
<ul>
<li><strong>Posterior Precision</strong>: <span class="math inline">\frac{1}{\sigma_*^2} = \frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}</span></li>
<li><strong>Posterior Mean</strong>: <span class="math inline">\theta_* = \sigma_*^2 \left( \frac{\theta_0}{\sigma_0^2} + \frac{n \bar{x}}{\sigma^2} \right)</span></li>
</ul>
</div>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>Precision (inverse variance) is additive: posterior precision = prior precision + data precision</li>
<li>The posterior mean is a precision-weighted average of prior mean and sample mean</li>
<li>More precise information gets more weight</li>
<li>The posterior mean interpolates between the prior mean and sample mean, with the weight given to the data increasing as <span class="math inline">n</span> increases</li>
<li>Posterior is always more precise than either prior or likelihood alone</li>
</ul>
<div id="44541d8d" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show code for Normal-Normal posterior calculation</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normal_normal_posterior(prior_mean, prior_var, data_mean, data_var, n):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate posterior parameters for Normal-Normal conjugate model.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    -----------</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    prior_mean : Prior mean θ₀</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    prior_var : Prior variance σ₀²</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    data_mean : Sample mean x̄</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    data_var : Known data variance σ²</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    n : Sample size</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">    --------</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">    post_mean : Posterior mean θ*</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">    post_var : Posterior variance σ*²</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to precisions (inverse variances)</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    prior_precision <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> prior_var</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    data_precision <span class="op">=</span> n <span class="op">/</span> data_var</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior precision is sum of precisions</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    post_precision <span class="op">=</span> prior_precision <span class="op">+</span> data_precision</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    post_var <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> post_precision</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior mean is precision-weighted average</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    post_mean <span class="op">=</span> post_var <span class="op">*</span> (prior_precision <span class="op">*</span> prior_mean <span class="op">+</span> </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                            data_precision <span class="op">*</span> data_mean)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> post_mean, post_var</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Example calculation</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>prior_mean, prior_var <span class="op">=</span> <span class="dv">0</span>, <span class="dv">4</span>  <span class="co"># Prior: N(0, 4)</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>data_var <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Known variance</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>data_mean <span class="op">=</span> <span class="fl">2.3</span>  <span class="co"># Observed sample mean</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>post_mean, post_var <span class="op">=</span> normal_normal_posterior(</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    prior_mean, prior_var, data_mean, data_var, n</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prior: N(</span><span class="sc">{</span>prior_mean<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>prior_var<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data: n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, x̄=</span><span class="sc">{</span>data_mean<span class="sc">}</span><span class="ss">, σ²=</span><span class="sc">{</span>data_var<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior: N(</span><span class="sc">{</span>post_mean<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>post_var<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Posterior mean is </span><span class="sc">{</span>post_mean<span class="sc">:.3f}</span><span class="ss">, between prior mean </span><span class="sc">{</span>prior_mean<span class="sc">}</span><span class="ss"> and MLE </span><span class="sc">{</span>data_mean<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Prior: N(0, 4)
Data: n=10, x̄=2.3, σ²=1
Posterior: N(2.244, 0.098)

Posterior mean is 2.244, between prior mean 0 and MLE 2.3</code></pre>
</div>
</div>
</section>
</section>
<section id="the-art-and-science-of-choosing-priors" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="the-art-and-science-of-choosing-priors"><span class="header-section-number">8.4.2</span> The Art and Science of Choosing Priors</h3>
<p>One of the most debated topics in Bayesian statistics is how to choose the prior distribution. Critics argue that priors introduce subjectivity; advocates counter that they make assumptions explicit. The reality is nuanced: prior choice is both an art requiring judgment and a science with established principles.</p>
<p><strong>Conjugate Priors</strong>: We’ve seen these in action – Beta for Bernoulli, Normal for Normal. They’re computationally convenient and have nice interpretations (like pseudo-counts), but they may not reflect genuine prior beliefs. Using them just for convenience can lead to misleading results.</p>
<p><strong>Non-Informative Priors</strong>: These attempt to be “objective” by letting the data speak for itself. Common choices include:</p>
<ul>
<li>Uniform priors: <span class="math inline">f(\theta) = \text{constant}</span></li>
<li>Jeffreys’ prior: <span class="math inline">f(\theta) \propto \sqrt{I(\theta)}</span> where <span class="math inline">I(\theta)</span> is the Fisher information</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Flat Prior Fallacy
</div>
</div>
<div class="callout-body-container callout-body">
<p>A uniform prior is not “uninformative”! Consider:</p>
<ol type="1">
<li><strong>Scale matters</strong>: A uniform prior on <span class="math inline">[-10^6, 10^6]</span> says <span class="math inline">|\theta|</span> is almost certainly large</li>
<li><strong>Not transformation invariant</strong>: If <span class="math inline">p \sim \text{Uniform}(0,1)</span>, then <span class="math inline">\log(p/(1-p))</span> is not uniform</li>
<li><strong>Can encode strong beliefs</strong>: Uniform on [0, 1000] for a rate parameter implies most mass is on very large values (highly informative!)</li>
</ol>
<p>The notion of “no information” is not well-defined mathematically.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Jeffreys’ Prior
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Jeffreys proposed using <span class="math inline">f(\theta) \propto \sqrt{I(\theta)}</span> where <span class="math inline">I(\theta)</span> is the Fisher information. This prior has a key property: it’s invariant to reparameterization. If we transform <span class="math inline">\theta</span> to <span class="math inline">\varphi = g(\theta)</span>, the Jeffreys prior for <span class="math inline">\varphi</span> is what we’d get by transforming the Jeffreys prior for <span class="math inline">\theta</span>.</p>
<p>For <span class="math inline">\text{Bernoulli}(p)</span>, the Jeffreys prior is <span class="math inline">\text{Beta}(1/2, 1/2)</span>, which is U-shaped, putting more mass near 0 and 1 than at 0.5 – hardly “uninformative”!</p>
</div>
</div>
</div>
<p><strong>Weakly Informative Priors</strong>: This is the recommended approach nowadays which balances several goals:</p>
<ul>
<li>Wide enough to not exclude plausible values</li>
<li>Tight enough to exclude absurd values</li>
<li>Regularize estimation to prevent overfitting</li>
</ul>
<p>For example, for a logistic regression coefficient, a <span class="math inline">\mathcal{N}(0, 2.5^2)</span> prior (mean 0, standard deviation 2.5) allows large effects but prevents numerical instability. Note: We use the <span class="math inline">\mathcal{N}(\mu, \sigma^2)</span> parameterization throughout these notes.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Challenge of High-Dimensional Priors
</div>
</div>
<div class="callout-body-container callout-body">
<p>Placing sensible priors becomes increasingly difficult as dimensionality grows:</p>
<ul>
<li><p><strong>The Gaussian bubble</strong>: In high dimensions, a multivariate standard normal <span class="math inline">\mathcal{N}(0, I)</span> concentrates its mass in a thin shell at radius <span class="math inline">\sqrt{d}</span> from the origin – almost no mass is near zero despite this being the “center” of the distribution. This concentration of measure phenomenon means our intuitions about priors break down (see this <a href="https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/">blog post</a>).</p></li>
<li><p><strong>Deep learning</strong>: Specifying priors for millions of neural network weights remains an open problem in Bayesian deep learning. Most practitioners resort to simple priors like <span class="math inline">\mathcal{N}(0, \sigma^2 I)</span> that don’t capture the true structure, or avoid fully Bayesian approaches altogether.</p></li>
</ul>
<p>High-dimensional Bayesian inference requires careful thought about what the prior actually implies when there are many parameters.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: The Bayesian Central Limit Theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A remarkable result shows that Bayesian and frequentist methods converge with enough data.</p>
<p>Under suitable regularity conditions, as <span class="math inline">n \to \infty</span>, the posterior distribution can be approximated by: <span class="math display">f(\theta | x^n) \approx \mathcal{N}(\hat{\theta}_{MLE}, \widehat{se}^2)</span></p>
<p>where <span class="math inline">\hat{\theta}_{MLE}</span> is the maximum likelihood estimate and <span class="math inline">\widehat{se}</span> is its standard error.</p>
<p><strong>Why this matters</strong>:</p>
<ol type="1">
<li>With enough data, the prior becomes irrelevant</li>
<li>Bayesian credible intervals ≈ Frequentist confidence intervals</li>
<li>Both approaches give essentially the same answer</li>
<li>The likelihood dominates both approaches in large samples</li>
</ol>
<p>This is reassuring: two philosophically different approaches converge to the same practical conclusions when we have sufficient evidence.</p>
</div>
</div>
</div>
</section>
<section id="implementing-bayesian-inference" class="level3" data-number="8.4.3">
<h3 data-number="8.4.3" class="anchored" data-anchor-id="implementing-bayesian-inference"><span class="header-section-number">8.4.3</span> Implementing Bayesian Inference</h3>
<p>For the conjugate models in this chapter, we can solve for the posterior distribution analytically. But what happens in more complex, real-world models where this is not possible?</p>
<p>The modern Bayesian workflow relies on powerful computational algorithms, most commonly <strong>Markov chain Monte Carlo (MCMC)</strong>. These algorithms allow us to generate a large collection of samples that are representative of the posterior distribution, even when we cannot solve for it directly. Once we have these samples, we can approximate any summary we need (like the mean or a credible interval) and easily get posteriors for transformed parameters.</p>
<p>Modern <strong>probabilistic programming frameworks</strong> such as <a href="https://mc-stan.org/">Stan</a> or <a href="https://www.pymc.io/welcome.html">PyMC</a> allow users to perform Bayesian inference relatively easily, exploiting modern machinery. This computational approach is incredibly powerful and flexible, and we will explore it in detail in Chapter 10. For now, the key takeaway is that the goal of Bayesian inference is always to obtain the posterior; the methods in this chapter do it with math, while later methods will do it with computation.</p>
</section>
</section>
<section id="statistical-decision-theory-a-framework-for-best" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="statistical-decision-theory-a-framework-for-best"><span class="header-section-number">8.5</span> Statistical Decision Theory: A Framework for “Best”</h2>
<p>We now shift from Bayesian inference to a more general question: given multiple ways to estimate a parameter, how do we choose the best one? Statistical decision theory provides a formal framework for comparing any estimators – Bayesian, frequentist, or otherwise.</p>
<section id="the-ingredients-loss-and-risk" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="the-ingredients-loss-and-risk"><span class="header-section-number">8.5.1</span> The Ingredients: Loss and Risk</h3>
<p>To compare estimators formally, we need to quantify how “wrong” an estimate is.</p>
<div class="definition">
<p><strong>Loss Function</strong> <span class="math inline">L(\theta, \hat{\theta})</span>: Quantifies the penalty for estimating <span class="math inline">\theta</span> with <span class="math inline">\hat{\theta}</span>.</p>
<p>Common examples:</p>
<ul>
<li><strong>Squared Error</strong>: <span class="math inline">L_2(\theta, \hat{\theta}) = (\theta - \hat{\theta})^2</span></li>
<li><strong>Absolute Error</strong>: <span class="math inline">L_1(\theta, \hat{\theta}) = |\theta - \hat{\theta}|</span></li>
<li><strong><span class="math inline">L_p</span> Loss</strong>: <span class="math inline">L_p(\theta, \hat{\theta}) = |\theta - \hat{\theta}|^p</span> for <span class="math inline">p \geq 1</span> (generalizes the above)</li>
<li><strong>Zero-One Loss</strong>: <span class="math inline">L_{0-1}(\theta, \hat{\theta}) = \begin{cases} 0 &amp; \text{if } \theta = \hat{\theta} \\ 1 &amp; \text{otherwise} \end{cases}</span></li>
</ul>
</div>
<p>Loss tells us the cost of a specific estimate for a specific parameter value. But estimators are random – they depend on data. So we need to average:</p>
<div class="definition">
<p><strong>Risk</strong> <span class="math inline">R(\theta, \hat{\theta})</span>: The expected loss over all possible datasets, for a given loss function <span class="math inline">\mathcal{L}</span> and fixed <span class="math inline">\theta</span>. <span class="math display">R(\theta, \hat{\theta}) = \mathbb{E}_{\theta}[L(\theta, \hat{\theta}(X))] = \int L(\theta, \hat{\theta}(x)) f(x;\theta) dx</span></p>
</div>
<p>For squared error loss, the risk equals the MSE: <span class="math display">R(\theta, \hat{\theta}) = \mathbb{E}_{\theta}[(\theta - \hat{\theta})^2] = \text{MSE} = \mathbb{V}(\hat{\theta}) + \text{Bias}^2(\hat{\theta}) </span></p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255605-198-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-198-1" role="tab" aria-controls="tabset-1757255605-198-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-198-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-198-2" role="tab" aria-controls="tabset-1757255605-198-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-198-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-198-3" role="tab" aria-controls="tabset-1757255605-198-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255605-198-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255605-198-1-tab"><p>Think of risk as the “average wrongness” of an estimator, for a
specific definition of “wrongness” (loss function). Imagine you could
repeat your experiment many times with the same true parameter value.
Each time, you’d get different data and thus a different estimate. The
risk tells you the average penalty you’d pay across all these
repetitions.</p><p>It’s like evaluating a weather forecaster: you don’t judge them on
one prediction, but on their average performance over many days. An
estimator with low risk is consistently good, even if it’s not perfect
on any single dataset.</p></div><div id="tabset-1757255605-198-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-198-2-tab"><p>Risk is the frequentist expectation of the loss function, treating
the estimator as a random variable (through its dependence on random
data) while holding the parameter fixed:</p><p><span class="math display">\[R(\theta, \hat{\theta}) = \mathbb{E}_{X \sim f(x;\theta)}[L(\theta, \hat{\theta}(X))]\]</span></p><p>This contrasts with Bayes risk, which averages over
<span class="math inline">\(\theta\)</span> according to a prior. The
risk function <span class="math inline">\(R(\theta, \cdot)\)</span> maps
each parameter value to a real number, creating a curve that
characterizes the estimator’s performance across the parameter
space.</p><p>For squared error loss, the bias-variance decomposition shows that
risk combines systematic error (bias) with variability (variance),
revealing the fundamental tradeoff in estimation.</p></div><div id="tabset-1757255605-198-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-198-3-tab"><p>We can understand risk concretely through simulation. The following
code demonstrates what risk actually means by repeatedly generating
datasets from the same distribution and computing the squared loss for
each one:</p><div id="71c3a350" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_risk_by_simulation(true_theta, estimator_func, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                                n_samples<span class="op">=</span><span class="dv">20</span>, n_simulations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Estimate risk via Monte Carlo simulation.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    This shows what risk really means: the average loss</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    over many possible datasets.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate a dataset (example: Normal distribution)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.normal(true_theta, <span class="dv">1</span>, n_samples)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the estimate for this dataset</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        estimate <span class="op">=</span> estimator_func(data)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the loss (using squared error)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (estimate <span class="op">-</span> true_theta)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Risk is the average loss</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    risk <span class="op">=</span> np.mean(losses)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"True parameter: </span><span class="sc">{</span>true_theta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Estimated risk: </span><span class="sc">{</span>risk<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Min loss seen: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(losses)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Max loss seen: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(losses)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> risk</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Risk of sample mean estimator</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>risk <span class="op">=</span> estimate_risk_by_simulation(</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    true_theta<span class="op">=</span><span class="fl">5.0</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    estimator_func<span class="op">=</span><span class="kw">lambda</span> data: np.mean(data),</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">20</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True parameter: 5.0
Estimated risk: 0.0503
Min loss seen: 0.0000
Max loss seen: 0.8506</code></pre>
</div>
</div><p><strong>Key Takeaway</strong>: Risk is the <em>average</em> loss
across all possible datasets. The simulation shows that while individual
losses vary widely (min to max), risk captures the expected performance
of an estimator.</p></div></div></div>
</section>
<section id="the-challenge-of-comparing-risk-functions" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="the-challenge-of-comparing-risk-functions"><span class="header-section-number">8.5.2</span> The Challenge of Comparing Risk Functions</h3>
<p>Risk functions are curves – one risk value for each possible <span class="math inline">\theta</span>. This creates a fundamental problem: estimators rarely dominate uniformly.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: A Simple Case
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">X \sim \mathcal{N}(\theta, 1)</span> and let’s assume squared error loss. Compare:</p>
<ul>
<li><span class="math inline">\hat{\theta}_1 = X</span> (the sensible estimator)</li>
<li><span class="math inline">\hat{\theta}_2 = 3</span> (a silly constant estimator)</li>
</ul>
<p>Risk calculations:</p>
<ul>
<li><p>For <span class="math inline">\hat{\theta}_1 = X</span>: <span class="math display">R(\theta, \hat{\theta}_1) = \mathbb{E}_\theta[(X - \theta)^2] = \text{Var}(X) = 1</span> This is constant for all <span class="math inline">\theta</span>.</p></li>
<li><p>For <span class="math inline">\hat{\theta}_2 = 3</span>: <span class="math display">R(\theta, \hat{\theta}_2) = \mathbb{E}_\theta[(3 - \theta)^2] = (3-\theta)^2</span> This depends on <span class="math inline">\theta</span> since the estimator is non-random.</p></li>
</ul>
<div id="210ae48b" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-bayesian-inference-decision-theory_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The constant estimator is actually better when <span class="math inline">\theta</span> is near 3 (“a broken clock is right twice a day”), but terrible elsewhere. Still, neither <em>uniformly</em> dominates the other.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Bernoulli Estimation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider <span class="math inline">X_1, \ldots, X_n \sim \text{Bernoulli}(p)</span> with squared error loss. Let <span class="math inline">S = \sum_{i=1}^n X_i</span> be the number of successes.</p>
<p>We’ll compare two natural estimators by computing their risk functions.</p>
<p><strong>Estimator 1: MLE</strong></p>
<p>The MLE is <span class="math inline">\hat{p}_1 = \bar{X} = \frac{S}{n}</span></p>
<p>Since <span class="math inline">S \sim \text{Binomial}(n, p)</span>, we have:</p>
<ul>
<li><span class="math inline">\mathbb{E}[S] = np</span>, so <span class="math inline">\mathbb{E}[\hat{p}_1] = p</span> (unbiased)</li>
<li><span class="math inline">\text{Var}(S) = np(1-p)</span>, so <span class="math inline">\text{Var}(\hat{p}_1) = \frac{p(1-p)}{n}</span></li>
</ul>
<p>The risk under squared error loss is: <span class="math display">R(p, \hat{p}_1) = \mathbb{E}[(\hat{p}_1 - p)^2] = \text{Var}(\hat{p}_1) + \text{Bias}^2(\hat{p}_1) = \frac{p(1-p)}{n} + 0 = \frac{p(1-p)}{n}</span></p>
<p>This is a parabola with maximum at <span class="math inline">p = 1/2</span>.</p>
<p><strong>Estimator 2: Bayesian posterior mean with Beta(α, β) prior</strong></p>
<p>Using Bayes’ theorem with prior <span class="math inline">p \sim \text{Beta}(\alpha, \beta)</span> and observing <span class="math inline">S</span> successes: <span class="math display">p | S \sim \text{Beta}(\alpha + S, \beta + n - S)</span></p>
<p>The posterior mean<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> is: <span class="math display">\hat{p}_2 = \frac{\alpha + S}{\alpha + \beta + n}</span></p>
<p>To find the risk, we compute bias and variance:</p>
<ul>
<li>Expected value: <span class="math inline">\mathbb{E}[\hat{p}_2] = \frac{\alpha + np}{\alpha + \beta + n}</span></li>
<li>Bias: <span class="math inline">\text{Bias}(\hat{p}_2) = \frac{\alpha + np}{\alpha + \beta + n} - p = \frac{\alpha - p(\alpha + \beta)}{\alpha + \beta + n}</span></li>
<li>Variance: <span class="math inline">\text{Var}(\hat{p}_2) = \text{Var}\left(\frac{S}{\alpha + \beta + n}\right) = \frac{np(1-p)}{(\alpha + \beta + n)^2}</span></li>
</ul>
<p>Therefore, the general risk formula is: <span class="math display">R(p, \hat{p}_2) = \frac{np(1-p)}{(\alpha + \beta + n)^2} + \left(\frac{\alpha - p(\alpha + \beta)}{\alpha + \beta + n}\right)^2</span></p>
<p><strong>Special case: Uniform prior Beta(1, 1)</strong></p>
<p>For <span class="math inline">\alpha = \beta = 1</span>, the posterior mean becomes: <span class="math display">\hat{p}_2 = \frac{1 + S}{2 + n} = \frac{n}{n+2} \cdot \frac{S}{n} + \frac{2}{n+2} \cdot \frac{1}{2}</span></p>
<p>This is a weighted average of the MLE and the prior mean 1/2.</p>
<p>The risk specializes to: <span class="math display">R(p, \hat{p}_2) = \frac{np(1-p)}{(n+2)^2} + \left(\frac{1 - 2p}{n + 2}\right)^2</span></p>
<p>Let’s plot both risk functions to see how they compare:</p>
<div id="73fd7bfe" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-bayesian-inference-decision-theory_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The risk functions cross! The MLE is better near the extremes (p near 0 or 1), while the Bayes estimator is better near the middle (p near 1/2). Neither estimator uniformly dominates the other.</p>
</div>
</div>
<p>Both examples above illustrate a fundamental challenge in decision theory: when risk functions cross, we cannot declare one estimator uniformly better than another. Different estimators excel in different regions of the parameter space.</p>
<p>To make a decision on the estimator to use, we must reduce these risk curves to single numbers that we can compare. But how should we summarize an entire function? Should we care most about average performance or worst-case performance? Different answers to this question lead to two distinct optimality criteria: <strong>Bayes estimators</strong> (optimizing average risk) and <strong>minimax estimators</strong> (optimizing worst-case risk), detailed in the next section.</p>
</section>
</section>
<section id="optimal-estimators-bayes-and-minimax-rules" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="optimal-estimators-bayes-and-minimax-rules"><span class="header-section-number">8.6</span> Optimal Estimators: Bayes and Minimax Rules</h2>
<section id="the-bayesian-approach-minimizing-average-risk" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="the-bayesian-approach-minimizing-average-risk"><span class="header-section-number">8.6.1</span> The Bayesian Approach: Minimizing Average Risk</h3>
<p>The Bayesian approach to decision theory averages the risk over a prior distribution, giving us a single number to minimize.</p>
<div class="definition">
<p><strong>Bayes Risk</strong>: The expected risk, averaged over the prior distribution <span class="math inline">f(\theta)</span>: <span class="math display">r(f, \hat{\theta}) = \int R(\theta, \hat{\theta}) f(\theta) d\theta</span></p>
</div>
<div class="definition">
<p><strong>Bayes Estimator</strong>: The estimator <span class="math inline">\hat{\theta}^B</span> that minimizes the Bayes risk: <span class="math display">\hat{\theta}^B = \arg\min_{\hat{\theta}} r(f, \hat{\theta})</span></p>
</div>
<p>The remarkable connection between Bayesian inference and decision theory:</p>
<div class="theorem" name="Finding the Bayes Estimator">
<p>The Bayes estimator can be found by minimizing the <em>posterior</em> expected loss for each observed <span class="math inline">x</span>. Specifically:</p>
<ul>
<li>For <strong>Squared Error Loss</strong>: The Bayes estimator is the <strong>Posterior Mean</strong></li>
<li>For <strong>Absolute Error Loss</strong>: The Bayes estimator is the <strong>Posterior Median</strong><br>
</li>
<li>For <strong>Zero-One Loss</strong>: The Bayes estimator is the <strong>Posterior Mode (MAP)</strong></li>
</ul>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof for Different Loss Functions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For any loss function <span class="math inline">L(\theta, \hat{\theta})</span>, the Bayes estimator minimizes the posterior expected loss: <span class="math display">\hat{\theta}^B(x) = \arg\min_a \mathbb{E}[L(\theta, a) | X = x] = \arg\min_a \int L(\theta, a) f(\theta|x) d\theta</span></p>
<p><strong>Squared Error Loss: <span class="math inline">L(\theta, a) = (\theta - a)^2</span></strong></p>
<p>We need to minimize: <span class="math display">\int (\theta - a)^2 f(\theta|x) d\theta</span></p>
<p>Taking the derivative with respect to <span class="math inline">a</span> and setting to zero: <span class="math display">\frac{d}{da} \int (\theta - a)^2 f(\theta|x) d\theta = -2 \int (\theta - a) f(\theta|x) d\theta = 0</span></p>
<p>This gives: <span class="math display">\int \theta f(\theta|x) d\theta = a \int f(\theta|x) d\theta = a</span></p>
<p>Therefore: <span class="math inline">\hat{\theta}^B(x) = \int \theta f(\theta|x) d\theta = \mathbb{E}[\theta | X = x]</span> (posterior mean)</p>
<p><strong>Absolute Error Loss: <span class="math inline">L(\theta, a) = |\theta - a|</span></strong></p>
<p>We need to minimize: <span class="math display">\int |\theta - a| f(\theta|x) d\theta = \int_{-\infty}^a (a - \theta) f(\theta|x) d\theta + \int_a^{\infty} (\theta - a) f(\theta|x) d\theta</span></p>
<p>Taking the derivative with respect to <span class="math inline">a</span>: <span class="math display">\frac{d}{da} = \int_{-\infty}^a f(\theta|x) d\theta - \int_a^{\infty} f(\theta|x) d\theta = F(a|x) - (1 - F(a|x)) = 2F(a|x) - 1</span></p>
<p>Setting to zero: <span class="math inline">F(a|x) = 1/2</span>, so <span class="math inline">\hat{\theta}^B(x)</span> is the posterior median.</p>
<p><strong>Zero-One Loss: <span class="math inline">L(\theta, a) = \mathbb{1}\{\theta \neq a\}</span></strong></p>
<p>The expected loss is: <span class="math display">\mathbb{E}[L(\theta, a) | X = x] = P(\theta \neq a | X = x) = 1 - P(\theta = a | X = x)</span></p>
<p>This is minimized when <span class="math inline">P(\theta = a | X = x)</span> is maximized, which occurs at the posterior mode.</p>
</div>
</div>
</div>
<p>This theorem reveals a profound insight: Bayesian inference naturally produces optimal estimators! The posterior summaries we compute for inference are exactly the estimators that minimize expected loss.</p>
</section>
<section id="the-frequentist-approach-minimizing-worst-case-risk" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="the-frequentist-approach-minimizing-worst-case-risk"><span class="header-section-number">8.6.2</span> The Frequentist Approach: Minimizing Worst-Case Risk</h3>
<p>The minimax approach takes a pessimistic view: prepare for the worst case.</p>
<div class="definition">
<p><strong>Maximum Risk</strong>: The worst-case risk over the entire parameter space <span class="math inline">\Theta</span>: <span class="math display">\bar{R}(\hat{\theta}) = \sup_{\theta \in \Theta} R(\theta, \hat{\theta})</span></p>
</div>
<div class="definition">
<p><strong>Minimax Estimator</strong>: The estimator <span class="math inline">\hat{\theta}^{MM}</span> with the smallest maximum risk: <span class="math display">\hat{\theta}^{MM} = \arg\min_{\hat{\theta}} \sup_{\theta} R(\theta, \hat{\theta})</span></p>
<p>It’s the “best of the worst-case” estimators.</p>
</div>
<p>Finding minimax estimators directly is usually difficult. However, there’s a powerful connection to Bayes estimators:</p>
<div class="theorem" name="Constant Risk Bayes Rules are Minimax">
<p>If a Bayes estimator has constant risk (the same risk for all <span class="math inline">\theta</span>), then it is minimax.</p>
</div>
<p>This gives us a recipe: find a prior such that the resulting Bayes estimator has constant risk.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Minimax Estimator for Bernoulli
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider <span class="math inline">X_1, \ldots, X_n \sim \text{Bernoulli}(p)</span> with squared error loss. We know from the previous example that the Bayes estimator with prior <span class="math inline">\text{Beta}(\alpha, \beta)</span> has risk:</p>
<p><span class="math display">R(p, \hat{p}) = \frac{np(1-p)}{(\alpha + \beta + n)^2} + \left(\frac{\alpha - p(\alpha + \beta)}{\alpha + \beta + n}\right)^2</span></p>
<p><strong>The key insight</strong>: Can we choose <span class="math inline">\alpha</span> and <span class="math inline">\beta</span> to make this risk constant (independent of <span class="math inline">p</span>)? If so, the constant risk theorem tells us the resulting estimator would be minimax.</p>
<p>It turns out that setting <span class="math inline">\alpha = \beta = \sqrt{n/4}</span> does exactly this!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Derivation of the minimax prior
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If we set <span class="math inline">\alpha = \beta</span>, the risk becomes: <span class="math display">R(p, \hat{p}) = \frac{1}{(2\alpha + n)^2}\left[np(1-p) + \alpha^2(1 - 2p)^2\right]</span></p>
<p>Expanding the term in brackets: <span class="math display">np(1-p) + \alpha^2(1 - 2p)^2 = np - np^2 + \alpha^2(1 - 4p + 4p^2)</span> <span class="math display">= \alpha^2 + (n - 4\alpha^2)p + (4\alpha^2 - n)p^2</span></p>
<p>This is constant if and only if the coefficients of <span class="math inline">p</span> and <span class="math inline">p^2</span> are both zero: - Coefficient of <span class="math inline">p</span>: <span class="math inline">n - 4\alpha^2 = 0 \Rightarrow \alpha^2 = n/4</span> - Coefficient of <span class="math inline">p^2</span>: <span class="math inline">4\alpha^2 - n = 0 \Rightarrow \alpha^2 = n/4</span> ✓</p>
<p>Both conditions give the same answer: <span class="math inline">\alpha = \sqrt{n/4}</span>.</p>
</div>
</div>
</div>
<p>With <span class="math inline">\alpha = \beta = \sqrt{n/4}</span>:</p>
<ul>
<li>The Bayes estimator is: <span class="math inline">\hat{p} = \frac{S + \sqrt{n/4}}{n + \sqrt{n}}</span></li>
<li>The risk is constant: <span class="math inline">R(p, \hat{p}) = \frac{n}{4(n + \sqrt{n})^2}</span> for all <span class="math inline">p</span></li>
<li>By the theorem above, this makes it minimax!</li>
</ul>
<p><strong>How does this minimax estimator compare to the MLE?</strong></p>
<p>By maximum risk (worst-case criterion):</p>
<ul>
<li>MLE: Maximum risk is <span class="math inline">\frac{1}{4n}</span> (at <span class="math inline">p = 1/2</span>)</li>
<li>Minimax: Constant risk <span class="math inline">\frac{n}{4(n+\sqrt{n})^2} &lt; \frac{1}{4n}</span></li>
</ul>
<p>The minimax estimator wins on worst-case performance - that’s what it was designed for!</p>
<p>But here’s the interesting part: even though the minimax estimator was derived from a Beta(<span class="math inline">\sqrt{n/4}, \sqrt{n/4}</span>) prior, we can ask how it performs on average under <em>any</em> prior. For instance, under a uniform prior:</p>
<ul>
<li>MLE: Bayes risk = <span class="math inline">\frac{1}{6n}</span></li>
<li>Minimax: Bayes risk = <span class="math inline">\frac{n}{4(n+\sqrt{n})^2}</span></li>
</ul>
<p>For <span class="math inline">n \geq 20</span>, the MLE has lower average risk under the uniform prior. This illustrates a key principle: <strong>the minimax estimator optimizes worst-case performance, but may sacrifice average-case performance to achieve this robustness</strong>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Minimax Estimator for Normal Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>For <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\theta, 1)</span>, the sample mean <span class="math inline">\bar{X}</span> has risk: <span class="math display">R(\theta, \bar{X}) = \mathbb{E}[(\bar{X} - \theta)^2] = \text{Var}(\bar{X}) = \frac{1}{n}</span></p>
<p>This risk is constant (doesn’t depend on <span class="math inline">\theta</span>). Furthermore, <span class="math inline">\bar{X}</span> can be shown to be admissible (as it is the limit of admissible Bayes estimators for Normal priors). An admissible estimator with constant risk is minimax. Therefore, <span class="math inline">\bar{X}</span> is minimax.</p>
<p>This result holds for any “well-behaved” loss function (convex and symmetric about the origin).</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Large Sample MLE
</div>
</div>
<div class="callout-body-container callout-body">
<p>In most parametric models with large <span class="math inline">n</span>, the MLE is approximately minimax. The intuition: as <span class="math inline">n \rightarrow \infty</span>, the MLE becomes approximately Normal with variance <span class="math inline">1/(nI(\theta))</span> where <span class="math inline">I(\theta)</span> is the Fisher information. In many regular models, this leads to approximately constant risk.</p>
<p><strong>Important caveat</strong>: This breaks down when the number of parameters grows with <span class="math inline">n</span>. For example, in the “many Normal means” problem where we estimate <span class="math inline">n</span> means from <span class="math inline">n</span> observations, the MLE is far from minimax.</p>
</div>
</div>
</section>
</section>
<section id="admissibility-ruling-out-bad-estimators" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="admissibility-ruling-out-bad-estimators"><span class="header-section-number">8.7</span> Admissibility: Ruling Out Bad Estimators</h2>
<p>Minimax and Bayes estimators tell us about optimality according to specific criteria. But there’s a more basic requirement: an estimator shouldn’t be uniformly worse than another.</p>
<section id="defining-admissibility" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1" class="anchored" data-anchor-id="defining-admissibility"><span class="header-section-number">8.7.1</span> Defining Admissibility</h3>
<div class="definition">
<p>An estimator <span class="math inline">\hat{\theta}</span> is <strong>inadmissible</strong> if there exists another estimator <span class="math inline">\hat{\theta}'</span> such that:</p>
<ol type="1">
<li><span class="math inline">R(\theta, \hat{\theta}') \le R(\theta, \hat{\theta})</span> for all <span class="math inline">\theta</span></li>
<li><span class="math inline">R(\theta, \hat{\theta}') &lt; R(\theta, \hat{\theta})</span> for at least one <span class="math inline">\theta</span></li>
</ol>
<p>Otherwise, <span class="math inline">\hat{\theta}</span> is <strong>admissible</strong>.</p>
</div>
<p>An inadmissible estimator is dominated – there’s another estimator that’s never worse and sometimes better. Using an inadmissible estimator is irrational.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Admissibility ≠ Good
</div>
</div>
<div class="callout-body-container callout-body">
<p>The constant estimator <span class="math inline">\hat{\theta} = 3</span> for <span class="math inline">X \sim \mathcal{N}(\theta, 1)</span> is admissible! Why? Any estimator that beats it at <span class="math inline">\theta = 3</span> must be worse elsewhere. But it’s still a terrible estimator for most purposes.</p>
<p>Admissibility is a necessary but not sufficient condition for a good estimator.</p>
</div>
</div>
</section>
<section id="key-properties-and-connections" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2" class="anchored" data-anchor-id="key-properties-and-connections"><span class="header-section-number">8.7.2</span> Key Properties and Connections</h3>
<div class="theorem" name="Bayes Rules are Admissible">
<p>A Bayes estimator for a prior with full support (positive density everywhere) is always admissible.</p>
</div>
<p>This is powerful: Bayesian methods automatically avoid inadmissible estimators.</p>
<p><strong>Other connections</strong>:</p>
<ul>
<li><strong>Constant Risk and Admissibility</strong>: An admissible estimator with constant risk is minimax</li>
<li><strong>Minimax and Admissibility</strong>: Minimax estimators are usually admissible or “nearly” admissible</li>
<li><strong>MLE and Admissibility</strong>: The MLE is not always admissible, especially in high dimensions</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Stein’s Paradox and Shrinkage
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider estimating <span class="math inline">k \geq 3</span> Normal means simultaneously. Let <span class="math inline">Y_i \sim \mathcal{N}(\theta_i, 1)</span> for <span class="math inline">i = 1, ..., k</span>.</p>
<p><strong>The Setup</strong>: We want to estimate <span class="math inline">\theta = (\theta_1, ..., \theta_k)</span> with total squared error loss: <span class="math display">L(\theta, \hat{\theta}) = \sum_{i=1}^k (\theta_i - \hat{\theta}_i)^2</span></p>
<p><strong>The Paradox</strong>: The “obvious” estimator <span class="math inline">\hat{\theta}_i = Y_i</span> (using each observation to estimate its own mean) is inadmissible when <span class="math inline">k \geq 3</span>!</p>
<p><strong>The Solution</strong>: The James-Stein estimator <span class="math display">\hat{\theta}_i^{JS} = \left(1 - \frac{k-2}{\sum_j Y_j^2}\right)^+ Y_i</span> “shrinks” estimates toward zero and has uniformly lower risk than the MLE.</p>
<p><strong>The Importance</strong>: This counterintuitive result revolutionized high-dimensional statistics. It shows that when estimating many parameters simultaneously, we can improve by “borrowing strength” across parameters. This is the foundation of modern regularization methods in machine learning.</p>
<p>The key insight: in high dimensions, the MLE can be improved by shrinkage toward a common value.</p>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">8.8</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="8.8.1">
<h3 data-number="8.8.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">8.8.1</span> Key Concepts Review</h3>
<p><strong>Bayesian Inference</strong>:</p>
<ul>
<li><strong>Posterior ∝ Likelihood × Prior</strong>: Bayes’ theorem provides the recipe for updating beliefs</li>
<li><strong>Conjugate models</strong>: Beta-Bernoulli and Normal-Normal give closed-form posteriors</li>
<li><strong>Prior choice matters</strong>: Conjugate (convenient), non-informative (problematic), weakly informative (recommended)</li>
<li><strong>Credible intervals</strong>: Direct probability statements about parameters</li>
</ul>
<p><strong>Statistical Decision Theory</strong>:</p>
<ul>
<li><strong>Loss functions</strong>: Quantify the cost of estimation errors</li>
<li><strong>Risk functions</strong>: Expected loss – curves that are hard to compare</li>
<li><strong>Bayes estimators</strong>: Minimize average risk over a prior</li>
<li><strong>Minimax estimators</strong>: Minimize worst-case risk</li>
</ul>
<p><strong>Key Connections</strong>:</p>
<ul>
<li>Posterior mean = Bayes estimator for squared error loss</li>
<li>Constant risk Bayes rules are minimax</li>
<li>Bayes rules are admissible</li>
<li>In large samples, Bayesian and frequentist methods converge</li>
</ul>
</section>
<section id="the-big-picture" class="level3" data-number="8.8.2">
<h3 data-number="8.8.2" class="anchored" data-anchor-id="the-big-picture"><span class="header-section-number">8.8.2</span> The Big Picture</h3>
<p>This chapter revealed two fundamental insights:</p>
<ol type="1">
<li><p><strong>Bayesian inference provides a unified, probabilistic framework</strong> for learning from data. By treating parameters as random variables, we can make direct probability statements and naturally incorporate prior knowledge.</p></li>
<li><p><strong>Decision theory provides a formal language</strong> for evaluating and comparing any statistical procedure. The posterior mean is not just an arbitrary summary – it’s the optimal estimator under squared error loss.</p></li>
</ol>
<p>The connection runs deeper: Bayesian methods naturally produce optimal estimators, while decision theory helps us understand when and why different approaches work well. Even frequentist stalwarts use decision theory, and the best frequentist estimators often have Bayesian interpretations.</p>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="8.8.3">
<h3 data-number="8.8.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">8.8.3</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><strong>Confusing credible and confidence intervals</strong>: A 95% credible interval contains <span class="math inline">\theta</span> with probability 0.95 given the data.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> A 95% confidence interval is produced by a procedure that traps <span class="math inline">\theta</span> in 95% of repeated experiments.</li>
<li><strong>Thinking uniform priors are “uninformative”</strong>: They encode specific beliefs and aren’t transformation invariant.</li>
<li><strong>Using conjugate priors blindly</strong>: Convenience shouldn’t override reasonable prior beliefs.</li>
<li><strong>Forgetting the prior’s influence diminishes</strong>: With enough data, different reasonable priors lead to similar posteriors.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></li>
<li><strong>Assuming admissible = good</strong>: The constant estimator <span class="math inline">\hat{\theta} = 3</span> is admissible but useless.</li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="8.8.4">
<h3 data-number="8.8.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">8.8.4</span> Chapter Connections</h3>
<ul>
<li><p><strong>Previous (Ch. 5-7)</strong>: We learned frequentist methods for finding and evaluating estimators. Now we have a completely different paradigm (Bayesian) and a unified theory (decision theory) for comparing estimators from any paradigm.</p></li>
<li><p><strong>This Chapter</strong>: Introduced Bayesian thinking and formal decision theory. These provide alternative and complementary approaches to the frequentist methods we’ve studied.</p></li>
<li><p><strong>Next (Ch. 10)</strong>: We’ll see how modern computational methods (MCMC, Stan) make Bayesian inference practical for complex models where conjugacy doesn’t help.</p></li>
<li><p><strong>Applications</strong>: Bayesian methods shine in hierarchical models, missing data problems, and anywhere prior information is valuable.</p></li>
</ul>
</section>
<section id="self-test-problems" class="level3" data-number="8.8.5">
<h3 data-number="8.8.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">8.8.5</span> Self-Test Problems</h3>
<ol type="1">
<li><p><strong>Bayesian Calculation</strong>: Given <span class="math inline">n=10</span> observations from a <span class="math inline">\text{Poisson}(\lambda)</span> distribution with <span class="math inline">\sum x_i = 30</span>, and a prior <span class="math inline">\lambda \sim \text{Gamma}(2, 1)</span>, find the posterior distribution for <span class="math inline">\lambda</span>. What is the Bayes estimator under squared error loss?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The Gamma distribution is conjugate to the Poisson. Using the shape-rate parameterization (where <span class="math inline">\beta</span> is the rate parameter), if <span class="math inline">\lambda \sim \text{Gamma}(\alpha, \beta)</span> and we observe data with sum <span class="math inline">S</span>, then <span class="math inline">\lambda | \text{data} \sim \text{Gamma}(\alpha + S, \beta + n)</span>. The posterior mean (Bayes estimator) is <span class="math inline">(\alpha + S)/(\beta + n)</span>.</p>
</div>
</div>
</div></li>
<li><p><strong>Decision Theory Concepts</strong>: Let <span class="math inline">X_1, \ldots, X_n \sim \mathcal{N}(\mu, 1)</span>. The MLE <span class="math inline">\hat{\mu} = \bar{X}</span> has risk <span class="math inline">1/n</span> under squared error loss.</p>
<ul>
<li><ol type="a">
<li>Is this risk constant?</li>
</ol></li>
<li><ol start="2" type="a">
<li>How does <span class="math inline">\hat{\mu}</span> relate to the Bayes estimator under a Normal prior? (One sentence.)</li>
</ol></li>
<li><ol start="3" type="a">
<li>Is <span class="math inline">\hat{\mu}</span> minimax? Give a one-line justification.</li>
</ol></li>
<li><ol start="4" type="a">
<li>Is <span class="math inline">\hat{\mu}</span> admissible? Give a one-line justification.</li>
</ol></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li>Yes, <span class="math inline">R(\mu, \bar{X}) = \text{Var}(\bar{X}) = 1/n</span> is constant.</li>
<li>With prior <span class="math inline">\mu \sim \mathcal{N}(a, b^2)</span>, the Bayes estimator is the posterior mean <span class="math inline">w\bar{X} + (1-w)a</span>, where <span class="math inline">w = \frac{b^2}{b^2 + 1/n}</span>; as <span class="math inline">b^2 \to \infty</span> (very diffuse prior), this approaches <span class="math inline">\bar{X}</span>.</li>
<li>Constant risk + admissibility ⇒ minimax (by results in the notes).</li>
<li>Yes, in the 1D Normal-mean problem with squared error, <span class="math inline">\bar{X}</span> is admissible (classical result), even though it’s a limit of Bayes rules.</li>
</ol>
</div>
</div>
</div></li>
<li><p><strong>Prior Choice</strong>: You’re estimating the probability <span class="math inline">p</span> that a new medical treatment works. You’re skeptical because most new treatments fail. What would be:</p>
<ul>
<li>A weakly informative prior for <span class="math inline">p</span>?</li>
<li>A strong prior reflecting your skepticism?</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Weakly informative: Beta(1, 3) or Beta(1, 5) - allows all values but slightly favors lower success rates. Strong skeptical prior: Beta(1, 10) or Beta(1, 20) - strongly concentrates mass near 0, reflecting belief that the treatment likely doesn’t work. Remember: Beta parameters can be interpreted as pseudo-counts of successes and failures.</p>
</div>
</div>
</div></li>
<li><p><strong>Conceptual Understanding</strong>: Why is the James-Stein estimator’s improvement over the MLE considered paradoxical? What does it tell us about estimating multiple parameters simultaneously?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The paradox: When estimating three or more unrelated means (e.g., baseball batting average, physics constants, and rainfall), using information from all of them together (via shrinkage) gives better estimates than treating them separately. This violates our intuition that unrelated problems should be solved independently. The lesson: In high dimensions, “borrowing strength” across parameters through shrinkage reduces overall risk, even for unrelated parameters.</p>
</div>
</div>
</div></li>
</ol>
</section>
<section id="python-and-r-reference" class="level3" data-number="8.8.6">
<h3 data-number="8.8.6" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">8.8.6</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255605-990-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-990-1" role="tab" aria-controls="tabset-1757255605-990-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255605-990-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255605-990-2" role="tab" aria-controls="tabset-1757255605-990-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255605-990-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255605-990-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Conjugate Bayesian Inference</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">## Beta-Bernoulli Model</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beta_bernoulli_posterior(n_successes, n_trials, alpha_prior<span class="op">=</span><span class="dv">1</span>, beta_prior<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute posterior parameters for Beta-Bernoulli model.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Prior: Beta(alpha_prior, beta_prior)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Data: n_successes in n_trials</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Posterior: Beta(alpha_post, beta_post)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    alpha_post <span class="op">=</span> alpha_prior <span class="op">+</span> n_successes</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    beta_post <span class="op">=</span> beta_prior <span class="op">+</span> (n_trials <span class="op">-</span> n_successes)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha_post, beta_post</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>n, s <span class="op">=</span> <span class="dv">20</span>, <span class="dv">12</span>  <span class="co"># 12 successes in 20 trials</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>alpha_post, beta_post <span class="op">=</span> beta_bernoulli_posterior(s, n)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior mean (Bayes estimator for squared error loss)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>posterior_mean <span class="op">=</span> alpha_post <span class="op">/</span> (alpha_post <span class="op">+</span> beta_post)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% credible interval</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ci_lower, ci_upper <span class="op">=</span> stats.beta.ppf([<span class="fl">0.025</span>, <span class="fl">0.975</span>], alpha_post, beta_post)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize posterior</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>p_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">200</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> stats.beta.pdf(p_range, alpha_post, beta_post)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.plot(p_range, posterior)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.fill_between(p_range, posterior, </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>                 where<span class="op">=</span>(p_range <span class="op">&gt;=</span> ci_lower) <span class="op">&amp;</span> (p_range <span class="op">&lt;=</span> ci_upper),</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'95% Credible Interval'</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Posterior density'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Beta(</span><span class="sc">{</span>alpha_post<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>beta_post<span class="sc">}</span><span class="ss">) Posterior'</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co">## Normal-Normal Model  </span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normal_normal_posterior(data, prior_mean<span class="op">=</span><span class="dv">0</span>, prior_var<span class="op">=</span><span class="dv">1</span>, data_var<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute posterior for Normal-Normal conjugate model.</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Prior: N(prior_mean, prior_var)</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co">    Likelihood: N(theta, data_var) for each observation</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    data_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Precision (1/variance) is additive</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    prior_precision <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>prior_var</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    data_precision <span class="op">=</span> n<span class="op">/</span>data_var</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    post_precision <span class="op">=</span> prior_precision <span class="op">+</span> data_precision</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior parameters</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    post_var <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>post_precision</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    post_mean <span class="op">=</span> post_var <span class="op">*</span> (prior_precision <span class="op">*</span> prior_mean <span class="op">+</span> </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>                            data_precision <span class="op">*</span> data_mean)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> post_mean, post_var</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Theory</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_risk(estimator_func, true_theta, n_simulations<span class="op">=</span><span class="dv">10000</span>, n_samples<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co">    Estimate risk via simulation for squared error loss.</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate data</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.normal(true_theta, <span class="dv">1</span>, n_samples)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute estimate</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>        estimate <span class="op">=</span> estimator_func(data)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (estimate <span class="op">-</span> true_theta)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(losses)  <span class="co"># Risk = expected loss</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Compare MLE and a shrinkage estimator</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mle_estimator(data):</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(data)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shrinkage_estimator(data, shrink_target<span class="op">=</span><span class="dv">0</span>, shrink_factor<span class="op">=</span><span class="fl">0.8</span>):</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    mle <span class="op">=</span> np.mean(data)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> shrink_factor <span class="op">*</span> mle <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> shrink_factor) <span class="op">*</span> shrink_target</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare risks</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>theta_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">50</span>)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>risk_mle <span class="op">=</span> []</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>risk_shrink <span class="op">=</span> []</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> theta <span class="kw">in</span> theta_values:</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    risk_mle.append(compute_risk(mle_estimator, theta))</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    risk_shrink.append(compute_risk(shrinkage_estimator, theta))</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, risk_mle, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, risk_shrink, label<span class="op">=</span><span class="st">'Shrinkage'</span>)</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True θ'</span>)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Risk'</span>)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Risk Functions Comparison'</span>)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255605-990-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255605-990-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conjugate Bayesian Inference</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Beta-Bernoulli Model</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>beta_bernoulli_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(n_successes, n_trials, </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">alpha_prior =</span> <span class="dv">1</span>, <span class="at">beta_prior =</span> <span class="dv">1</span>) {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute posterior parameters</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  alpha_post <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> n_successes</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  beta_post <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> (n_trials <span class="sc">-</span> n_successes)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">alpha =</span> alpha_post, <span class="at">beta =</span> beta_post,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">mean =</span> alpha_post <span class="sc">/</span> (alpha_post <span class="sc">+</span> beta_post))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">beta_bernoulli_posterior</span>(s, n)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% credible interval</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>ci <span class="ot">&lt;-</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), posterior<span class="sc">$</span>alpha, posterior<span class="sc">$</span>beta)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize posterior</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>p_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>posterior_density <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(p_seq, posterior<span class="sc">$</span>alpha, posterior<span class="sc">$</span>beta)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">p =</span> p_seq, <span class="at">density =</span> posterior_density)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> density)) <span class="sc">+</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, p <span class="sc">&gt;=</span> ci[<span class="dv">1</span>] <span class="sc">&amp;</span> p <span class="sc">&lt;=</span> ci[<span class="dv">2</span>]),</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            <span class="at">alpha =</span> <span class="fl">0.3</span>, <span class="at">fill =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"p"</span>, <span class="at">y =</span> <span class="st">"Posterior density"</span>,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="fu">sprintf</span>(<span class="st">"Beta(%g, %g) Posterior"</span>, </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                      posterior<span class="sc">$</span>alpha, posterior<span class="sc">$</span>beta)) <span class="sc">+</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="do">## Normal-Normal Model</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>normal_normal_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">prior_mean =</span> <span class="dv">0</span>, <span class="at">prior_var =</span> <span class="dv">1</span>, </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">data_var =</span> <span class="dv">1</span>) {</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(data)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>  data_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(data)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Precision is additive</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>  prior_precision <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>prior_var</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>  data_precision <span class="ot">&lt;-</span> n<span class="sc">/</span>data_var</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>  post_precision <span class="ot">&lt;-</span> prior_precision <span class="sc">+</span> data_precision</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Posterior parameters</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  post_var <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>post_precision</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>  post_mean <span class="ot">&lt;-</span> post_var <span class="sc">*</span> (prior_precision <span class="sc">*</span> prior_mean <span class="sc">+</span> </span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>                           data_precision <span class="sc">*</span> data_mean)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">mean =</span> post_mean, <span class="at">var =</span> post_var, <span class="at">sd =</span> <span class="fu">sqrt</span>(post_var))</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Theory</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>compute_risk <span class="ot">&lt;-</span> <span class="cf">function</span>(estimator_func, true_theta, </span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>                         <span class="at">n_simulations =</span> <span class="dv">10000</span>, <span class="at">n_samples =</span> <span class="dv">20</span>) {</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Estimate risk via simulation</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_simulations, {</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_samples, true_theta, <span class="dv">1</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    estimate <span class="ot">&lt;-</span> <span class="fu">estimator_func</span>(data)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    (estimate <span class="sc">-</span> true_theta)<span class="sc">^</span><span class="dv">2</span>  <span class="co"># Squared error loss</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(losses)  <span class="co"># Risk = expected loss</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Example estimators</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>mle_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(data) <span class="fu">mean</span>(data)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>shrinkage_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">target =</span> <span class="dv">0</span>, <span class="at">factor =</span> <span class="fl">0.8</span>) {</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>  factor <span class="sc">*</span> <span class="fu">mean</span>(data) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> factor) <span class="sc">*</span> target</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare risk functions</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>theta_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">50</span>)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>risk_mle <span class="ot">&lt;-</span> <span class="fu">sapply</span>(theta_seq, <span class="cf">function</span>(theta) </span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_risk</span>(mle_estimator, theta))</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>risk_shrink <span class="ot">&lt;-</span> <span class="fu">sapply</span>(theta_seq, <span class="cf">function</span>(theta) </span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_risk</span>(shrinkage_estimator, theta))</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>df_risk <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">rep</span>(theta_seq, <span class="dv">2</span>),</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>  <span class="at">risk =</span> <span class="fu">c</span>(risk_mle, risk_shrink),</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimator =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"MLE"</span>, <span class="st">"Shrinkage"</span>), <span class="at">each =</span> <span class="fu">length</span>(theta_seq))</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_risk, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> risk, <span class="at">color =</span> estimator)) <span class="sc">+</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"True θ"</span>, <span class="at">y =</span> <span class="st">"Risk"</span>, </span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"Risk Functions Comparison"</span>) <span class="sc">+</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="8.8.7">
<h3 data-number="8.8.7" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">8.8.7</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<colgroup>
<col style="width: 47%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding Source(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction: A Different Way of Thinking</strong></td>
<td style="text-align: left;">From slides and AF447 case study from <span class="citation" data-cites="stone2014search">Stone et al. (<a href="../references.html#ref-stone2014search" role="doc-biblioref">2014</a>)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Two Philosophies of Statistics</td>
<td style="text-align: left;">AoS §11.1 and slides</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>The Bayesian Method: Updating Beliefs with Data</strong></td>
<td style="text-align: left;">AoS §11.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Engine: Bayes’ Theorem for Inference</td>
<td style="text-align: left;">AoS §11.2 plus slides</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Summarizing the Posterior</td>
<td style="text-align: left;">AoS §11.2; AoS §12.3 (for median/mode)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Credible vs.&nbsp;Confidence Intervals</td>
<td style="text-align: left;">AoS §11.9 and expanded from lecture material</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Bayesian Inference in Action</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Conjugate Models and Conjugate Priors</td>
<td style="text-align: left;">AoS §11.2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Example: The Bernoulli-Beta Model</td>
<td style="text-align: left;">AoS Example 11.1 and slides</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Example: The Normal-Normal Model</td>
<td style="text-align: left;">AoS Example 11.2 and slides</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Art and Science of Choosing Priors</td>
<td style="text-align: left;">AoS §11.6 expanded with modern views</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Implementing Bayesian Inference</td>
<td style="text-align: left;">AoS §11.4 expanded with modern tools</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Statistical Decision Theory</strong></td>
<td style="text-align: left;">AoS Ch 12</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Ingredients: Loss and Risk</td>
<td style="text-align: left;">AoS §12.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Challenge of Comparing Risk Functions</td>
<td style="text-align: left;">AoS §12.2 (Examples 12.2, 12.3)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Optimal Estimators: Bayes and Minimax Rules</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Bayesian Approach: Minimizing Average Risk</td>
<td style="text-align: left;">AoS §12.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Frequentist Approach: Minimizing Worst-Case Risk</td>
<td style="text-align: left;">AoS §12.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Example: Minimax Estimator for Bernoulli</td>
<td style="text-align: left;">AoS Example 12.12</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Example: Minimax Estimator for Normal Mean</td>
<td style="text-align: left;">AoS Theorem 12.14, Theorem 12.22</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Example: Large Sample MLE</td>
<td style="text-align: left;">AoS §12.5</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Admissibility: Ruling Out Bad Estimators</strong></td>
<td style="text-align: left;">AoS §12.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Defining Admissibility</td>
<td style="text-align: left;">AoS Definition 12.17</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Key Properties and Connections</td>
<td style="text-align: left;">AoS Theorem 12.19, Theorem 12.21</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Advanced: Stein’s Paradox and Shrinkage</td>
<td style="text-align: left;">AoS §12.7</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Self-Test Problems</strong></td>
<td style="text-align: left;">Based on AoS Ch 11/12 exercises and concepts</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-materials" class="level3" data-number="8.8.8">
<h3 data-number="8.8.8" class="anchored" data-anchor-id="further-materials"><span class="header-section-number">8.8.8</span> Further Materials</h3>
<ul>
<li><strong>Foundational Text</strong>: Gelman et al., “Bayesian Data Analysis” (3rd ed.)</li>
<li><strong>The Air France Search Case Study</strong>: <span class="citation" data-cites="stone2014search">Stone et al. (<a href="../references.html#ref-stone2014search" role="doc-biblioref">2014</a>)</span>.</li>
<li><strong>Prior Choice</strong>: See the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan wiki</a>.</li>
</ul>
<hr>
<p><em>Remember: Bayesian inference is about updating beliefs with data. Decision theory is about choosing the best estimator. Together, they provide a complete framework for statistical inference that complements and enriches the frequentist approach. Master both paradigms – they each have their place in the modern statistician’s toolkit!</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-stone2014search" class="csl-entry" role="listitem">
Stone, Lawrence D, Colleen M Keller, Thomas M Kratzke, and Johan P Strumpfer. 2014. <span>“Search for the Wreckage of <span>Air France</span> Flight <span>AF</span> 447.”</span> <em>Statistical Science</em> 29 (1): 69–80. <a href="https://doi.org/10.1214/13-STS420">https://doi.org/10.1214/13-STS420</a>.
</div>
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The marginal likelihood is not <em>just</em> a constant. Since it encodes the probability of the data under a specific statistical model, it can be used as a metric for comparing <em>different models</em>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The term “a posteriori” is Latin meaning “from what comes after” or “from the latter,” referring to knowledge that comes <em>after</em> observing evidence. This contrasts with “a priori” meaning “from what comes before” – knowledge <em>before</em> seeing data.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Also called a <strong>posterior interval</strong> in some texts, particularly older or more theoretical works. Both terms are correct and refer to the same concept.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The posterior mean is the Bayes estimator under squared error loss, as we will see in the following section.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For a given model and prior.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In regular, identifiable parametric models; this can fail in high dimensions or weakly identified settings.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/07-hypothesis-testing.html" class="pagination-link" aria-label="Hypothesis Testing and p-values">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/09-linear-logistic-regression.html" class="pagination-link" aria-label="Linear and Logistic Regression">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Bayesian Inference and Statistical Decision Theory</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply Bayes' theorem to compute posterior distributions** from prior and likelihood, and interpret credible intervals vs. confidence intervals.</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Work with conjugate models** (Beta-Bernoulli, Normal-Normal) to derive posteriors and understand how data and prior beliefs combine.</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Choose appropriate priors** and explain their impact on inference, particularly as sample size increases.</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Use decision theory to compare estimators** via loss functions and risk, understanding why we need scalar summaries (Bayes risk, maximum risk).</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Identify optimal estimators** by connecting posterior summaries to Bayes estimators, finding minimax estimators via constant risk, and determining admissibility.</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>This chapter introduces two deeply connected topics: Bayesian inference, which provides a principled way to update beliefs with data, and statistical decision theory, which gives us a formal framework for comparing any statistical procedure. The material is adapted from Chapters 11 and 12 of @wasserman2013all and supplemented with modern perspectives and computational examples.</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction: A Different Way of Thinking</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Search for Air France Flight 447</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>On June 1, 2009, Air France Flight 447 vanished over the Atlantic Ocean. The Airbus A330, carrying 228 people, disappeared from radar while flying from Rio de Janeiro to Paris, leaving behind only automated messages indicating system failures. What followed was one of the most challenging search operations in aviation history -- and ultimately, a powerful demonstration of how Bayesian inference succeeds by integrating multiple sources of uncertain information. This remarkable story is documented in detail in @stone2014search, from which the figures and search details in this section are taken.</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>::: {.column width="55%"}</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>Modern airliners transmit their position every 10 minutes via satellite. When AF447's transmissions stopped at 2:14 AM, it created a circular search area with a 40 nautical mile (74 km) radius -- still covering over 5,000 square nautical miles of ocean.</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>The depth in this region reaches 14,000 feet, with underwater mountains and valleys making detection extremely challenging. The flight data and cockpit voice recorders, crucial for understanding what happened, emit acoustic beacons that function for about 40 days and have a detection range of about 2,000 meters.</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>This wasn't just a search problem -- it was a problem of combining uncertain, conflicting information from multiple sources.</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>::: {.column width="45%"}</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="al">![The intended flight path of AF447 and the 40 NM radius circle centered on the last known position (LKP). The circle represents the maximum distance the aircraft could have traveled after its last transmission. Figure from @stone2014search.](../images/Stone2014_fig1.png)</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Initial Search Efforts </span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>Throughout 2009 and 2010, search teams employed sophisticated statistical models including oceanographic drift analysis and confidence regions. However, each search operation focused on a single line of evidence rather than integrating all available information. These efforts, while extensive and expensive, all failed to locate the wreckage:</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="fu">## Surface Search (June 2009)</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>The first phase focused on finding floating debris. After six days, search aircraft spotted debris and bodies approximately 38 NM north of the last known position. Scientists then used reverse drift modeling (working backwards from where debris was found, using ocean current data to estimate where it originated) to predict where the wreckage might be.</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>**Result**: No wreckage found in the predicted areas.</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Acoustic Search (June 2009)</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>Teams deployed sensitive hydrophones to listen for the flight recorders' acoustic beacons. They concentrated along the intended flight path, reasoning the aircraft was likely on course when it crashed.</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="al">![The vertical and horizontal search lines showing the passive acoustic search paths for the flight recorder beacons. The circles show the 20 and 40 NM radius from the last known position. Figure from Stone et al. (2014).](../images/Stone2014_fig5.png)</span>{width=80%}</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>**Result**: No signals detected. The search assumed the beacons were functioning -- a reasonable but ultimately incorrect assumption.</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="fu">## Active Sonar (August 2009)</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>A limited side-scan sonar search was conducted south of the last known position in areas not covered in June.</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="al">![Regions searched by active side-looking sonar. The small rectangle shows the limited August 2009 coverage, while the larger areas show April-May 2010 coverage. Figure from Stone et al. (2014).](../images/Stone2014_fig6.png)</span>{width=80%}</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>**Result**: No wreckage found.</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## Confidence Region (April-May 2010)</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>Scientists computed a 95% confidence region by reverse drift modeling from where bodies and debris were recovered. By simulating ocean currents backwards in time, they estimated where the crash most likely occurred, producing a search zone north and west of the last known position.</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="al">![The 95% confidence zone recommended for the 2010 search, located north and west of the LKP, based on reverse drift modeling. Figure from Stone et al. (2014).](../images/Stone2014_fig2.png)</span>{width=80%}</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>**Result**: No wreckage found. The confidence region, while statistically valid, relied heavily on ocean current models and didn't integrate other sources of evidence like historical crash locations or search effectiveness.</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why the Initial Approaches Failed</span></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>Each search used valid and sophisticated statistical reasoning but treated evidence in isolation:</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Drift models didn't account for prior crash locations</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Passive acoustic searches couldn't distinguish between beacon failure and absence of wreckage</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Search patterns didn't incorporate the probability of missing the wreckage</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No unified framework was used to combine these different sources of uncertainty</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Bayesian Strategy</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>In July 2010, after four unsuccessful search operations, the French aviation authority (BEA) assembled a new team of statisticians to design a search strategy for 2011. This team took a fundamentally different approach: instead of treating each piece of evidence separately, they used Bayesian inference to combine *all* sources of information into a single probability distribution.</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>**The Bayesian Framework**</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>The team constructed a posterior probability distribution for the wreckage location by combining:</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Prior Distribution**: Historical data showed that aircraft are usually found close to their last known position. This gave higher prior probability to areas near the center of the circle.</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Drift Model Likelihood**: Bodies found north of the LKP implied certain starting positions were more likely than others -- but with significant uncertainty.</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Search Effectiveness**: Previous searches weren't perfect. The team modeled the probability of missing the wreckage in searched areas, particularly accounting for terrain difficulty.</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Beacon Failure Possibility**: The lack of acoustic signals could mean either the wreckage wasn't in searched areas OR the beacons had failed. Bayesian analysis could incorporate both possibilities.</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a><span class="al">![Reverse drift distribution showing the probability density of potential crash locations based on where bodies and debris were found. This was one key input to the Bayesian analysis. Figure from @stone2014search.](../images/Stone2014_fig3.png)</span></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a><span class="fu">## Technical Detail: Computing the Posterior</span></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>The posterior distribution was computed using:</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>$$P(\text{location} | \text{all evidence}) \propto P(\text{all evidence} | \text{location}) \times P(\text{location})$$</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>Where the evidence included:</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Negative search results (no detection in searched areas)</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Positive drift data (bodies found at specific locations)</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Timing constraints (time between crash and debris discovery)</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>The likelihood $P(\text{all evidence} | \text{location})$ was itself a product of multiple conditional probabilities, each capturing different aspects of the search problem. Monte Carlo methods were used to integrate over unknown parameters like ocean current variations and detection probabilities.</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Breakthrough</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>The Bayesian analysis produced a surprising result: the highest probability areas were very close to the last known position. Although these areas had been covered by passive acoustic searches in 2009, the active sonar efforts in 2009-2010 had focused elsewhere based on drift models.</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution}</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Key Insight</span></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>The Bayesian approach revealed that multiple pieces of weak evidence all pointed to the same conclusion:</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Historical data suggested searching near the LKP</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Debris drift models had high uncertainty and conflicting predictions</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The failure to find wreckage in extensively searched areas increased relative probability elsewhere</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Beacon failure was historically more likely than initially assumed</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>No single piece of evidence was conclusive, but together they pointed strongly to areas near the last known position.</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Discovery and Vindication</span></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>The new search began in 2011, focusing on the high-probability areas identified by the Bayesian analysis. After just one week of searching, on April 3, 2011, the wreckage was found at a depth of approximately 14,000 feet, very close to the last known position.</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a><span class="al">![Posterior distribution from the Bayesian analysis, showing the actual wreck location marked. The dark area near the center shows the highest probability zone, which correctly identified the area where the wreckage was ultimately found. Figure from @stone2014search.](../images/Stone2014_fig8.png)</span>{width=80%}</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Bayesian Methods Succeeded</span></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>The Bayesian approach succeeded where the initial methods failed for three fundamental reasons:</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Coherent Information Integration**: While the initial searches treated each piece of evidence separately, Bayesian inference combined them into a single, coherent picture.</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Uncertainty Quantification**: The approach explicitly modeled multiple sources of uncertainty -- from ocean currents to sensor reliability -- rather than assuming point estimates were correct.</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Prior Knowledge Utilization**: Historical data about crash locations provided valuable information that pure data-driven approaches ignored.</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>This case demonstrates the power of Bayesian thinking: when faced with multiple sources of imperfect information, Bayesian methods provide the mathematical framework to combine them optimally.</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Two Philosophies of Statistics</span></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>In the world of statistical inference, there are two major philosophical schools of thought about probability, parameters, and how we should make inferences from data. These aren't just abstract philosophical debates -- they lead to fundamentally different methods, interpretations, and answers. Understanding both perspectives is crucial for modern data scientists.</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a><span class="fu">## Frequentist View</span></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>We've been working primarily within the frequentist framework throughout this course. Let's formalize its key principles:</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>**F1. Probability as Frequency**: Probability refers to limiting relative frequencies in repeated experiments. Probabilities are objective properties of the real world. When we say a coin has probability 0.5 of landing heads, we mean that in an infinite sequence of flips, exactly half would be heads.</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>**F2. Fixed Parameters**: Parameters are fixed, unknown constants. They are not random variables. Because they don't vary, we cannot make probability statements about them. We can't say "there's a 95% probability that $\mu$ is between 2 and 4" -- either it is or it isn't.</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>**F3. Long-Run Performance**: Statistical methods should have well-defined long-run frequency properties. A 95% confidence interval should trap the true parameter in 95% of repeated experiments. This is a statement about the procedure, not about any particular interval.</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>**F4. Point-Conditioned Prediction**: Predictions are typically conditioned on a single parameter value, often an estimate like the MLE. We predict future data assuming our estimate is correct.</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>**F5. Separate Theories**: There's no single, overarching theory unifying all aspects of frequentist inference. Estimation theory, hypothesis testing, and prediction each have their own frameworks and optimality criteria.</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian View</span></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>The Bayesian approach starts from fundamentally different assumptions:</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>**B1. Probability as Belief**: Probability describes degree of belief or confidence. Probabilities can be subjective and represent our uncertainty about anything -- including fixed events. We can meaningfully say "I'm 70% confident it rained in Paris on January 1, 1850" even though this is a fixed historical fact.</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>**B2. Probabilistic Parameters**: We can make probability statements about parameters, treating our uncertainty about them as something to be described by a probability distribution. Even though $\theta$ is fixed, our knowledge about it is uncertain, and we quantify this uncertainty with probabilities.</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>**B3. Inference as Belief Updating**: The core of inference is updating our beliefs about parameters by producing a posterior probability distribution after observing data. This posterior encapsulates everything we know about the parameter.</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>**B4. Averaged Prediction**: Predictions are made by averaging over all parameter values, weighted by their posterior probability. Instead of picking one "best" parameter value, we consider all plausible values.</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>**B5. Unified Theory**: The framework has a strong, unified theoretical foundation based on the rules of probability. Bayes' theorem provides a single coherent approach to all inference problems.</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a><span class="fu">### This Chapter's Goal</span></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>We will explore two deeply connected topics:</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bayesian Inference**: The machinery for updating our beliefs about parameters using data. We'll see how prior knowledge combines with observed data to produce posterior distributions.</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Statistical Decision Theory**: A formal framework for choosing the "best" estimator under any paradigm. This theory, which applies to both frequentist and Bayesian methods, gives us a rigorous way to compare different statistical procedures.</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>These topics are connected because Bayesian inference naturally leads to optimal estimators under decision theory, while decision theory helps us understand when and why Bayesian methods work well.</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here's a reference table of key terms in this chapter:</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>| Bayesian inference | Bayesiläinen päättely | Main inferential framework |</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>| Prior distribution | Priorijakauma | Beliefs before seeing data |</span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>| Posterior distribution | Posteriorijakauma | Updated beliefs after data |</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>| Likelihood | Uskottavuus | Probability of data given parameters |</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>| Credible interval | Uskottavuusväli | Bayesian confidence interval |</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>| Loss function | Tappiofunktio | Measure of estimation error |</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>| Risk | Riski | Expected loss |</span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>| Bayes estimator | Bayes-estimaattori | Minimizes Bayes risk |</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>| Minimax estimator | Minimax-estimaattori | Minimizes maximum risk |</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>| Admissible | Käypä, kelvollinen | Cannot be uniformly improved |</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Bayesian Method: Updating Beliefs with Data</span></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Engine: Bayes' Theorem for Inference</span></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>The Bayesian method centers on a fundamental question: **how do we make predictions about unknown quantities** when we have uncertain knowledge about the parameters that govern them?</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>Consider predicting some unknown quantity $x^*$ (which could be future data, or properties of the parameter itself) when we have:</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A model with unknown parameter $\theta$</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Observed data $x^n = (x_1, \ldots, x_n)$ that provides information about $\theta$</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>Using the rules of probability, we can write:</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>$$f(x^* | x^n) = \int f(x^* | \theta, x^n) f(\theta | x^n) d\theta$$</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>If $x^*$ depends on the data only through $\theta$ (a common assumption), this simplifies to:</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>$$f(x^* | x^n) = \int f(x^* | \theta) f(\theta | x^n) d\theta$$</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>This equation reveals the key insight: **to make predictions, we need the posterior distribution** $f(\theta | x^n)$. The posterior tells us which parameter values are plausible given the data, and we average our predictions over all these plausible values.</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>**The Components of Bayesian Inference**</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>To compute the posterior distribution $f(\theta | x^n)$, we need:</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Prior Distribution $f(\theta)$**: What we believe about $\theta$ *before* seeing the data. This encodes our initial knowledge or assumptions. We will see later how the prior is chosen.</span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Likelihood $f(x^n | \theta)$ or $\mathcal{L}_n(\theta)$**: The probability of observing our data given different parameter values. This is the same likelihood function used in maximum likelihood estimation.</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior Distribution $f(\theta | x^n)$**: Our updated belief about $\theta$ *after* seeing the data, obtained via Bayes' theorem.</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Bayes' Theorem for Distributions"}</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>The posterior distribution is computed as:</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>$$ f(\theta | x^n) = \frac{f(x^n | \theta) f(\theta)}{\int f(x^n | \theta) f(\theta) d\theta} $$</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>The denominator $\int f(x^n | \theta) f(\theta) d\theta$ is called the **marginal likelihood** or **evidence**. It's a normalizing constant that ensures the posterior integrates to 1.^<span class="co">[</span><span class="ot">The marginal likelihood is not *just* a constant. Since it encodes the probability of the data under a specific statistical model, it can be used as a metric for comparing *different models*.</span><span class="co">]</span></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>We often do not specifically care about the normalizing constant, and write:</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>$$ f(\theta | x^n) \propto f(x^n | \theta) f(\theta) $$</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>denoting that the posterior is **proportional** to Likelihood times Prior.</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>When the observations $X_1, \ldots, X_n$ are IID given $\theta$, the likelihood factorizes:</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>$$ f(\theta | x^n) \propto \mathcal{L}_n(\theta) f(\theta) = \left[\prod_{i=1}^n f(x_i | \theta)\right] f(\theta) $$</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>This product structure is what allows evidence to accumulate across independent observations.</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Do We Care About the Posterior?</span></span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>The posterior distribution serves two distinct purposes:</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>**1. Direct Parameter Inference**: Sometimes the parameters themselves are what we want to know:</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's the true efficacy of a vaccine?</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's the rate of climate change?</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What's a manufacturing process's defect rate?</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>Here, we examine the posterior directly to understand the parameter values.</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>**2. Prediction**: Other times, parameters are just a means to predict future observations:</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimating weather model parameters to forecast tomorrow's conditions</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning user preferences to recommend movies</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimating volatility to predict financial risk</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>For prediction, we integrate over the posterior, incorporating parameter uncertainty into our forecasts rather than conditioning on a single estimate.</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summarizing the Posterior</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>The posterior distribution $f(\theta | x^n)$ contains all our knowledge about $\theta$ after seeing the data. It's the complete Bayesian answer to an inference problem. However, we often need to summarize this distribution with a single point -- a **point estimate** -- for communication or decision-making.</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>**Point Estimates**:</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior Mean**: $\bar{\theta}_n = \mathbb{E}<span class="co">[</span><span class="ot">\theta | x^n</span><span class="co">]</span> = \int \theta f(\theta | x^n) d\theta$</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>  The center of our posterior beliefs, weighting all possible values by their posterior probability.</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior Median**: The value $\theta_m$ such that $\mathbb{P}(\theta \le \theta_m | x^n) = 0.5$</span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>  The value that splits the posterior distribution in half.</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior Mode (MAP)**: $\hat{\theta}_{MAP} = \arg\max_{\theta} f(\theta | x^n)$</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>  The most probable value according to the posterior. MAP stands for "Maximum A Posteriori."^<span class="co">[</span><span class="ot">The term "a posteriori" is Latin meaning "from what comes after" or "from the latter," referring to knowledge that comes *after* observing evidence. This contrasts with "a priori" meaning "from what comes before" – knowledge *before* seeing data.</span><span class="co">]</span></span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>**Interval Estimates**:</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Credible Interval**: A $(1-\alpha)$ credible interval^[Also called a **posterior interval** in some texts, particularly older or more theoretical works. Both terms are correct and refer to the same concept.] is a range $(a, b)$ such that:</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>  $$\mathbb{P}(a &lt; \theta &lt; b | x^n) = 1-\alpha$$</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>  Typically computed as an equal-tailed interval by finding $a$ and $b$ where $\int_{-\infty}^a f(\theta | x^n) d\theta = \int_b^{\infty} f(\theta | x^n) d\theta = \alpha/2$.</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a><span class="fu">## Credible vs. Confidence Intervals</span></span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>A crucial distinction:</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Credible interval** (Bayesian): "Given the data, there's a 95% probability that $\theta$ lies in this interval."</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Confidence interval** (Frequentist): "This procedure produces intervals that trap the true $\theta$ in 95% of repeated experiments."</span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a>The credible interval makes a direct probability statement about the parameter, which is what most people incorrectly think confidence intervals do!</span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a>Imagine you're trying to estimate the average height in a population. You take a sample and compute an interval.</span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a>**Confidence Interval (Frequentist)**: "If I repeated this sampling procedure 100 times, about 95 of those intervals would contain the true average height." It's a statement about the reliability of the *method*, not about any specific interval. Once computed, the true value is either in it or not -- there's no probability involved.</span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a>**Credible Interval (Bayesian)**: "Based on the data I observed and my prior knowledge, I'm 95% confident the true average height is in this interval." It's a direct probability statement about where the parameter lies, given what we've learned.</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>The confidence interval is like a fishing net manufacturer's guarantee: "95% of our nets catch fish." The credible interval is like a weather forecast: "95% chance of rain tomorrow." One describes a long-run property of a procedure; the other describes belief about a specific unknown.</span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>Let $\theta$ be the parameter and $X^n$ the observed data.</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a>**Confidence Interval**: Find functions $L(X^n)$ and $U(X^n)$ such that:</span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}_\theta(L(X^n) \leq \theta \leq U(X^n)) = 1-\alpha \text{ for all } \theta$$</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a>The probability is over the random data $X^n$, with $\theta$ fixed. Different data gives different intervals.</span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a>**Credible Interval**: Find constants $a$ and $b$ such that:</span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a>$$\int_a^b f(\theta|X^n) d\theta = 1-\alpha$$</span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>The probability is over the parameter $\theta$ given fixed, observed data $X^n$. The interval quantifies our posterior uncertainty about $\theta$.</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a>Key difference: In confidence intervals, data is random and parameter is fixed. In credible intervals, data is fixed (observed) and parameter is treated as random (uncertain).</span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a>Let's simulate both types of intervals to see their fundamental difference. We'll generate many datasets to show the frequentist coverage property, then compute a single credible interval to show the Bayesian probability statement:</span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the difference between confidence and credible intervals</span></span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter</span></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a>true_std <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate many datasets to show confidence interval behavior</span></span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>confidence_intervals <span class="op">=</span> []</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a dataset</span></span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.random.normal(true_mean, true_std, n)</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a>    sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>    sample_se <span class="op">=</span> true_std <span class="op">/</span> np.sqrt(n)  <span class="co"># Known variance case</span></span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 95% Confidence interval</span></span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>    ci_lower <span class="op">=</span> sample_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> sample_se</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>    ci_upper <span class="op">=</span> sample_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> sample_se</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a>    confidence_intervals.append((ci_lower, ci_upper))</span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a><span class="co"># Count how many contain the true parameter</span></span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>coverage <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> (l, u) <span class="kw">in</span> confidence_intervals <span class="cf">if</span> l <span class="op">&lt;=</span> true_mean <span class="op">&lt;=</span> u)</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Confidence Interval Coverage: </span><span class="sc">{</span>coverage<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_simulations<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>coverage<span class="op">/</span>n_simulations<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This demonstrates the frequentist guarantee: ~95</span><span class="sc">% c</span><span class="st">overage in repeated sampling"</span>)</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize all 100 confidence intervals</span></span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a><span class="co"># Top panel: Show all confidence intervals</span></span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>ax1.axhline(true_mean, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, label<span class="op">=</span><span class="st">'True mean'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot all intervals, colored by whether they contain the true mean</span></span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>    l, u <span class="op">=</span> confidence_intervals[i]</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a>    contains_true <span class="op">=</span> l <span class="op">&lt;=</span> true_mean <span class="op">&lt;=</span> u</span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> <span class="st">'#0173B2'</span> <span class="cf">if</span> contains_true <span class="cf">else</span> <span class="st">'#DE8F05'</span>  <span class="co"># Blue vs Orange (high contrast, colorblind safe)</span></span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use thinner lines and transparency for better visualization</span></span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>    ax1.plot([i, i], [l, u], color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="fl">0.8</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Small dots for interval centers</span></span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>    ax1.plot(i, (l<span class="op">+</span>u)<span class="op">/</span><span class="dv">2</span>, <span class="st">'.'</span>, color<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a><span class="co"># Add summary statistics</span></span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a>n_containing <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> (l,u) <span class="kw">in</span> confidence_intervals <span class="cf">if</span> l <span class="op">&lt;=</span> true_mean <span class="op">&lt;=</span> u)</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Dataset number'</span>)</span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="ss">f'All </span><span class="sc">{</span>n_simulations<span class="sc">}</span><span class="ss"> Confidence Intervals</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a>             <span class="ss">f'</span><span class="sc">{</span>n_containing<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_simulations<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>n_containing<span class="op">/</span>n_simulations<span class="sc">:.1%}</span><span class="ss">) contain true mean • '</span></span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>             <span class="ss">f'Blue = contains true mean, Orange = misses'</span>)</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="dv">1</span>, n_simulations)</span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a><span class="co"># Now show a single Bayesian credible interval</span></span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>single_dataset <span class="op">=</span> np.random.normal(true_mean, true_std, n)</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> np.mean(single_dataset)</span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a><span class="co"># With a Normal prior N(0, 10) and known variance</span></span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>prior_mean, prior_var <span class="op">=</span> <span class="dv">0</span>, <span class="dv">10</span></span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a>posterior_var <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span><span class="op">/</span>prior_var <span class="op">+</span> n<span class="op">/</span>true_std<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>posterior_mean <span class="op">=</span> posterior_var <span class="op">*</span> (prior_mean<span class="op">/</span>prior_var <span class="op">+</span> n<span class="op">*</span>sample_mean<span class="op">/</span>true_std<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>posterior_std <span class="op">=</span> np.sqrt(posterior_var)</span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% Credible interval</span></span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>cred_lower <span class="op">=</span> posterior_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a>cred_upper <span class="op">=</span> posterior_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std</span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom panel: Show posterior distribution</span></span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(posterior_mean <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>posterior_std, posterior_mean <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>posterior_std, <span class="dv">200</span>)</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>posterior_density <span class="op">=</span> stats.norm.pdf(x_range, posterior_mean, posterior_std)</span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_range, posterior_density, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Posterior'</span>)</span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a>ax2.fill_between(x_range, posterior_density, </span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a>                 where<span class="op">=</span>(x_range <span class="op">&gt;=</span> cred_lower) <span class="op">&amp;</span> (x_range <span class="op">&lt;=</span> cred_upper),</span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'95% Credible Interval'</span>)</span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a>ax2.axvline(true_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'True mean'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a>ax2.axvline(sample_mean, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, label<span class="op">=</span><span class="st">'Sample mean'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Posterior density'</span>)</span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="ss">f'Posterior Distribution for One Dataset</span><span class="ch">\n</span><span class="ss">P(</span><span class="sc">{</span>cred_lower<span class="sc">:.2f}</span><span class="ss"> &lt; θ &lt; </span><span class="sc">{</span>cred_upper<span class="sc">:.2f}</span><span class="ss"> | data) = 0.95'</span>)</span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">For this specific dataset:"</span>)</span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Sample mean: </span><span class="sc">{</span>sample_mean<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  95% Credible Interval: [</span><span class="sc">{</span>cred_lower<span class="sc">:.2f}</span><span class="ss">, </span><span class="sc">{</span>cred_upper<span class="sc">:.2f}</span><span class="ss">]"</span>)</span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  This is a direct probability statement about the parameter!"</span>)</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a>**Key Takeaway**: Confidence intervals achieve 95% coverage *across many experiments* (a procedure property), while credible intervals give 95% probability *for this specific dataset* (a parameter property). Same numbers, fundamentally different meanings.</span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian Inference in Action</span></span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conjugate Models and Conjugate Priors</span></span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a>In principle, Bayesian inference requires us to compute integrals to normalize the posterior distribution. In practice, these integrals are often intractable. However, for certain combinations of priors and likelihoods, the posterior has a nice closed form. These special cases are called **conjugate models**.</span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is a Conjugate Prior?</span></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a>A prior is **conjugate** to a likelihood if the resulting posterior distribution is in the same family as the prior. This means:</span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If the prior is Beta, the posterior is also Beta</span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If the prior is Normal, the posterior is also Normal</span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a>Conjugacy provides a convenient analytical shortcut, though modern computational methods have reduced its importance.</span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Example: The Bernoulli-Beta Model</span></span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a>Consider the fundamental problem of estimating a probability from binary data. Let $X_i \sim \text{Bernoulli}(p)$ for $i = 1, ..., n$, where we observe $s$ successes out of $n$ trials.</span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a>**Starting with a Uniform Prior:**</span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a>Since $p$ is a probability, it must lie in $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. If we have no prior information, a natural choice is the uniform prior: $f(p) = 1$ for $p \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$.</span>
<span id="cb4-495"><a href="#cb4-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-496"><a href="#cb4-496" aria-hidden="true" tabindex="-1"></a>**Likelihood**: With $s$ successes in $n$ trials, the likelihood is:</span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}_n(p) \propto p^s(1-p)^{n-s}$$</span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a>**Posterior calculation**:</span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a>$$f(p | x^n) \propto f(p) \times \mathcal{L}_n(p) = 1 \times p^s(1-p)^{n-s} = p^{(s+1)-1}(1-p)^{(n-s+1)-1}$$</span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a>This has the form of a <span class="co">[</span><span class="ot">Beta distribution</span><span class="co">](https://en.wikipedia.org/wiki/Beta_distribution)</span>! Specifically, if we match the parameters:</span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a>$$p | x^n \sim \text{Beta}(s+1, n-s+1)$$</span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a>The mean of $\text{Beta}(\alpha, \beta)$ is $\alpha / (\alpha + \beta)$, so the posterior mean here is $\bar{p} = \frac{s+1}{n+2}$, which can be written as:</span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a>$$\bar{p} = \frac{n}{n+2} \cdot \frac{s}{n} + \frac{2}{n+2} \cdot \frac{1}{2}$$</span>
<span id="cb4-507"><a href="#cb4-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-508"><a href="#cb4-508" aria-hidden="true" tabindex="-1"></a>This is a weighted average of the MLE $\hat{p} = s/n$ and the prior mean $1/2$, with the data getting more weight as $n$ increases.</span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a>**The General Beta Prior:**</span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a>The uniform prior is actually a special case of the Beta distribution. In general, if we use a $\text{Beta}(\alpha, \beta)$ prior:</span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a>$$f(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} p^{\alpha-1}(1-p)^{\beta-1}$$</span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a>Then the posterior is:</span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a>$$p | x^n \sim \text{Beta}(\alpha + s, \beta + n - s)$$</span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a>**Key insights:**</span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The Beta distribution is **conjugate** to the Bernoulli likelihood - the posterior stays in the Beta family</span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The parameters $\alpha$ and $\beta$ act as "pseudo-counts": $\alpha$ prior successes, $\beta$ prior failures</span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The uniform prior is $\text{Beta}(1, 1)$ - one pseudo-success and one pseudo-failure</span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The posterior mean $\bar{p} = \frac{\alpha + s}{\alpha + \beta + n}$ combines prior pseudo-counts with observed counts</span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>As $n \to \infty$, the data dominates and the prior's influence vanishes</span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a>Let's visualize how the posterior evolves with data:</span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-535"><a href="#cb4-535" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-536"><a href="#cb4-536" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb4-537"><a href="#cb4-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-538"><a href="#cb4-538" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the figure</span></span>
<span id="cb4-539"><a href="#cb4-539" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb4-540"><a href="#cb4-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-541"><a href="#cb4-541" aria-hidden="true" tabindex="-1"></a><span class="co"># Top panel: Effect of sample size</span></span>
<span id="cb4-542"><a href="#cb4-542" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb4-543"><a href="#cb4-543" aria-hidden="true" tabindex="-1"></a>p_true <span class="op">=</span> <span class="fl">0.7</span>  <span class="co"># True probability</span></span>
<span id="cb4-544"><a href="#cb4-544" aria-hidden="true" tabindex="-1"></a>alpha_prior, beta_prior <span class="op">=</span> <span class="dv">1</span>, <span class="dv">1</span>  <span class="co"># Uniform prior</span></span>
<span id="cb4-545"><a href="#cb4-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-546"><a href="#cb4-546" aria-hidden="true" tabindex="-1"></a><span class="co"># Different sample sizes</span></span>
<span id="cb4-547"><a href="#cb4-547" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">200</span>]</span>
<span id="cb4-548"><a href="#cb4-548" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'gray'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'red'</span>]</span>
<span id="cb4-549"><a href="#cb4-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-550"><a href="#cb4-550" aria-hidden="true" tabindex="-1"></a>p_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">200</span>)</span>
<span id="cb4-551"><a href="#cb4-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-552"><a href="#cb4-552" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed once outside the loop for reproducible results</span></span>
<span id="cb4-553"><a href="#cb4-553" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-554"><a href="#cb4-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-555"><a href="#cb4-555" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, color <span class="kw">in</span> <span class="bu">zip</span>(sample_sizes, colors):</span>
<span id="cb4-556"><a href="#cb4-556" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-557"><a href="#cb4-557" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Just the prior</span></span>
<span id="cb4-558"><a href="#cb4-558" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> stats.beta.pdf(p_range, alpha_prior, beta_prior)</span>
<span id="cb4-559"><a href="#cb4-559" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="st">'Prior'</span></span>
<span id="cb4-560"><a href="#cb4-560" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-561"><a href="#cb4-561" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate data</span></span>
<span id="cb4-562"><a href="#cb4-562" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> np.random.binomial(n, p_true)</span>
<span id="cb4-563"><a href="#cb4-563" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior parameters</span></span>
<span id="cb4-564"><a href="#cb4-564" aria-hidden="true" tabindex="-1"></a>        alpha_post <span class="op">=</span> alpha_prior <span class="op">+</span> s</span>
<span id="cb4-565"><a href="#cb4-565" aria-hidden="true" tabindex="-1"></a>        beta_post <span class="op">=</span> beta_prior <span class="op">+</span> n <span class="op">-</span> s</span>
<span id="cb4-566"><a href="#cb4-566" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> stats.beta.pdf(p_range, alpha_post, beta_post)</span>
<span id="cb4-567"><a href="#cb4-567" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="ss">f'n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, s=</span><span class="sc">{</span>s<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb4-568"><a href="#cb4-568" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-569"><a href="#cb4-569" aria-hidden="true" tabindex="-1"></a>    ax1.plot(p_range, y, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>label)</span>
<span id="cb4-570"><a href="#cb4-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-571"><a href="#cb4-571" aria-hidden="true" tabindex="-1"></a>ax1.axvline(p_true, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True p'</span>)</span>
<span id="cb4-572"><a href="#cb4-572" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb4-573"><a href="#cb4-573" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb4-574"><a href="#cb4-574" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Posterior Becomes More Concentrated with More Data'</span>)</span>
<span id="cb4-575"><a href="#cb4-575" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb4-576"><a href="#cb4-576" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-577"><a href="#cb4-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-578"><a href="#cb4-578" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom panel: Effect of different priors</span></span>
<span id="cb4-579"><a href="#cb4-579" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb4-580"><a href="#cb4-580" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb4-581"><a href="#cb4-581" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">10</span>  <span class="co"># 50% success rate in data</span></span>
<span id="cb4-582"><a href="#cb4-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-583"><a href="#cb4-583" aria-hidden="true" tabindex="-1"></a>priors <span class="op">=</span> [</span>
<span id="cb4-584"><a href="#cb4-584" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">1</span>, <span class="st">'Uniform: Beta(1,1)'</span>),</span>
<span id="cb4-585"><a href="#cb4-585" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="dv">10</span>, <span class="st">'Informative: Beta(10,10)'</span>),</span>
<span id="cb4-586"><a href="#cb4-586" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">10</span>, <span class="st">'Skeptical: Beta(1,10)'</span>)</span>
<span id="cb4-587"><a href="#cb4-587" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-588"><a href="#cb4-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-589"><a href="#cb4-589" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (alpha, beta, label) <span class="kw">in</span> priors:</span>
<span id="cb4-590"><a href="#cb4-590" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior</span></span>
<span id="cb4-591"><a href="#cb4-591" aria-hidden="true" tabindex="-1"></a>    prior_y <span class="op">=</span> stats.beta.pdf(p_range, alpha, beta)</span>
<span id="cb4-592"><a href="#cb4-592" aria-hidden="true" tabindex="-1"></a>    ax2.plot(p_range, prior_y, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-593"><a href="#cb4-593" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-594"><a href="#cb4-594" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior</span></span>
<span id="cb4-595"><a href="#cb4-595" aria-hidden="true" tabindex="-1"></a>    alpha_post <span class="op">=</span> alpha <span class="op">+</span> s</span>
<span id="cb4-596"><a href="#cb4-596" aria-hidden="true" tabindex="-1"></a>    beta_post <span class="op">=</span> beta <span class="op">+</span> n <span class="op">-</span> s</span>
<span id="cb4-597"><a href="#cb4-597" aria-hidden="true" tabindex="-1"></a>    post_y <span class="op">=</span> stats.beta.pdf(p_range, alpha_post, beta_post)</span>
<span id="cb4-598"><a href="#cb4-598" aria-hidden="true" tabindex="-1"></a>    ax2.plot(p_range, post_y, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>label)</span>
<span id="cb4-599"><a href="#cb4-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-600"><a href="#cb4-600" aria-hidden="true" tabindex="-1"></a>ax2.axvline(<span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb4-601"><a href="#cb4-601" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'p'</span>)</span>
<span id="cb4-602"><a href="#cb4-602" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb4-603"><a href="#cb4-603" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="ss">f'Different Priors, Same Data (n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, s=</span><span class="sc">{</span>s<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb4-604"><a href="#cb4-604" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb4-605"><a href="#cb4-605" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-606"><a href="#cb4-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-607"><a href="#cb4-607" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-608"><a href="#cb4-608" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-609"><a href="#cb4-609" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-610"><a href="#cb4-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-611"><a href="#cb4-611" aria-hidden="true" tabindex="-1"></a>The plots illustrate two key principles:</span>
<span id="cb4-612"><a href="#cb4-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-613"><a href="#cb4-613" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Top panel**: As we collect more data, the posterior becomes increasingly concentrated around the true value, regardless of the prior.</span>
<span id="cb4-614"><a href="#cb4-614" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Bottom panel**: Different priors lead to different posteriors (but this effect diminishes with larger sample sizes).</span>
<span id="cb4-615"><a href="#cb4-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-616"><a href="#cb4-616" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Example: The Normal-Normal Model</span></span>
<span id="cb4-617"><a href="#cb4-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-618"><a href="#cb4-618" aria-hidden="true" tabindex="-1"></a>Now consider estimating the mean of a Normal distribution with known variance. Let $X_i \sim \mathcal{N}(\theta, \sigma^2)$ where $\sigma^2$ is known (this assumption simplifies the math and is commonly used in introductory examples).</span>
<span id="cb4-619"><a href="#cb4-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-620"><a href="#cb4-620" aria-hidden="true" tabindex="-1"></a>**Prior**: $\theta \sim \mathcal{N}(\theta_0, \sigma_0^2)$</span>
<span id="cb4-621"><a href="#cb4-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-622"><a href="#cb4-622" aria-hidden="true" tabindex="-1"></a>**Likelihood**: For IID data, the sufficient statistic is the sample mean $\bar{x}$, and:</span>
<span id="cb4-623"><a href="#cb4-623" aria-hidden="true" tabindex="-1"></a>$$\bar{x} | \theta \sim \mathcal{N}(\theta, \sigma^2/n)$$</span>
<span id="cb4-624"><a href="#cb4-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-625"><a href="#cb4-625" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Posterior for Normal-Normal Model"}</span>
<span id="cb4-626"><a href="#cb4-626" aria-hidden="true" tabindex="-1"></a>Given a likelihood $X_i | \theta \sim \mathcal{N}(\theta, \sigma^2)$ (known $\sigma^2$) and a prior $\theta \sim \mathcal{N}(\theta_0, \sigma_0^2)$, the posterior is:</span>
<span id="cb4-627"><a href="#cb4-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-628"><a href="#cb4-628" aria-hidden="true" tabindex="-1"></a>$$\theta | x^n \sim \mathcal{N}(\theta_*, \sigma_*^2)$$</span>
<span id="cb4-629"><a href="#cb4-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-630"><a href="#cb4-630" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb4-631"><a href="#cb4-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-632"><a href="#cb4-632" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior Precision**: $\frac{1}{\sigma_*^2} = \frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}$</span>
<span id="cb4-633"><a href="#cb4-633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior Mean**: $\theta_* = \sigma_*^2 \left( \frac{\theta_0}{\sigma_0^2} + \frac{n \bar{x}}{\sigma^2} \right)$</span>
<span id="cb4-634"><a href="#cb4-634" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-635"><a href="#cb4-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-636"><a href="#cb4-636" aria-hidden="true" tabindex="-1"></a>**Interpretation**:</span>
<span id="cb4-637"><a href="#cb4-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-638"><a href="#cb4-638" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Precision (inverse variance) is additive: posterior precision = prior precision + data precision</span>
<span id="cb4-639"><a href="#cb4-639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The posterior mean is a precision-weighted average of prior mean and sample mean</span>
<span id="cb4-640"><a href="#cb4-640" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More precise information gets more weight</span>
<span id="cb4-641"><a href="#cb4-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The posterior mean interpolates between the prior mean and sample mean, with the weight given to the data increasing as $n$ increases</span>
<span id="cb4-642"><a href="#cb4-642" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Posterior is always more precise than either prior or likelihood alone</span>
<span id="cb4-643"><a href="#cb4-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-646"><a href="#cb4-646" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-647"><a href="#cb4-647" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb4-648"><a href="#cb4-648" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb4-649"><a href="#cb4-649" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code for Normal-Normal posterior calculation"</span></span>
<span id="cb4-650"><a href="#cb4-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-651"><a href="#cb4-651" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normal_normal_posterior(prior_mean, prior_var, data_mean, data_var, n):</span>
<span id="cb4-652"><a href="#cb4-652" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-653"><a href="#cb4-653" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate posterior parameters for Normal-Normal conjugate model.</span></span>
<span id="cb4-654"><a href="#cb4-654" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-655"><a href="#cb4-655" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb4-656"><a href="#cb4-656" aria-hidden="true" tabindex="-1"></a><span class="co">    -----------</span></span>
<span id="cb4-657"><a href="#cb4-657" aria-hidden="true" tabindex="-1"></a><span class="co">    prior_mean : Prior mean θ₀</span></span>
<span id="cb4-658"><a href="#cb4-658" aria-hidden="true" tabindex="-1"></a><span class="co">    prior_var : Prior variance σ₀²</span></span>
<span id="cb4-659"><a href="#cb4-659" aria-hidden="true" tabindex="-1"></a><span class="co">    data_mean : Sample mean x̄</span></span>
<span id="cb4-660"><a href="#cb4-660" aria-hidden="true" tabindex="-1"></a><span class="co">    data_var : Known data variance σ²</span></span>
<span id="cb4-661"><a href="#cb4-661" aria-hidden="true" tabindex="-1"></a><span class="co">    n : Sample size</span></span>
<span id="cb4-662"><a href="#cb4-662" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-663"><a href="#cb4-663" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-664"><a href="#cb4-664" aria-hidden="true" tabindex="-1"></a><span class="co">    --------</span></span>
<span id="cb4-665"><a href="#cb4-665" aria-hidden="true" tabindex="-1"></a><span class="co">    post_mean : Posterior mean θ*</span></span>
<span id="cb4-666"><a href="#cb4-666" aria-hidden="true" tabindex="-1"></a><span class="co">    post_var : Posterior variance σ*²</span></span>
<span id="cb4-667"><a href="#cb4-667" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-668"><a href="#cb4-668" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to precisions (inverse variances)</span></span>
<span id="cb4-669"><a href="#cb4-669" aria-hidden="true" tabindex="-1"></a>    prior_precision <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> prior_var</span>
<span id="cb4-670"><a href="#cb4-670" aria-hidden="true" tabindex="-1"></a>    data_precision <span class="op">=</span> n <span class="op">/</span> data_var</span>
<span id="cb4-671"><a href="#cb4-671" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-672"><a href="#cb4-672" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior precision is sum of precisions</span></span>
<span id="cb4-673"><a href="#cb4-673" aria-hidden="true" tabindex="-1"></a>    post_precision <span class="op">=</span> prior_precision <span class="op">+</span> data_precision</span>
<span id="cb4-674"><a href="#cb4-674" aria-hidden="true" tabindex="-1"></a>    post_var <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> post_precision</span>
<span id="cb4-675"><a href="#cb4-675" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-676"><a href="#cb4-676" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior mean is precision-weighted average</span></span>
<span id="cb4-677"><a href="#cb4-677" aria-hidden="true" tabindex="-1"></a>    post_mean <span class="op">=</span> post_var <span class="op">*</span> (prior_precision <span class="op">*</span> prior_mean <span class="op">+</span> </span>
<span id="cb4-678"><a href="#cb4-678" aria-hidden="true" tabindex="-1"></a>                            data_precision <span class="op">*</span> data_mean)</span>
<span id="cb4-679"><a href="#cb4-679" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-680"><a href="#cb4-680" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> post_mean, post_var</span>
<span id="cb4-681"><a href="#cb4-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-682"><a href="#cb4-682" aria-hidden="true" tabindex="-1"></a><span class="co"># Example calculation</span></span>
<span id="cb4-683"><a href="#cb4-683" aria-hidden="true" tabindex="-1"></a>prior_mean, prior_var <span class="op">=</span> <span class="dv">0</span>, <span class="dv">4</span>  <span class="co"># Prior: N(0, 4)</span></span>
<span id="cb4-684"><a href="#cb4-684" aria-hidden="true" tabindex="-1"></a>data_var <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Known variance</span></span>
<span id="cb4-685"><a href="#cb4-685" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-686"><a href="#cb4-686" aria-hidden="true" tabindex="-1"></a>data_mean <span class="op">=</span> <span class="fl">2.3</span>  <span class="co"># Observed sample mean</span></span>
<span id="cb4-687"><a href="#cb4-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-688"><a href="#cb4-688" aria-hidden="true" tabindex="-1"></a>post_mean, post_var <span class="op">=</span> normal_normal_posterior(</span>
<span id="cb4-689"><a href="#cb4-689" aria-hidden="true" tabindex="-1"></a>    prior_mean, prior_var, data_mean, data_var, n</span>
<span id="cb4-690"><a href="#cb4-690" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-691"><a href="#cb4-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-692"><a href="#cb4-692" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prior: N(</span><span class="sc">{</span>prior_mean<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>prior_var<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb4-693"><a href="#cb4-693" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data: n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, x̄=</span><span class="sc">{</span>data_mean<span class="sc">}</span><span class="ss">, σ²=</span><span class="sc">{</span>data_var<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-694"><a href="#cb4-694" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior: N(</span><span class="sc">{</span>post_mean<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>post_var<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb4-695"><a href="#cb4-695" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Posterior mean is </span><span class="sc">{</span>post_mean<span class="sc">:.3f}</span><span class="ss">, between prior mean </span><span class="sc">{</span>prior_mean<span class="sc">}</span><span class="ss"> and MLE </span><span class="sc">{</span>data_mean<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-696"><a href="#cb4-696" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-697"><a href="#cb4-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-698"><a href="#cb4-698" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Art and Science of Choosing Priors</span></span>
<span id="cb4-699"><a href="#cb4-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-700"><a href="#cb4-700" aria-hidden="true" tabindex="-1"></a>One of the most debated topics in Bayesian statistics is how to choose the prior distribution. Critics argue that priors introduce subjectivity; advocates counter that they make assumptions explicit. The reality is nuanced: prior choice is both an art requiring judgment and a science with established principles.</span>
<span id="cb4-701"><a href="#cb4-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-702"><a href="#cb4-702" aria-hidden="true" tabindex="-1"></a>**Conjugate Priors**: We've seen these in action -- Beta for Bernoulli, Normal for Normal. They're computationally convenient and have nice interpretations (like pseudo-counts), but they may not reflect genuine prior beliefs. Using them just for convenience can lead to misleading results.</span>
<span id="cb4-703"><a href="#cb4-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-704"><a href="#cb4-704" aria-hidden="true" tabindex="-1"></a>**Non-Informative Priors**: These attempt to be "objective" by letting the data speak for itself. Common choices include:</span>
<span id="cb4-705"><a href="#cb4-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-706"><a href="#cb4-706" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Uniform priors: $f(\theta) = \text{constant}$</span>
<span id="cb4-707"><a href="#cb4-707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Jeffreys' prior: $f(\theta) \propto \sqrt{I(\theta)}$ where $I(\theta)$ is the Fisher information</span>
<span id="cb4-708"><a href="#cb4-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-709"><a href="#cb4-709" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb4-710"><a href="#cb4-710" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Flat Prior Fallacy</span></span>
<span id="cb4-711"><a href="#cb4-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-712"><a href="#cb4-712" aria-hidden="true" tabindex="-1"></a>A uniform prior is not "uninformative"! Consider:</span>
<span id="cb4-713"><a href="#cb4-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-714"><a href="#cb4-714" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Scale matters**: A uniform prior on $<span class="co">[</span><span class="ot">-10^6, 10^6</span><span class="co">]</span>$ says $|\theta|$ is almost certainly large</span>
<span id="cb4-715"><a href="#cb4-715" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Not transformation invariant**: If $p \sim \text{Uniform}(0,1)$, then $\log(p/(1-p))$ is not uniform</span>
<span id="cb4-716"><a href="#cb4-716" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Can encode strong beliefs**: Uniform on <span class="co">[</span><span class="ot">0, 1000</span><span class="co">]</span> for a rate parameter implies most mass is on very large values (highly informative!)</span>
<span id="cb4-717"><a href="#cb4-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-718"><a href="#cb4-718" aria-hidden="true" tabindex="-1"></a>The notion of "no information" is not well-defined mathematically.</span>
<span id="cb4-719"><a href="#cb4-719" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-720"><a href="#cb4-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-721"><a href="#cb4-721" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-722"><a href="#cb4-722" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Jeffreys' Prior</span></span>
<span id="cb4-723"><a href="#cb4-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-724"><a href="#cb4-724" aria-hidden="true" tabindex="-1"></a>Jeffreys proposed using $f(\theta) \propto \sqrt{I(\theta)}$ where $I(\theta)$ is the Fisher information. This prior has a key property: it's invariant to reparameterization. If we transform $\theta$ to $\varphi = g(\theta)$, the Jeffreys prior for $\varphi$ is what we'd get by transforming the Jeffreys prior for $\theta$.</span>
<span id="cb4-725"><a href="#cb4-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-726"><a href="#cb4-726" aria-hidden="true" tabindex="-1"></a>For $\text{Bernoulli}(p)$, the Jeffreys prior is $\text{Beta}(1/2, 1/2)$, which is U-shaped, putting more mass near 0 and 1 than at 0.5 -- hardly "uninformative"!</span>
<span id="cb4-727"><a href="#cb4-727" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-728"><a href="#cb4-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-729"><a href="#cb4-729" aria-hidden="true" tabindex="-1"></a>**Weakly Informative Priors**: This is the recommended approach nowadays which balances several goals:</span>
<span id="cb4-730"><a href="#cb4-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-731"><a href="#cb4-731" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wide enough to not exclude plausible values</span>
<span id="cb4-732"><a href="#cb4-732" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tight enough to exclude absurd values</span>
<span id="cb4-733"><a href="#cb4-733" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regularize estimation to prevent overfitting</span>
<span id="cb4-734"><a href="#cb4-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-735"><a href="#cb4-735" aria-hidden="true" tabindex="-1"></a>For example, for a logistic regression coefficient, a $\mathcal{N}(0, 2.5^2)$ prior (mean 0, standard deviation 2.5) allows large effects but prevents numerical instability. Note: We use the $\mathcal{N}(\mu, \sigma^2)$ parameterization throughout these notes.</span>
<span id="cb4-736"><a href="#cb4-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-737"><a href="#cb4-737" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution}</span>
<span id="cb4-738"><a href="#cb4-738" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Challenge of High-Dimensional Priors</span></span>
<span id="cb4-739"><a href="#cb4-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-740"><a href="#cb4-740" aria-hidden="true" tabindex="-1"></a>Placing sensible priors becomes increasingly difficult as dimensionality grows:</span>
<span id="cb4-741"><a href="#cb4-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-742"><a href="#cb4-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The Gaussian bubble**: In high dimensions, a multivariate standard normal $\mathcal{N}(0, I)$ concentrates its mass in a thin shell at radius $\sqrt{d}$ from the origin -- almost no mass is near zero despite this being the "center" of the distribution. This concentration of measure phenomenon means our intuitions about priors break down (see this <span class="co">[</span><span class="ot">blog post</span><span class="co">](https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/)</span>).</span>
<span id="cb4-743"><a href="#cb4-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-744"><a href="#cb4-744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep learning**: Specifying priors for millions of neural network weights remains an open problem in Bayesian deep learning. Most practitioners resort to simple priors like $\mathcal{N}(0, \sigma^2 I)$ that don't capture the true structure, or avoid fully Bayesian approaches altogether.</span>
<span id="cb4-745"><a href="#cb4-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-746"><a href="#cb4-746" aria-hidden="true" tabindex="-1"></a>High-dimensional Bayesian inference requires careful thought about what the prior actually implies when there are many parameters.</span>
<span id="cb4-747"><a href="#cb4-747" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-748"><a href="#cb4-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-749"><a href="#cb4-749" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-750"><a href="#cb4-750" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: The Bayesian Central Limit Theorem</span></span>
<span id="cb4-751"><a href="#cb4-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-752"><a href="#cb4-752" aria-hidden="true" tabindex="-1"></a>A remarkable result shows that Bayesian and frequentist methods converge with enough data.</span>
<span id="cb4-753"><a href="#cb4-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-754"><a href="#cb4-754" aria-hidden="true" tabindex="-1"></a>Under suitable regularity conditions, as $n \to \infty$, the posterior distribution can be approximated by:</span>
<span id="cb4-755"><a href="#cb4-755" aria-hidden="true" tabindex="-1"></a>$$f(\theta | x^n) \approx \mathcal{N}(\hat{\theta}_{MLE}, \widehat{se}^2)$$</span>
<span id="cb4-756"><a href="#cb4-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-757"><a href="#cb4-757" aria-hidden="true" tabindex="-1"></a>where $\hat{\theta}_{MLE}$ is the maximum likelihood estimate and $\widehat{se}$ is its standard error.</span>
<span id="cb4-758"><a href="#cb4-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-759"><a href="#cb4-759" aria-hidden="true" tabindex="-1"></a>**Why this matters**:</span>
<span id="cb4-760"><a href="#cb4-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-761"><a href="#cb4-761" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>With enough data, the prior becomes irrelevant</span>
<span id="cb4-762"><a href="#cb4-762" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Bayesian credible intervals ≈ Frequentist confidence intervals</span>
<span id="cb4-763"><a href="#cb4-763" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Both approaches give essentially the same answer</span>
<span id="cb4-764"><a href="#cb4-764" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>The likelihood dominates both approaches in large samples</span>
<span id="cb4-765"><a href="#cb4-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-766"><a href="#cb4-766" aria-hidden="true" tabindex="-1"></a>This is reassuring: two philosophically different approaches converge to the same practical conclusions when we have sufficient evidence.</span>
<span id="cb4-767"><a href="#cb4-767" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-768"><a href="#cb4-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-769"><a href="#cb4-769" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementing Bayesian Inference</span></span>
<span id="cb4-770"><a href="#cb4-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-771"><a href="#cb4-771" aria-hidden="true" tabindex="-1"></a>For the conjugate models in this chapter, we can solve for the posterior distribution analytically. But what happens in more complex, real-world models where this is not possible?</span>
<span id="cb4-772"><a href="#cb4-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-773"><a href="#cb4-773" aria-hidden="true" tabindex="-1"></a>The modern Bayesian workflow relies on powerful computational algorithms, most commonly **Markov chain Monte Carlo (MCMC)**. These algorithms allow us to generate a large collection of samples that are representative of the posterior distribution, even when we cannot solve for it directly. Once we have these samples, we can approximate any summary we need (like the mean or a credible interval) and easily get posteriors for transformed parameters.</span>
<span id="cb4-774"><a href="#cb4-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-775"><a href="#cb4-775" aria-hidden="true" tabindex="-1"></a>Modern **probabilistic programming frameworks** such as <span class="co">[</span><span class="ot">Stan</span><span class="co">](https://mc-stan.org/)</span> or <span class="co">[</span><span class="ot">PyMC</span><span class="co">](https://www.pymc.io/welcome.html)</span> allow users to perform Bayesian inference relatively easily, exploiting modern machinery. This computational approach is incredibly powerful and flexible, and we will explore it in detail in Chapter 10. For now, the key takeaway is that the goal of Bayesian inference is always to obtain the posterior; the methods in this chapter do it with math, while later methods will do it with computation.</span>
<span id="cb4-776"><a href="#cb4-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-777"><a href="#cb4-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-778"><a href="#cb4-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-779"><a href="#cb4-779" aria-hidden="true" tabindex="-1"></a><span class="fu">## Statistical Decision Theory: A Framework for "Best"</span></span>
<span id="cb4-780"><a href="#cb4-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-781"><a href="#cb4-781" aria-hidden="true" tabindex="-1"></a>We now shift from Bayesian inference to a more general question: given multiple ways to estimate a parameter, how do we choose the best one? Statistical decision theory provides a formal framework for comparing any estimators -- Bayesian, frequentist, or otherwise.</span>
<span id="cb4-782"><a href="#cb4-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-783"><a href="#cb4-783" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Ingredients: Loss and Risk</span></span>
<span id="cb4-784"><a href="#cb4-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-785"><a href="#cb4-785" aria-hidden="true" tabindex="-1"></a>To compare estimators formally, we need to quantify how "wrong" an estimate is.</span>
<span id="cb4-786"><a href="#cb4-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-787"><a href="#cb4-787" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-788"><a href="#cb4-788" aria-hidden="true" tabindex="-1"></a>**Loss Function** $L(\theta, \hat{\theta})$: Quantifies the penalty for estimating $\theta$ with $\hat{\theta}$.</span>
<span id="cb4-789"><a href="#cb4-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-790"><a href="#cb4-790" aria-hidden="true" tabindex="-1"></a>Common examples:</span>
<span id="cb4-791"><a href="#cb4-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-792"><a href="#cb4-792" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Squared Error**: $L_2(\theta, \hat{\theta}) = (\theta - \hat{\theta})^2$</span>
<span id="cb4-793"><a href="#cb4-793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Absolute Error**: $L_1(\theta, \hat{\theta}) = |\theta - \hat{\theta}|$</span>
<span id="cb4-794"><a href="#cb4-794" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**$L_p$ Loss**: $L_p(\theta, \hat{\theta}) = |\theta - \hat{\theta}|^p$ for $p \geq 1$ (generalizes the above)</span>
<span id="cb4-795"><a href="#cb4-795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zero-One Loss**: $L_{0-1}(\theta, \hat{\theta}) = \begin{cases} 0 &amp; \text{if } \theta = \hat{\theta} <span class="sc">\\</span> 1 &amp; \text{otherwise} \end{cases}$</span>
<span id="cb4-796"><a href="#cb4-796" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-797"><a href="#cb4-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-798"><a href="#cb4-798" aria-hidden="true" tabindex="-1"></a>Loss tells us the cost of a specific estimate for a specific parameter value. But estimators are random -- they depend on data. So we need to average:</span>
<span id="cb4-799"><a href="#cb4-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-800"><a href="#cb4-800" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-801"><a href="#cb4-801" aria-hidden="true" tabindex="-1"></a>**Risk** $R(\theta, \hat{\theta})$: The expected loss over all possible datasets, for a given loss function $\mathcal{L}$ and fixed $\theta$.</span>
<span id="cb4-802"><a href="#cb4-802" aria-hidden="true" tabindex="-1"></a>$$R(\theta, \hat{\theta}) = \mathbb{E}_{\theta}<span class="co">[</span><span class="ot">L(\theta, \hat{\theta}(X))</span><span class="co">]</span> = \int L(\theta, \hat{\theta}(x)) f(x;\theta) dx$$</span>
<span id="cb4-803"><a href="#cb4-803" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-804"><a href="#cb4-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-805"><a href="#cb4-805" aria-hidden="true" tabindex="-1"></a>For squared error loss, the risk equals the MSE:</span>
<span id="cb4-806"><a href="#cb4-806" aria-hidden="true" tabindex="-1"></a>$$R(\theta, \hat{\theta}) = \mathbb{E}_{\theta}<span class="co">[</span><span class="ot">(\theta - \hat{\theta})^2</span><span class="co">]</span> = \text{MSE} = \mathbb{V}(\hat{\theta}) + \text{Bias}^2(\hat{\theta}) $$</span>
<span id="cb4-807"><a href="#cb4-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-808"><a href="#cb4-808" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb4-809"><a href="#cb4-809" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb4-810"><a href="#cb4-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-811"><a href="#cb4-811" aria-hidden="true" tabindex="-1"></a>Think of risk as the "average wrongness" of an estimator, for a specific definition of "wrongness" (loss function). Imagine you could repeat your experiment many times with the same true parameter value. Each time, you'd get different data and thus a different estimate. The risk tells you the average penalty you'd pay across all these repetitions.</span>
<span id="cb4-812"><a href="#cb4-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-813"><a href="#cb4-813" aria-hidden="true" tabindex="-1"></a>It's like evaluating a weather forecaster: you don't judge them on one prediction, but on their average performance over many days. An estimator with low risk is consistently good, even if it's not perfect on any single dataset.</span>
<span id="cb4-814"><a href="#cb4-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-815"><a href="#cb4-815" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb4-816"><a href="#cb4-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-817"><a href="#cb4-817" aria-hidden="true" tabindex="-1"></a>Risk is the frequentist expectation of the loss function, treating the estimator as a random variable (through its dependence on random data) while holding the parameter fixed:</span>
<span id="cb4-818"><a href="#cb4-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-819"><a href="#cb4-819" aria-hidden="true" tabindex="-1"></a>$$R(\theta, \hat{\theta}) = \mathbb{E}_{X \sim f(x;\theta)}<span class="co">[</span><span class="ot">L(\theta, \hat{\theta}(X))</span><span class="co">]</span>$$</span>
<span id="cb4-820"><a href="#cb4-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-821"><a href="#cb4-821" aria-hidden="true" tabindex="-1"></a>This contrasts with Bayes risk, which averages over $\theta$ according to a prior. The risk function $R(\theta, \cdot)$ maps each parameter value to a real number, creating a curve that characterizes the estimator's performance across the parameter space.</span>
<span id="cb4-822"><a href="#cb4-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-823"><a href="#cb4-823" aria-hidden="true" tabindex="-1"></a>For squared error loss, the bias-variance decomposition shows that risk combines systematic error (bias) with variability (variance), revealing the fundamental tradeoff in estimation.</span>
<span id="cb4-824"><a href="#cb4-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-825"><a href="#cb4-825" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb4-826"><a href="#cb4-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-827"><a href="#cb4-827" aria-hidden="true" tabindex="-1"></a>We can understand risk concretely through simulation. The following code demonstrates what risk actually means by repeatedly generating datasets from the same distribution and computing the squared loss for each one:</span>
<span id="cb4-828"><a href="#cb4-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-831"><a href="#cb4-831" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-832"><a href="#cb4-832" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb4-833"><a href="#cb4-833" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-834"><a href="#cb4-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-835"><a href="#cb4-835" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_risk_by_simulation(true_theta, estimator_func, </span>
<span id="cb4-836"><a href="#cb4-836" aria-hidden="true" tabindex="-1"></a>                                n_samples<span class="op">=</span><span class="dv">20</span>, n_simulations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb4-837"><a href="#cb4-837" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-838"><a href="#cb4-838" aria-hidden="true" tabindex="-1"></a><span class="co">    Estimate risk via Monte Carlo simulation.</span></span>
<span id="cb4-839"><a href="#cb4-839" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-840"><a href="#cb4-840" aria-hidden="true" tabindex="-1"></a><span class="co">    This shows what risk really means: the average loss</span></span>
<span id="cb4-841"><a href="#cb4-841" aria-hidden="true" tabindex="-1"></a><span class="co">    over many possible datasets.</span></span>
<span id="cb4-842"><a href="#cb4-842" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-843"><a href="#cb4-843" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb4-844"><a href="#cb4-844" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-845"><a href="#cb4-845" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb4-846"><a href="#cb4-846" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate a dataset (example: Normal distribution)</span></span>
<span id="cb4-847"><a href="#cb4-847" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.normal(true_theta, <span class="dv">1</span>, n_samples)</span>
<span id="cb4-848"><a href="#cb4-848" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-849"><a href="#cb4-849" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the estimate for this dataset</span></span>
<span id="cb4-850"><a href="#cb4-850" aria-hidden="true" tabindex="-1"></a>        estimate <span class="op">=</span> estimator_func(data)</span>
<span id="cb4-851"><a href="#cb4-851" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-852"><a href="#cb4-852" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the loss (using squared error)</span></span>
<span id="cb4-853"><a href="#cb4-853" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (estimate <span class="op">-</span> true_theta)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-854"><a href="#cb4-854" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb4-855"><a href="#cb4-855" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-856"><a href="#cb4-856" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Risk is the average loss</span></span>
<span id="cb4-857"><a href="#cb4-857" aria-hidden="true" tabindex="-1"></a>    risk <span class="op">=</span> np.mean(losses)</span>
<span id="cb4-858"><a href="#cb4-858" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-859"><a href="#cb4-859" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"True parameter: </span><span class="sc">{</span>true_theta<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-860"><a href="#cb4-860" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Estimated risk: </span><span class="sc">{</span>risk<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-861"><a href="#cb4-861" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Min loss seen: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(losses)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-862"><a href="#cb4-862" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Max loss seen: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">max</span>(losses)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-863"><a href="#cb4-863" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-864"><a href="#cb4-864" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> risk</span>
<span id="cb4-865"><a href="#cb4-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-866"><a href="#cb4-866" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Risk of sample mean estimator</span></span>
<span id="cb4-867"><a href="#cb4-867" aria-hidden="true" tabindex="-1"></a>risk <span class="op">=</span> estimate_risk_by_simulation(</span>
<span id="cb4-868"><a href="#cb4-868" aria-hidden="true" tabindex="-1"></a>    true_theta<span class="op">=</span><span class="fl">5.0</span>,</span>
<span id="cb4-869"><a href="#cb4-869" aria-hidden="true" tabindex="-1"></a>    estimator_func<span class="op">=</span><span class="kw">lambda</span> data: np.mean(data),</span>
<span id="cb4-870"><a href="#cb4-870" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">20</span></span>
<span id="cb4-871"><a href="#cb4-871" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-872"><a href="#cb4-872" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-873"><a href="#cb4-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-874"><a href="#cb4-874" aria-hidden="true" tabindex="-1"></a>**Key Takeaway**: Risk is the *average* loss across all possible datasets. The simulation shows that while individual losses vary widely (min to max), risk captures the expected performance of an estimator.</span>
<span id="cb4-875"><a href="#cb4-875" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-876"><a href="#cb4-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-877"><a href="#cb4-877" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Challenge of Comparing Risk Functions</span></span>
<span id="cb4-878"><a href="#cb4-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-879"><a href="#cb4-879" aria-hidden="true" tabindex="-1"></a>Risk functions are curves -- one risk value for each possible $\theta$. This creates a fundamental problem: estimators rarely dominate uniformly.</span>
<span id="cb4-880"><a href="#cb4-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-881"><a href="#cb4-881" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb4-882"><a href="#cb4-882" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: A Simple Case</span></span>
<span id="cb4-883"><a href="#cb4-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-884"><a href="#cb4-884" aria-hidden="true" tabindex="-1"></a>Let $X \sim \mathcal{N}(\theta, 1)$ and let's assume squared error loss. Compare:</span>
<span id="cb4-885"><a href="#cb4-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-886"><a href="#cb4-886" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\theta}_1 = X$ (the sensible estimator)</span>
<span id="cb4-887"><a href="#cb4-887" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\theta}_2 = 3$ (a silly constant estimator)</span>
<span id="cb4-888"><a href="#cb4-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-889"><a href="#cb4-889" aria-hidden="true" tabindex="-1"></a>Risk calculations:</span>
<span id="cb4-890"><a href="#cb4-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-891"><a href="#cb4-891" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For $\hat{\theta}_1 = X$: </span>
<span id="cb4-892"><a href="#cb4-892" aria-hidden="true" tabindex="-1"></a>  $$R(\theta, \hat{\theta}_1) = \mathbb{E}_\theta<span class="co">[</span><span class="ot">(X - \theta)^2</span><span class="co">]</span> = \text{Var}(X) = 1$$</span>
<span id="cb4-893"><a href="#cb4-893" aria-hidden="true" tabindex="-1"></a>  This is constant for all $\theta$.</span>
<span id="cb4-894"><a href="#cb4-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-895"><a href="#cb4-895" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For $\hat{\theta}_2 = 3$:</span>
<span id="cb4-896"><a href="#cb4-896" aria-hidden="true" tabindex="-1"></a>  $$R(\theta, \hat{\theta}_2) = \mathbb{E}_\theta<span class="co">[</span><span class="ot">(3 - \theta)^2</span><span class="co">]</span> = (3-\theta)^2$$</span>
<span id="cb4-897"><a href="#cb4-897" aria-hidden="true" tabindex="-1"></a>  This depends on $\theta$ since the estimator is non-random.</span>
<span id="cb4-898"><a href="#cb4-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-899"><a href="#cb4-899" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-902"><a href="#cb4-902" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-903"><a href="#cb4-903" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb4-904"><a href="#cb4-904" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb4-905"><a href="#cb4-905" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb4-906"><a href="#cb4-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-907"><a href="#cb4-907" aria-hidden="true" tabindex="-1"></a>theta_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">200</span>)</span>
<span id="cb4-908"><a href="#cb4-908" aria-hidden="true" tabindex="-1"></a>risk1 <span class="op">=</span> np.ones_like(theta_range)  <span class="co"># Risk of X</span></span>
<span id="cb4-909"><a href="#cb4-909" aria-hidden="true" tabindex="-1"></a>risk2 <span class="op">=</span> (<span class="dv">3</span> <span class="op">-</span> theta_range)<span class="op">**</span><span class="dv">2</span>       <span class="co"># Risk of constant estimator 3</span></span>
<span id="cb4-910"><a href="#cb4-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-911"><a href="#cb4-911" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb4-912"><a href="#cb4-912" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_range, risk1, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="vs">r'$\hat{\theta}_1 = X$'</span>)</span>
<span id="cb4-913"><a href="#cb4-913" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_range, risk2, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="vs">r'$\hat{\theta}_2 = 3$'</span>)</span>
<span id="cb4-914"><a href="#cb4-914" aria-hidden="true" tabindex="-1"></a>plt.axvspan(<span class="dv">2</span>, <span class="dv">4</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'green'</span>, label<span class="op">=</span><span class="vs">r'$\hat{\theta}_2$ is better'</span>)</span>
<span id="cb4-915"><a href="#cb4-915" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'$\theta$'</span>)</span>
<span id="cb4-916"><a href="#cb4-916" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Risk'</span>)</span>
<span id="cb4-917"><a href="#cb4-917" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Risk Functions Can Cross'</span>)</span>
<span id="cb4-918"><a href="#cb4-918" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-919"><a href="#cb4-919" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-920"><a href="#cb4-920" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb4-921"><a href="#cb4-921" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-922"><a href="#cb4-922" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-923"><a href="#cb4-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-924"><a href="#cb4-924" aria-hidden="true" tabindex="-1"></a>The constant estimator is actually better when $\theta$ is near 3 ("a broken clock is right twice a day"), but terrible elsewhere. Still, neither *uniformly* dominates the other.</span>
<span id="cb4-925"><a href="#cb4-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-926"><a href="#cb4-926" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-927"><a href="#cb4-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-928"><a href="#cb4-928" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb4-929"><a href="#cb4-929" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Bernoulli Estimation</span></span>
<span id="cb4-930"><a href="#cb4-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-931"><a href="#cb4-931" aria-hidden="true" tabindex="-1"></a>Consider $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$ with squared error loss. Let $S = \sum_{i=1}^n X_i$ be the number of successes. </span>
<span id="cb4-932"><a href="#cb4-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-933"><a href="#cb4-933" aria-hidden="true" tabindex="-1"></a>We'll compare two natural estimators by computing their risk functions.</span>
<span id="cb4-934"><a href="#cb4-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-935"><a href="#cb4-935" aria-hidden="true" tabindex="-1"></a>**Estimator 1: MLE**</span>
<span id="cb4-936"><a href="#cb4-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-937"><a href="#cb4-937" aria-hidden="true" tabindex="-1"></a>The MLE is $\hat{p}_1 = \bar{X} = \frac{S}{n}$</span>
<span id="cb4-938"><a href="#cb4-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-939"><a href="#cb4-939" aria-hidden="true" tabindex="-1"></a>Since $S \sim \text{Binomial}(n, p)$, we have:</span>
<span id="cb4-940"><a href="#cb4-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-941"><a href="#cb4-941" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}<span class="co">[</span><span class="ot">S</span><span class="co">]</span> = np$, so $\mathbb{E}<span class="co">[</span><span class="ot">\hat{p}_1</span><span class="co">]</span> = p$ (unbiased)</span>
<span id="cb4-942"><a href="#cb4-942" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(S) = np(1-p)$, so $\text{Var}(\hat{p}_1) = \frac{p(1-p)}{n}$</span>
<span id="cb4-943"><a href="#cb4-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-944"><a href="#cb4-944" aria-hidden="true" tabindex="-1"></a>The risk under squared error loss is:</span>
<span id="cb4-945"><a href="#cb4-945" aria-hidden="true" tabindex="-1"></a>$$R(p, \hat{p}_1) = \mathbb{E}<span class="co">[</span><span class="ot">(\hat{p}_1 - p)^2</span><span class="co">]</span> = \text{Var}(\hat{p}_1) + \text{Bias}^2(\hat{p}_1) = \frac{p(1-p)}{n} + 0 = \frac{p(1-p)}{n}$$</span>
<span id="cb4-946"><a href="#cb4-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-947"><a href="#cb4-947" aria-hidden="true" tabindex="-1"></a>This is a parabola with maximum at $p = 1/2$.</span>
<span id="cb4-948"><a href="#cb4-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-949"><a href="#cb4-949" aria-hidden="true" tabindex="-1"></a>**Estimator 2: Bayesian posterior mean with Beta(α, β) prior**</span>
<span id="cb4-950"><a href="#cb4-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-951"><a href="#cb4-951" aria-hidden="true" tabindex="-1"></a>Using Bayes' theorem with prior $p \sim \text{Beta}(\alpha, \beta)$ and observing $S$ successes:</span>
<span id="cb4-952"><a href="#cb4-952" aria-hidden="true" tabindex="-1"></a>$$p | S \sim \text{Beta}(\alpha + S, \beta + n - S)$$</span>
<span id="cb4-953"><a href="#cb4-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-954"><a href="#cb4-954" aria-hidden="true" tabindex="-1"></a>The posterior mean^<span class="co">[</span><span class="ot">The posterior mean is the Bayes estimator under squared error loss, as we will see in the following section.</span><span class="co">]</span> is:</span>
<span id="cb4-955"><a href="#cb4-955" aria-hidden="true" tabindex="-1"></a>$$\hat{p}_2 = \frac{\alpha + S}{\alpha + \beta + n}$$</span>
<span id="cb4-956"><a href="#cb4-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-957"><a href="#cb4-957" aria-hidden="true" tabindex="-1"></a>To find the risk, we compute bias and variance:</span>
<span id="cb4-958"><a href="#cb4-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-959"><a href="#cb4-959" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Expected value: $\mathbb{E}<span class="co">[</span><span class="ot">\hat{p}_2</span><span class="co">]</span> = \frac{\alpha + np}{\alpha + \beta + n}$</span>
<span id="cb4-960"><a href="#cb4-960" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias: $\text{Bias}(\hat{p}_2) = \frac{\alpha + np}{\alpha + \beta + n} - p = \frac{\alpha - p(\alpha + \beta)}{\alpha + \beta + n}$</span>
<span id="cb4-961"><a href="#cb4-961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variance: $\text{Var}(\hat{p}_2) = \text{Var}\left(\frac{S}{\alpha + \beta + n}\right) = \frac{np(1-p)}{(\alpha + \beta + n)^2}$</span>
<span id="cb4-962"><a href="#cb4-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-963"><a href="#cb4-963" aria-hidden="true" tabindex="-1"></a>Therefore, the general risk formula is:</span>
<span id="cb4-964"><a href="#cb4-964" aria-hidden="true" tabindex="-1"></a>$$R(p, \hat{p}_2) = \frac{np(1-p)}{(\alpha + \beta + n)^2} + \left(\frac{\alpha - p(\alpha + \beta)}{\alpha + \beta + n}\right)^2$$</span>
<span id="cb4-965"><a href="#cb4-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-966"><a href="#cb4-966" aria-hidden="true" tabindex="-1"></a>**Special case: Uniform prior Beta(1, 1)**</span>
<span id="cb4-967"><a href="#cb4-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-968"><a href="#cb4-968" aria-hidden="true" tabindex="-1"></a>For $\alpha = \beta = 1$, the posterior mean becomes:</span>
<span id="cb4-969"><a href="#cb4-969" aria-hidden="true" tabindex="-1"></a>$$\hat{p}_2 = \frac{1 + S}{2 + n} = \frac{n}{n+2} \cdot \frac{S}{n} + \frac{2}{n+2} \cdot \frac{1}{2}$$</span>
<span id="cb4-970"><a href="#cb4-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-971"><a href="#cb4-971" aria-hidden="true" tabindex="-1"></a>This is a weighted average of the MLE and the prior mean 1/2.</span>
<span id="cb4-972"><a href="#cb4-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-973"><a href="#cb4-973" aria-hidden="true" tabindex="-1"></a>The risk specializes to:</span>
<span id="cb4-974"><a href="#cb4-974" aria-hidden="true" tabindex="-1"></a>$$R(p, \hat{p}_2) = \frac{np(1-p)}{(n+2)^2} + \left(\frac{1 - 2p}{n + 2}\right)^2$$</span>
<span id="cb4-975"><a href="#cb4-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-976"><a href="#cb4-976" aria-hidden="true" tabindex="-1"></a>Let's plot both risk functions to see how they compare:</span>
<span id="cb4-977"><a href="#cb4-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-980"><a href="#cb4-980" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-981"><a href="#cb4-981" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb4-982"><a href="#cb4-982" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb4-983"><a href="#cb4-983" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb4-984"><a href="#cb4-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-985"><a href="#cb4-985" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb4-986"><a href="#cb4-986" aria-hidden="true" tabindex="-1"></a>p_range <span class="op">=</span> np.linspace(<span class="fl">0.001</span>, <span class="fl">0.999</span>, <span class="dv">200</span>)</span>
<span id="cb4-987"><a href="#cb4-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-988"><a href="#cb4-988" aria-hidden="true" tabindex="-1"></a><span class="co"># Risk for MLE: R(p, p̂₁) = p(1-p)/n</span></span>
<span id="cb4-989"><a href="#cb4-989" aria-hidden="true" tabindex="-1"></a>risk_mle <span class="op">=</span> p_range <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_range) <span class="op">/</span> n</span>
<span id="cb4-990"><a href="#cb4-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-991"><a href="#cb4-991" aria-hidden="true" tabindex="-1"></a><span class="co"># Risk for Bayes with uniform prior Beta(1,1)</span></span>
<span id="cb4-992"><a href="#cb4-992" aria-hidden="true" tabindex="-1"></a>alpha, beta <span class="op">=</span> <span class="dv">1</span>, <span class="dv">1</span></span>
<span id="cb4-993"><a href="#cb4-993" aria-hidden="true" tabindex="-1"></a>risk_bayes <span class="op">=</span> []</span>
<span id="cb4-994"><a href="#cb4-994" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> p_range:</span>
<span id="cb4-995"><a href="#cb4-995" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The Bayes estimator is (nX̄ + 1)/(n + 2)</span></span>
<span id="cb4-996"><a href="#cb4-996" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Its expected value is (np + 1)/(n + 2)</span></span>
<span id="cb4-997"><a href="#cb4-997" aria-hidden="true" tabindex="-1"></a>    bayes_mean <span class="op">=</span> (n<span class="op">*</span>p <span class="op">+</span> alpha) <span class="op">/</span> (n <span class="op">+</span> alpha <span class="op">+</span> beta)</span>
<span id="cb4-998"><a href="#cb4-998" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias = E[estimator] - true value</span></span>
<span id="cb4-999"><a href="#cb4-999" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> bayes_mean <span class="op">-</span> p</span>
<span id="cb4-1000"><a href="#cb4-1000" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Variance of the Bayes estimator</span></span>
<span id="cb4-1001"><a href="#cb4-1001" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> (n <span class="op">*</span> p <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>p)) <span class="op">/</span> ((n <span class="op">+</span> alpha <span class="op">+</span> beta)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-1002"><a href="#cb4-1002" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Risk = Variance + Bias²</span></span>
<span id="cb4-1003"><a href="#cb4-1003" aria-hidden="true" tabindex="-1"></a>    risk_bayes.append(var <span class="op">+</span> bias<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-1004"><a href="#cb4-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1005"><a href="#cb4-1005" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb4-1006"><a href="#cb4-1006" aria-hidden="true" tabindex="-1"></a>plt.plot(p_range, risk_mle, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb4-1007"><a href="#cb4-1007" aria-hidden="true" tabindex="-1"></a>plt.plot(p_range, risk_bayes, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Bayes (uniform prior)'</span>)</span>
<span id="cb4-1008"><a href="#cb4-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1009"><a href="#cb4-1009" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight where each is better</span></span>
<span id="cb4-1010"><a href="#cb4-1010" aria-hidden="true" tabindex="-1"></a>plt.fill_between(p_range, <span class="dv">0</span>, risk_mle, where<span class="op">=</span>(np.array(risk_mle) <span class="op">&gt;</span> np.array(risk_bayes)), </span>
<span id="cb4-1011"><a href="#cb4-1011" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Bayes better here'</span>)</span>
<span id="cb4-1012"><a href="#cb4-1012" aria-hidden="true" tabindex="-1"></a>plt.fill_between(p_range, <span class="dv">0</span>, risk_mle, where<span class="op">=</span>(np.array(risk_mle) <span class="op">&lt;=</span> np.array(risk_bayes)), </span>
<span id="cb4-1013"><a href="#cb4-1013" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'MLE better here'</span>)</span>
<span id="cb4-1014"><a href="#cb4-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1015"><a href="#cb4-1015" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p'</span>)</span>
<span id="cb4-1016"><a href="#cb4-1016" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Risk'</span>)</span>
<span id="cb4-1017"><a href="#cb4-1017" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Risk Functions Cross - Neither Estimator Dominates (n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb4-1018"><a href="#cb4-1018" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-1019"><a href="#cb4-1019" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-1020"><a href="#cb4-1020" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-1021"><a href="#cb4-1021" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-1022"><a href="#cb4-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1023"><a href="#cb4-1023" aria-hidden="true" tabindex="-1"></a>The risk functions cross! The MLE is better near the extremes (p near 0 or 1), while the Bayes estimator is better near the middle (p near 1/2). Neither estimator uniformly dominates the other.</span>
<span id="cb4-1024"><a href="#cb4-1024" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1025"><a href="#cb4-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1026"><a href="#cb4-1026" aria-hidden="true" tabindex="-1"></a>Both examples above illustrate a fundamental challenge in decision theory: when risk functions cross, we cannot declare one estimator uniformly better than another. Different estimators excel in different regions of the parameter space. </span>
<span id="cb4-1027"><a href="#cb4-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1028"><a href="#cb4-1028" aria-hidden="true" tabindex="-1"></a>To make a decision on the estimator to use, we must reduce these risk curves to single numbers that we can compare.</span>
<span id="cb4-1029"><a href="#cb4-1029" aria-hidden="true" tabindex="-1"></a>But how should we summarize an entire function? Should we care most about average performance or worst-case performance? Different answers to this question lead to two distinct optimality criteria: **Bayes estimators** (optimizing average risk) and **minimax estimators** (optimizing worst-case risk), detailed in the next section.</span>
<span id="cb4-1030"><a href="#cb4-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1031"><a href="#cb4-1031" aria-hidden="true" tabindex="-1"></a><span class="fu">## Optimal Estimators: Bayes and Minimax Rules</span></span>
<span id="cb4-1032"><a href="#cb4-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1033"><a href="#cb4-1033" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Bayesian Approach: Minimizing Average Risk</span></span>
<span id="cb4-1034"><a href="#cb4-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1035"><a href="#cb4-1035" aria-hidden="true" tabindex="-1"></a>The Bayesian approach to decision theory averages the risk over a prior distribution, giving us a single number to minimize.</span>
<span id="cb4-1036"><a href="#cb4-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1037"><a href="#cb4-1037" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-1038"><a href="#cb4-1038" aria-hidden="true" tabindex="-1"></a>**Bayes Risk**: The expected risk, averaged over the prior distribution $f(\theta)$:</span>
<span id="cb4-1039"><a href="#cb4-1039" aria-hidden="true" tabindex="-1"></a>$$r(f, \hat{\theta}) = \int R(\theta, \hat{\theta}) f(\theta) d\theta$$</span>
<span id="cb4-1040"><a href="#cb4-1040" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1041"><a href="#cb4-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1042"><a href="#cb4-1042" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-1043"><a href="#cb4-1043" aria-hidden="true" tabindex="-1"></a>**Bayes Estimator**: The estimator $\hat{\theta}^B$ that minimizes the Bayes risk:</span>
<span id="cb4-1044"><a href="#cb4-1044" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}^B = \arg\min_{\hat{\theta}} r(f, \hat{\theta})$$</span>
<span id="cb4-1045"><a href="#cb4-1045" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1046"><a href="#cb4-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1047"><a href="#cb4-1047" aria-hidden="true" tabindex="-1"></a>The remarkable connection between Bayesian inference and decision theory:</span>
<span id="cb4-1048"><a href="#cb4-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1049"><a href="#cb4-1049" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Finding the Bayes Estimator"}</span>
<span id="cb4-1050"><a href="#cb4-1050" aria-hidden="true" tabindex="-1"></a>The Bayes estimator can be found by minimizing the *posterior* expected loss for each observed $x$. Specifically:</span>
<span id="cb4-1051"><a href="#cb4-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1052"><a href="#cb4-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For **Squared Error Loss**: The Bayes estimator is the **Posterior Mean**</span>
<span id="cb4-1053"><a href="#cb4-1053" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For **Absolute Error Loss**: The Bayes estimator is the **Posterior Median**  </span>
<span id="cb4-1054"><a href="#cb4-1054" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For **Zero-One Loss**: The Bayes estimator is the **Posterior Mode (MAP)**</span>
<span id="cb4-1055"><a href="#cb4-1055" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1056"><a href="#cb4-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1057"><a href="#cb4-1057" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-1058"><a href="#cb4-1058" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof for Different Loss Functions</span></span>
<span id="cb4-1059"><a href="#cb4-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1060"><a href="#cb4-1060" aria-hidden="true" tabindex="-1"></a>For any loss function $L(\theta, \hat{\theta})$, the Bayes estimator minimizes the posterior expected loss:</span>
<span id="cb4-1061"><a href="#cb4-1061" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}^B(x) = \arg\min_a \mathbb{E}<span class="co">[</span><span class="ot">L(\theta, a) | X = x</span><span class="co">]</span> = \arg\min_a \int L(\theta, a) f(\theta|x) d\theta$$</span>
<span id="cb4-1062"><a href="#cb4-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1063"><a href="#cb4-1063" aria-hidden="true" tabindex="-1"></a>**Squared Error Loss: $L(\theta, a) = (\theta - a)^2$**</span>
<span id="cb4-1064"><a href="#cb4-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1065"><a href="#cb4-1065" aria-hidden="true" tabindex="-1"></a>We need to minimize:</span>
<span id="cb4-1066"><a href="#cb4-1066" aria-hidden="true" tabindex="-1"></a>$$\int (\theta - a)^2 f(\theta|x) d\theta$$</span>
<span id="cb4-1067"><a href="#cb4-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1068"><a href="#cb4-1068" aria-hidden="true" tabindex="-1"></a>Taking the derivative with respect to $a$ and setting to zero:</span>
<span id="cb4-1069"><a href="#cb4-1069" aria-hidden="true" tabindex="-1"></a>$$\frac{d}{da} \int (\theta - a)^2 f(\theta|x) d\theta = -2 \int (\theta - a) f(\theta|x) d\theta = 0$$</span>
<span id="cb4-1070"><a href="#cb4-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1071"><a href="#cb4-1071" aria-hidden="true" tabindex="-1"></a>This gives:</span>
<span id="cb4-1072"><a href="#cb4-1072" aria-hidden="true" tabindex="-1"></a>$$\int \theta f(\theta|x) d\theta = a \int f(\theta|x) d\theta = a$$</span>
<span id="cb4-1073"><a href="#cb4-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1074"><a href="#cb4-1074" aria-hidden="true" tabindex="-1"></a>Therefore: $\hat{\theta}^B(x) = \int \theta f(\theta|x) d\theta = \mathbb{E}<span class="co">[</span><span class="ot">\theta | X = x</span><span class="co">]</span>$ (posterior mean)</span>
<span id="cb4-1075"><a href="#cb4-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1076"><a href="#cb4-1076" aria-hidden="true" tabindex="-1"></a>**Absolute Error Loss: $L(\theta, a) = |\theta - a|$**</span>
<span id="cb4-1077"><a href="#cb4-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1078"><a href="#cb4-1078" aria-hidden="true" tabindex="-1"></a>We need to minimize:</span>
<span id="cb4-1079"><a href="#cb4-1079" aria-hidden="true" tabindex="-1"></a>$$\int |\theta - a| f(\theta|x) d\theta = \int_{-\infty}^a (a - \theta) f(\theta|x) d\theta + \int_a^{\infty} (\theta - a) f(\theta|x) d\theta$$</span>
<span id="cb4-1080"><a href="#cb4-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1081"><a href="#cb4-1081" aria-hidden="true" tabindex="-1"></a>Taking the derivative with respect to $a$:</span>
<span id="cb4-1082"><a href="#cb4-1082" aria-hidden="true" tabindex="-1"></a>$$\frac{d}{da} = \int_{-\infty}^a f(\theta|x) d\theta - \int_a^{\infty} f(\theta|x) d\theta = F(a|x) - (1 - F(a|x)) = 2F(a|x) - 1$$</span>
<span id="cb4-1083"><a href="#cb4-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1084"><a href="#cb4-1084" aria-hidden="true" tabindex="-1"></a>Setting to zero: $F(a|x) = 1/2$, so $\hat{\theta}^B(x)$ is the posterior median.</span>
<span id="cb4-1085"><a href="#cb4-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1086"><a href="#cb4-1086" aria-hidden="true" tabindex="-1"></a>**Zero-One Loss: $L(\theta, a) = \mathbb{1}\{\theta \neq a\}$**</span>
<span id="cb4-1087"><a href="#cb4-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1088"><a href="#cb4-1088" aria-hidden="true" tabindex="-1"></a>The expected loss is:</span>
<span id="cb4-1089"><a href="#cb4-1089" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">L(\theta, a) | X = x</span><span class="co">]</span> = P(\theta \neq a | X = x) = 1 - P(\theta = a | X = x)$$</span>
<span id="cb4-1090"><a href="#cb4-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1091"><a href="#cb4-1091" aria-hidden="true" tabindex="-1"></a>This is minimized when $P(\theta = a | X = x)$ is maximized, which occurs at the posterior mode.</span>
<span id="cb4-1092"><a href="#cb4-1092" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1093"><a href="#cb4-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1094"><a href="#cb4-1094" aria-hidden="true" tabindex="-1"></a>This theorem reveals a profound insight: Bayesian inference naturally produces optimal estimators! The posterior summaries we compute for inference are exactly the estimators that minimize expected loss.</span>
<span id="cb4-1095"><a href="#cb4-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1096"><a href="#cb4-1096" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Frequentist Approach: Minimizing Worst-Case Risk</span></span>
<span id="cb4-1097"><a href="#cb4-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1098"><a href="#cb4-1098" aria-hidden="true" tabindex="-1"></a>The minimax approach takes a pessimistic view: prepare for the worst case.</span>
<span id="cb4-1099"><a href="#cb4-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1100"><a href="#cb4-1100" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-1101"><a href="#cb4-1101" aria-hidden="true" tabindex="-1"></a>**Maximum Risk**: The worst-case risk over the entire parameter space $\Theta$:</span>
<span id="cb4-1102"><a href="#cb4-1102" aria-hidden="true" tabindex="-1"></a>$$\bar{R}(\hat{\theta}) = \sup_{\theta \in \Theta} R(\theta, \hat{\theta})$$</span>
<span id="cb4-1103"><a href="#cb4-1103" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1104"><a href="#cb4-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1105"><a href="#cb4-1105" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-1106"><a href="#cb4-1106" aria-hidden="true" tabindex="-1"></a>**Minimax Estimator**: The estimator $\hat{\theta}^{MM}$ with the smallest maximum risk:</span>
<span id="cb4-1107"><a href="#cb4-1107" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}^{MM} = \arg\min_{\hat{\theta}} \sup_{\theta} R(\theta, \hat{\theta})$$</span>
<span id="cb4-1108"><a href="#cb4-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1109"><a href="#cb4-1109" aria-hidden="true" tabindex="-1"></a>It's the "best of the worst-case" estimators.</span>
<span id="cb4-1110"><a href="#cb4-1110" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1111"><a href="#cb4-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1112"><a href="#cb4-1112" aria-hidden="true" tabindex="-1"></a>Finding minimax estimators directly is usually difficult. However, there's a powerful connection to Bayes estimators:</span>
<span id="cb4-1113"><a href="#cb4-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1114"><a href="#cb4-1114" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Constant Risk Bayes Rules are Minimax"}</span>
<span id="cb4-1115"><a href="#cb4-1115" aria-hidden="true" tabindex="-1"></a>If a Bayes estimator has constant risk (the same risk for all $\theta$), then it is minimax.</span>
<span id="cb4-1116"><a href="#cb4-1116" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1117"><a href="#cb4-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1118"><a href="#cb4-1118" aria-hidden="true" tabindex="-1"></a>This gives us a recipe: find a prior such that the resulting Bayes estimator has constant risk.</span>
<span id="cb4-1119"><a href="#cb4-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1120"><a href="#cb4-1120" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb4-1121"><a href="#cb4-1121" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Minimax Estimator for Bernoulli</span></span>
<span id="cb4-1122"><a href="#cb4-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1123"><a href="#cb4-1123" aria-hidden="true" tabindex="-1"></a>Consider $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$ with squared error loss. We know from the previous example that the Bayes estimator with prior $\text{Beta}(\alpha, \beta)$ has risk:</span>
<span id="cb4-1124"><a href="#cb4-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1125"><a href="#cb4-1125" aria-hidden="true" tabindex="-1"></a>$$R(p, \hat{p}) = \frac{np(1-p)}{(\alpha + \beta + n)^2} + \left(\frac{\alpha - p(\alpha + \beta)}{\alpha + \beta + n}\right)^2$$</span>
<span id="cb4-1126"><a href="#cb4-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1127"><a href="#cb4-1127" aria-hidden="true" tabindex="-1"></a>**The key insight**: Can we choose $\alpha$ and $\beta$ to make this risk constant (independent of $p$)? If so, the constant risk theorem tells us the resulting estimator would be minimax.</span>
<span id="cb4-1128"><a href="#cb4-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1129"><a href="#cb4-1129" aria-hidden="true" tabindex="-1"></a>It turns out that setting $\alpha = \beta = \sqrt{n/4}$ does exactly this!</span>
<span id="cb4-1130"><a href="#cb4-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1131"><a href="#cb4-1131" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-1132"><a href="#cb4-1132" aria-hidden="true" tabindex="-1"></a><span class="fu">## Derivation of the minimax prior</span></span>
<span id="cb4-1133"><a href="#cb4-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1134"><a href="#cb4-1134" aria-hidden="true" tabindex="-1"></a>If we set $\alpha = \beta$, the risk becomes:</span>
<span id="cb4-1135"><a href="#cb4-1135" aria-hidden="true" tabindex="-1"></a>$$R(p, \hat{p}) = \frac{1}{(2\alpha + n)^2}\left<span class="co">[</span><span class="ot">np(1-p) + \alpha^2(1 - 2p)^2\right</span><span class="co">]</span>$$</span>
<span id="cb4-1136"><a href="#cb4-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1137"><a href="#cb4-1137" aria-hidden="true" tabindex="-1"></a>Expanding the term in brackets:</span>
<span id="cb4-1138"><a href="#cb4-1138" aria-hidden="true" tabindex="-1"></a>$$np(1-p) + \alpha^2(1 - 2p)^2 = np - np^2 + \alpha^2(1 - 4p + 4p^2)$$</span>
<span id="cb4-1139"><a href="#cb4-1139" aria-hidden="true" tabindex="-1"></a>$$= \alpha^2 + (n - 4\alpha^2)p + (4\alpha^2 - n)p^2$$</span>
<span id="cb4-1140"><a href="#cb4-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1141"><a href="#cb4-1141" aria-hidden="true" tabindex="-1"></a>This is constant if and only if the coefficients of $p$ and $p^2$ are both zero:</span>
<span id="cb4-1142"><a href="#cb4-1142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coefficient of $p$: $n - 4\alpha^2 = 0 \Rightarrow \alpha^2 = n/4$</span>
<span id="cb4-1143"><a href="#cb4-1143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coefficient of $p^2$: $4\alpha^2 - n = 0 \Rightarrow \alpha^2 = n/4$ ✓</span>
<span id="cb4-1144"><a href="#cb4-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1145"><a href="#cb4-1145" aria-hidden="true" tabindex="-1"></a>Both conditions give the same answer: $\alpha = \sqrt{n/4}$.</span>
<span id="cb4-1146"><a href="#cb4-1146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1147"><a href="#cb4-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1148"><a href="#cb4-1148" aria-hidden="true" tabindex="-1"></a>With $\alpha = \beta = \sqrt{n/4}$:</span>
<span id="cb4-1149"><a href="#cb4-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1150"><a href="#cb4-1150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The Bayes estimator is: $\hat{p} = \frac{S + \sqrt{n/4}}{n + \sqrt{n}}$</span>
<span id="cb4-1151"><a href="#cb4-1151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The risk is constant: $R(p, \hat{p}) = \frac{n}{4(n + \sqrt{n})^2}$ for all $p$</span>
<span id="cb4-1152"><a href="#cb4-1152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>By the theorem above, this makes it minimax!</span>
<span id="cb4-1153"><a href="#cb4-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1154"><a href="#cb4-1154" aria-hidden="true" tabindex="-1"></a>**How does this minimax estimator compare to the MLE?**</span>
<span id="cb4-1155"><a href="#cb4-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1156"><a href="#cb4-1156" aria-hidden="true" tabindex="-1"></a>By maximum risk (worst-case criterion):</span>
<span id="cb4-1157"><a href="#cb4-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1158"><a href="#cb4-1158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MLE: Maximum risk is $\frac{1}{4n}$ (at $p = 1/2$)</span>
<span id="cb4-1159"><a href="#cb4-1159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Minimax: Constant risk $\frac{n}{4(n+\sqrt{n})^2} &lt; \frac{1}{4n}$</span>
<span id="cb4-1160"><a href="#cb4-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1161"><a href="#cb4-1161" aria-hidden="true" tabindex="-1"></a>The minimax estimator wins on worst-case performance - that's what it was designed for!</span>
<span id="cb4-1162"><a href="#cb4-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1163"><a href="#cb4-1163" aria-hidden="true" tabindex="-1"></a>But here's the interesting part: even though the minimax estimator was derived from a Beta($\sqrt{n/4}, \sqrt{n/4}$) prior, we can ask how it performs on average under *any* prior. For instance, under a uniform prior:</span>
<span id="cb4-1164"><a href="#cb4-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1165"><a href="#cb4-1165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MLE: Bayes risk = $\frac{1}{6n}$</span>
<span id="cb4-1166"><a href="#cb4-1166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Minimax: Bayes risk = $\frac{n}{4(n+\sqrt{n})^2}$</span>
<span id="cb4-1167"><a href="#cb4-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1168"><a href="#cb4-1168" aria-hidden="true" tabindex="-1"></a>For $n \geq 20$, the MLE has lower average risk under the uniform prior. This illustrates a key principle: **the minimax estimator optimizes worst-case performance, but may sacrifice average-case performance to achieve this robustness**.</span>
<span id="cb4-1169"><a href="#cb4-1169" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1170"><a href="#cb4-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1171"><a href="#cb4-1171" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb4-1172"><a href="#cb4-1172" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Minimax Estimator for Normal Mean</span></span>
<span id="cb4-1173"><a href="#cb4-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1174"><a href="#cb4-1174" aria-hidden="true" tabindex="-1"></a>For $X_1, \ldots, X_n \sim \mathcal{N}(\theta, 1)$, the sample mean $\bar{X}$ has risk:</span>
<span id="cb4-1175"><a href="#cb4-1175" aria-hidden="true" tabindex="-1"></a>$$R(\theta, \bar{X}) = \mathbb{E}<span class="co">[</span><span class="ot">(\bar{X} - \theta)^2</span><span class="co">]</span> = \text{Var}(\bar{X}) = \frac{1}{n}$$</span>
<span id="cb4-1176"><a href="#cb4-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1177"><a href="#cb4-1177" aria-hidden="true" tabindex="-1"></a>This risk is constant (doesn't depend on $\theta$). Furthermore, $\bar{X}$ can be shown to be admissible (as it is the limit of admissible Bayes estimators for Normal priors). An admissible estimator with constant risk is minimax. Therefore, $\bar{X}$ is minimax.</span>
<span id="cb4-1178"><a href="#cb4-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1179"><a href="#cb4-1179" aria-hidden="true" tabindex="-1"></a>This result holds for any "well-behaved" loss function (convex and symmetric about the origin).</span>
<span id="cb4-1180"><a href="#cb4-1180" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1181"><a href="#cb4-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1182"><a href="#cb4-1182" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb4-1183"><a href="#cb4-1183" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Large Sample MLE</span></span>
<span id="cb4-1184"><a href="#cb4-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1185"><a href="#cb4-1185" aria-hidden="true" tabindex="-1"></a>In most parametric models with large $n$, the MLE is approximately minimax. The intuition: as $n \rightarrow \infty$, the MLE becomes approximately Normal with variance $1/(nI(\theta))$ where $I(\theta)$ is the Fisher information. In many regular models, this leads to approximately constant risk.</span>
<span id="cb4-1186"><a href="#cb4-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1187"><a href="#cb4-1187" aria-hidden="true" tabindex="-1"></a>**Important caveat**: This breaks down when the number of parameters grows with $n$. For example, in the "many Normal means" problem where we estimate $n$ means from $n$ observations, the MLE is far from minimax.</span>
<span id="cb4-1188"><a href="#cb4-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1189"><a href="#cb4-1189" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1190"><a href="#cb4-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1191"><a href="#cb4-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">## Admissibility: Ruling Out Bad Estimators</span></span>
<span id="cb4-1192"><a href="#cb4-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1193"><a href="#cb4-1193" aria-hidden="true" tabindex="-1"></a>Minimax and Bayes estimators tell us about optimality according to specific criteria. But there's a more basic requirement: an estimator shouldn't be uniformly worse than another.</span>
<span id="cb4-1194"><a href="#cb4-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1195"><a href="#cb4-1195" aria-hidden="true" tabindex="-1"></a><span class="fu">### Defining Admissibility</span></span>
<span id="cb4-1196"><a href="#cb4-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1197"><a href="#cb4-1197" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb4-1198"><a href="#cb4-1198" aria-hidden="true" tabindex="-1"></a>An estimator $\hat{\theta}$ is **inadmissible** if there exists another estimator $\hat{\theta}'$ such that:</span>
<span id="cb4-1199"><a href="#cb4-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1200"><a href="#cb4-1200" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$R(\theta, \hat{\theta}') \le R(\theta, \hat{\theta})$ for all $\theta$</span>
<span id="cb4-1201"><a href="#cb4-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$R(\theta, \hat{\theta}') &lt; R(\theta, \hat{\theta})$ for at least one $\theta$</span>
<span id="cb4-1202"><a href="#cb4-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1203"><a href="#cb4-1203" aria-hidden="true" tabindex="-1"></a>Otherwise, $\hat{\theta}$ is **admissible**.</span>
<span id="cb4-1204"><a href="#cb4-1204" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1205"><a href="#cb4-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1206"><a href="#cb4-1206" aria-hidden="true" tabindex="-1"></a>An inadmissible estimator is dominated -- there's another estimator that's never worse and sometimes better. Using an inadmissible estimator is irrational.</span>
<span id="cb4-1207"><a href="#cb4-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1208"><a href="#cb4-1208" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-1209"><a href="#cb4-1209" aria-hidden="true" tabindex="-1"></a><span class="fu">## Admissibility ≠ Good</span></span>
<span id="cb4-1210"><a href="#cb4-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1211"><a href="#cb4-1211" aria-hidden="true" tabindex="-1"></a>The constant estimator $\hat{\theta} = 3$ for $X \sim \mathcal{N}(\theta, 1)$ is admissible! Why? Any estimator that beats it at $\theta = 3$ must be worse elsewhere. But it's still a terrible estimator for most purposes.</span>
<span id="cb4-1212"><a href="#cb4-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1213"><a href="#cb4-1213" aria-hidden="true" tabindex="-1"></a>Admissibility is a necessary but not sufficient condition for a good estimator.</span>
<span id="cb4-1214"><a href="#cb4-1214" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1215"><a href="#cb4-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1216"><a href="#cb4-1216" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Properties and Connections</span></span>
<span id="cb4-1217"><a href="#cb4-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1218"><a href="#cb4-1218" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Bayes Rules are Admissible"}</span>
<span id="cb4-1219"><a href="#cb4-1219" aria-hidden="true" tabindex="-1"></a>A Bayes estimator for a prior with full support (positive density everywhere) is always admissible.</span>
<span id="cb4-1220"><a href="#cb4-1220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1221"><a href="#cb4-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1222"><a href="#cb4-1222" aria-hidden="true" tabindex="-1"></a>This is powerful: Bayesian methods automatically avoid inadmissible estimators.</span>
<span id="cb4-1223"><a href="#cb4-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1224"><a href="#cb4-1224" aria-hidden="true" tabindex="-1"></a>**Other connections**:</span>
<span id="cb4-1225"><a href="#cb4-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1226"><a href="#cb4-1226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Constant Risk and Admissibility**: An admissible estimator with constant risk is minimax</span>
<span id="cb4-1227"><a href="#cb4-1227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Minimax and Admissibility**: Minimax estimators are usually admissible or "nearly" admissible</span>
<span id="cb4-1228"><a href="#cb4-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MLE and Admissibility**: The MLE is not always admissible, especially in high dimensions</span>
<span id="cb4-1229"><a href="#cb4-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1230"><a href="#cb4-1230" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-1231"><a href="#cb4-1231" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Stein's Paradox and Shrinkage</span></span>
<span id="cb4-1232"><a href="#cb4-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1233"><a href="#cb4-1233" aria-hidden="true" tabindex="-1"></a>Consider estimating $k \geq 3$ Normal means simultaneously. Let $Y_i \sim \mathcal{N}(\theta_i, 1)$ for $i = 1, ..., k$.</span>
<span id="cb4-1234"><a href="#cb4-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1235"><a href="#cb4-1235" aria-hidden="true" tabindex="-1"></a>**The Setup**: We want to estimate $\theta = (\theta_1, ..., \theta_k)$ with total squared error loss:</span>
<span id="cb4-1236"><a href="#cb4-1236" aria-hidden="true" tabindex="-1"></a>$$L(\theta, \hat{\theta}) = \sum_{i=1}^k (\theta_i - \hat{\theta}_i)^2$$</span>
<span id="cb4-1237"><a href="#cb4-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1238"><a href="#cb4-1238" aria-hidden="true" tabindex="-1"></a>**The Paradox**: The "obvious" estimator $\hat{\theta}_i = Y_i$ (using each observation to estimate its own mean) is inadmissible when $k \geq 3$!</span>
<span id="cb4-1239"><a href="#cb4-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1240"><a href="#cb4-1240" aria-hidden="true" tabindex="-1"></a>**The Solution**: The James-Stein estimator</span>
<span id="cb4-1241"><a href="#cb4-1241" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_i^{JS} = \left(1 - \frac{k-2}{\sum_j Y_j^2}\right)^+ Y_i$$</span>
<span id="cb4-1242"><a href="#cb4-1242" aria-hidden="true" tabindex="-1"></a>"shrinks" estimates toward zero and has uniformly lower risk than the MLE.</span>
<span id="cb4-1243"><a href="#cb4-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1244"><a href="#cb4-1244" aria-hidden="true" tabindex="-1"></a>**The Importance**: This counterintuitive result revolutionized high-dimensional statistics. It shows that when estimating many parameters simultaneously, we can improve by "borrowing strength" across parameters. This is the foundation of modern regularization methods in machine learning.</span>
<span id="cb4-1245"><a href="#cb4-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1246"><a href="#cb4-1246" aria-hidden="true" tabindex="-1"></a>The key insight: in high dimensions, the MLE can be improved by shrinkage toward a common value.</span>
<span id="cb4-1247"><a href="#cb4-1247" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1248"><a href="#cb4-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1249"><a href="#cb4-1249" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb4-1250"><a href="#cb4-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1251"><a href="#cb4-1251" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb4-1252"><a href="#cb4-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1253"><a href="#cb4-1253" aria-hidden="true" tabindex="-1"></a>**Bayesian Inference**:</span>
<span id="cb4-1254"><a href="#cb4-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1255"><a href="#cb4-1255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Posterior ∝ Likelihood × Prior**: Bayes' theorem provides the recipe for updating beliefs</span>
<span id="cb4-1256"><a href="#cb4-1256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Conjugate models**: Beta-Bernoulli and Normal-Normal give closed-form posteriors</span>
<span id="cb4-1257"><a href="#cb4-1257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Prior choice matters**: Conjugate (convenient), non-informative (problematic), weakly informative (recommended)</span>
<span id="cb4-1258"><a href="#cb4-1258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Credible intervals**: Direct probability statements about parameters</span>
<span id="cb4-1259"><a href="#cb4-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1260"><a href="#cb4-1260" aria-hidden="true" tabindex="-1"></a>**Statistical Decision Theory**:</span>
<span id="cb4-1261"><a href="#cb4-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1262"><a href="#cb4-1262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Loss functions**: Quantify the cost of estimation errors</span>
<span id="cb4-1263"><a href="#cb4-1263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Risk functions**: Expected loss -- curves that are hard to compare</span>
<span id="cb4-1264"><a href="#cb4-1264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bayes estimators**: Minimize average risk over a prior</span>
<span id="cb4-1265"><a href="#cb4-1265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Minimax estimators**: Minimize worst-case risk</span>
<span id="cb4-1266"><a href="#cb4-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1267"><a href="#cb4-1267" aria-hidden="true" tabindex="-1"></a>**Key Connections**:</span>
<span id="cb4-1268"><a href="#cb4-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1269"><a href="#cb4-1269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Posterior mean = Bayes estimator for squared error loss</span>
<span id="cb4-1270"><a href="#cb4-1270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Constant risk Bayes rules are minimax</span>
<span id="cb4-1271"><a href="#cb4-1271" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayes rules are admissible</span>
<span id="cb4-1272"><a href="#cb4-1272" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In large samples, Bayesian and frequentist methods converge</span>
<span id="cb4-1273"><a href="#cb4-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1274"><a href="#cb4-1274" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Big Picture</span></span>
<span id="cb4-1275"><a href="#cb4-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1276"><a href="#cb4-1276" aria-hidden="true" tabindex="-1"></a>This chapter revealed two fundamental insights:</span>
<span id="cb4-1277"><a href="#cb4-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1278"><a href="#cb4-1278" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bayesian inference provides a unified, probabilistic framework** for learning from data. By treating parameters as random variables, we can make direct probability statements and naturally incorporate prior knowledge.</span>
<span id="cb4-1279"><a href="#cb4-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1280"><a href="#cb4-1280" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Decision theory provides a formal language** for evaluating and comparing any statistical procedure. The posterior mean is not just an arbitrary summary -- it's the optimal estimator under squared error loss.</span>
<span id="cb4-1281"><a href="#cb4-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1282"><a href="#cb4-1282" aria-hidden="true" tabindex="-1"></a>The connection runs deeper: Bayesian methods naturally produce optimal estimators, while decision theory helps us understand when and why different approaches work well. Even frequentist stalwarts use decision theory, and the best frequentist estimators often have Bayesian interpretations.</span>
<span id="cb4-1283"><a href="#cb4-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1284"><a href="#cb4-1284" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb4-1285"><a href="#cb4-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1286"><a href="#cb4-1286" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Confusing credible and confidence intervals**: A 95% credible interval contains $\theta$ with probability 0.95 given the data.^<span class="co">[</span><span class="ot">For a given model and prior.</span><span class="co">]</span> A 95% confidence interval is produced by a procedure that traps $\theta$ in 95% of repeated experiments.</span>
<span id="cb4-1287"><a href="#cb4-1287" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Thinking uniform priors are "uninformative"**: They encode specific beliefs and aren't transformation invariant.</span>
<span id="cb4-1288"><a href="#cb4-1288" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Using conjugate priors blindly**: Convenience shouldn't override reasonable prior beliefs.</span>
<span id="cb4-1289"><a href="#cb4-1289" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Forgetting the prior's influence diminishes**: With enough data, different reasonable priors lead to similar posteriors.^<span class="co">[</span><span class="ot">In regular, identifiable parametric models; this can fail in high dimensions or weakly identified settings.</span><span class="co">]</span></span>
<span id="cb4-1290"><a href="#cb4-1290" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Assuming admissible = good**: The constant estimator $\hat{\theta} = 3$ is admissible but useless.</span>
<span id="cb4-1291"><a href="#cb4-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1292"><a href="#cb4-1292" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb4-1293"><a href="#cb4-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1294"><a href="#cb4-1294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Previous (Ch. 5-7)**: We learned frequentist methods for finding and evaluating estimators. Now we have a completely different paradigm (Bayesian) and a unified theory (decision theory) for comparing estimators from any paradigm.</span>
<span id="cb4-1295"><a href="#cb4-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1296"><a href="#cb4-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**This Chapter**: Introduced Bayesian thinking and formal decision theory. These provide alternative and complementary approaches to the frequentist methods we've studied.</span>
<span id="cb4-1297"><a href="#cb4-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1298"><a href="#cb4-1298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next (Ch. 10)**: We'll see how modern computational methods (MCMC, Stan) make Bayesian inference practical for complex models where conjugacy doesn't help.</span>
<span id="cb4-1299"><a href="#cb4-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1300"><a href="#cb4-1300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications**: Bayesian methods shine in hierarchical models, missing data problems, and anywhere prior information is valuable.</span>
<span id="cb4-1301"><a href="#cb4-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1302"><a href="#cb4-1302" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb4-1303"><a href="#cb4-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1304"><a href="#cb4-1304" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bayesian Calculation**: Given $n=10$ observations from a $\text{Poisson}(\lambda)$ distribution with $\sum x_i = 30$, and a prior $\lambda \sim \text{Gamma}(2, 1)$, find the posterior distribution for $\lambda$. What is the Bayes estimator under squared error loss?</span>
<span id="cb4-1305"><a href="#cb4-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1306"><a href="#cb4-1306" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb4-1307"><a href="#cb4-1307" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb4-1308"><a href="#cb4-1308" aria-hidden="true" tabindex="-1"></a>   The Gamma distribution is conjugate to the Poisson. Using the shape-rate parameterization (where $\beta$ is the rate parameter), if $\lambda \sim \text{Gamma}(\alpha, \beta)$ and we observe data with sum $S$, then $\lambda | \text{data} \sim \text{Gamma}(\alpha + S, \beta + n)$. The posterior mean (Bayes estimator) is $(\alpha + S)/(\beta + n)$.</span>
<span id="cb4-1309"><a href="#cb4-1309" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb4-1310"><a href="#cb4-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1311"><a href="#cb4-1311" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Decision Theory Concepts**: Let $X_1, \ldots, X_n \sim \mathcal{N}(\mu, 1)$. The MLE $\hat{\mu} = \bar{X}$ has risk $1/n$ under squared error loss.</span>
<span id="cb4-1312"><a href="#cb4-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1313"><a href="#cb4-1313" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>(a) Is this risk constant?</span>
<span id="cb4-1314"><a href="#cb4-1314" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>(b) How does $\hat{\mu}$ relate to the Bayes estimator under a Normal prior? (One sentence.)</span>
<span id="cb4-1315"><a href="#cb4-1315" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>(c) Is $\hat{\mu}$ minimax? Give a one-line justification.</span>
<span id="cb4-1316"><a href="#cb4-1316" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>(d) Is $\hat{\mu}$ admissible? Give a one-line justification.</span>
<span id="cb4-1317"><a href="#cb4-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1318"><a href="#cb4-1318" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb4-1319"><a href="#cb4-1319" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb4-1320"><a href="#cb4-1320" aria-hidden="true" tabindex="-1"></a>   (a) Yes, $R(\mu, \bar{X}) = \text{Var}(\bar{X}) = 1/n$ is constant.</span>
<span id="cb4-1321"><a href="#cb4-1321" aria-hidden="true" tabindex="-1"></a>   (b) With prior $\mu \sim \mathcal{N}(a, b^2)$, the Bayes estimator is the posterior mean $w\bar{X} + (1-w)a$, where $w = \frac{b^2}{b^2 + 1/n}$; as $b^2 \to \infty$ (very diffuse prior), this approaches $\bar{X}$.</span>
<span id="cb4-1322"><a href="#cb4-1322" aria-hidden="true" tabindex="-1"></a>   (c) Constant risk + admissibility ⇒ minimax (by results in the notes).</span>
<span id="cb4-1323"><a href="#cb4-1323" aria-hidden="true" tabindex="-1"></a>   (d) Yes, in the 1D Normal-mean problem with squared error, $\bar{X}$ is admissible (classical result), even though it's a limit of Bayes rules.</span>
<span id="cb4-1324"><a href="#cb4-1324" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb4-1325"><a href="#cb4-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1326"><a href="#cb4-1326" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Prior Choice**: You're estimating the probability $p$ that a new medical treatment works. You're skeptical because most new treatments fail. What would be:</span>
<span id="cb4-1327"><a href="#cb4-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1328"><a href="#cb4-1328" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>A weakly informative prior for $p$?</span>
<span id="cb4-1329"><a href="#cb4-1329" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>A strong prior reflecting your skepticism?</span>
<span id="cb4-1330"><a href="#cb4-1330" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb4-1331"><a href="#cb4-1331" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb4-1332"><a href="#cb4-1332" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb4-1333"><a href="#cb4-1333" aria-hidden="true" tabindex="-1"></a>   Weakly informative: Beta(1, 3) or Beta(1, 5) - allows all values but slightly favors lower success rates. Strong skeptical prior: Beta(1, 10) or Beta(1, 20) - strongly concentrates mass near 0, reflecting belief that the treatment likely doesn't work. Remember: Beta parameters can be interpreted as pseudo-counts of successes and failures.</span>
<span id="cb4-1334"><a href="#cb4-1334" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb4-1335"><a href="#cb4-1335" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb4-1336"><a href="#cb4-1336" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Conceptual Understanding**: Why is the James-Stein estimator's improvement over the MLE considered paradoxical? What does it tell us about estimating multiple parameters simultaneously?</span>
<span id="cb4-1337"><a href="#cb4-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1338"><a href="#cb4-1338" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb4-1339"><a href="#cb4-1339" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb4-1340"><a href="#cb4-1340" aria-hidden="true" tabindex="-1"></a>   The paradox: When estimating three or more unrelated means (e.g., baseball batting average, physics constants, and rainfall), using information from all of them together (via shrinkage) gives better estimates than treating them separately. This violates our intuition that unrelated problems should be solved independently. The lesson: In high dimensions, "borrowing strength" across parameters through shrinkage reduces overall risk, even for unrelated parameters.</span>
<span id="cb4-1341"><a href="#cb4-1341" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb4-1342"><a href="#cb4-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1343"><a href="#cb4-1343" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb4-1344"><a href="#cb4-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1345"><a href="#cb4-1345" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb4-1346"><a href="#cb4-1346" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb4-1347"><a href="#cb4-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1348"><a href="#cb4-1348" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb4-1349"><a href="#cb4-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1350"><a href="#cb4-1350" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-1351"><a href="#cb4-1351" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-1352"><a href="#cb4-1352" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-1353"><a href="#cb4-1353" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb4-1354"><a href="#cb4-1354" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-1355"><a href="#cb4-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1356"><a href="#cb4-1356" aria-hidden="true" tabindex="-1"></a><span class="co"># Conjugate Bayesian Inference</span></span>
<span id="cb4-1357"><a href="#cb4-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1358"><a href="#cb4-1358" aria-hidden="true" tabindex="-1"></a><span class="co">## Beta-Bernoulli Model</span></span>
<span id="cb4-1359"><a href="#cb4-1359" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beta_bernoulli_posterior(n_successes, n_trials, alpha_prior<span class="op">=</span><span class="dv">1</span>, beta_prior<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb4-1360"><a href="#cb4-1360" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-1361"><a href="#cb4-1361" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute posterior parameters for Beta-Bernoulli model.</span></span>
<span id="cb4-1362"><a href="#cb4-1362" aria-hidden="true" tabindex="-1"></a><span class="co">    Prior: Beta(alpha_prior, beta_prior)</span></span>
<span id="cb4-1363"><a href="#cb4-1363" aria-hidden="true" tabindex="-1"></a><span class="co">    Data: n_successes in n_trials</span></span>
<span id="cb4-1364"><a href="#cb4-1364" aria-hidden="true" tabindex="-1"></a><span class="co">    Posterior: Beta(alpha_post, beta_post)</span></span>
<span id="cb4-1365"><a href="#cb4-1365" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-1366"><a href="#cb4-1366" aria-hidden="true" tabindex="-1"></a>    alpha_post <span class="op">=</span> alpha_prior <span class="op">+</span> n_successes</span>
<span id="cb4-1367"><a href="#cb4-1367" aria-hidden="true" tabindex="-1"></a>    beta_post <span class="op">=</span> beta_prior <span class="op">+</span> (n_trials <span class="op">-</span> n_successes)</span>
<span id="cb4-1368"><a href="#cb4-1368" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha_post, beta_post</span>
<span id="cb4-1369"><a href="#cb4-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1370"><a href="#cb4-1370" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb4-1371"><a href="#cb4-1371" aria-hidden="true" tabindex="-1"></a>n, s <span class="op">=</span> <span class="dv">20</span>, <span class="dv">12</span>  <span class="co"># 12 successes in 20 trials</span></span>
<span id="cb4-1372"><a href="#cb4-1372" aria-hidden="true" tabindex="-1"></a>alpha_post, beta_post <span class="op">=</span> beta_bernoulli_posterior(s, n)</span>
<span id="cb4-1373"><a href="#cb4-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1374"><a href="#cb4-1374" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior mean (Bayes estimator for squared error loss)</span></span>
<span id="cb4-1375"><a href="#cb4-1375" aria-hidden="true" tabindex="-1"></a>posterior_mean <span class="op">=</span> alpha_post <span class="op">/</span> (alpha_post <span class="op">+</span> beta_post)</span>
<span id="cb4-1376"><a href="#cb4-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1377"><a href="#cb4-1377" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% credible interval</span></span>
<span id="cb4-1378"><a href="#cb4-1378" aria-hidden="true" tabindex="-1"></a>ci_lower, ci_upper <span class="op">=</span> stats.beta.ppf([<span class="fl">0.025</span>, <span class="fl">0.975</span>], alpha_post, beta_post)</span>
<span id="cb4-1379"><a href="#cb4-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1380"><a href="#cb4-1380" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize posterior</span></span>
<span id="cb4-1381"><a href="#cb4-1381" aria-hidden="true" tabindex="-1"></a>p_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">200</span>)</span>
<span id="cb4-1382"><a href="#cb4-1382" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> stats.beta.pdf(p_range, alpha_post, beta_post)</span>
<span id="cb4-1383"><a href="#cb4-1383" aria-hidden="true" tabindex="-1"></a>plt.plot(p_range, posterior)</span>
<span id="cb4-1384"><a href="#cb4-1384" aria-hidden="true" tabindex="-1"></a>plt.fill_between(p_range, posterior, </span>
<span id="cb4-1385"><a href="#cb4-1385" aria-hidden="true" tabindex="-1"></a>                 where<span class="op">=</span>(p_range <span class="op">&gt;=</span> ci_lower) <span class="op">&amp;</span> (p_range <span class="op">&lt;=</span> ci_upper),</span>
<span id="cb4-1386"><a href="#cb4-1386" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'95% Credible Interval'</span>)</span>
<span id="cb4-1387"><a href="#cb4-1387" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p'</span>)</span>
<span id="cb4-1388"><a href="#cb4-1388" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Posterior density'</span>)</span>
<span id="cb4-1389"><a href="#cb4-1389" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Beta(</span><span class="sc">{</span>alpha_post<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>beta_post<span class="sc">}</span><span class="ss">) Posterior'</span>)</span>
<span id="cb4-1390"><a href="#cb4-1390" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-1391"><a href="#cb4-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1392"><a href="#cb4-1392" aria-hidden="true" tabindex="-1"></a><span class="co">## Normal-Normal Model  </span></span>
<span id="cb4-1393"><a href="#cb4-1393" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normal_normal_posterior(data, prior_mean<span class="op">=</span><span class="dv">0</span>, prior_var<span class="op">=</span><span class="dv">1</span>, data_var<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb4-1394"><a href="#cb4-1394" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-1395"><a href="#cb4-1395" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute posterior for Normal-Normal conjugate model.</span></span>
<span id="cb4-1396"><a href="#cb4-1396" aria-hidden="true" tabindex="-1"></a><span class="co">    Prior: N(prior_mean, prior_var)</span></span>
<span id="cb4-1397"><a href="#cb4-1397" aria-hidden="true" tabindex="-1"></a><span class="co">    Likelihood: N(theta, data_var) for each observation</span></span>
<span id="cb4-1398"><a href="#cb4-1398" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-1399"><a href="#cb4-1399" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb4-1400"><a href="#cb4-1400" aria-hidden="true" tabindex="-1"></a>    data_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb4-1401"><a href="#cb4-1401" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-1402"><a href="#cb4-1402" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Precision (1/variance) is additive</span></span>
<span id="cb4-1403"><a href="#cb4-1403" aria-hidden="true" tabindex="-1"></a>    prior_precision <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>prior_var</span>
<span id="cb4-1404"><a href="#cb4-1404" aria-hidden="true" tabindex="-1"></a>    data_precision <span class="op">=</span> n<span class="op">/</span>data_var</span>
<span id="cb4-1405"><a href="#cb4-1405" aria-hidden="true" tabindex="-1"></a>    post_precision <span class="op">=</span> prior_precision <span class="op">+</span> data_precision</span>
<span id="cb4-1406"><a href="#cb4-1406" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-1407"><a href="#cb4-1407" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior parameters</span></span>
<span id="cb4-1408"><a href="#cb4-1408" aria-hidden="true" tabindex="-1"></a>    post_var <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>post_precision</span>
<span id="cb4-1409"><a href="#cb4-1409" aria-hidden="true" tabindex="-1"></a>    post_mean <span class="op">=</span> post_var <span class="op">*</span> (prior_precision <span class="op">*</span> prior_mean <span class="op">+</span> </span>
<span id="cb4-1410"><a href="#cb4-1410" aria-hidden="true" tabindex="-1"></a>                            data_precision <span class="op">*</span> data_mean)</span>
<span id="cb4-1411"><a href="#cb4-1411" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-1412"><a href="#cb4-1412" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> post_mean, post_var</span>
<span id="cb4-1413"><a href="#cb4-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1414"><a href="#cb4-1414" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Theory</span></span>
<span id="cb4-1415"><a href="#cb4-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1416"><a href="#cb4-1416" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_risk(estimator_func, true_theta, n_simulations<span class="op">=</span><span class="dv">10000</span>, n_samples<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb4-1417"><a href="#cb4-1417" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-1418"><a href="#cb4-1418" aria-hidden="true" tabindex="-1"></a><span class="co">    Estimate risk via simulation for squared error loss.</span></span>
<span id="cb4-1419"><a href="#cb4-1419" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-1420"><a href="#cb4-1420" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb4-1421"><a href="#cb4-1421" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb4-1422"><a href="#cb4-1422" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate data</span></span>
<span id="cb4-1423"><a href="#cb4-1423" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.normal(true_theta, <span class="dv">1</span>, n_samples)</span>
<span id="cb4-1424"><a href="#cb4-1424" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute estimate</span></span>
<span id="cb4-1425"><a href="#cb4-1425" aria-hidden="true" tabindex="-1"></a>        estimate <span class="op">=</span> estimator_func(data)</span>
<span id="cb4-1426"><a href="#cb4-1426" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss</span></span>
<span id="cb4-1427"><a href="#cb4-1427" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (estimate <span class="op">-</span> true_theta)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-1428"><a href="#cb4-1428" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb4-1429"><a href="#cb4-1429" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-1430"><a href="#cb4-1430" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(losses)  <span class="co"># Risk = expected loss</span></span>
<span id="cb4-1431"><a href="#cb4-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1432"><a href="#cb4-1432" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Compare MLE and a shrinkage estimator</span></span>
<span id="cb4-1433"><a href="#cb4-1433" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mle_estimator(data):</span>
<span id="cb4-1434"><a href="#cb4-1434" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(data)</span>
<span id="cb4-1435"><a href="#cb4-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1436"><a href="#cb4-1436" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shrinkage_estimator(data, shrink_target<span class="op">=</span><span class="dv">0</span>, shrink_factor<span class="op">=</span><span class="fl">0.8</span>):</span>
<span id="cb4-1437"><a href="#cb4-1437" aria-hidden="true" tabindex="-1"></a>    mle <span class="op">=</span> np.mean(data)</span>
<span id="cb4-1438"><a href="#cb4-1438" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> shrink_factor <span class="op">*</span> mle <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> shrink_factor) <span class="op">*</span> shrink_target</span>
<span id="cb4-1439"><a href="#cb4-1439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1440"><a href="#cb4-1440" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare risks</span></span>
<span id="cb4-1441"><a href="#cb4-1441" aria-hidden="true" tabindex="-1"></a>theta_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">50</span>)</span>
<span id="cb4-1442"><a href="#cb4-1442" aria-hidden="true" tabindex="-1"></a>risk_mle <span class="op">=</span> []</span>
<span id="cb4-1443"><a href="#cb4-1443" aria-hidden="true" tabindex="-1"></a>risk_shrink <span class="op">=</span> []</span>
<span id="cb4-1444"><a href="#cb4-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1445"><a href="#cb4-1445" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> theta <span class="kw">in</span> theta_values:</span>
<span id="cb4-1446"><a href="#cb4-1446" aria-hidden="true" tabindex="-1"></a>    risk_mle.append(compute_risk(mle_estimator, theta))</span>
<span id="cb4-1447"><a href="#cb4-1447" aria-hidden="true" tabindex="-1"></a>    risk_shrink.append(compute_risk(shrinkage_estimator, theta))</span>
<span id="cb4-1448"><a href="#cb4-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1449"><a href="#cb4-1449" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, risk_mle, label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb4-1450"><a href="#cb4-1450" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, risk_shrink, label<span class="op">=</span><span class="st">'Shrinkage'</span>)</span>
<span id="cb4-1451"><a href="#cb4-1451" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True θ'</span>)</span>
<span id="cb4-1452"><a href="#cb4-1452" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Risk'</span>)</span>
<span id="cb4-1453"><a href="#cb4-1453" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Risk Functions Comparison'</span>)</span>
<span id="cb4-1454"><a href="#cb4-1454" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-1455"><a href="#cb4-1455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-1456"><a href="#cb4-1456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1457"><a href="#cb4-1457" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb4-1458"><a href="#cb4-1458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1459"><a href="#cb4-1459" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb4-1460"><a href="#cb4-1460" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-1461"><a href="#cb4-1461" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-1462"><a href="#cb4-1462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1463"><a href="#cb4-1463" aria-hidden="true" tabindex="-1"></a><span class="co"># Conjugate Bayesian Inference</span></span>
<span id="cb4-1464"><a href="#cb4-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1465"><a href="#cb4-1465" aria-hidden="true" tabindex="-1"></a><span class="do">## Beta-Bernoulli Model</span></span>
<span id="cb4-1466"><a href="#cb4-1466" aria-hidden="true" tabindex="-1"></a>beta_bernoulli_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(n_successes, n_trials, </span>
<span id="cb4-1467"><a href="#cb4-1467" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">alpha_prior =</span> <span class="dv">1</span>, <span class="at">beta_prior =</span> <span class="dv">1</span>) {</span>
<span id="cb4-1468"><a href="#cb4-1468" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute posterior parameters</span></span>
<span id="cb4-1469"><a href="#cb4-1469" aria-hidden="true" tabindex="-1"></a>  alpha_post <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> n_successes</span>
<span id="cb4-1470"><a href="#cb4-1470" aria-hidden="true" tabindex="-1"></a>  beta_post <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> (n_trials <span class="sc">-</span> n_successes)</span>
<span id="cb4-1471"><a href="#cb4-1471" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-1472"><a href="#cb4-1472" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">alpha =</span> alpha_post, <span class="at">beta =</span> beta_post,</span>
<span id="cb4-1473"><a href="#cb4-1473" aria-hidden="true" tabindex="-1"></a>       <span class="at">mean =</span> alpha_post <span class="sc">/</span> (alpha_post <span class="sc">+</span> beta_post))</span>
<span id="cb4-1474"><a href="#cb4-1474" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-1475"><a href="#cb4-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1476"><a href="#cb4-1476" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb4-1477"><a href="#cb4-1477" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb4-1478"><a href="#cb4-1478" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb4-1479"><a href="#cb4-1479" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">beta_bernoulli_posterior</span>(s, n)</span>
<span id="cb4-1480"><a href="#cb4-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1481"><a href="#cb4-1481" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% credible interval</span></span>
<span id="cb4-1482"><a href="#cb4-1482" aria-hidden="true" tabindex="-1"></a>ci <span class="ot">&lt;-</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), posterior<span class="sc">$</span>alpha, posterior<span class="sc">$</span>beta)</span>
<span id="cb4-1483"><a href="#cb4-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1484"><a href="#cb4-1484" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize posterior</span></span>
<span id="cb4-1485"><a href="#cb4-1485" aria-hidden="true" tabindex="-1"></a>p_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb4-1486"><a href="#cb4-1486" aria-hidden="true" tabindex="-1"></a>posterior_density <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(p_seq, posterior<span class="sc">$</span>alpha, posterior<span class="sc">$</span>beta)</span>
<span id="cb4-1487"><a href="#cb4-1487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1488"><a href="#cb4-1488" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">p =</span> p_seq, <span class="at">density =</span> posterior_density)</span>
<span id="cb4-1489"><a href="#cb4-1489" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> density)) <span class="sc">+</span></span>
<span id="cb4-1490"><a href="#cb4-1490" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb4-1491"><a href="#cb4-1491" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, p <span class="sc">&gt;=</span> ci[<span class="dv">1</span>] <span class="sc">&amp;</span> p <span class="sc">&lt;=</span> ci[<span class="dv">2</span>]),</span>
<span id="cb4-1492"><a href="#cb4-1492" aria-hidden="true" tabindex="-1"></a>            <span class="at">alpha =</span> <span class="fl">0.3</span>, <span class="at">fill =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb4-1493"><a href="#cb4-1493" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"p"</span>, <span class="at">y =</span> <span class="st">"Posterior density"</span>,</span>
<span id="cb4-1494"><a href="#cb4-1494" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="fu">sprintf</span>(<span class="st">"Beta(%g, %g) Posterior"</span>, </span>
<span id="cb4-1495"><a href="#cb4-1495" aria-hidden="true" tabindex="-1"></a>                      posterior<span class="sc">$</span>alpha, posterior<span class="sc">$</span>beta)) <span class="sc">+</span></span>
<span id="cb4-1496"><a href="#cb4-1496" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb4-1497"><a href="#cb4-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1498"><a href="#cb4-1498" aria-hidden="true" tabindex="-1"></a><span class="do">## Normal-Normal Model</span></span>
<span id="cb4-1499"><a href="#cb4-1499" aria-hidden="true" tabindex="-1"></a>normal_normal_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">prior_mean =</span> <span class="dv">0</span>, <span class="at">prior_var =</span> <span class="dv">1</span>, </span>
<span id="cb4-1500"><a href="#cb4-1500" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">data_var =</span> <span class="dv">1</span>) {</span>
<span id="cb4-1501"><a href="#cb4-1501" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(data)</span>
<span id="cb4-1502"><a href="#cb4-1502" aria-hidden="true" tabindex="-1"></a>  data_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(data)</span>
<span id="cb4-1503"><a href="#cb4-1503" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-1504"><a href="#cb4-1504" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Precision is additive</span></span>
<span id="cb4-1505"><a href="#cb4-1505" aria-hidden="true" tabindex="-1"></a>  prior_precision <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>prior_var</span>
<span id="cb4-1506"><a href="#cb4-1506" aria-hidden="true" tabindex="-1"></a>  data_precision <span class="ot">&lt;-</span> n<span class="sc">/</span>data_var</span>
<span id="cb4-1507"><a href="#cb4-1507" aria-hidden="true" tabindex="-1"></a>  post_precision <span class="ot">&lt;-</span> prior_precision <span class="sc">+</span> data_precision</span>
<span id="cb4-1508"><a href="#cb4-1508" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-1509"><a href="#cb4-1509" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Posterior parameters</span></span>
<span id="cb4-1510"><a href="#cb4-1510" aria-hidden="true" tabindex="-1"></a>  post_var <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>post_precision</span>
<span id="cb4-1511"><a href="#cb4-1511" aria-hidden="true" tabindex="-1"></a>  post_mean <span class="ot">&lt;-</span> post_var <span class="sc">*</span> (prior_precision <span class="sc">*</span> prior_mean <span class="sc">+</span> </span>
<span id="cb4-1512"><a href="#cb4-1512" aria-hidden="true" tabindex="-1"></a>                           data_precision <span class="sc">*</span> data_mean)</span>
<span id="cb4-1513"><a href="#cb4-1513" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-1514"><a href="#cb4-1514" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">mean =</span> post_mean, <span class="at">var =</span> post_var, <span class="at">sd =</span> <span class="fu">sqrt</span>(post_var))</span>
<span id="cb4-1515"><a href="#cb4-1515" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-1516"><a href="#cb4-1516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1517"><a href="#cb4-1517" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Theory</span></span>
<span id="cb4-1518"><a href="#cb4-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1519"><a href="#cb4-1519" aria-hidden="true" tabindex="-1"></a>compute_risk <span class="ot">&lt;-</span> <span class="cf">function</span>(estimator_func, true_theta, </span>
<span id="cb4-1520"><a href="#cb4-1520" aria-hidden="true" tabindex="-1"></a>                         <span class="at">n_simulations =</span> <span class="dv">10000</span>, <span class="at">n_samples =</span> <span class="dv">20</span>) {</span>
<span id="cb4-1521"><a href="#cb4-1521" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Estimate risk via simulation</span></span>
<span id="cb4-1522"><a href="#cb4-1522" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_simulations, {</span>
<span id="cb4-1523"><a href="#cb4-1523" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_samples, true_theta, <span class="dv">1</span>)</span>
<span id="cb4-1524"><a href="#cb4-1524" aria-hidden="true" tabindex="-1"></a>    estimate <span class="ot">&lt;-</span> <span class="fu">estimator_func</span>(data)</span>
<span id="cb4-1525"><a href="#cb4-1525" aria-hidden="true" tabindex="-1"></a>    (estimate <span class="sc">-</span> true_theta)<span class="sc">^</span><span class="dv">2</span>  <span class="co"># Squared error loss</span></span>
<span id="cb4-1526"><a href="#cb4-1526" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb4-1527"><a href="#cb4-1527" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-1528"><a href="#cb4-1528" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(losses)  <span class="co"># Risk = expected loss</span></span>
<span id="cb4-1529"><a href="#cb4-1529" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-1530"><a href="#cb4-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1531"><a href="#cb4-1531" aria-hidden="true" tabindex="-1"></a><span class="co"># Example estimators</span></span>
<span id="cb4-1532"><a href="#cb4-1532" aria-hidden="true" tabindex="-1"></a>mle_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(data) <span class="fu">mean</span>(data)</span>
<span id="cb4-1533"><a href="#cb4-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1534"><a href="#cb4-1534" aria-hidden="true" tabindex="-1"></a>shrinkage_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">target =</span> <span class="dv">0</span>, <span class="at">factor =</span> <span class="fl">0.8</span>) {</span>
<span id="cb4-1535"><a href="#cb4-1535" aria-hidden="true" tabindex="-1"></a>  factor <span class="sc">*</span> <span class="fu">mean</span>(data) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> factor) <span class="sc">*</span> target</span>
<span id="cb4-1536"><a href="#cb4-1536" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-1537"><a href="#cb4-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1538"><a href="#cb4-1538" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare risk functions</span></span>
<span id="cb4-1539"><a href="#cb4-1539" aria-hidden="true" tabindex="-1"></a>theta_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">50</span>)</span>
<span id="cb4-1540"><a href="#cb4-1540" aria-hidden="true" tabindex="-1"></a>risk_mle <span class="ot">&lt;-</span> <span class="fu">sapply</span>(theta_seq, <span class="cf">function</span>(theta) </span>
<span id="cb4-1541"><a href="#cb4-1541" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_risk</span>(mle_estimator, theta))</span>
<span id="cb4-1542"><a href="#cb4-1542" aria-hidden="true" tabindex="-1"></a>risk_shrink <span class="ot">&lt;-</span> <span class="fu">sapply</span>(theta_seq, <span class="cf">function</span>(theta) </span>
<span id="cb4-1543"><a href="#cb4-1543" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_risk</span>(shrinkage_estimator, theta))</span>
<span id="cb4-1544"><a href="#cb4-1544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1545"><a href="#cb4-1545" aria-hidden="true" tabindex="-1"></a>df_risk <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb4-1546"><a href="#cb4-1546" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">rep</span>(theta_seq, <span class="dv">2</span>),</span>
<span id="cb4-1547"><a href="#cb4-1547" aria-hidden="true" tabindex="-1"></a>  <span class="at">risk =</span> <span class="fu">c</span>(risk_mle, risk_shrink),</span>
<span id="cb4-1548"><a href="#cb4-1548" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimator =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"MLE"</span>, <span class="st">"Shrinkage"</span>), <span class="at">each =</span> <span class="fu">length</span>(theta_seq))</span>
<span id="cb4-1549"><a href="#cb4-1549" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-1550"><a href="#cb4-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1551"><a href="#cb4-1551" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_risk, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> risk, <span class="at">color =</span> estimator)) <span class="sc">+</span></span>
<span id="cb4-1552"><a href="#cb4-1552" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb4-1553"><a href="#cb4-1553" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"True θ"</span>, <span class="at">y =</span> <span class="st">"Risk"</span>, </span>
<span id="cb4-1554"><a href="#cb4-1554" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"Risk Functions Comparison"</span>) <span class="sc">+</span></span>
<span id="cb4-1555"><a href="#cb4-1555" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb4-1556"><a href="#cb4-1556" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-1557"><a href="#cb4-1557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1558"><a href="#cb4-1558" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1559"><a href="#cb4-1559" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1560"><a href="#cb4-1560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1561"><a href="#cb4-1561" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb4-1562"><a href="#cb4-1562" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-1563"><a href="#cb4-1563" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb4-1564"><a href="#cb4-1564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1565"><a href="#cb4-1565" aria-hidden="true" tabindex="-1"></a>Python and R code examples for Bayesian inference and decision theory can be found in the HTML version of these notes.</span>
<span id="cb4-1566"><a href="#cb4-1566" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1567"><a href="#cb4-1567" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1568"><a href="#cb4-1568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1569"><a href="#cb4-1569" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb4-1570"><a href="#cb4-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1571"><a href="#cb4-1571" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb4-1572"><a href="#cb4-1572" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb4-1573"><a href="#cb4-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1574"><a href="#cb4-1574" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding Source(s) |</span>
<span id="cb4-1575"><a href="#cb4-1575" aria-hidden="true" tabindex="-1"></a>|:---------------------|:-----------------------|</span>
<span id="cb4-1576"><a href="#cb4-1576" aria-hidden="true" tabindex="-1"></a>| **Introduction: A Different Way of Thinking** | From slides and AF447 case study from @stone2014search |</span>
<span id="cb4-1577"><a href="#cb4-1577" aria-hidden="true" tabindex="-1"></a>| ↳ The Two Philosophies of Statistics | AoS §11.1 and slides |</span>
<span id="cb4-1578"><a href="#cb4-1578" aria-hidden="true" tabindex="-1"></a>| **The Bayesian Method: Updating Beliefs with Data** | AoS §11.2 |</span>
<span id="cb4-1579"><a href="#cb4-1579" aria-hidden="true" tabindex="-1"></a>| ↳ The Engine: Bayes' Theorem for Inference | AoS §11.2 plus slides |</span>
<span id="cb4-1580"><a href="#cb4-1580" aria-hidden="true" tabindex="-1"></a>| ↳ Summarizing the Posterior | AoS §11.2; AoS §12.3 (for median/mode) |</span>
<span id="cb4-1581"><a href="#cb4-1581" aria-hidden="true" tabindex="-1"></a>| ↳ Credible vs. Confidence Intervals | AoS §11.9 and expanded from lecture material |</span>
<span id="cb4-1582"><a href="#cb4-1582" aria-hidden="true" tabindex="-1"></a>| **Bayesian Inference in Action** | |</span>
<span id="cb4-1583"><a href="#cb4-1583" aria-hidden="true" tabindex="-1"></a>| ↳ Conjugate Models and Conjugate Priors | AoS §11.2 |</span>
<span id="cb4-1584"><a href="#cb4-1584" aria-hidden="true" tabindex="-1"></a>| ↳ Example: The Bernoulli-Beta Model | AoS Example 11.1 and slides |</span>
<span id="cb4-1585"><a href="#cb4-1585" aria-hidden="true" tabindex="-1"></a>| ↳ Example: The Normal-Normal Model | AoS Example 11.2 and slides |</span>
<span id="cb4-1586"><a href="#cb4-1586" aria-hidden="true" tabindex="-1"></a>| ↳ The Art and Science of Choosing Priors | AoS §11.6 expanded with modern views |</span>
<span id="cb4-1587"><a href="#cb4-1587" aria-hidden="true" tabindex="-1"></a>| ↳ Implementing Bayesian Inference | AoS §11.4 expanded with modern tools |</span>
<span id="cb4-1588"><a href="#cb4-1588" aria-hidden="true" tabindex="-1"></a>| **Statistical Decision Theory** | AoS Ch 12 |</span>
<span id="cb4-1589"><a href="#cb4-1589" aria-hidden="true" tabindex="-1"></a>| ↳ The Ingredients: Loss and Risk | AoS §12.1 |</span>
<span id="cb4-1590"><a href="#cb4-1590" aria-hidden="true" tabindex="-1"></a>| ↳ The Challenge of Comparing Risk Functions | AoS §12.2 (Examples 12.2, 12.3) |</span>
<span id="cb4-1591"><a href="#cb4-1591" aria-hidden="true" tabindex="-1"></a>| **Optimal Estimators: Bayes and Minimax Rules** | |</span>
<span id="cb4-1592"><a href="#cb4-1592" aria-hidden="true" tabindex="-1"></a>| ↳ The Bayesian Approach: Minimizing Average Risk | AoS §12.3 |</span>
<span id="cb4-1593"><a href="#cb4-1593" aria-hidden="true" tabindex="-1"></a>| ↳ The Frequentist Approach: Minimizing Worst-Case Risk | AoS §12.4 |</span>
<span id="cb4-1594"><a href="#cb4-1594" aria-hidden="true" tabindex="-1"></a>| ↳ Example: Minimax Estimator for Bernoulli | AoS Example 12.12 |</span>
<span id="cb4-1595"><a href="#cb4-1595" aria-hidden="true" tabindex="-1"></a>| ↳ Example: Minimax Estimator for Normal Mean | AoS Theorem 12.14, Theorem 12.22 |</span>
<span id="cb4-1596"><a href="#cb4-1596" aria-hidden="true" tabindex="-1"></a>| ↳ Example: Large Sample MLE | AoS §12.5 |</span>
<span id="cb4-1597"><a href="#cb4-1597" aria-hidden="true" tabindex="-1"></a>| **Admissibility: Ruling Out Bad Estimators** | AoS §12.6 |</span>
<span id="cb4-1598"><a href="#cb4-1598" aria-hidden="true" tabindex="-1"></a>| ↳ Defining Admissibility | AoS Definition 12.17 |</span>
<span id="cb4-1599"><a href="#cb4-1599" aria-hidden="true" tabindex="-1"></a>| ↳ Key Properties and Connections | AoS Theorem 12.19, Theorem 12.21 |</span>
<span id="cb4-1600"><a href="#cb4-1600" aria-hidden="true" tabindex="-1"></a>| ↳ Advanced: Stein's Paradox and Shrinkage | AoS §12.7 |</span>
<span id="cb4-1601"><a href="#cb4-1601" aria-hidden="true" tabindex="-1"></a>| **Self-Test Problems** | Based on AoS Ch 11/12 exercises and concepts |</span>
<span id="cb4-1602"><a href="#cb4-1602" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-1603"><a href="#cb4-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1604"><a href="#cb4-1604" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Materials</span></span>
<span id="cb4-1605"><a href="#cb4-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1606"><a href="#cb4-1606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Foundational Text**: Gelman et al., "Bayesian Data Analysis" (3rd ed.)</span>
<span id="cb4-1607"><a href="#cb4-1607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The Air France Search Case Study**: @stone2014search.</span>
<span id="cb4-1608"><a href="#cb4-1608" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Prior Choice**: See the <span class="co">[</span><span class="ot">Stan wiki</span><span class="co">](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)</span>.</span>
<span id="cb4-1609"><a href="#cb4-1609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1610"><a href="#cb4-1610" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-1611"><a href="#cb4-1611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-1612"><a href="#cb4-1612" aria-hidden="true" tabindex="-1"></a>*Remember: Bayesian inference is about updating beliefs with data. Decision theory is about choosing the best estimator. Together, they provide a complete framework for statistical inference that complements and enriches the frequentist approach. Master both paradigms -- they each have their place in the modern statistician's toolkit!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>