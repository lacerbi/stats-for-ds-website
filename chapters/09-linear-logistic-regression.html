<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 9&nbsp; Linear and Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../references.html" rel="next">
<link href="../chapters/08-bayesian-inference-decision-theory.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/09-linear-logistic-regression.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">9.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-why-linear-models-still-matter" id="toc-introduction-why-linear-models-still-matter" class="nav-link" data-scroll-target="#introduction-why-linear-models-still-matter"><span class="header-section-number">9.2</span> Introduction: Why Linear Models Still Matter</a>
  <ul class="collapse">
  <li><a href="#the-power-of-interpretability" id="toc-the-power-of-interpretability" class="nav-link" data-scroll-target="#the-power-of-interpretability"><span class="header-section-number">9.2.1</span> The Power of Interpretability</a></li>
  <li><a href="#linear-models-as-building-blocks" id="toc-linear-models-as-building-blocks" class="nav-link" data-scroll-target="#linear-models-as-building-blocks"><span class="header-section-number">9.2.2</span> Linear Models as Building Blocks</a></li>
  <li><a href="#this-chapters-goal" id="toc-this-chapters-goal" class="nav-link" data-scroll-target="#this-chapters-goal"><span class="header-section-number">9.2.3</span> This Chapter’s Goal</a></li>
  </ul></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"><span class="header-section-number">9.3</span> Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#regression-models" id="toc-regression-models" class="nav-link" data-scroll-target="#regression-models"><span class="header-section-number">9.3.1</span> Regression Models</a></li>
  <li><a href="#regression-notation" id="toc-regression-notation" class="nav-link" data-scroll-target="#regression-notation"><span class="header-section-number">9.3.2</span> Regression Notation</a></li>
  <li><a href="#the-simple-linear-regression-model" id="toc-the-simple-linear-regression-model" class="nav-link" data-scroll-target="#the-simple-linear-regression-model"><span class="header-section-number">9.3.3</span> The Simple Linear Regression Model</a></li>
  <li><a href="#estimating-parameters-the-method-of-least-squares" id="toc-estimating-parameters-the-method-of-least-squares" class="nav-link" data-scroll-target="#estimating-parameters-the-method-of-least-squares"><span class="header-section-number">9.3.4</span> Estimating Parameters: The Method of Least Squares</a></li>
  <li><a href="#connection-to-maximum-likelihood-estimation" id="toc-connection-to-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#connection-to-maximum-likelihood-estimation"><span class="header-section-number">9.3.5</span> Connection to Maximum Likelihood Estimation</a></li>
  <li><a href="#properties-of-the-least-squares-estimators" id="toc-properties-of-the-least-squares-estimators" class="nav-link" data-scroll-target="#properties-of-the-least-squares-estimators"><span class="header-section-number">9.3.6</span> Properties of the Least Squares Estimators</a></li>
  <li><a href="#simple-linear-regression-in-practice" id="toc-simple-linear-regression-in-practice" class="nav-link" data-scroll-target="#simple-linear-regression-in-practice"><span class="header-section-number">9.3.7</span> Simple Linear Regression in Practice</a></li>
  </ul></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">9.4</span> Multiple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#extending-the-model-to-multiple-predictors" id="toc-extending-the-model-to-multiple-predictors" class="nav-link" data-scroll-target="#extending-the-model-to-multiple-predictors"><span class="header-section-number">9.4.1</span> Extending the Model to Multiple Predictors</a></li>
  <li><a href="#multiple-regression-in-practice" id="toc-multiple-regression-in-practice" class="nav-link" data-scroll-target="#multiple-regression-in-practice"><span class="header-section-number">9.4.2</span> Multiple Regression in Practice</a></li>
  <li><a href="#model-selection-choosing-the-right-predictors" id="toc-model-selection-choosing-the-right-predictors" class="nav-link" data-scroll-target="#model-selection-choosing-the-right-predictors"><span class="header-section-number">9.4.3</span> Model Selection: Choosing the Right Predictors</a></li>
  <li><a href="#regression-assumptions-and-diagnostics" id="toc-regression-assumptions-and-diagnostics" class="nav-link" data-scroll-target="#regression-assumptions-and-diagnostics"><span class="header-section-number">9.4.4</span> Regression Assumptions and Diagnostics</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">9.5</span> Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#modeling-binary-outcomes" id="toc-modeling-binary-outcomes" class="nav-link" data-scroll-target="#modeling-binary-outcomes"><span class="header-section-number">9.5.1</span> Modeling Binary Outcomes</a></li>
  <li><a href="#the-logistic-regression-model" id="toc-the-logistic-regression-model" class="nav-link" data-scroll-target="#the-logistic-regression-model"><span class="header-section-number">9.5.2</span> The Logistic Regression Model</a></li>
  <li><a href="#logistic-regression-in-practice" id="toc-logistic-regression-in-practice" class="nav-link" data-scroll-target="#logistic-regression-in-practice"><span class="header-section-number">9.5.3</span> Logistic Regression in Practice</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-connections" id="toc-chapter-summary-and-connections" class="nav-link" data-scroll-target="#chapter-summary-and-connections"><span class="header-section-number">9.6</span> Chapter Summary and Connections</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">9.6.1</span> Key Concepts Review</a></li>
  <li><a href="#the-big-picture" id="toc-the-big-picture" class="nav-link" data-scroll-target="#the-big-picture"><span class="header-section-number">9.6.2</span> The Big Picture</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">9.6.3</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">9.6.4</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">9.6.5</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">9.6.6</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">9.6.7</span> Connections to Source Material</a></li>
  <li><a href="#further-materials" id="toc-further-materials" class="nav-link" data-scroll-target="#further-materials"><span class="header-section-number">9.6.8</span> Further Materials</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">9.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li><strong>Build and interpret linear regression models</strong> to understand relationships between variables, connecting the geometric intuition of least squares with the statistical framework of maximum likelihood estimation.</li>
<li><strong>Evaluate regression coefficients meaningfully</strong>, including their practical interpretation, statistical significance via confidence intervals and hypothesis tests, and the crucial distinction between association and causation.</li>
<li><strong>Diagnose and address model inadequacies</strong> using residual plots to check assumptions, applying transformations when relationships are nonlinear, and selecting predictors using principled criteria (AIC, BIC, cross-validation).</li>
<li><strong>Extend regression to binary outcomes</strong> through logistic regression, understanding how the logit link enables probability modeling and interpreting coefficients as odds ratios.</li>
<li><strong>Apply regression for both prediction and explanation</strong>, recognizing when linear models excel (interpretability needs, moderate sample sizes) versus when more complex methods are warranted.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter introduces the two most fundamental and widely used models in statistics: linear and logistic regression. While modern machine learning offers powerful black-box predictors, linear models remain essential for their interpretability and foundational role in statistical inference. The material is adapted from Chapter 13 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span>, supplemented with modern perspectives on model interpretability and practical examples.</p>
</div>
</div>
</section>
<section id="introduction-why-linear-models-still-matter" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="introduction-why-linear-models-still-matter"><span class="header-section-number">9.2</span> Introduction: Why Linear Models Still Matter</h2>
<section id="the-power-of-interpretability" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="the-power-of-interpretability"><span class="header-section-number">9.2.1</span> The Power of Interpretability</h3>
<p>In an age dominated by complex machine learning models – neural networks with millions of parameters, random forests with thousands of trees, gradient boosting machines with intricate interactions – one might wonder: why dedicate an entire chapter to something as simple as linear regression?</p>
<p>The answer lies in a fundamental trade-off in statistical modeling: <strong>complexity versus interpretability</strong>. While a deep neural network might achieve better prediction accuracy on a complex dataset, it operates as a “black box”. We feed in inputs, we get outputs, but the mechanism connecting them remains opaque. This opacity becomes problematic when we need to:</p>
<ul>
<li><strong>Explain predictions to stakeholders</strong>: “Why was this loan application denied?”</li>
<li><strong>Identify which features matter most</strong>: “Which factors most strongly predict patient readmission?”</li>
<li><strong>Ensure fairness and avoid bias</strong>: “Is our model discriminating based on protected attributes?”</li>
<li><strong>Debug unexpected behavior</strong>: “Why did the model fail on this particular case?”</li>
</ul>
<p>Linear models excel at all these tasks. Every coefficient has a clear interpretation: it tells us exactly how a one-unit change in a predictor affects the outcome, holding all else constant. This interpretability has made linear models the reference tools in fields where understanding relationships is as important as making predictions – from economics to medicine to social sciences.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Are Neural Networks Black Boxes?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The story we presented above of neural networks being completely black boxes is a simplification nowadays. The field of <em>interpretability research</em> has developed sophisticated methods to understand neural networks, including techniques like mechanistic interpretability, activation analysis, and circuit discovery. These approaches have yielded notable insights, for example for <a href="https://www.anthropic.com/research/tracing-thoughts-language-model">large language models</a>.</p>
<p>Still, there’s a crucial distinction: linear models are transparent by construction – each coefficient directly tells us how a unit change in input affects output. Neural networks must be <strong>reverse-engineered</strong> through complex analysis requiring specialized tools and expertise. Think of it as the difference between reading a recipe versus doing forensic analysis on a finished cake.</p>
<p>In short, while interpretability research continues to advance, linear models remain uniquely valuable when interpretability is a primary requirement rather than an afterthought – and linear models can be used as tools to understand more complex models, as seen in the next paragraph.</p>
</div>
</div>
</div>
</section>
<section id="linear-models-as-building-blocks" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="linear-models-as-building-blocks"><span class="header-section-number">9.2.2</span> Linear Models as Building Blocks</h3>
<p>Beyond their direct application, linear models serve as the foundation for building and understanding more complex methods:</p>
<ul>
<li><strong>Generalized Linear Models (GLMs)</strong> extend linear regression to handle non-normal outcomes</li>
<li><strong>Mixed Effects Models</strong> add random effects to account for hierarchical data structures</li>
<li><strong>Regularized Regression</strong> (Ridge, LASSO, Elastic Net) adds penalties to handle high-dimensional data</li>
<li><strong>Local Linear Methods</strong> like LOESS use linear regression in small neighborhoods for flexible curve fitting</li>
</ul>
<p>Even in the realm of “black-box” machine learning, linear models play a crucial role in model interpretation. Consider <strong>LIME</strong> (Local Interpretable Model-Agnostic Explanations) <span class="citation" data-cites="ribeiro2016lime">(<a href="../references.html#ref-ribeiro2016lime" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>, a popular technique for explaining individual predictions from any complex model. LIME works by:</p>
<ol type="1">
<li>Taking a prediction from a complex model (e.g., “This image is 92% likely to contain a cat”)</li>
<li>Generating perturbed samples around the input of interest</li>
<li>Getting predictions from the complex model for these perturbed samples</li>
<li><strong>Fitting a simple linear model</strong> to approximate the complex model’s behavior locally</li>
<li>Using the linear model’s coefficients to explain <strong>which features drove the prediction</strong></li>
</ol>
<p>In essence, LIME uses the interpretability of linear models to shed light on the darkness of black-box predictions. When we can’t understand the global behavior of a complex model, we can at least understand its local behavior through the lens of linear approximation.</p>
</section>
<section id="this-chapters-goal" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="this-chapters-goal"><span class="header-section-number">9.2.3</span> This Chapter’s Goal</h3>
<p>Our goal in this chapter is to master both the theory and practice of linear and logistic regression. We’ll develop the mathematical framework, explore the connections to maximum likelihood estimation, and learn how to implement and interpret these models in practice. Along the way, we’ll address critical questions like:</p>
<ul>
<li>How do we estimate regression coefficients and quantify our uncertainty about them?</li>
<li>What assumptions underlie linear regression, and what happens when they’re violated?</li>
<li>How do we choose which predictors to include when we have many candidates?</li>
<li>How do we extend the framework to binary outcomes?</li>
</ul>
<p>By the end of this chapter, you’ll have a thorough understanding of these fundamental models – knowledge that will serve you whether you’re building interpretable models for scientific research or using LIME to explain neural network predictions.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Historical Note: The Origins of “Regression”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The term “regression” might seem odd for a method that predicts one variable from others. Its origin lies in the work of <a href="https://en.wikipedia.org/wiki/Francis_Galton">Sir Francis Galton</a> (1822-1911), who studied the relationship between parents’ heights and their children’s heights. Galton observed that while tall parents tended to have tall children and short parents short children, the children’s heights tended to be closer to the population mean than their parents’ heights.</p>
<p>He called this phenomenon “regression towards mediocrity” (later “regression to the mean”). Though the name stuck, modern regression analysis is far more general than Galton’s original application – it’s a comprehensive framework for modeling relationships between variables.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here’s a concise reference table of key terms in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td>Regressio</td>
<td>General statistical method</td>
</tr>
<tr class="even">
<td>Linear regression</td>
<td>Lineaarinen regressio</td>
<td>Predicting continuous outcomes</td>
</tr>
<tr class="odd">
<td>Simple linear regression</td>
<td>Yhden selittäjän lineaarinen regressio</td>
<td>One predictor variable</td>
</tr>
<tr class="even">
<td>Multiple regression</td>
<td>Usean selittäjän lineaarinen regressio</td>
<td>Multiple predictor variables</td>
</tr>
<tr class="odd">
<td>Logistic regression</td>
<td>Logistinen regressio</td>
<td>Predicting binary outcomes</td>
</tr>
<tr class="even">
<td>Regression function</td>
<td>Regressiofunktio</td>
<td><span class="math inline">r(x) = \mathbb{E}(Y \mid X=x)</span></td>
</tr>
<tr class="odd">
<td>Response variable</td>
<td>Vastemuuttuja</td>
<td>Dependent variable</td>
</tr>
<tr class="even">
<td>Predictor variable</td>
<td>Selittävä muuttuja (myös: kovariaatti, piirre)</td>
<td>Independent variable</td>
</tr>
<tr class="odd">
<td>Least squares</td>
<td>Pienimmän neliösumman menetelmä</td>
<td>Parameter estimation method</td>
</tr>
<tr class="even">
<td>Residual Sum of Squares (RSS)</td>
<td>Jäännösneliösumma</td>
<td>Fit criterion minimized by OLS</td>
</tr>
<tr class="odd">
<td>Residual</td>
<td>Residuaali, jäännös</td>
<td>Difference between observed and predicted</td>
</tr>
<tr class="even">
<td>Fitted value</td>
<td>Sovitettu arvo (myös: sovite; ennustettu arvo)</td>
<td>Model prediction for a data point</td>
</tr>
<tr class="odd">
<td>Fitted line</td>
<td>Sovitettu suora</td>
<td>Line of best fit</td>
</tr>
<tr class="even">
<td>Coefficient</td>
<td>Kerroin</td>
<td>Parameter in regression equation</td>
</tr>
<tr class="odd">
<td>Intercept</td>
<td>Vakiotermi</td>
<td>Constant term in regression</td>
</tr>
<tr class="even">
<td>Slope</td>
<td>Kulmakerroin</td>
<td>Rate of change parameter</td>
</tr>
<tr class="odd">
<td>Standard error</td>
<td>Keskivirhe</td>
<td>Uncertainty of an estimate</td>
</tr>
<tr class="even">
<td>Conditional likelihood</td>
<td>Ehdollinen uskottavuus</td>
<td>Likelihood given covariates</td>
</tr>
<tr class="odd">
<td>R-squared</td>
<td>Selitysaste (<span class="math inline">R^2</span>)</td>
<td>Proportion of variance explained</td>
</tr>
<tr class="even">
<td>Training error</td>
<td>Opetusvirhe</td>
<td>In-sample error</td>
</tr>
<tr class="odd">
<td>Overfitting</td>
<td>Ylisovitus (tai: liikasovitus)</td>
<td>Model too complex for data</td>
</tr>
<tr class="even">
<td>Underfitting</td>
<td>Vajaasovitus</td>
<td>Model too simple for data</td>
</tr>
<tr class="odd">
<td>AIC</td>
<td>Akaiken informaatiokriteeri</td>
<td>Model selection criterion</td>
</tr>
<tr class="even">
<td>BIC</td>
<td>Bayes-informaatiokriteeri</td>
<td>Model selection criterion</td>
</tr>
<tr class="odd">
<td>Forward search/selection</td>
<td>Etenevä haku (myös: eteenpäin valinta)</td>
<td>Greedy model search</td>
</tr>
<tr class="even">
<td>Statistical control</td>
<td>Vakiointi</td>
<td>Including background variables</td>
</tr>
<tr class="odd">
<td>Cross-validation</td>
<td>Ristiinvalidointi</td>
<td>Model evaluation technique</td>
</tr>
<tr class="even">
<td>Leave-one-out cross-validation</td>
<td>Yksi-pois-ristiinvalidointi (myös: yksittäisristiinvalidointi)</td>
<td>CV with one held-out point</td>
</tr>
<tr class="odd">
<td>p-value</td>
<td>p-arvo</td>
<td>Significance measure for tests</td>
</tr>
<tr class="even">
<td>Confidence interval</td>
<td>Luottamusväli</td>
<td>Uncertainty quantification</td>
</tr>
<tr class="odd">
<td>Odds ratio</td>
<td>Vetosuhde</td>
<td>Effect measure in logistic regression</td>
</tr>
<tr class="even">
<td>Logit (log-odds)</td>
<td>Logit-muunnos (logaritminen vetosuhde)</td>
<td>Link in logistic regression</td>
</tr>
<tr class="odd">
<td>Interaction term</td>
<td>Vuorovaikutustermi (interaktiotermi)</td>
<td>Effect modification between predictors</td>
</tr>
<tr class="even">
<td>Homoscedasticity / Heteroscedasticity</td>
<td>Homo-/heteroskedastisuus</td>
<td>(Non-)constant error variance</td>
</tr>
<tr class="odd">
<td>Multicollinearity</td>
<td>Multikollineaarisuus</td>
<td>Correlation among predictors</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="simple-linear-regression" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="simple-linear-regression"><span class="header-section-number">9.3</span> Simple Linear Regression</h2>
<section id="regression-models" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="regression-models"><span class="header-section-number">9.3.1</span> Regression Models</h3>
<p><strong>Regression</strong> is a method for studying the relationship between a <strong>response variable</strong> <span class="math inline">Y</span> and a <strong>covariate</strong> <span class="math inline">X</span>. The response variable (also called the dependent variable) is what we’re trying to understand or predict – exam scores, blood pressure, sales revenue. The covariate is also called a <strong>predictor variable</strong> or <strong>feature</strong> – these are the variables we use to explain or predict the response, such as study hours, weight, or advertising spend.</p>
<p>When we observe pairs of these variables, we naturally ask: how does <span class="math inline">Y</span> tend to change as <span class="math inline">X</span> varies? The answer lies in the <strong>regression function</strong>:</p>
<p><span class="math display">r(x) = \mathbb{E}(Y \mid X=x) = \int y f(y \mid x) \, dy</span></p>
<p>This function tells us the expected (average) value of <span class="math inline">Y</span> for any given value of <span class="math inline">X</span>. Think of it as the systematic part of the relationship – the signal beneath the noise. If we knew <span class="math inline">r(x)</span> perfectly, we could say “when <span class="math inline">X = x</span>, we expect <span class="math inline">Y</span> to be around <span class="math inline">r(x)</span>, though individual observations will vary.”</p>
<p>Our goal is to estimate the regression function <span class="math inline">r(x)</span> from data of the form: <span class="math display">(X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n) \sim F_{X,Y}</span></p>
<p>where each pair represents one observation drawn from some joint distribution. In this chapter, we take a parametric approach and assume that <span class="math inline">r</span> has a specific functional form – namely, that it’s linear.</p>
</section>
<section id="regression-notation" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="regression-notation"><span class="header-section-number">9.3.2</span> Regression Notation</h3>
<p>Before diving into linear regression specifically, let’s establish a basic notation. <em>Any</em> regression model can be written as:</p>
<p><span class="math display">Y = \underbrace{r(X)}_{\text{signal}} + \underbrace{\epsilon}_{\text{noise}}</span></p>
<p>where <span class="math inline">\mathbb{E}(\epsilon | X) = 0</span>. This decomposition is fundamental: the observed response equals the <strong>signal</strong> (the systematic component we can predict from <span class="math inline">X</span>) plus <strong>noise</strong> or <strong>error</strong> (the random variation we cannot predict).<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why can we always write it this way?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The proof is simple. Define <span class="math inline">\epsilon = Y - r(X)</span>, then: <span class="math display">Y = Y + r(X) - r(X) = r(X) + \epsilon.</span></p>
<p>Moreover, since <span class="math inline">r(X) = \mathbb{E}(Y \mid X)</span> by definition, we have: <span class="math display">\mathbb{E}(\epsilon \mid X) = \mathbb{E}(Y - r(X) \mid X) = \mathbb{E}(Y \mid X) - r(X) = 0</span></p>
<p>This shows that the decomposition isn’t just a notational convenience – it’s a mathematical fact that any joint distribution of <span class="math inline">(X, Y)</span> can be decomposed into a predictable part (the conditional expectation) and an unpredictable part (the zero-mean error).</p>
</div>
</div>
</div>
<p>This notation separates what’s predictable (the regression function) from what’s unpredictable (the error term). The art and science of regression lies in finding good estimates for <span class="math inline">r(x)</span> from finite data.</p>
</section>
<section id="the-simple-linear-regression-model" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="the-simple-linear-regression-model"><span class="header-section-number">9.3.3</span> The Simple Linear Regression Model</h3>
<p>In <strong>simple linear regression</strong>, we assume <span class="math inline">r(x)</span> is a linear function of one-dimensional <span class="math inline">X</span>:</p>
<p><span class="math display">r(x) = \beta_0 + \beta_1 x</span></p>
<p>This defines the simple linear regression model:</p>
<div class="definition">
<p><strong>Simple Linear Regression Model</strong></p>
<p><span class="math display">Y_i = \beta_0 + \beta_1 X_i + \epsilon_i</span></p>
<p>where <span class="math inline">\mathbb{E}(\epsilon_i \mid X_i) = 0</span> and <span class="math inline">\mathbb{V}(\epsilon_i \mid X_i) = \sigma^2</span>.</p>
<p>The parameters have specific interpretations:</p>
<ul>
<li><span class="math inline">\beta_0</span> is the <strong>intercept</strong>: the expected value of <span class="math inline">Y</span> when <span class="math inline">X = 0</span></li>
<li><span class="math inline">\beta_1</span> is the <strong>slope</strong>: the expected change in <span class="math inline">Y</span> for a one-unit increase in <span class="math inline">X</span></li>
<li><span class="math inline">\sigma^2</span> is the <strong>error variance</strong>: how much individual observations vary around the line</li>
</ul>
</div>
<p>This model makes a bold claim: the relationship between <span class="math inline">X</span> and <span class="math inline">Y</span> can be adequately captured by a straight line, with all deviations from this line being random noise with constant variance. The assumption <span class="math inline">\mathbb{E}(\epsilon_i \mid X_i) = 0</span> ensures the line goes through the “middle” of the data at each value of <span class="math inline">X</span>, while <span class="math inline">\mathbb{V}(\epsilon_i \mid X_i) = \sigma^2</span> (homoscedasticity) means the scatter around the line is equally variable across all values of <span class="math inline">X</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When is Linearity Reasonable?
</div>
</div>
<div class="callout-body-container callout-body">
<p>More often than you might think! Many relationships are approximately linear, at least over the range of observed data. Even when the true relationship is nonlinear, linear regression often provides a useful first-order approximation – much like how we can approximate curves with tangent lines in calculus.</p>
<p>Consider height and weight, income and spending, or temperature and ice cream sales. While none of these relationships are perfectly linear across all possible values, they’re often reasonably linear within the range we observe. And sometimes that’s all we need for useful predictions and insights.</p>
</div>
</div>
<p>Once we’ve estimated the parameters – which we’ll discuss next –, we can make predictions and assess our model:</p>
<div class="definition">
<p><strong>Regression Terminology</strong></p>
<p>Let <span class="math inline">\hat{\beta}_0</span> and <span class="math inline">\hat{\beta}_1</span> denote estimates of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>. Then:</p>
<ul>
<li>The <strong>fitted line</strong> is: <span class="math inline">\hat{r}(x) = \hat{\beta}_0 + \hat{\beta}_1 x</span></li>
<li>The <strong>predicted values</strong> or <strong>fitted values</strong> are: <span class="math inline">\hat{Y}_i = \hat{r}(X_i) = \hat{\beta}_0 + \hat{\beta}_1 X_i</span></li>
<li>The <strong>residuals</strong> are: <span class="math inline">\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X_i)</span></li>
</ul>
</div>
<p>The residuals are crucial – they represent what our model doesn’t explain. Small residuals mean our line fits well; large residuals suggest either a poor fit or inherent variability in the relationship.</p>
</section>
<section id="estimating-parameters-the-method-of-least-squares" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="estimating-parameters-the-method-of-least-squares"><span class="header-section-number">9.3.4</span> Estimating Parameters: The Method of Least Squares</h3>
<p>Now comes the central question: given our data, how do we find the “best” line? What values should we choose for <span class="math inline">\hat{\beta}_0</span> and <span class="math inline">\hat{\beta}_1</span>?</p>
<p>The most common answer is the <strong>method of least squares</strong>. The idea is to find the line that minimizes the <strong>Residual Sum of Squares (RSS)</strong>:</p>
<p><span class="math display">\text{RSS} = \sum_{i=1}^{n} \hat{\epsilon}_i^2 = \sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)^2</span></p>
<p>The RSS measures how well the line fits the data – it’s the sum of squared vertical distances from the points to the line. The least squares estimates are the values of <span class="math inline">\hat{\beta}_0</span> and <span class="math inline">\hat{\beta}_1</span> that make RSS as small as possible.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-306-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-306-1" role="tab" aria-controls="tabset-1757255612-306-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-306-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-306-2" role="tab" aria-controls="tabset-1757255612-306-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-306-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-306-3" role="tab" aria-controls="tabset-1757255612-306-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255612-306-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-306-1-tab"><p>Imagine you’re trying to draw a line through a cloud of points on a
scatter plot. You want the line to be “close” to all the points
simultaneously. But what does “close” mean?</p><p>For each point, we have an error: how far the point lies above the
line (positive error) or below the line (negative error). We need to
aggregate these errors into a single measure of fit. We have several
options:</p><ol type="1">
<li><p><strong>Sum of errors</strong>: Won’t work – positive and
negative errors cancel out. A terrible line far above half the points
and far below the others could have zero total error!</p></li>
<li><p><strong>Sum of absolute errors</strong>: This works (no
cancellation), but absolute values are mathematically inconvenient –
they’re not differentiable at zero, making optimization harder.</p></li>
<li><p><strong>Sum of squared errors</strong>: This is the winner!
Squaring prevents cancellation, penalizes large errors more than small
ones (outliers matter), and is mathematically convenient allowing
closed-form solutions.</p></li>
</ol><p>The least squares line is the one that minimizes this sum of squared
errors. It’s the line that best “threads through” the cloud of points in
the squared-error sense.</p></div><div id="tabset-1757255612-306-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-306-2-tab"><p>To find the least squares estimates, we minimize RSS with respect to
<span class="math inline">\(\hat{\beta}_0\)</span> and
<span class="math inline">\(\hat{\beta}_1\)</span>. Taking partial
derivatives and setting them to zero:</p><p><span class="math display">\[\frac{\partial \text{RSS}}{\partial \hat{\beta}_0} = -2\sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0\]</span></p><p><span class="math display">\[\frac{\partial \text{RSS}}{\partial \hat{\beta}_1} = -2\sum_{i=1}^{n} X_i(Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0\]</span></p><p>These are called the <strong>normal equations</strong>. From the
first equation:
<span class="math display">\[\sum_{i=1}^{n} Y_i = n\hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^{n} X_i\]</span></p><p>Dividing by <span class="math inline">\(n\)</span> gives:
<span class="math inline">\(\bar{Y}_n = \hat{\beta}_0 + \hat{\beta}_1 \bar{X}_n\)</span>,
which shows the fitted line passes through
<span class="math inline">\((\bar{X}_n, \bar{Y}_n)\)</span>.</p><p>From the second equation and some algebra (expanding the products and
using the first equation), we get:
<span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n} X_i Y_i - n\bar{X}_n\bar{Y}_n}{\sum_{i=1}^{n} X_i^2 - n\bar{X}_n^2} = \frac{\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n)}{\sum_{i=1}^{n}(X_i - \bar{X}_n)^2}\]</span></p><p>This is the sample covariance of
<span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> divided by the sample variance of
<span class="math inline">\(X\)</span>.</p></div><div id="tabset-1757255612-306-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-306-3-tab"><p>Let’s see least squares in action. We’ll generate some synthetic data
with a known linear relationship plus noise, then find the least squares
line. The visualization will show two key perspectives:</p><ol type="1">
<li><strong>The fitted line with residuals</strong>: How well the line
fits the data and what the residuals look like</li>
<li><strong>The optimization landscape</strong>: How RSS changes as we
vary the slope, showing that our formula finds the minimum</li>
</ol><div id="5f0f27b1" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" data-code-fold="true" data-code-summary="Show code for visualization"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some example data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, n)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>true_beta0, true_beta1 <span class="op">=</span> <span class="dv">2</span>, <span class="fl">1.5</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> true_beta0 <span class="op">+</span> true_beta1 <span class="op">*</span> X <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">2</span>, n)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate least squares estimates</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X_mean <span class="op">=</span> np.mean(X)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Y_mean <span class="op">=</span> np.mean(Y)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>beta1_hat <span class="op">=</span> np.<span class="bu">sum</span>((X <span class="op">-</span> X_mean) <span class="op">*</span> (Y <span class="op">-</span> Y_mean)) <span class="op">/</span> np.<span class="bu">sum</span>((X <span class="op">-</span> X_mean)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>beta0_hat <span class="op">=</span> Y_mean <span class="op">-</span> beta1_hat <span class="op">*</span> X_mean</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization with stacked subplots</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Top panel: Show the residuals</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax1.scatter(X, Y, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>Y_plot <span class="op">=</span> beta0_hat <span class="op">+</span> beta1_hat <span class="op">*</span> X_plot</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>ax1.plot(X_plot, Y_plot, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Fitted line: Y = </span><span class="sc">{</span>beta0_hat<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>beta1_hat<span class="sc">:.2f}</span><span class="ss">X'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw residuals</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    Y_pred <span class="op">=</span> beta0_hat <span class="op">+</span> beta1_hat <span class="op">*</span> X[i]</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    ax1.plot([X[i], X[i]], [Y[i], Y_pred], <span class="st">'g--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Y'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Least Squares Minimizes Squared Residuals'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom panel: Show RSS as a function of slope</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>slopes <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="fl">2.5</span>, <span class="dv">100</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>rss_values <span class="op">=</span> []</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> slope <span class="kw">in</span> slopes:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    intercept <span class="op">=</span> Y_mean <span class="op">-</span> slope <span class="op">*</span> X_mean  <span class="co"># Best intercept given slope</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> Y <span class="op">-</span> (intercept <span class="op">+</span> slope <span class="op">*</span> X)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    rss <span class="op">=</span> np.<span class="bu">sum</span>(residuals<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    rss_values.append(rss)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>ax2.plot(slopes, rss_values, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>ax2.axvline(beta1_hat, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Optimal slope = </span><span class="sc">{</span>beta1_hat<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>ax2.scatter([beta1_hat], [<span class="bu">min</span>(rss_values)], color<span class="op">=</span><span class="st">'r'</span>, s<span class="op">=</span><span class="dv">100</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Slope (β₁)'</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Residual Sum of Squares'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'RSS as a Function of Slope'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Least squares estimates: β₀ = </span><span class="sc">{</span>beta0_hat<span class="sc">:.3f}</span><span class="ss">, β₁ = </span><span class="sc">{</span>beta1_hat<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimum RSS = </span><span class="sc">{</span><span class="bu">min</span>(rss_values)<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-2-output-1.png"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Least squares estimates: β₀ = 2.858, β₁ = 1.221
Minimum RSS = 80.61</code></pre>
</div>
</div><p>The top panel shows the fitted line and the residuals (green dashed
lines). Notice how the line “threads through” the cloud of points,
balancing the errors above and below. The residuals visualize what we’re
minimizing – we want these vertical distances (squared) to be as small
as possible in total.</p><p>The bottom panel reveals the optimization landscape. As we vary the
slope <span class="math inline">\(\beta_1\)</span> (while adjusting
<span class="math inline">\(\beta_0\)</span> optimally for each slope to
maintain the constraint that the line passes through
<span class="math inline">\((\bar{X}, \bar{Y})\)</span>), the RSS forms
a parabola with a clear minimum. The red dashed line marks where our
formula places us – exactly at the minimum! This confirms that the least
squares formula genuinely finds the best fit in terms of minimizing
squared errors.</p></div></div></div>
<div class="theorem" name="Least Squares Estimates">
<p>The values of <span class="math inline">\hat{\beta}_0</span> and <span class="math inline">\hat{\beta}_1</span> that minimize the RSS are:</p>
<p><span class="math display">\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n)}{\sum_{i=1}^{n}(X_i - \bar{X}_n)^2} \quad \text{(slope)}</span></p>
<p><span class="math display">\hat{\beta}_0 = \bar{Y}_n - \hat{\beta}_1 \bar{X}_n \quad \text{(intercept)}</span></p>
<p>where <span class="math inline">\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i</span> and <span class="math inline">\bar{Y}_n = \frac{1}{n}\sum_{i=1}^{n} Y_i</span> are the sample means.</p>
</div>
<p>These formulas have intuitive interpretations:</p>
<ul>
<li>The slope <span class="math inline">\hat{\beta}_1</span> is essentially the sample covariance between <span class="math inline">X</span> and <span class="math inline">Y</span> divided by the sample variance of <span class="math inline">X</span></li>
<li>The intercept <span class="math inline">\hat{\beta}_0</span> is chosen so the fitted line passes through the point <span class="math inline">(\bar{X}_n, \bar{Y}_n)</span> – the center of the data</li>
</ul>
<p>Least squares is very convenient because it has a closed-form solution. We don’t need iterative algorithms or numerical optimization<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> – just plug the data into our formulas and we get the optimal answer.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Estimating Error Variance
</div>
</div>
<div class="callout-body-container callout-body">
<p>We also need to estimate the error variance <span class="math inline">\sigma^2</span>. An unbiased estimator is:</p>
<p><span class="math display">\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^{n} \hat{\epsilon}_i^2</span></p>
<p>Why divide by <span class="math inline">n-2</span> instead of <span class="math inline">n</span>? We’ve estimated two parameters (<span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>), which costs us two degrees of freedom. This adjustment ensures our variance estimate is unbiased.</p>
</div>
</div>
</section>
<section id="connection-to-maximum-likelihood-estimation" class="level3" data-number="9.3.5">
<h3 data-number="9.3.5" class="anchored" data-anchor-id="connection-to-maximum-likelihood-estimation"><span class="header-section-number">9.3.5</span> Connection to Maximum Likelihood Estimation</h3>
<p>So far, we’ve motivated least squares geometrically – it finds the line that minimizes squared distances. But there’s a deeper connection to the likelihood principle we studied in previous chapters.</p>
<p><strong>Adding the Normality Assumption</strong></p>
<p>Suppose we strengthen our assumptions by specifying that the errors are normally distributed: <span class="math display">\epsilon_i \mid X_i \sim \mathcal{N}(0, \sigma^2)</span></p>
<p>This implies that: <span class="math display">Y_i \mid X_i \sim \mathcal{N}(\beta_0 + \beta_1 X_i, \sigma^2)</span></p>
<p>Each observation follows a normal distribution centered at the regression line, with constant variance <span class="math inline">\sigma^2</span>.</p>
<p><strong>The Likelihood Function</strong></p>
<p>Under these assumptions, we can write the likelihood function. The joint density of all observations is:</p>
<p><span class="math display">\prod_{i=1}^{n} f(X_i, Y_i) = \prod_{i=1}^{n} f_X(X_i) \times \prod_{i=1}^{n} f_{Y|X}(Y_i | X_i)</span></p>
<p>The first term doesn’t involve our parameters <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>, so we focus on the second term – the <strong>conditional likelihood</strong>:</p>
<p><span class="math display">\mathcal{L}(\beta_0, \beta_1, \sigma) = \prod_{i=1}^{n} f_{Y|X}(Y_i | X_i) \propto \sigma^{-n} \exp\left\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2\right\}</span></p>
<p>The conditional log-likelihood is:</p>
<p><span class="math display">\ell(\beta_0, \beta_1, \sigma) = -n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>To maximize the log-likelihood with respect to <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>, we must minimize the sum: <span class="math display">\sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2</span></p>
<p>But this is exactly the RSS! Therefore, under the normality assumption, <strong>the least squares estimators are also the maximum likelihood estimators</strong>.</p>
<p>This is a profound connection: the simple geometric idea of minimizing squared distances coincides with the principled statistical approach of maximum likelihood, at least when errors are normal.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What about estimating σ²?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>From the conditional log-likelihood above we can also derive the MLE for the error variance: <span class="math display">\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^{n} \hat{\epsilon}_i^2</span></p>
<p>However, this estimator is biased – it systematically underestimates the true variance. In practice, we use the unbiased estimator: <span class="math display">\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^{n} \hat{\epsilon}_i^2</span></p>
<p>The difference is small for large <span class="math inline">n</span>, but the unbiased version provides more accurate confidence intervals and hypothesis tests, which is why it’s standard in linear regression.</p>
</div>
</div>
</div>
</section>
<section id="properties-of-the-least-squares-estimators" class="level3" data-number="9.3.6">
<h3 data-number="9.3.6" class="anchored" data-anchor-id="properties-of-the-least-squares-estimators"><span class="header-section-number">9.3.6</span> Properties of the Least Squares Estimators</h3>
<p>Understanding the statistical properties of our estimates is crucial for inference. How accurate are they? How does accuracy improve with more data? Can we quantify our uncertainty?</p>
<p><strong>Finite Sample Properties</strong></p>
<div class="theorem">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_0, \hat{\beta}_1)^T</span> denote the least squares estimators. Assume that:</p>
<ol type="1">
<li><span class="math inline">\mathbb{E}(\epsilon_i \mid X^n) = 0</span> for all <span class="math inline">i</span></li>
<li><span class="math inline">\mathbb{V}(\epsilon_i \mid X^n) = \sigma^2</span> for all <span class="math inline">i</span> (homoscedasticity)</li>
<li><span class="math inline">\text{Cov}(\epsilon_i, \epsilon_j \mid X^n) = 0</span> for <span class="math inline">i \neq j</span> (uncorrelated errors)</li>
</ol>
<p>Then, conditional on <span class="math inline">X^n = (X_1, \ldots, X_n)</span>:</p>
<p><span class="math display">\mathbb{E}(\hat{\beta} \mid X^n) = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}</span></p>
<p><span class="math display">\mathbb{V}(\hat{\beta} \mid X^n) = \frac{\sigma^2}{n s_X^2} \begin{pmatrix} \frac{1}{n}\sum_{i=1}^{n} X_i^2 &amp; -\bar{X}_n \\ -\bar{X}_n &amp; 1 \end{pmatrix}</span></p>
<p>where <span class="math inline">s_X^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2</span> is the sample variance of <span class="math inline">X</span>.</p>
</div>
<p>This theorem tells us that the least squares estimators are <strong>unbiased</strong> – on average, they hit the true values. The variance formula allows us to compute standard errors:</p>
<p><span class="math display">\widehat{\text{se}}(\hat{\beta}_0) = \frac{\hat{\sigma}}{s_X \sqrt{n}} \sqrt{\frac{\sum_{i=1}^{n} X_i^2}{n}} \quad \text{and} \quad \widehat{\text{se}}(\hat{\beta}_1) = \frac{\hat{\sigma}}{s_X \sqrt{n}}</span></p>
<p>These standard errors quantify the uncertainty in our estimates. Notice that both decrease with <span class="math inline">\sqrt{n}</span> – more data means more precision.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced: Derivation of Standard Errors
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The variance formula comes from the fact that <span class="math inline">\hat{\beta}_1</span> is a linear combination of the <span class="math inline">Y_i</span>’s: <span class="math display">\hat{\beta}_1 = \sum_{i=1}^{n} w_i Y_i</span> where <span class="math inline">w_i = \frac{X_i - \bar{X}_n}{\sum_{j=1}^{n}(X_j - \bar{X}_n)^2}</span>.</p>
<p>Since the <span class="math inline">Y_i</span>’s are independent with variance <span class="math inline">\sigma^2</span>: <span class="math display">\text{Var}(\hat{\beta}_1 \mid X^n) = \sigma^2 \sum_{i=1}^{n} w_i^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(X_i - \bar{X}_n)^2} = \frac{\sigma^2}{n s_X^2}</span></p>
<p>The derivation for <span class="math inline">\hat{\beta}_0</span> is similar but more involved due to its dependence on both <span class="math inline">\bar{Y}_n</span> and <span class="math inline">\hat{\beta}_1</span>.</p>
</div>
</div>
</div>
<p><strong>Asymptotic Properties and Inference</strong></p>
<div class="theorem">
<p>Under appropriate regularity conditions, as <span class="math inline">n \to \infty</span>:</p>
<ol type="1">
<li><p><strong>Consistency</strong>: <span class="math inline">\hat{\beta}_0 \xrightarrow{P} \beta_0</span> and <span class="math inline">\hat{\beta}_1 \xrightarrow{P} \beta_1</span></p></li>
<li><p><strong>Asymptotic Normality</strong>: <span class="math display">\frac{\hat{\beta}_0 - \beta_0}{\widehat{\text{se}}(\hat{\beta}_0)} \rightsquigarrow \mathcal{N}(0, 1) \quad \text{and} \quad \frac{\hat{\beta}_1 - \beta_1}{\widehat{\text{se}}(\hat{\beta}_1)} \rightsquigarrow \mathcal{N}(0, 1)</span></p></li>
<li><p><strong>Confidence Intervals</strong>: An approximate <span class="math inline">(1-\alpha)</span> confidence interval for <span class="math inline">\beta_j</span> is: <span class="math display">\hat{\beta}_j \pm z_{\alpha/2} \cdot \widehat{\text{se}}(\hat{\beta}_j)</span></p></li>
<li><p><strong>Hypothesis Testing</strong>: The Wald test for <span class="math inline">H_0: \beta_1 = 0</span> vs.&nbsp;<span class="math inline">H_1: \beta_1 \neq 0</span> is: reject <span class="math inline">H_0</span> if <span class="math inline">|W| &gt; z_{\alpha/2}</span> where <span class="math inline">W = \hat{\beta}_1 / \widehat{\text{se}}(\hat{\beta}_1)</span></p></li>
</ol>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finite Sample Refinement
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For finite <span class="math inline">n</span>, a more accurate test uses the Student’s <span class="math inline">t</span>-distribution with <span class="math inline">n-2</span> degrees of freedom rather than the normal distribution. This accounts for the additional uncertainty from estimating <span class="math inline">\sigma^2</span>. Most statistical software uses the <span class="math inline">t</span>-distribution by default for regression inference.</p>
</div>
</div>
</div>
</section>
<section id="simple-linear-regression-in-practice" class="level3" data-number="9.3.7">
<h3 data-number="9.3.7" class="anchored" data-anchor-id="simple-linear-regression-in-practice"><span class="header-section-number">9.3.7</span> Simple Linear Regression in Practice</h3>
<p>Let’s see how these concepts work with real data. We’ll use the classic Framingham Heart Study dataset to explore the relationship between weight and blood pressure, building our understanding step by step.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: The Framingham Heart Study
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Framingham Heart Study is a long-term cardiovascular study that began in 1948 in Framingham, Massachusetts. This landmark study has provided crucial insights into cardiovascular disease risk factors. We’ll examine the relationship between <strong>relative weight</strong> (FRW - a normalized weight measure where 100 represents median weight for height) and <strong>systolic blood pressure</strong> (SBP - the pressure when the heart beats).</p>
</div>
</div>
<section id="step-1-loading-and-exploring-the-data" class="level4">
<h4 class="anchored" data-anchor-id="step-1-loading-and-exploring-the-data">Step 1: Loading and Exploring the Data</h4>
<p>First, let’s load the data and understand what we’re working with:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-805-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-805-1" role="tab" aria-controls="tabset-1757255612-805-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-805-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-805-2" role="tab" aria-controls="tabset-1757255612-805-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-805-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-805-1-tab"><div id="72dd648f" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Framingham data</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>fram <span class="op">=</span> pd.read_csv(<span class="st">'../data/fram.txt'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first few rows to understand the structure</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 6 rows of the Framingham data:"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fram.head(<span class="dv">6</span>))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Dataset shape: </span><span class="sc">{</span>fram<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Columns: </span><span class="sc">{</span><span class="bu">list</span>(fram.columns)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>First 6 rows of the Framingham data:
         SEX  AGE  FRW  SBP  SBP10  DBP  CHOL  CIG  CHD YRS_CHD  DEATH  \
ID                                                                       
4988  female   57  135  186    NaN  120   150    0    1     pre      7   
3001  female   60  123  165    NaN  100   167   25    0      16     10   
5079  female   54  115  140    NaN   90   213    5    0       8      8   
5162  female   52  102  170    NaN  104   280   15    0      10      7   
4672  female   45   99  185    NaN  105   326   20    0       8     10   
5822  female   51   93  142    NaN   90   234   35    0       4      8   

      YRS_DTH    CAUSE  
ID                      
4988       11  unknown  
3001       17  unknown  
5079       13  unknown  
5162       11  unknown  
4672       17  unknown  
5822       13  unknown  

Dataset shape: (1394, 13)
Columns: ['SEX', 'AGE', 'FRW', 'SBP', 'SBP10', 'DBP', 'CHOL', 'CIG', 'CHD', 'YRS_CHD', 'DEATH', 'YRS_DTH', 'CAUSE']</code></pre>
</div>
</div><div id="b3497e84" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Focus on our variables of interest</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Summary statistics for key variables:"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fram[[<span class="st">'FRW'</span>, <span class="st">'SBP'</span>]].describe())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Summary statistics for key variables:
               FRW          SBP
count  1394.000000  1394.000000
mean    105.365136   148.086083
std      17.752489    28.022062
min      52.000000    90.000000
25%      94.000000   130.000000
50%     103.000000   142.000000
75%     114.000000   160.000000
max     222.000000   300.000000</code></pre>
</div>
</div></div><div id="tabset-1757255612-805-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-805-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fram <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'../data/fram.txt'</span>, <span class="at">sep=</span><span class="st">'</span><span class="sc">\t</span><span class="st">'</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first few rows to understand the structure</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"First 6 rows of the Framingham data:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(fram)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Dataset dimensions:"</span>, <span class="fu">dim</span>(fram), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Column names:"</span>, <span class="fu">names</span>(fram), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary statistics for key variables</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fram[<span class="fu">c</span>(<span class="st">"FRW"</span>, <span class="st">"SBP"</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding the Variables
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The dataset contains 1,394 observations with 13 variables including demographics, behaviors, and health outcomes.</li>
<li><strong>FRW (Framingham Relative Weight)</strong>: A normalized measure where 100 represents the median weight for a given height. Values above 100 indicate above-median weight.</li>
<li><strong>SBP (Systolic Blood Pressure)</strong>: Measured in mmHg, normal range is typically below 120. Values ≥140 indicate hypertension.</li>
</ul>
</div>
</div>
</section>
<section id="step-2-initial-visualization" class="level4">
<h4 class="anchored" data-anchor-id="step-2-initial-visualization">Step 2: Initial Visualization</h4>
<p>Before fitting any model, let’s visualize the relationship between weight and blood pressure:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-415-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-415-1" role="tab" aria-controls="tabset-1757255612-415-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-415-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-415-2" role="tab" aria-controls="tabset-1757255612-415-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-415-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-415-1-tab"><div id="1c735a95" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scatter plot to explore the relationship</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'FRW'</span>], fram[<span class="st">'SBP'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Relative Weight (FRW)'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Systolic Blood Pressure (SBP)'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Relationship between Relative Weight and Blood Pressure'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-5-output-1.png"></p>
</div>
</div></div><div id="tabset-1757255612-415-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-415-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scatter plot to explore the relationship</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fram<span class="sc">$</span>FRW, fram<span class="sc">$</span>SBP, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Relative Weight (FRW)"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Systolic Blood Pressure (SBP)"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Relationship between Relative Weight and Blood Pressure"</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What the Visualization Tells Us
</div>
</div>
<div class="callout-body-container callout-body">
<p>Looking at this scatter plot, we can observe:</p>
<ul>
<li>There appears to be a <strong>positive relationship</strong>: As relative weight increases, blood pressure tends to increase.</li>
<li>There’s <strong>substantial variation</strong>: The wide spread of points suggests that weight alone won’t perfectly predict blood pressure - other factors must also be important.</li>
<li>The pattern looks <strong>compatible with the linearity hypothesis</strong>, in the sense that there’s no obvious curve or nonlinear pattern that would suggest a straight line is inappropriate.</li>
</ul>
<p>This visualization motivates our next step: fitting a linear model to quantify this relationship.</p>
</div>
</div>
</section>
<section id="step-3-fitting-the-linear-regression-model" class="level4">
<h4 class="anchored" data-anchor-id="step-3-fitting-the-linear-regression-model">Step 3: Fitting the Linear Regression Model</h4>
<p>Now let’s fit the simple linear regression model: <span class="math inline">\text{SBP} = \beta_0 + \beta_1 \cdot \text{FRW} + \epsilon</span>.</p>
<p>Linear regression with a quadratic loss is also called “ordinary least squares” (OLS), a term which can be found in statistical software like in Python’s <code>statsmodel</code> package.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-393-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-393-1" role="tab" aria-controls="tabset-1757255612-393-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-393-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-393-2" role="tab" aria-controls="tabset-1757255612-393-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-393-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-393-1-tab"><div id="fcffaa31" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear regression model using statsmodels</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW'</span>, data<span class="op">=</span>fram)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> model.fit()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the fitted equation</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fitted equation: SBP = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">'Intercept'</span>]<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss"> * FRW"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitted equation: SBP = 92.87 + 0.524 * FRW</code></pre>
</div>
</div></div><div id="tabset-1757255612-393-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-393-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the linear regression model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW, <span class="at">data =</span> fram)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the fitted equation</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Fitted equation: SBP = %.2f + %.3f * FRW</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>            <span class="fu">coef</span>(fit)[<span class="dv">1</span>], <span class="fu">coef</span>(fit)[<span class="dv">2</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<p>We’ve now fitted our linear regression model. The fitted equation shows the estimated relationship between relative weight and blood pressure. In the following steps, we’ll use this model to make predictions, visualize the fit, check our assumptions, and interpret the statistical significance of our findings.</p>
</section>
<section id="step-4-making-predictions" class="level4">
<h4 class="anchored" data-anchor-id="step-4-making-predictions">Step 4: Making Predictions</h4>
<p>Let’s use our model to make predictions for specific weight values:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-130-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-130-1" role="tab" aria-controls="tabset-1757255612-130-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-130-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-130-2" role="tab" aria-controls="tabset-1757255612-130-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-130-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-130-1-tab"><div id="8bea42c0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions for specific FRW values</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>test_weights <span class="op">=</span> pd.DataFrame({<span class="st">'FRW'</span>: [<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>]})</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> results.predict(test_weights)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predictions for new data points:"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">40</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> frw, sbp <span class="kw">in</span> <span class="bu">zip</span>(test_weights[<span class="st">'FRW'</span>], predictions):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"FRW = </span><span class="sc">{</span>frw<span class="sc">:3d}</span><span class="ss"> → Predicted SBP = </span><span class="sc">{</span>sbp<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual calculation to show the mechanics</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Manual calculation (verifying our understanding):"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>beta0 <span class="op">=</span> results.params[<span class="st">'Intercept'</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> results.params[<span class="st">'FRW'</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> frw <span class="kw">in</span> [<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>]:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    manual_pred <span class="op">=</span> beta0 <span class="op">+</span> beta1 <span class="op">*</span> frw</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"FRW = </span><span class="sc">{</span>frw<span class="sc">:3d}</span><span class="ss"> → </span><span class="sc">{</span>beta0<span class="sc">:.3f}</span><span class="ss"> + </span><span class="sc">{</span>beta1<span class="sc">:.3f}</span><span class="ss"> × </span><span class="sc">{</span>frw<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>manual_pred<span class="sc">:.1f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predictions for new data points:
========================================
FRW =  80 → Predicted SBP = 134.8
FRW = 100 → Predicted SBP = 145.3
FRW = 120 → Predicted SBP = 155.8

Manual calculation (verifying our understanding):
FRW =  80 → 92.866 + 0.524 × 80 = 134.8
FRW = 100 → 92.866 + 0.524 × 100 = 145.3
FRW = 120 → 92.866 + 0.524 × 120 = 155.8</code></pre>
</div>
</div></div><div id="tabset-1757255612-130-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-130-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions for specific FRW values</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>test_weights <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">FRW =</span> <span class="fu">c</span>(<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> test_weights)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Predictions for new data points:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="fu">rep</span>(<span class="st">"="</span>, <span class="dv">40</span>), <span class="at">collapse=</span><span class="st">""</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(test_weights)) {</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"FRW = %3d → Predicted SBP = %.1f</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                test_weights<span class="sc">$</span>FRW[i], predictions[i]))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual calculation</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)[<span class="dv">1</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Manual calculation (verifying our understanding):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(frw <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>)) {</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    manual_pred <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> frw</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"FRW = %3d → %.3f + %.3f × %d = %.1f</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>                frw, beta0, beta1, frw, manual_pred))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="step-5-visualizing-the-fitted-model" class="level4">
<h4 class="anchored" data-anchor-id="step-5-visualizing-the-fitted-model">Step 5: Visualizing the Fitted Model</h4>
<p>Let’s plot the fitted model:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-604-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-604-1" role="tab" aria-controls="tabset-1757255612-604-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-604-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-604-2" role="tab" aria-controls="tabset-1757255612-604-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-604-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-604-1-tab"><div id="cd8ad68d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic fitted line visualization</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'FRW'</span>], fram[<span class="st">'SBP'</span>], alpha<span class="op">=</span><span class="fl">0.4</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(fram[<span class="st">'FRW'</span>].<span class="bu">min</span>(), fram[<span class="st">'FRW'</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> results.params[<span class="st">'Intercept'</span>] <span class="op">+</span> results.params[<span class="st">'FRW'</span>] <span class="op">*</span> x_range</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, y_pred, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted line'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Relative Weight (FRW)'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Systolic Blood Pressure (SBP)'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Fitted Regression Line'</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Add equation to the plot</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>equation <span class="op">=</span> <span class="ss">f'SBP = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"Intercept"</span>]<span class="sc">:.1f}</span><span class="ss"> + </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"FRW"</span>]<span class="sc">:.3f}</span><span class="ss"> × FRW'</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, equation, transform<span class="op">=</span>plt.gca().transAxes, fontsize<span class="op">=</span><span class="dv">11</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>         verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-8-output-1.png"></p>
</div>
</div></div><div id="tabset-1757255612-604-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-604-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic fitted line visualization</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fram<span class="sc">$</span>FRW, fram<span class="sc">$</span>SBP, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Relative Weight (FRW)"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Systolic Blood Pressure (SBP)"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Fitted Regression Line"</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.4</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add equation to the plot</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>equation <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"SBP ="</span>, <span class="fu">round</span>(<span class="fu">coef</span>(fit)[<span class="dv">1</span>], <span class="dv">1</span>), <span class="st">"+"</span>, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">round</span>(<span class="fu">coef</span>(fit)[<span class="dv">2</span>], <span class="dv">3</span>), <span class="st">"× FRW"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> equation, </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">bty =</span> <span class="st">"n"</span>, <span class="at">cex =</span> <span class="fl">1.1</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.col =</span> <span class="st">"black"</span>, <span class="at">bg =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="fl">0.96</span>, <span class="fl">0.8</span>, <span class="fl">0.8</span>))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<p>The plot above shows our fitted regression line. The equation in the box gives us the specific relationship: for each unit increase in relative weight (FRW), systolic blood pressure increases by approximately half a mmHg on average (see the exact coefficient in the equation above).</p>
<p>The plot below shows the geometric interpretation of intercept (<span class="math inline">\beta_0</span>) and slope (<span class="math inline">\beta_1</span>) parameters:</p>
<div id="bc4fa64e" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Geometric interpretation of intercept and slope</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'FRW'</span>], fram[<span class="st">'SBP'</span>], alpha<span class="op">=</span><span class="fl">0.4</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extend x range to show intercept</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x_extended <span class="op">=</span> np.linspace(<span class="dv">0</span>, fram[<span class="st">'FRW'</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>y_extended <span class="op">=</span> results.params[<span class="st">'Intercept'</span>] <span class="op">+</span> results.params[<span class="st">'FRW'</span>] <span class="op">*</span> x_extended</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x_extended, y_extended, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Show intercept (β₀) at x=0</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, results.params[<span class="st">'Intercept'</span>]], <span class="st">'green'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">0</span>, results.params[<span class="st">'Intercept'</span>], <span class="st">'go'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="vs">r'$\beta_0$'</span> <span class="op">+</span> <span class="ss">f' = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"Intercept"</span>]<span class="sc">:.1f}</span><span class="ss">'</span>, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>            xy<span class="op">=</span>(<span class="dv">0</span>, results.params[<span class="st">'Intercept'</span>]), xytext<span class="op">=</span>(<span class="dv">10</span>, results.params[<span class="st">'Intercept'</span>] <span class="op">-</span> <span class="dv">10</span>),</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">'green'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Show slope (β₁) interpretation</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>x_demo <span class="op">=</span> [<span class="dv">40</span>, <span class="dv">60</span>]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>y_demo <span class="op">=</span> [results.params[<span class="st">'Intercept'</span>] <span class="op">+</span> results.params[<span class="st">'FRW'</span>] <span class="op">*</span> x <span class="cf">for</span> x <span class="kw">in</span> x_demo]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.plot(x_demo, [y_demo[<span class="dv">0</span>], y_demo[<span class="dv">0</span>]], <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.plot([x_demo[<span class="dv">1</span>], x_demo[<span class="dv">1</span>]], y_demo, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Δx = 20'</span>, xy<span class="op">=</span>(<span class="dv">50</span>, y_demo[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">15</span>), fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'blue'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Δy = </span><span class="sc">{</span><span class="dv">20</span> <span class="op">*</span> results<span class="sc">.</span>params[<span class="st">"FRW"</span>]<span class="sc">:.1f}</span><span class="ss">'</span>, xy<span class="op">=</span>(x_demo[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">2</span>, np.mean(y_demo)), </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'blue'</span>, rotation<span class="op">=</span><span class="dv">90</span>, va<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="dv">20</span>, y_demo[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">25</span>, <span class="vs">r'$\beta_1$'</span> <span class="op">+</span> <span class="ss">f' = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"FRW"</span>]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>         fontsize<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">'blue'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">5</span>, fram[<span class="st">'FRW'</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">5</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, fram[<span class="st">'SBP'</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Relative Weight (FRW)'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Systolic Blood Pressure (SBP)'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r'Geometric Interpretation: $\beta_0$ (intercept) and $\beta_1$ (slope)'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="step-6-model-diagnostics" class="level4">
<h4 class="anchored" data-anchor-id="step-6-model-diagnostics">Step 6: Model Diagnostics</h4>
<p>Before interpreting our regression results, let’s check if our model assumptions are satisfied using diagnostic plots:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-522-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-522-1" role="tab" aria-controls="tabset-1757255612-522-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-522-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-522-2" role="tab" aria-controls="tabset-1757255612-522-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-522-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-522-1-tab"><div id="cb0accf0" class="cell" data-fig-height="8" data-execution_count="9">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create diagnostic plots</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get residuals and fitted values</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> results.resid</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fitted <span class="op">=</span> results.fittedvalues</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Residuals vs Fitted Values</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>ax1.scatter(fitted, residuals, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Residuals vs Fitted Values'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Q-Q plot for normality check</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>stats.probplot(residuals, dist<span class="op">=</span><span class="st">"norm"</span>, plot<span class="op">=</span>ax2)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Normal Q-Q Plot'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-10-output-1.png"></p>
</div>
</div></div><div id="tabset-1757255612-522-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-522-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create diagnostic plots</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Residuals vs Fitted</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Q-Q plot</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">which =</span> <span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset plot layout</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What the Diagnostic Plots Tell Us
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Residuals vs Fitted</strong>: The residuals are approximately centered around zero with reasonably random scatter, though we can see some evidence of skew (asymmetry) in the distribution.</p>
<p><strong>Q-Q Plot (Quantile-Quantile Plot)</strong>: This plot compares the distribution of our residuals to a normal distribution. If residuals were perfectly normal, all points would fall exactly on the diagonal line. Here we see the points roughly follow the line but with some deviation at the extremes (the tails), confirming that the residuals are not perfectly normally distributed.</p>
<p><strong>Important</strong>: These minor violations of assumptions are common with real data and not necessarily dealbreakers. Linear regression is quite robust to modest departures from normality, especially with large samples like ours (n=1,394). However, we should keep these limitations in mind when interpreting results and consider transformations or robust methods if violations become severe.</p>
</div>
</div>
</section>
<section id="step-7-model-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="step-7-model-interpretation">Step 7: Model Interpretation</h4>
<p>Now let’s examine and interpret the full regression output:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-141-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-141-1" role="tab" aria-controls="tabset-1757255612-141-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-141-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-141-2" role="tab" aria-controls="tabset-1757255612-141-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-141-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-141-1-tab"><div id="dabd8ab1" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the full regression summary</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    SBP   R-squared:                       0.110
Model:                            OLS   Adj. R-squared:                  0.110
Method:                 Least Squares   F-statistic:                     172.5
Date:                Sun, 07 Sep 2025   Prob (F-statistic):           3.18e-37
Time:                        17:33:30   Log-Likelihood:                -6542.3
No. Observations:                1394   AIC:                         1.309e+04
Df Residuals:                    1392   BIC:                         1.310e+04
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     92.8658      4.264     21.778      0.000      84.501     101.231
FRW            0.5241      0.040     13.132      0.000       0.446       0.602
==============================================================================
Omnibus:                      338.464   Durbin-Watson:                   1.756
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              883.998
Skew:                           1.271   Prob(JB):                    1.10e-192
Kurtosis:                       5.959   Cond. No.                         643.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div><p><strong>Note:</strong> Python’s statsmodels output is more verbose
than R’s. Focus on these key sections:</p><ul>
<li><strong>Coefficients table</strong> (middle): Shows estimates,
standard errors, t-statistics, and p-values</li>
<li><strong>R-squared</strong> (top-right): Proportion of variance
explained</li>
<li><strong>F-statistic</strong> (top-right): Tests overall model
significance</li>
</ul></div><div id="tabset-1757255612-141-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-141-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the regression summary</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpreting the Regression Output
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Key Statistical Tests:</strong></p>
<ol type="1">
<li>The reported p-values for individual coefficients use the <strong>Wald Test</strong>:
<ul>
<li><span class="math inline">H_0: \beta_j = 0</span> (coefficient equals zero, no effect)</li>
<li>Test statistic: <span class="math inline">t = \hat{\beta}_j / \text{SE}(\hat{\beta}_j)</span></li>
<li>Our result: FRW p-value &lt; 0.001 → reject <span class="math inline">H_0</span> → weight significantly affects blood pressure</li>
</ul></li>
<li><strong>F-Test</strong> is used for overall model significance
<ul>
<li><span class="math inline">H_0: \beta_1 = \beta_2 = \ldots = \beta_k = 0</span> (no predictors have any effect)</li>
<li>Tests if <strong>any</strong> coefficient is non-zero</li>
<li>Our result: F = 172.5, p &lt; 0.001 → reject <span class="math inline">H_0</span> → model is useful</li>
</ul></li>
<li><strong>R-Squared</strong> (<span class="math inline">R^2</span>, coefficient of determination)
<ul>
<li>Proportion of variance in <span class="math inline">Y</span> explained by the model (typically between 0 and 1)</li>
<li>Measures accuracy on training data</li>
<li>In our example: model explains roughly 11% of blood pressure variation (see exact value in output above)</li>
<li>Low <span class="math inline">R^2</span> common when many unmeasured factors affect outcome, which happens for many real-world data</li>
</ul></li>
</ol>
<p><strong>Reading the Coefficients Table:</strong></p>
<ul>
<li><strong>Intercept (<span class="math inline">\beta_0</span>)</strong>: Expected SBP when FRW = 0 (extrapolation - not meaningful here)</li>
<li><strong>FRW coefficient (<span class="math inline">\beta_1</span>)</strong>: Each unit increase in relative weight increases SBP by approximately 0.5 mmHg on average (see exact value in output)</li>
<li><strong>Standard errors</strong>: Quantify uncertainty in estimates</li>
<li><strong>95% CI for <span class="math inline">\beta_1</span></strong>: Check the confidence interval in the output - it quantifies our uncertainty about the true slope</li>
</ul>
<p><strong>Model Quality Assessment:</strong></p>
<ul>
<li><strong>Residual Standard Error</strong>: Typical prediction error (see output for exact value)</li>
<li><strong>Statistical vs.&nbsp;Practical Significance</strong>: While statistically significant (p &lt; 0.001), the effect size is modest</li>
<li><strong>Predictive Power</strong>: The relatively low <span class="math inline">R^2</span> indicates that weight alone is not a strong predictor of blood pressure</li>
</ul>
<p><strong>Key Takeaway</strong>: The regression confirms a statistically significant positive relationship between weight and blood pressure. However, the modest <span class="math inline">R^2</span> reminds us that blood pressure is multifactorial – diet, exercise, genetics, stress, and many other factors play important roles.</p>
</div>
</div>
</section>
<section id="summary-what-weve-learned" class="level4">
<h4 class="anchored" data-anchor-id="summary-what-weve-learned">Summary: What We’ve Learned</h4>
<p>Through this comprehensive example, we’ve followed a complete regression workflow:</p>
<ol type="1">
<li><strong>Data Exploration</strong>: Understood our variables and their context</li>
<li><strong>Initial Visualization</strong>: Examined the relationship visually before modeling</li>
<li><strong>Model Fitting</strong>: Applied least squares to find the best-fitting line</li>
<li><strong>Making Predictions</strong>: Used the model to predict new values</li>
<li><strong>Visualizing the Fit</strong>: Showed the fitted line</li>
<li><strong>Model Diagnostics</strong>: Checked assumptions through residual plots</li>
<li><strong>Model Interpretation</strong>: Understood the statistical tests and what they tell us</li>
</ol>
<p>The Framingham data reveals an important finding: weight has a statistically significant effect on blood pressure (p &lt; 0.001), with each unit increase in relative weight associated with approximately a 0.5 mmHg increase in systolic blood pressure (see the exact coefficient in the regression output above).</p>
<p>The modest <span class="math inline">R^2</span> (around 11% of variance explained) doesn’t diminish the medical importance of this relationship. Rather, it reminds us that:</p>
<ul>
<li>Blood pressure is multifactorial - weight is one important factor among many (age, diet, exercise, genetics, stress)</li>
<li>Even when individual predictors explain modest variance, they can still be clinically meaningful</li>
<li>Simple linear regression effectively quantifies real-world relationships and their uncertainty</li>
</ul>
</section>
</section>
</section>
<section id="multiple-linear-regression" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">9.4</span> Multiple Linear Regression</h2>
<section id="extending-the-model-to-multiple-predictors" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="extending-the-model-to-multiple-predictors"><span class="header-section-number">9.4.1</span> Extending the Model to Multiple Predictors</h3>
<p>Real-world phenomena rarely depend on just one predictor. A person’s blood pressure isn’t determined solely by their weight – age, diet, exercise, genetics, and countless other factors play roles. Multiple linear regression extends our framework to handle multiple predictors simultaneously.</p>
<section id="the-model" class="level4">
<h4 class="anchored" data-anchor-id="the-model">The Model</h4>
<p>With <span class="math inline">k</span> predictors (covariates), the multiple linear regression model becomes:</p>
<p><span class="math display">Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \epsilon_i, \quad i = 1, \ldots, n</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">Y_i</span> is the response for observation <span class="math inline">i</span></li>
<li><span class="math inline">X_{ij}</span> is the value of the <span class="math inline">j</span>-th covariate for observation <span class="math inline">i</span></li>
<li><span class="math inline">\beta_0</span> is the intercept</li>
<li><span class="math inline">\beta_j</span> is the coefficient for the <span class="math inline">j</span>-th covariate (for <span class="math inline">j = 1, \ldots, k</span>)</li>
<li><span class="math inline">\epsilon_i</span> is the error term with <span class="math inline">\mathbb{E}(\epsilon_i \mid X_i) = 0</span> and <span class="math inline">\mathbb{V}(\epsilon_i \mid X_i) = \sigma^2</span></li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Convention: Incorporating the Intercept
</div>
</div>
<div class="callout-body-container callout-body">
<p>The intercept <span class="math inline">\beta_0</span> is often incorporated directly into the covariate notation by defining <span class="math inline">X_{i0} = 1</span> for all observations <span class="math inline">i = 1, \ldots, n</span>. This allows us to write the model more compactly as: <span class="math display">Y_i = \sum_{j=0}^{k} \beta_j X_{ij} + \epsilon_i</span></p>
<p>This convention simplifies matrix notation and many derivations. When you see design matrices or covariate vectors, check whether the intercept is handled explicitly (with a column of ones) or implicitly (assumed but not shown).</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Indexing Confusion: Starting from 0 vs 1
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You’ll encounter two indexing conventions in the literature:</p>
<p><strong>Convention 1 (0-indexed, used here in the lecture notes):</strong></p>
<ul>
<li><span class="math inline">X_{i0} = 1</span> for the intercept</li>
<li><span class="math inline">X_{i1}, X_{i2}, \ldots, X_{ik}</span> for the <span class="math inline">k</span> actual covariates<br>
</li>
<li>Design matrix <span class="math inline">\mathbf{X}</span> is <span class="math inline">n \times (k+1)</span></li>
<li>Model: <span class="math inline">Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_k X_{ik} + \epsilon_i</span></li>
</ul>
<p><strong>Convention 2 (1-indexed, common in some texts):</strong></p>
<ul>
<li><span class="math inline">X_{i1} = 1</span> for the intercept</li>
<li><span class="math inline">X_{i2}, X_{i3}, \ldots, X_{i,k+1}</span> for the <span class="math inline">k</span> actual covariates</li>
<li>Design matrix <span class="math inline">\mathbf{X}</span> is still <span class="math inline">n \times (k+1)</span></li>
<li>Model: <span class="math inline">Y_i = \beta_1 + \beta_2 X_{i2} + \cdots + \beta_{k+1} X_{i,k+1} + \epsilon_i</span></li>
</ul>
<p>Both are correct! The key is consistency within a given analysis. Software typically handles this transparently – R’s <code>lm()</code> and Python’s <code>statsmodels</code> automatically add the intercept column regardless of your indexing preference.</p>
</div>
</div>
</div>
<div class="definition">
<p><strong>Multiple Linear Regression in Matrix Form</strong></p>
<p>The model is more elegantly expressed using matrix notation: <span class="math display">\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\mathbf{Y}</span> is an <span class="math inline">n \times 1</span> vector of responses</li>
<li><span class="math inline">\mathbf{X}</span> is an <span class="math inline">n \times (k+1)</span> design matrix (often written as <span class="math inline">n \times k</span> when the intercept is implicit)</li>
<li><span class="math inline">\boldsymbol{\beta}</span> is a <span class="math inline">(k+1) \times 1</span> vector of coefficients (or <span class="math inline">k \times 1</span> when intercept is implicit)</li>
<li><span class="math inline">\boldsymbol{\epsilon}</span> is an <span class="math inline">n \times 1</span> vector of errors</li>
</ul>
</div>
<p>Explicitly (showing the column of 1s separately from the actual covariates): <span class="math display">\begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix} = \begin{pmatrix} 1 &amp; X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1k} \\ 1 &amp; X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2k} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{nk} \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_k \end{pmatrix} + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}</span></p>
<p>Here:</p>
<ul>
<li>The first column of 1s corresponds to what we defined as <span class="math inline">X_{i0} = 1</span> for the intercept</li>
<li><span class="math inline">X_{i1}, X_{i2}, \ldots, X_{ik}</span> are the values of the <span class="math inline">k</span> actual covariates for observation <span class="math inline">i</span></li>
<li>The <span class="math inline">n</span> rows correspond to different observations</li>
<li>The <span class="math inline">k+1</span> columns correspond to the intercept and <span class="math inline">k</span> covariates</li>
</ul>
</section>
<section id="least-squares-in-matrix-form" class="level4">
<h4 class="anchored" data-anchor-id="least-squares-in-matrix-form">Least Squares in Matrix Form</h4>
<p>The least squares criterion remains the same – minimize RSS – but the solution is now expressed in matrix form:</p>
<div class="theorem" name="Least Squares Estimate (Matrix Form)">
<p>Assuming <span class="math inline">\mathbf{X}^T\mathbf{X}</span> is invertible, the least squares estimate is:</p>
<p><span class="math display">\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}</span></p>
<p>with variance-covariance matrix:</p>
<p><span class="math display">\mathbb{V}(\hat{\boldsymbol{\beta}} \mid \mathbf{X}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}</span></p>
</div>
<p><strong>Key Results from the Least Squares Solution:</strong></p>
<ol type="1">
<li><p><strong>The estimated regression function</strong> is: <span class="math display">\hat{r}(x) = \hat{\beta}_0 + \sum_{j=1}^{k} \hat{\beta}_j x_j</span></p></li>
<li><p><strong>An unbiased estimate of the error variance</strong> <span class="math inline">\sigma^2</span> is: <span class="math display">\hat{\sigma}^2 = \frac{1}{n-k-1} \sum_{i=1}^{n} \hat{\epsilon}_i^2 = \frac{1}{n-k-1} ||\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}||^2</span></p>
<p>We divide by <span class="math inline">n-k-1</span> because we’ve estimated <span class="math inline">k+1</span> parameters (including the intercept).</p></li>
<li><p><strong>Confidence intervals</strong> for individual coefficients: <span class="math display">\hat{\beta}_j \pm z_{\alpha/2} \cdot \widehat{\text{se}}(\hat{\beta}_j)</span></p>
<p>where <span class="math inline">\widehat{\text{se}}^2(\hat{\beta}_j)</span> is the <span class="math inline">j</span>-th diagonal element of <span class="math inline">\hat{\sigma}^2(\mathbf{X}^T\mathbf{X})^{-1}</span>.</p></li>
</ol>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Meaning of a Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<p>In multiple regression, the interpretation of coefficients becomes more subtle. The coefficient <span class="math inline">\beta_j</span> represents the expected change in <span class="math inline">Y</span> for a one-unit change in <span class="math inline">X_j</span>, <strong>holding all other covariates constant</strong>.</p>
<p>This is the crucial concept of <strong>statistical control</strong> or <strong>adjustment</strong>. We’re estimating the partial effect of each predictor, after accounting for the linear effects of all other predictors in the model.</p>
<p>This interpretation assumes:</p>
<ol type="1">
<li>The other variables can actually be held constant (may not be realistic)</li>
<li>The relationship is truly linear</li>
<li>No important interactions exist between predictors</li>
</ol>
</div>
</div>
</section>
</section>
<section id="multiple-regression-in-practice" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="multiple-regression-in-practice"><span class="header-section-number">9.4.2</span> Multiple Regression in Practice</h3>
<p>We’ll now add two biologically relevant predictors to our weight-only model:</p>
<ul>
<li><strong>SEX</strong>: Categorical variable (“female”/“male” in the data)
<ul>
<li>R and Python automatically encode with female = 0 (reference), male = 1</li>
<li>Output shows as “SEX[T.male]” (Python) or “SEXmale” (R)</li>
<li>Coefficient interpretation: difference in mean SBP for males vs females</li>
</ul></li>
<li><strong>CHOL</strong>: Total serum cholesterol in mg/dL (typical range: 150-250 mg/dL)
<ul>
<li>A known cardiovascular risk factor</li>
<li>Coefficient interpretation: change in SBP per 1 mg/dL increase in cholesterol</li>
</ul></li>
</ul>
<p>We’ll build three models:</p>
<ol type="1">
<li><strong>Model 1</strong>: <code>SBP ~ FRW</code> (baseline simple regression)</li>
<li><strong>Model 2</strong>: <code>SBP ~ FRW + SEX + CHOL</code> (multiple regression)
<ul>
<li>Adds <code>SEX</code> and <code>CHOL</code> predictors</li>
</ul></li>
<li><strong>Model 3</strong>: <code>SBP ~ FRW + SEX + CHOL + FRW:SEX</code> (with interaction/cross term)
<ul>
<li><code>FRW:SEX</code> is an <strong>interaction term</strong> - that is the product FRW × SEX (weight × 0/1 for female/male)</li>
<li>This tests: “Does weight affect blood pressure differently for males vs females?”</li>
<li>For females (<code>SEX = 0</code>): Effect of weight = <span class="math inline">\beta_{\text{FRW}}</span></li>
<li>For males (<code>SEX = 1</code>): Effect of weight = <span class="math inline">\beta_{\text{FRW}} + \beta_{\text{FRW:SEX}}</span></li>
<li>If <span class="math inline">\beta_{\text{FRW:SEX}} &gt; 0</span>: weight increases BP more for males than females</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: From Simple to Multiple Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s build on our simple regression model by adding sex and cholesterol as predictors:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-949-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-949-1" role="tab" aria-controls="tabset-1757255612-949-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-949-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-949-2" role="tab" aria-controls="tabset-1757255612-949-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-949-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-949-1-tab"><div id="f507015c" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the same Framingham data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fram <span class="op">=</span> pd.read_csv(<span class="st">'../data/fram.txt'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 1: Simple regression (for comparison)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW'</span>, data<span class="op">=</span>fram).fit()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 2: Multiple regression</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW + SEX + CHOL'</span>, data<span class="op">=</span>fram).fit()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 3: With interaction (does weight affect BP differently for males/females?)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW + SEX + CHOL + FRW:SEX'</span>, data<span class="op">=</span>fram).fit()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Simple Regression (Model 1):"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  FRW coefficient: </span><span class="sc">{</span>model1<span class="sc">.</span>params[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss"> (SE=</span><span class="sc">{</span>model1<span class="sc">.</span>bse[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  R-squared: </span><span class="sc">{</span>model1<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Multiple Regression (Model 2):"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  FRW coefficient: </span><span class="sc">{</span>model2<span class="sc">.</span>params[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss"> (SE=</span><span class="sc">{</span>model2<span class="sc">.</span>bse[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  SEX[T.male] coefficient: </span><span class="sc">{</span>model2<span class="sc">.</span>params[<span class="st">'SEX[T.male]'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  CHOL coefficient: </span><span class="sc">{</span>model2<span class="sc">.</span>params[<span class="st">'CHOL'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  R-squared: </span><span class="sc">{</span>model2<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">With Interaction (Model 3):"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'FRW:SEX[T.male]'</span> <span class="kw">in</span> model3.params:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    interaction_coef <span class="op">=</span> model3.params[<span class="st">'FRW:SEX[T.male]'</span>]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    interaction_pval <span class="op">=</span> model3.pvalues[<span class="st">'FRW:SEX[T.male]'</span>]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  FRW:SEX interaction: </span><span class="sc">{</span>interaction_coef<span class="sc">:.3f}</span><span class="ss"> (p=</span><span class="sc">{</span>interaction_pval<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> interaction_pval <span class="op">&gt;</span> <span class="fl">0.05</span>:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Interpretation: No significant interaction - weight affects BP similarly for both sexes"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Interpretation: Weight has </span><span class="sc">{</span><span class="st">'stronger'</span> <span class="cf">if</span> interaction_coef <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'weaker'</span><span class="sc">}</span><span class="ss"> effect for males"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Simple Regression (Model 1):
  FRW coefficient: 0.524 (SE=0.040)
  R-squared: 0.110

Multiple Regression (Model 2):
  FRW coefficient: 0.499 (SE=0.040)
  SEX[T.male] coefficient: -4.066
  CHOL coefficient: 0.053
  R-squared: 0.125

With Interaction (Model 3):
  FRW:SEX interaction: -0.004 (p=0.967)
  Interpretation: No significant interaction - weight affects BP similarly for both sexes</code></pre>
</div>
</div></div><div id="tabset-1757255612-949-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-949-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fram <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'../data/fram.txt'</span>, <span class="at">sep=</span><span class="st">'</span><span class="sc">\t</span><span class="st">'</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Build models</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW, <span class="at">data =</span> fram)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW <span class="sc">+</span> SEX <span class="sc">+</span> CHOL, <span class="at">data =</span> fram)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW <span class="sc">+</span> SEX <span class="sc">+</span> CHOL <span class="sc">+</span> FRW<span class="sc">:</span>SEX, <span class="at">data =</span> fram)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare coefficients</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Simple Regression (Model 1):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  FRW coefficient: %.3f (SE=%.3f)</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            <span class="fu">coef</span>(model1)[<span class="st">"FRW"</span>], <span class="fu">summary</span>(model1)<span class="sc">$</span>coef[<span class="st">"FRW"</span>, <span class="st">"Std. Error"</span>]))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  R-squared: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">summary</span>(model1)<span class="sc">$</span>r.squared))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Multiple Regression (Model 2):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  FRW coefficient: %.3f (SE=%.3f)</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            <span class="fu">coef</span>(model2)[<span class="st">"FRW"</span>], <span class="fu">summary</span>(model2)<span class="sc">$</span>coef[<span class="st">"FRW"</span>, <span class="st">"Std. Error"</span>]))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  SEXmale coefficient: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">coef</span>(model2)[<span class="st">"SEXmale"</span>]))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  CHOL coefficient: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">coef</span>(model2)[<span class="st">"CHOL"</span>]))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  R-squared: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">summary</span>(model2)<span class="sc">$</span>r.squared))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">With Interaction (Model 3):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(<span class="st">"FRW:SEXmale"</span> <span class="sc">%in%</span> <span class="fu">names</span>(<span class="fu">coef</span>(model3))) {</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  interaction_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(model3)[<span class="st">"FRW:SEXmale"</span>]</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  interaction_pval <span class="ot">&lt;-</span> <span class="fu">summary</span>(model3)<span class="sc">$</span>coef[<span class="st">"FRW:SEXmale"</span>, <span class="st">"Pr(&gt;|t|)"</span>]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  FRW:SEX interaction: %.3f (p=%.3f)</span><span class="sc">\n</span><span class="st">"</span>, interaction_coef, interaction_pval))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(interaction_pval <span class="sc">&gt;</span> <span class="fl">0.05</span>) {</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">"  Interpretation: No significant interaction - weight affects BP similarly for both sexes</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Interpretation: Weight has %s effect for males</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>                <span class="fu">ifelse</span>(interaction_coef <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">"stronger"</span>, <span class="st">"weaker"</span>)))</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</div>
</div>
<p><strong>Key Observations from the Multiple Regression Results:</strong></p>
<ol type="1">
<li><strong>Minimal coefficient change</strong>: The FRW coefficient changes only slightly when adding other predictors (compare Model 1 vs Model 2 outputs above). This stability suggests weight has a robust relationship with blood pressure that isn’t confounded by sex or cholesterol.</li>
<li><strong>Sex effect in this cohort</strong>: In this Framingham cohort, the model shows males have somewhat lower blood pressure than females (see the SEX coefficient in Model 2 output). This pattern in the 1950s-60s data (mean age ≈ 52 years) may reflect post-menopausal effects in women.</li>
<li><strong>Modest R-squared improvement</strong>: Adding sex and cholesterol only marginally improves <span class="math inline">R^2</span> (compare the R-squared values between Model 1 and Model 2). This teaches us that more predictors don’t always mean much better predictions.</li>
<li><strong>No meaningful interaction</strong>: The interaction term in Model 3 is near zero (see FRW:SEX coefficient), suggesting weight affects blood pressure similarly for both sexes. Not all hypothesized interactions turn out to be important!</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Real Data Lessons
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>These Framingham results illustrate important realities of data analysis:</p>
<ul>
<li><strong>Effects can be counterintuitive</strong>: Women having higher blood pressure in this 1950s-60s cohort (mean age 52) surprises many, but it’s consistent across all age groups in the data. Historical context and demographics matter!</li>
<li><strong>More predictors ≠ much better fit</strong>: Despite adding two predictors and an interaction, we only explained an additional 1.5% of variance.</li>
<li><strong>Most interactions are null</strong>: We often hypothesize interactions that don’t materialize. That’s fine – testing and rejecting hypotheses is part of science.</li>
<li><strong>Biological systems are complex</strong>: Even our best model explains only 12.5% of blood pressure variation. The remaining 87.5% comes from genetics, lifestyle, measurement error, and countless other factors.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="model-selection-choosing-the-right-predictors" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="model-selection-choosing-the-right-predictors"><span class="header-section-number">9.4.3</span> Model Selection: Choosing the Right Predictors</h3>
<p>With many potential predictors, a critical question arises: which ones should we include? Including too few predictors (<strong>underfitting</strong>) leads to bias; including too many (<strong>overfitting</strong>) increases variance and reduces interpretability. <strong>Model selection</strong> seeks the sweet spot.</p>
<section id="the-core-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-core-problem">The Core Problem</h4>
<p>When we have <span class="math inline">k</span> potential predictors, there are <span class="math inline">2^k</span> possible models (each predictor is either in or out). With just 10 predictors, that’s 1,024 models; with 20 predictors, over a million! We need:</p>
<ol type="1">
<li>A way to score each model’s quality</li>
<li>An efficient search strategy to find the best model</li>
</ol>
</section>
<section id="scoring-models-the-bias-variance-trade-off" class="level4">
<h4 class="anchored" data-anchor-id="scoring-models-the-bias-variance-trade-off">Scoring Models: The Bias-Variance Trade-off</h4>
<p>The fundamental challenge is that training error – how well the model fits the data used to build it – is a bad guide to how well it will predict new data. Complex models always fit training data better, but they may perform poorly on new data. This is a manifestation of the bias-variance tradeoff we studied in Chapter 3: as we add more predictors to a regression, bias decreases (better fit to the true relationship) but variance increases (more sensitivity to the particular sample). Too few predictors leads to <strong>underfitting</strong> (high bias), while too many leads to <strong>overfitting</strong> (high variance).</p>
<div class="definition">
<p><strong>Prediction Risk (Quadratic Loss)</strong></p>
<p>For a model <span class="math inline">S</span> with predictors <span class="math inline">\mathcal{X}_S</span>, the prediction risk under quadratic loss is: <span class="math display">R(S) = \sum_{i=1}^{n} \mathbb{E}[(\hat{Y}_i(S) - Y_i^*)^2]</span></p>
<p>where <span class="math inline">Y_i^*</span> is a future observation at covariate value <span class="math inline">X_i</span>, and <span class="math inline">\hat{Y}_i(S)</span> is the prediction from model <span class="math inline">S</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Squared Error Loss?
</div>
</div>
<div class="callout-body-container callout-body">
<p>We use quadratic loss throughout model selection because:</p>
<ol type="1">
<li>It matches our least squares estimation method (consistency across model fitting and model evaluation)</li>
<li>It leads to tractable bias-variance decompositions<br>
</li>
<li>It penalizes large errors more than small ones (often desirable in practice)</li>
</ol>
<p>Other loss functions (absolute error, 0-1 loss) are valid but lead to different optimal models.</p>
</div>
</div>
<p>Since we can’t directly compute prediction risk (we don’t have future data!), we need estimates. Many model selection criteria follow a similar form which we want to maximize:</p>
<p><span class="math display">\text{Model Score} = \underbrace{\text{Goodness of Fit}}_{\text{how well model fits data}} - \underbrace{\text{Complexity Penalty}}_{\text{penalty for too many parameters}}</span></p>
<p>Equivalently, some model selection metrics aim to minimize:</p>
<p><span class="math display">\text{Model Score Loss} = \text{Training Error} + \text{Complexity Penalty}</span></p>
<p>This fundamental trade-off appears in different guises across the methods we’ll examine. The key insight is that we must balance how well we fit the current data against the danger of overfitting.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-581-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-581-1" role="tab" aria-controls="tabset-1757255612-581-1" aria-selected="true" href="">Mallow’s Cp</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-581-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-581-2" role="tab" aria-controls="tabset-1757255612-581-2" aria-selected="false" href="">AIC</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-581-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-581-3" role="tab" aria-controls="tabset-1757255612-581-3" aria-selected="false" href="">BIC</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-581-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-581-4" role="tab" aria-controls="tabset-1757255612-581-4" aria-selected="false" href="">Cross-Validation</a></li></ul><div class="tab-content"><div id="tabset-1757255612-581-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-581-1-tab"><p><strong>Mallow’s <span class="math inline">\(C_p\)</span>
Statistic</strong> provides an estimate of prediction risk:</p><p><span class="math display">\[\hat{R}(S) = \text{RSS}(S) + 2|S|\hat{\sigma}^2\]</span></p><p>where:</p><ul>
<li><span class="math inline">\(\text{RSS}(S)\)</span> = residual sum of
squares (training error)</li>
<li><span class="math inline">\(|S|\)</span> = number of parameters in
model <span class="math inline">\(S\)</span></li>
<li><span class="math inline">\(\hat{\sigma}^2\)</span> = error variance
estimate from the full model</li>
</ul><p><strong>Interpretation</strong>: The first term measures lack of fit,
the second penalizes complexity. Named after statistician <a href="https://en.wikipedia.org/wiki/Colin_Lingwood_Mallows">Colin
Mallows</a> who developed it.</p><p><strong>When to use</strong>: Linear regression with normal errors
when you want an unbiased estimate of prediction risk.</p></div><div id="tabset-1757255612-581-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-581-2-tab"><p><strong>AIC (Akaike Information Criterion)</strong> takes an
information-theoretic approach. The standard definition used in
statistical software is:</p><p><span class="math display">\[\text{AIC}(S) = -2\ell_S + 2|S|\]</span></p><p>where: - <span class="math inline">\(\ell_S\)</span> is the
log-likelihood at the MLE - <span class="math inline">\(|S|\)</span> is
the number of parameters in model <span class="math inline">\(S\)</span>
(including the intercept)</p><p>We <strong>minimize</strong> AIC to select the best model.
Conceptually, this is equivalent to maximizing “goodness of fit minus
complexity penalty” since minimizing
<span class="math inline">\(-2\ell_S + 2|S|\)</span> is the same as
maximizing <span class="math inline">\(\ell_S - |S|\)</span>.</p><p><strong>Key insight</strong>: For linear regression with normal
errors, AIC is equivalent to Mallow’s
<span class="math inline">\(C_p\)</span> (they select the same
model).</p><p><strong>Philosophy</strong>: AIC was developed by statistician <a href="https://en.wikipedia.org/wiki/Hirotugu_Akaike">Hirotugu Akaike</a>
to approximate the Kullback-Leibler divergence between the true and
fitted models. It aims to minimize prediction error, not find the “true”
model – recognizing that the true model might be too complex to express
mathematically.</p><p><strong>When to use</strong>: When prediction accuracy is the primary
goal and you believe the true model may be complex.</p></div><div id="tabset-1757255612-581-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-581-3-tab"><p><strong>BIC (Bayesian Information Criterion)</strong> adds a stronger
complexity penalty. The standard definition is:</p><p><span class="math display">\[\text{BIC}(S) = -2\ell_S + |S|\log n\]</span></p><p>where <span class="math inline">\(n\)</span> is the sample size. We
<strong>minimize</strong> BIC to select the best model.</p><p>Note that the penalty <span class="math inline">\(|S|\log n\)</span>
grows with sample size, making BIC more conservative than AIC (which has
a fixed penalty of <span class="math inline">\(2|S|\)</span>). For
<span class="math inline">\(n &gt; e^2 \approx 7.4\)</span>, BIC penalizes
complexity more heavily than AIC.</p><p><strong>Philosophy</strong>: BIC, developed by statistician Gideon E.
Schwarz, has a Bayesian interpretation – it approximates the log
posterior probability of the model. As
<span class="math inline">\(n \to \infty\)</span>, BIC selects the true
model with probability 1 (consistency), <em>if the true model belongs to
the candidate model set</em>.</p><p><strong>Key difference from AIC</strong>: Stronger penalty leads to
simpler models. BIC assumes a true, relatively simple model exists among
the candidates.</p><p><strong>When to use</strong>: When you believe a relatively simple
true model exists and want consistency.</p></div><div id="tabset-1757255612-581-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-581-4-tab"><p>Alternatively, we can approximate prediction error by training our
model on a <strong>subset</strong> of the data, and testing on the
remaining (held-out) set. Repeating this procedure multiple times for
different partitions of the data is called
<strong>cross-validation</strong> (CV).</p><p><strong>Leave-one-out Cross-Validation (LOO-CV)</strong> directly
estimates prediction error on one held-out data point at a time:</p><p><span class="math display">\[\hat{R}_{CV}(S) = \sum_{i=1}^{n} (Y_i - \hat{Y}_{(i)})^2\]</span></p><p>where <span class="math inline">\(\hat{Y}_{(i)}\)</span> is the
prediction for observation <span class="math inline">\(i\)</span> from a
model fit without observation <span class="math inline">\(i\)</span>.
LOO-CV can be very expensive (requires refitting the model
<span class="math inline">\(n\)</span> times!) but for linear models it
can be computed efficiently.</p><p><strong><span class="math inline">\(k\)</span>-fold CV</strong>:
Divide data into <span class="math inline">\(k\)</span> groups called
“folds” (often <span class="math inline">\(k=5\)</span> or
<span class="math inline">\(k=10\)</span>), train on
<span class="math inline">\(k-1\)</span> folds, test on the held-out
fold, repeat and average. This only requires retraining the model
<span class="math inline">\(k\)</span> times.</p><p><strong>When to use</strong>: When you want a direct, model-agnostic
estimate of prediction performance. Essential for complex models where
AIC/BIC aren’t available. CV is a common evaluation technique in machine
learning.</p></div></div></div>
</section>
<section id="search-strategies" class="level4">
<h4 class="anchored" data-anchor-id="search-strategies">Search Strategies</h4>
<p>Even with a scoring criterion, we can’t check all <span class="math inline">2^k</span> models when <span class="math inline">k</span> is large. Common search strategies include:</p>
<ul>
<li><strong>Forward Stepwise Selection</strong>: Start with no predictors, add the best one at each step</li>
<li><strong>Backward Stepwise Selection</strong>: Start with all predictors, remove the worst one at each step</li>
<li><strong>Best Subset Selection</strong>: Check all models of each size (computationally intensive)</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Greedy Search Limitations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Stepwise methods are greedy algorithms – they make locally optimal choices without considering the global picture. They may miss the best model. For example, two predictors might be useless alone but powerful together due to interaction effects.</p>
</div>
</div>
</section>
<section id="comparing-predictor-importance" class="level4">
<h4 class="anchored" data-anchor-id="comparing-predictor-importance">Comparing Predictor Importance</h4>
<p>Once we’ve selected a model, we often want to know: which predictors have the most impact? The raw regression coefficients can be misleading because predictors are on different scales. A predictor measured in millimeters will have a much smaller coefficient than one measured in kilometers, even if they have the same actual importance.</p>
<p>The solution is to <strong>standardize predictors before comparing coefficients</strong>. After standardization:</p>
<ul>
<li>All predictors have mean 0 and the same spread</li>
<li>Coefficients become directly comparable</li>
<li>The coefficient magnitude indicates relative importance</li>
</ul>
<p><strong>Gelman &amp; Hill’s recommendation</strong>: <span class="citation" data-cites="gelman2007data">Gelman and Hill (<a href="../references.html#ref-gelman2007data" role="doc-biblioref">2007</a>)</span> recommend dividing continuous predictors by 2 standard deviations (not 1). This makes binary and continuous predictors more comparable, since a binary predictor’s standard deviation is at most 0.5, so dividing by 2 SDs puts it on a similar scale.</p>
<p>After standardization, predictors with larger coefficient magnitudes have stronger effects on the outcome (in standard deviation units). However, when predictors are correlated, standardized coefficients don’t directly measure the unique variance explained by each predictor. For that, consider partial <span class="math inline">R^2</span> or other variance decomposition methods.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Heuristics for Model Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Beyond automated criteria, <span class="citation" data-cites="gelman2007data">Gelman and Hill (<a href="../references.html#ref-gelman2007data" role="doc-biblioref">2007</a>)</span> suggest these practical guidelines:</p>
<ol type="1">
<li><strong>Include predictors you expect to be important</strong> based on subject knowledge</li>
<li><strong>Consider creating composite predictors</strong>: Not all related variables need separate inclusion – you can combine multiple covariates into meaningful composites (e.g., a socioeconomic index from income, education, and occupation)</li>
<li><strong>Add interaction terms for strong predictors</strong>: When predictors have large effects, their interactions often matter too</li>
<li><strong>Use statistical significance and sign to guide decisions</strong>:
<ul>
<li><strong>Significant with expected sign</strong>: Keep these predictors</li>
<li><strong>Significant with unexpected sign</strong>: Investigate further – may indicate model misspecification, confounding, or data issues</li>
<li><strong>Non-significant with expected sign</strong>: Often worth keeping if theoretically important</li>
<li><strong>Non-significant with unexpected sign</strong>: Generally drop these</li>
</ul></li>
</ol>
<p>Remember: statistical significance isn’t everything. A predictor’s theoretical importance and practical significance matter too.</p>
</div>
</div>
</section>
<section id="controlling-for-background-variables" class="level4">
<h4 class="anchored" data-anchor-id="controlling-for-background-variables">Controlling for Background Variables</h4>
<p>Many studies <strong>control</strong> for background variables (age, sex, education, socioeconomic status, etc.). This simply means <strong>including these variables as predictors</strong> in the model to “remove their impact” on the relationship of interest.</p>
<p>For example, in our <a href="#multiple-regression-in-practice">earlier Framingham analysis</a>, Model 2 controlled for sex and cholesterol when examining the weight-blood pressure relationship:</p>
<ul>
<li>Model 1: <code>SBP ~ FRW</code> (simple regression with weight only)<br>
</li>
<li>Model 2: <code>SBP ~ FRW + SEX + CHOL</code> (controlling for sex and cholesterol)</li>
</ul>
<p>The weight coefficient changed only slightly between models (see the outputs above), suggesting the relationship isn’t confounded by these variables.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Limitation of Statistical Control
</div>
</div>
<div class="callout-body-container callout-body">
<p>Controlling only captures <strong>linear effects</strong> of the control variables. If age has a nonlinear effect on the outcome (e.g., quadratic), simply including age as a linear term won’t fully control for it. Background variables can still affect inferences if their effects are nonlinear.</p>
<p>Consider including polynomial terms or splines for control variables when you suspect nonlinear relationships.</p>
</div>
</div>
</section>
</section>
<section id="regression-assumptions-and-diagnostics" class="level3" data-number="9.4.4">
<h3 data-number="9.4.4" class="anchored" data-anchor-id="regression-assumptions-and-diagnostics"><span class="header-section-number">9.4.4</span> Regression Assumptions and Diagnostics</h3>
<p>Linear regression makes strong assumptions. When violated, our inferences may be invalid.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Five Assumptions of Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>In decreasing order of importance <span class="citation" data-cites="gelman2007data">(per <a href="../references.html#ref-gelman2007data" role="doc-biblioref">Gelman and Hill 2007</a>)</span>:</p>
<ol type="1">
<li><p><strong>Validity</strong>: Are the data relevant to your research question? Does the outcome measure what you think it measures? Are all important predictors included? Missing key variables can invalidate all conclusions.</p></li>
<li><p><strong>Additivity and Linearity</strong>: The model assumes <span class="math inline">Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...</span>. If relationships are nonlinear, consider transformations (<span class="math inline">\log</span>, square root), polynomial terms, or interaction terms.</p></li>
<li><p><strong>Independence of Errors</strong>: Each observation’s error should be independent of others. Watch out for time series (temporal correlation), spatial data (geographic clustering), or grouped data (students within schools).</p></li>
<li><p><strong>Equal Variance (Homoscedasticity)</strong>: Error variance should be constant across all predictor values. Violation makes standard errors and confidence intervals unreliable.</p></li>
<li><p><strong>Normality of Errors</strong>: Errors should follow a normal distribution. This is the least important – with large samples, the Central Limit Theorem ensures valid inference even with non-normal errors. Outliers matter more than the exact distribution shape.</p></li>
</ol>
</div>
</div>
<section id="checking-assumptions-residual-plots" class="level4">
<h4 class="anchored" data-anchor-id="checking-assumptions-residual-plots">Checking Assumptions: Residual Plots</h4>
<p>Since the statistical assumptions of linear regression focus on the errors <span class="math inline">\epsilon_i = Y_i - \hat{Y}_i</span>, visualizing the residuals provides our primary diagnostic tool. The most important plot is <strong>residuals versus fitted values</strong>, which can reveal multiple assumption violations at once.</p>
<p>We demonstrated this in <a href="#step-6-model-diagnostics">Step 6 of our Framingham analysis</a>, where we created two key diagnostic plots:</p>
<ul>
<li><strong>Residuals vs Fitted</strong>: Reveals problems with linearity and constant variance assumptions.</li>
<li><strong>Q-Q Plot</strong>: Checks whether residuals follow a normal distribution.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpreting Residual Patterns
</div>
</div>
<div class="callout-body-container callout-body">
<p>Good residuals look like <strong>random noise</strong> around zero – no patterns, just scatter. Specific patterns reveal specific problems:</p>
<ul>
<li><strong>Curved patterns</strong> (U-shape, waves): Nonlinearity detected. The true relationship isn’t straight. Try transformations or polynomial terms.</li>
<li><strong>Funnel shape</strong> (variance changes with fitted values): Heteroscedasticity. Errors have unequal variance. Consider log-transforming <span class="math inline">Y</span> or weighted least squares.</li>
<li><strong>Outliers or extreme points</strong>: Can dominate the entire regression. Check if they’re data errors or reveal model limitations.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-28-contents" aria-controls="callout-28" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Correlation vs.&nbsp;Causation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-28" class="callout-28-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A significant regression coefficient does <strong>not</strong> imply causation! Regression finds associations, not causal relationships. For example, ice cream sales and swimming pool drownings are positively correlated (both increase in summer), but ice cream doesn’t cause drowning.</p>
<p>Establishing causation requires theoretical justification, temporal precedence, ruling out confounders, and ideally randomized experiments. We’ll explore causal inference in detail in Chapter 11.</p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="logistic-regression" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">9.5</span> Logistic Regression</h2>
<section id="modeling-binary-outcomes" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="modeling-binary-outcomes"><span class="header-section-number">9.5.1</span> Modeling Binary Outcomes</h3>
<p>So far, we’ve assumed the response variable <span class="math inline">Y</span> is continuous. But what if <span class="math inline">Y</span> is binary? Consider:</p>
<ul>
<li>Does a patient have the disease? (Yes/No)</li>
<li>Will a customer churn? (Yes/No)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
<li>Did the email get clicked? (Yes/No)</li>
<li>Will a loan default? (Yes/No)</li>
</ul>
<p>For these binary outcomes, we need <strong>logistic regression</strong>.</p>
</section>
<section id="the-logistic-regression-model" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="the-logistic-regression-model"><span class="header-section-number">9.5.2</span> The Logistic Regression Model</h3>
<div class="definition">
<p><strong>The Logistic Regression Model</strong></p>
<p>For a binary outcome <span class="math inline">Y_i \in \{0, 1\}</span> and predictors <span class="math inline">X_i</span>, the logistic regression model specifies:</p>
<p><span class="math display">p_i \equiv \mathbb{P}(Y_i = 1 \mid X_i) = \frac{e^{\beta_0 + \sum_{j=1}^k \beta_j X_{ij}}}{1 + e^{\beta_0 + \sum_{j=1}^k \beta_j X_{ij}}}</span></p>
<p>Equivalently, using the <strong>logit</strong> (log-odds) transformation:</p>
<p><span class="math display">\text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \sum_{j=1}^k \beta_j X_{ij}</span></p>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-22-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-22-1" role="tab" aria-controls="tabset-1757255612-22-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-22-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-22-2" role="tab" aria-controls="tabset-1757255612-22-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-22-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-22-3" role="tab" aria-controls="tabset-1757255612-22-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255612-22-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-22-1-tab"><p><strong>Where does this formula come from? Why “logistic”
regression?</strong></p><p>Point is, when <span class="math inline">\(Y\)</span> is binary,
linear regression produces continuous predictions – not the 0s and 1s we
observe with binary data. The natural approach is to model the
probability
<span class="math inline">\(p = \mathbb{P}(Y = 1 \mid X)\)</span>
instead of <span class="math inline">\(Y\)</span>.</p><p>We could try to model the probability
<span class="math inline">\(p\)</span> with a linear model such as
<span class="math inline">\(p = \beta_0 + \beta_1 X\)</span>. However,
we would immediately hit two problems:</p><ol type="1">
<li>Linear functions are unbounded: when
<span class="math inline">\(X\)</span> is large,
<span class="math inline">\(\beta_0 + \beta_1 X\)</span> can exceed 1;
when <span class="math inline">\(X\)</span> is small, it can fall below
0. Both give impossible “probabilities.”</li>
<li>Binary data strongly violates the homoscedasticity assumption – a
Bernoulli (i.e., binary) variable with probability
<span class="math inline">\(p\)</span> has variance
<span class="math inline">\(p(1-p)\)</span>, which depends on
<span class="math inline">\(X\)</span>.</li>
</ol><p>The trick is that instead of modelling directly
<span class="math inline">\(p\)</span>, we model the
<strong>log-odds</strong> (logarithm of the probability ratio), that is
<span class="math inline">\(\text{logit}(p) = \log\left(\frac{p}{1-p}\right)\)</span>,
which is an unbounded quantity that lives in
<span class="math inline">\((-\infty, \infty)\)</span>.</p><p>The <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic
function</a> is then the function that allows us to map the logit values
back to <span class="math inline">\((0,1)\)</span>, ensuring valid
probabilities regardless of predictor values.</p></div><div id="tabset-1757255612-22-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-22-2-tab"><p>The logistic function emerges naturally from maximum likelihood with
Bernoulli data. Since
<span class="math inline">\(Y_i \sim \text{Bernoulli}(p_i)\)</span>, the
likelihood is:</p><p><span class="math display">\[\mathcal{L} = \prod_{i=1}^n p_i^{Y_i}(1-p_i)^{1-Y_i}\]</span></p><p>The log-likelihood becomes:
<span class="math display">\[\ell = \sum_{i=1}^n \left[Y_i \log p_i + (1-Y_i)\log(1-p_i)\right]\]</span></p><p>We need to connect <span class="math inline">\(p_i\)</span> to the
predictors <span class="math inline">\(X_i\)</span>. We could try
various transformations, but the <strong>logit</strong> turns out to be
special – it’s the “canonical link” for Bernoulli distributions, meaning
it makes the mathematics particularly elegant. Specifically, if we set:
<span class="math display">\[\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 X_i\]</span></p><p>then the log-likelihood becomes <strong>concave</strong> in the
parameters <span class="math inline">\(\beta_0, \beta_1\)</span>. This
is crucial: a concave function has a unique maximum, so we’re guaranteed
to find the best fit without worrying about local maxima. Solving the
logit equation for <span class="math inline">\(p_i\)</span> gives us the
logistic function.</p></div><div id="tabset-1757255612-22-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-22-3-tab"><p>Let’s visualize the logistic function to build intuition for how it
transforms linear predictors into probabilities:</p><div id="9521740c" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb1" data-code-fold="true" data-code-summary="Show visualization code"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">200</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Linear predictor: β₀ + β₁X'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability: P(Y=1|X)'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'The Logistic Function'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark key point</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'When β₀ + β₁X = 0,</span><span class="ch">\n</span><span class="st">probability = 0.5'</span>, </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            xy<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), xytext<span class="op">=</span>(<span class="dv">2</span>, <span class="fl">0.3</span>),</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'red'</span>))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-13-output-1.png"></p>
</div>
</div><p>The S-shaped curve is the logistic function in action. Notice three
critical features:</p><ol type="1">
<li><strong>Bounded output</strong>: No matter how extreme the input (β₀
+ β₁X), the output stays strictly between 0 and 1</li>
<li><strong>Decision boundary</strong>: When the linear predictor equals
0, the probability equals 0.5 – this is the natural decision
threshold</li>
<li><strong>Smooth transitions</strong>: Unlike a hard step function,
the logistic provides gradual probability changes, reflecting
uncertainty near the boundary</li>
</ol><p>This smooth mapping from
<span class="math inline">\((-\infty, \infty) \to (0,1)\)</span> is what
makes logistic regression both mathematically tractable and practically
interpretable.</p></div></div></div>
<p>Unlike linear regression, logistic regression has no closed-form solution. The parameters are estimated via maximum likelihood using numerical optimization algorithms, as implemented in common statistical packages.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpreting Coefficients: Odds Ratios
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In logistic regression, coefficients have a specific interpretation:</p>
<ul>
<li><span class="math inline">\beta_j</span> is the change in <strong>log-odds</strong> for a one-unit increase in <span class="math inline">X_j</span>, holding other variables constant</li>
<li><span class="math inline">e^{\beta_j}</span> is the <strong>odds ratio</strong>: the factor by which odds are multiplied for a one-unit increase in <span class="math inline">X_j</span></li>
</ul>
<p>For example, if <span class="math inline">\beta_{\text{age}} = 0.05</span>, then:</p>
<ul>
<li>Each additional year of age increases log-odds by 0.05</li>
<li>Each additional year multiplies odds by <span class="math inline">e^{0.05} \approx 1.051</span> (5.1% increase)</li>
</ul>
<p>Remember: odds = <span class="math inline">p/(1-p)</span>. If <span class="math inline">p = 0.2</span>, odds = 0.25. If <span class="math inline">p = 0.8</span>, odds = 4.</p>
</div>
</div>
</div>
</section>
<section id="logistic-regression-in-practice" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3" class="anchored" data-anchor-id="logistic-regression-in-practice"><span class="header-section-number">9.5.3</span> Logistic Regression in Practice</h3>
<p>Let’s apply logistic regression to the Framingham data to predict high blood pressure.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Predicting High Blood Pressure
</div>
</div>
<div class="callout-body-container callout-body">
<p>We’ll create a <strong>binary outcome</strong> for high blood pressure using the clinical definition: systolic blood pressure (<code>SBP</code>) ≥ 140 mmHg or diastolic blood pressure (<code>DBP</code>) ≥ 90 mmHg. This is the standard threshold used in medical practice.</p>
<p>To model the probability of high blood pressure, we’ll:</p>
<ol type="1">
<li>Standardize continuous predictors (weight, age, cholesterol) by dividing by 2 standard deviations – this makes coefficients comparable across predictors.</li>
<li>Fit logistic regression models, starting with a single predictor then adding multiple variables.</li>
<li>Interpret the results through odds ratios.</li>
</ol>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-300-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-300-1" role="tab" aria-controls="tabset-1757255612-300-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-300-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-300-2" role="tab" aria-controls="tabset-1757255612-300-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-300-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-300-1-tab"><div id="24d79457" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Framingham data</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>fram <span class="op">=</span> pd.read_csv(<span class="st">'../data/fram.txt'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define high blood pressure (standard clinical threshold)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'HIGH_BP'</span>] <span class="op">=</span> ((fram[<span class="st">'SBP'</span>] <span class="op">&gt;=</span> <span class="dv">140</span>) <span class="op">|</span> (fram[<span class="st">'DBP'</span>] <span class="op">&gt;=</span> <span class="dv">90</span>)).astype(<span class="bu">int</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prevalence of high BP: </span><span class="sc">{</span>fram[<span class="st">'HIGH_BP'</span>]<span class="sc">.</span>mean()<span class="sc">:.1%}</span><span class="ss"> (</span><span class="sc">{</span>fram[<span class="st">'HIGH_BP'</span>]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> of </span><span class="sc">{</span><span class="bu">len</span>(fram)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize predictors (following Gelman &amp; Hill's recommendation)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividing by 2*SD makes binary and continuous predictors comparable</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardize(x):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">-</span> x.mean()) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> x.std())</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'sFRW'</span>] <span class="op">=</span> standardize(fram[<span class="st">'FRW'</span>])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'sAGE'</span>] <span class="op">=</span> standardize(fram[<span class="st">'AGE'</span>])</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'sCHOL'</span>] <span class="op">=</span> standardize(fram[<span class="st">'CHOL'</span>])</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a simple logistic regression with standardized weight</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.logit(<span class="st">'HIGH_BP ~ sFRW'</span>, data<span class="op">=</span>fram).fit(disp<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression: HIGH_BP ~ sFRW"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.summary2().tables[<span class="dv">1</span>])</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fitted model</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points (with slight jitter for visibility)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>y_jitter <span class="op">=</span> fram[<span class="st">'HIGH_BP'</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>, <span class="bu">len</span>(fram))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'sFRW'</span>], y_jitter, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the fitted probability curve</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(fram[<span class="st">'sFRW'</span>].<span class="bu">min</span>(), fram[<span class="st">'sFRW'</span>].<span class="bu">max</span>(), <span class="dv">200</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>X_pred <span class="op">=</span> pd.DataFrame({<span class="st">'sFRW'</span>: x_range})</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_pred)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, y_pred, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted probability'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Standardized Weight (sFRW)'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(High BP = 1)'</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Probability of High Blood Pressure vs Weight'</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpret the coefficient as odds ratio</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>beta_sfrw <span class="op">=</span> model.params[<span class="st">'sFRW'</span>]</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>or_sfrw <span class="op">=</span> np.exp(beta_sfrw)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Coefficient for sFRW: </span><span class="sc">{</span>beta_sfrw<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Odds ratio: </span><span class="sc">{</span>or_sfrw<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Interpretation: A 2-SD increase in weight multiplies odds of high BP by </span><span class="sc">{</span>or_sfrw<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prevalence of high BP: 65.0% (906 of 1394)

==================================================
Logistic Regression: HIGH_BP ~ sFRW
==================================================
              Coef.  Std.Err.          z         P&gt;|z|    [0.025    0.975]
Intercept  0.675629  0.059255  11.402124  4.080404e-30  0.559492  0.791766
sFRW       1.201906  0.138943   8.650323  5.135384e-18  0.929582  1.474230

Coefficient for sFRW: 1.2019
Odds ratio: 3.3265
Interpretation: A 2-SD increase in weight multiplies odds of high BP by 3.3265</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-14-output-2.png"></p>
</div>
</div></div><div id="tabset-1757255612-300-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-300-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)  <span class="co"># For rescale and invlogit functions</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>fram <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'../data/fram.txt'</span>, <span class="at">sep=</span><span class="st">'</span><span class="sc">\t</span><span class="st">'</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define high blood pressure</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>HIGH_BP <span class="ot">&lt;-</span> (fram<span class="sc">$</span>SBP <span class="sc">&gt;=</span> <span class="dv">140</span>) <span class="sc">|</span> (fram<span class="sc">$</span>DBP <span class="sc">&gt;=</span> <span class="dv">90</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Prevalence of high BP: %.1f%% (%d of %d)</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            <span class="fu">mean</span>(fram<span class="sc">$</span>HIGH_BP)<span class="sc">*</span><span class="dv">100</span>, <span class="fu">sum</span>(fram<span class="sc">$</span>HIGH_BP), <span class="fu">nrow</span>(fram)))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize predictors (rescale divides by 2*SD as recommended by Gelman &amp; Hill)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>sFRW <span class="ot">&lt;-</span> <span class="fu">rescale</span>(fram<span class="sc">$</span>FRW)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>sAGE <span class="ot">&lt;-</span> <span class="fu">rescale</span>(fram<span class="sc">$</span>AGE)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>sCHOL <span class="ot">&lt;-</span> <span class="fu">rescale</span>(fram<span class="sc">$</span>CHOL)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit logistic regression with standardized weight</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(HIGH_BP <span class="sc">~</span> sFRW, <span class="at">data =</span> fram, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">'logit'</span>))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fitted model</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fram<span class="sc">$</span>sFRW, <span class="fu">jitter</span>(<span class="fu">as.numeric</span>(fram<span class="sc">$</span>HIGH_BP), <span class="fl">0.05</span>), </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Standardized Weight (sFRW)"</span>, </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"P(High BP = 1)"</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Probability of High Blood Pressure vs Weight"</span>,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted curve</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">invlogit</span>(<span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>]<span class="sc">*</span>x), </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="st">"Fitted probability"</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpret as odds ratio</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>beta_sfrw <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)[<span class="st">"sFRW"</span>]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>or_sfrw <span class="ot">&lt;-</span> <span class="fu">exp</span>(beta_sfrw)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Coefficient for sFRW: %.4f</span><span class="sc">\n</span><span class="st">"</span>, beta_sfrw))</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Odds ratio: %.4f</span><span class="sc">\n</span><span class="st">"</span>, or_sfrw))</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Interpretation: A 2-SD increase in weight multiplies odds of high BP by %.4f</span><span class="sc">\n</span><span class="st">"</span>, or_sfrw))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<p>The fitted logistic curve shows how the probability of high blood pressure increases with weight. Unlike linear regression, the relationship is nonlinear – the effect of weight on probability is strongest in the middle range where probabilities are near 0.5.</p>
<section id="adding-multiple-predictors" class="level4">
<h4 class="anchored" data-anchor-id="adding-multiple-predictors">Adding Multiple Predictors</h4>
<p>Now let’s include age and sex to improve our model:</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-376-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-376-1" role="tab" aria-controls="tabset-1757255612-376-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-376-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-376-2" role="tab" aria-controls="tabset-1757255612-376-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-376-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-376-1-tab"><div id="d14e5b93" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with multiple predictors (using standardized variables)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> smf.logit(<span class="st">'HIGH_BP ~ sFRW + sAGE + SEX'</span>, data<span class="op">=</span>fram).fit(disp<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multiple Logistic Regression"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model2.summary2().tables[<span class="dv">1</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and display odds ratios</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ODDS RATIOS"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> model2.params.index:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    or_val <span class="op">=</span> np.exp(model2.params[var])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    ci <span class="op">=</span> model2.conf_int().loc[var]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    ci_low, ci_high <span class="op">=</span> np.exp(ci[<span class="dv">0</span>]), np.exp(ci[<span class="dv">1</span>])</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> var <span class="op">!=</span> <span class="st">'Intercept'</span>:</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>var<span class="sc">:10s}</span><span class="ss">: OR = </span><span class="sc">{</span>or_val<span class="sc">:5.3f}</span><span class="ss"> (95% CI: </span><span class="sc">{</span>ci_low<span class="sc">:5.3f}</span><span class="ss">-</span><span class="sc">{</span>ci_high<span class="sc">:5.3f}</span><span class="ss">)"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the effect of sex on the weight-BP relationship</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the actual data points with jitter, separated by sex</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>females <span class="op">=</span> fram[fram[<span class="st">'SEX'</span>] <span class="op">==</span> <span class="st">'female'</span>]</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>males <span class="op">=</span> fram[fram[<span class="st">'SEX'</span>] <span class="op">==</span> <span class="st">'male'</span>]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Add jitter to binary outcome for visibility</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>jitter_f <span class="op">=</span> females[<span class="st">'HIGH_BP'</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>, <span class="bu">len</span>(females))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>jitter_m <span class="op">=</span> males[<span class="st">'HIGH_BP'</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>, <span class="bu">len</span>(males))</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.scatter(females[<span class="st">'sFRW'</span>], jitter_f, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Female (data)'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.scatter(males[<span class="st">'sFRW'</span>], jitter_m, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Male (data)'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction data: vary weight, hold age at mean (0 for standardized)</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>weight_range <span class="op">=</span> np.linspace(fram[<span class="st">'sFRW'</span>].<span class="bu">min</span>(), fram[<span class="st">'sFRW'</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions for females</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>pred_data_f <span class="op">=</span> pd.DataFrame({<span class="st">'sFRW'</span>: weight_range, <span class="st">'sAGE'</span>: <span class="dv">0</span>, <span class="st">'SEX'</span>: <span class="st">'female'</span>})</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>pred_f <span class="op">=</span> model2.predict(pred_data_f)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions for males  </span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>pred_data_m <span class="op">=</span> pd.DataFrame({<span class="st">'sFRW'</span>: weight_range, <span class="st">'sAGE'</span>: <span class="dv">0</span>, <span class="st">'SEX'</span>: <span class="st">'male'</span>})</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>pred_m <span class="op">=</span> model2.predict(pred_data_m)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the fitted curves</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.plot(weight_range, pred_f, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Female (fitted)'</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.plot(weight_range, pred_m, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Male (fitted)'</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Standardized Weight (sFRW)'</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(High BP = 1)'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Probability of High BP by Weight and Sex</span><span class="ch">\n</span><span class="st">(Age held at mean)'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>==================================================
Multiple Logistic Regression
==================================================
                Coef.  Std.Err.         z         P&gt;|z|    [0.025    0.975]
Intercept    0.780125  0.083319  9.363138  7.740317e-21  0.616823  0.943426
SEX[T.male] -0.201763  0.116912 -1.725767  8.438929e-02 -0.430907  0.027380
sFRW         1.153445  0.140824  8.190707  2.596960e-16  0.877436  1.429454
sAGE         0.373807  0.117219  3.188958  1.427867e-03  0.144062  0.603552

==================================================
ODDS RATIOS
==================================================
SEX[T.male]: OR = 0.817 (95% CI: 0.650-1.028)
sFRW      : OR = 3.169 (95% CI: 2.405-4.176)
sAGE      : OR = 1.453 (95% CI: 1.155-1.829)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="09-linear-logistic-regression_files/figure-html/cell-15-output-2.png"></p>
</div>
</div></div><div id="tabset-1757255612-376-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-376-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with multiple predictors (using standardized variables)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(HIGH_BP <span class="sc">~</span> sFRW <span class="sc">+</span> sAGE <span class="sc">+</span> SEX, <span class="at">data =</span> fram, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">'logit'</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate odds ratios with confidence intervals</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>or_table <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(fit2), <span class="fu">confint</span>(fit2)))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(or_table)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the effect of sex on the weight-BP relationship</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the actual data points with jitter, separated by sex</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>females <span class="ot">&lt;-</span> fram[fram<span class="sc">$</span>SEX <span class="sc">==</span> <span class="st">"female"</span>, ]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>males <span class="ot">&lt;-</span> fram[fram<span class="sc">$</span>SEX <span class="sc">==</span> <span class="st">"male"</span>, ]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(females<span class="sc">$</span>sFRW, <span class="fu">jitter</span>(<span class="fu">as.numeric</span>(females<span class="sc">$</span>HIGH_BP), <span class="fl">0.05</span>), </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>), <span class="at">pch =</span> <span class="dv">16</span>, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Standardized Weight (sFRW)"</span>, </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"P(High BP = 1)"</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Probability of High BP by Weight and Sex</span><span class="sc">\n</span><span class="st">(Age held at mean)"</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(males<span class="sc">$</span>sFRW, <span class="fu">jitter</span>(<span class="fu">as.numeric</span>(males<span class="sc">$</span>HIGH_BP), <span class="fl">0.05</span>), </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.3</span>), <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted curves for females and males (age at mean = 0 for standardized)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Female curve</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">invlogit</span>(<span class="fu">coef</span>(fit2)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sFRW"</span>]<span class="sc">*</span>x <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sAGE"</span>]<span class="sc">*</span><span class="dv">0</span>), </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Male curve  </span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">invlogit</span>(<span class="fu">coef</span>(fit2)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"SEXmale"</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sFRW"</span>]<span class="sc">*</span>x <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sAGE"</span>]<span class="sc">*</span><span class="dv">0</span>),</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">"Female (data)"</span>, <span class="st">"Male (data)"</span>, <span class="st">"Female (fitted)"</span>, <span class="st">"Male (fitted)"</span>), </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>), <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.5</span>), <span class="st">"red"</span>, <span class="st">"blue"</span>), </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="cn">NA</span>, <span class="cn">NA</span>),</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fl">2.5</span>, <span class="fl">2.5</span>))</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
<p><strong>Interpretation of Results:</strong></p>
<ul>
<li><strong>sFRW (Weight)</strong>: A 2-SD increase in weight multiplies the odds of high BP by 3.17 (95% CI: 2.41-4.18) – a very strong effect</li>
<li><strong>sAGE (Age)</strong>: A 2-SD increase in age multiplies the odds of high BP by 1.45 (95% CI: 1.16-1.83) – significant but weaker than weight</li>
<li><strong>SEX</strong>: Being male decreases the odds by about 18% (OR = 0.82), but this is not statistically significant (95% CI: 0.65-1.03 includes 1)</li>
</ul>
<p>The visualization shows how the probability curves are parallel on the logit scale – males have consistently lower probability across all weight values (the blue curve sits below the red curve). The standardization allows direct comparison: weight has the strongest association with high blood pressure in this model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-31-contents" aria-controls="callout-31" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remarks About Logistic Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-31" class="callout-31-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>No R-squared</strong>: The usual <span class="math inline">R^2</span> doesn’t apply. Pseudo-<span class="math inline">R^2</span> measures exist but are less interpretable.</li>
<li><strong>Classification vs.&nbsp;Probability</strong>: Logistic regression estimates probabilities. Classification (yes/no) requires choosing a threshold (often 0.5, but domain-specific considerations matter).</li>
<li><strong>Separation Problem</strong>: If predictors perfectly separate the classes, the MLE doesn’t exist (coefficients go to ±∞). Regularization or Bayesian methods can help.</li>
<li><strong>Sample Size Requirements</strong>: Need more data than linear regression. Rule of thumb: 10-20 events per predictor for the less common outcome.</li>
<li><strong>Link Functions</strong>: The logit is just one choice, another choice is for example the <a href="https://en.wikipedia.org/wiki/Probit">probit</a> (normal CDF).</li>
</ol>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-summary-and-connections" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="chapter-summary-and-connections"><span class="header-section-number">9.6</span> Chapter Summary and Connections</h2>
<section id="key-concepts-review" class="level3" data-number="9.6.1">
<h3 data-number="9.6.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">9.6.1</span> Key Concepts Review</h3>
<p>We’ve covered the two fundamental models in statistical learning:</p>
<p><strong>Linear Regression</strong>:</p>
<ul>
<li>Models the expected value of a continuous response as a linear function of predictors</li>
<li>Estimated via least squares, which coincides with MLE under normality</li>
<li>Provides interpretable coefficients with well-understood inference procedures</li>
<li>Extends naturally to multiple predictors, with matrix formulation</li>
<li>Requires careful attention to assumptions and model selection</li>
</ul>
<p><strong>Logistic Regression</strong>:</p>
<ul>
<li>Extends the linear framework to binary outcomes via the logit link</li>
<li>Models probabilities, not the outcomes directly</li>
<li>Coefficients represent changes in log-odds, interpretable as odds ratios</li>
<li>Estimated via maximum likelihood with iterative algorithms</li>
<li>Shares the interpretability advantages of linear models</li>
</ul>
<p><strong>Key Connections</strong>:</p>
<p>Both models exemplify the fundamental statistical modeling workflow:</p>
<ol type="1">
<li>Specify a model (assumptions about the data-generating process)</li>
<li>Estimate parameters (least squares or maximum likelihood)</li>
<li>Quantify uncertainty (standard errors, confidence intervals)</li>
<li>Check assumptions (diagnostic plots)</li>
<li>Make predictions and interpret results</li>
</ol>
</section>
<section id="the-big-picture" class="level3" data-number="9.6.2">
<h3 data-number="9.6.2" class="anchored" data-anchor-id="the-big-picture"><span class="header-section-number">9.6.2</span> The Big Picture</h3>
<p>This chapter has taken you through the fundamentals of linear and logistic regression, from basic concepts to advanced applications. These models exemplify the core statistical modeling workflow: specify a model, estimate parameters, quantify uncertainty, check assumptions, and make predictions.</p>
<p>But why do these simple linear models remain so important in the era of deep learning? The answer lies in their unique ability to explain complex predictions. <strong>LIME (Local Interpretable Model-Agnostic Explanations)</strong> <span class="citation" data-cites="ribeiro2016lime">(<a href="../references.html#ref-ribeiro2016lime" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> demonstrates this perfectly: it explains any complex model’s predictions by fitting simple linear models locally around points of interest.</p>
<div id="fig-lime" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lime-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/lime.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lime-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: LIME: Local linear models explain complex predictions by approximating them in small neighborhoods. Figure reproduced from <span class="citation" data-cites="ribeiro2016lime">Ribeiro, Singh, and Guestrin (<a href="../references.html#ref-ribeiro2016lime" role="doc-biblioref">2016</a>)</span>.
</figcaption>
</figure>
</div>
<p>This principle – that complex functions are locally linear – makes linear models useful for understanding predictions from any model, no matter how complex. The techniques you’ve learned in this chapter (least squares, coefficient interpretation, diagnostics) aren’t just historical artifacts; they’re the foundation for both classical statistical analysis and modern interpretable machine learning.</p>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="9.6.3">
<h3 data-number="9.6.3" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">9.6.3</span> Common Pitfalls to Avoid</h3>
<p>When working with linear and logistic regression, watch out for these critical mistakes:</p>
<p><strong>1. Interpretation Errors</strong></p>
<ul>
<li><strong>Correlation ≠ Causation</strong>: A significant coefficient shows association, not causation (that’s Chapter 11’s topic!)</li>
<li><strong>Statistical ≠ Practical significance</strong>: With n=10,000, even tiny effects become “significant”</li>
<li><strong>Logistic coefficients are log-odds</strong>: A coefficient of 0.5 doesn’t mean “50% increase in probability”</li>
</ul>
<p><strong>2. Model Selection Traps</strong></p>
<ul>
<li><strong>Overfitting</strong>: Using training error to select models guarantees disappointment on new data</li>
<li><strong>Automation without thinking</strong>: If your model says ice cream sales <em>decrease</em> temperatures, something’s wrong</li>
<li><strong>Ignoring validation</strong>: Always hold out data – a perfect training fit often means terrible generalization</li>
</ul>
<p><strong>3. Technical Violations</strong></p>
<ul>
<li><strong>Ignoring diagnostic plots</strong>: That funnel-shaped residual plot? Your model needs help</li>
<li><strong>Multicollinearity chaos</strong>: When predictors correlate highly, coefficients become unstable and standard errors explode</li>
</ul>
<p><strong>4. The Big One: Context Blindness</strong></p>
<ul>
<li><strong>Extrapolation</strong>: Linear trends rarely continue forever (no, humans won’t be 20 feet tall in year 3000)</li>
<li><strong>Domain knowledge matters</strong>: Statistical criteria (AIC/BIC) are guides, not gospel</li>
</ul>
</section>
<section id="chapter-connections" class="level3" data-number="9.6.4">
<h3 data-number="9.6.4" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">9.6.4</span> Chapter Connections</h3>
<p><strong>Previous (Chapters 5-8):</strong></p>
<ul>
<li>Chapters 5-6 introduced parametric inference – linear regression is the quintessential parametric model, with least squares achieving the Cramér-Rao bound under normality</li>
<li>Chapter 7’s hypothesis testing framework directly applies to testing regression coefficients via Wald tests and F-tests</li>
<li>Chapter 8’s Bayesian paradigm offers an alternative: Bayesian linear regression incorporates prior knowledge about parameters</li>
</ul>
<p><strong>This Chapter:</strong> Introduced the two workhorses of statistical modeling: linear and logistic regression. We saw how least squares connects to maximum likelihood, how to handle multiple predictors, select models, and extend to binary outcomes. The focus on interpretability makes these models essential even in the era of complex machine learning.</p>
<p><strong>Next (Ch. 10-11):</strong></p>
<ul>
<li>Chapter 10 will show Bayesian regression in practice using probabilistic programming languages, with complex priors and hierarchical structures</li>
<li>Chapter 11 will distinguish association from causation – regression finds associations, not causal effects</li>
</ul>
<p><strong>Applications:</strong> Linear models remain indispensable across fields: economics (modeling market relationships), medicine (risk factor analysis), social sciences (understanding social determinants), machine learning (LIME and interpretability), and A/B testing (variance reduction through regression adjustment).</p>
</section>
<section id="self-test-problems" class="level3" data-number="9.6.5">
<h3 data-number="9.6.5" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">9.6.5</span> Self-Test Problems</h3>
<ol type="1">
<li><p><strong>Centroid Property</strong>: Show that the least squares regression line always passes through the point <span class="math inline">(\bar{X}, \bar{Y})</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Use the normal equations: from <span class="math inline">\frac{\partial \text{RSS}}{\partial \hat{\beta}_0} = 0</span> you get <span class="math inline">\sum_i(Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0</span>, which gives <span class="math inline">\bar{Y} = \hat{\beta}_0 + \hat{\beta}_1\bar{X}</span>.</p>
</div>
</div>
</div></li>
<li><p><strong>Multicollinearity Inflates Standard Errors</strong>: Explain why high correlation between <span class="math inline">X_j</span> and the other predictors increases the standard error of <span class="math inline">\hat{\beta}_j</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-33-contents" aria-controls="callout-33" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-33" class="callout-33-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In multiple regression, <span class="math inline">\text{Var}(\hat{\beta}_j) \propto (1-R_j^2)^{-1}</span>, where <span class="math inline">R_j^2</span> is from regressing <span class="math inline">X_j</span> on the other <span class="math inline">X</span>’s. As <span class="math inline">R_j^2 \to 1</span>, the variance (and SE) blows up.</p>
</div>
</div>
</div></li>
<li><p><strong>Binary Outcomes and Heteroscedasticity</strong>: For <span class="math inline">Y \sim \text{Bernoulli}(p)</span>, compute <span class="math inline">\text{Var}(Y)</span> and explain why applying linear regression to binary <span class="math inline">Y</span> violates the constant-variance assumption.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-34-contents" aria-controls="callout-34" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-34" class="callout-34-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\text{Var}(Y) = p(1-p)</span>, which depends on <span class="math inline">X</span> if <span class="math inline">p = \mathbb{P}(Y=1 \mid X)</span> changes with <span class="math inline">X</span>. The variance is maximized at <span class="math inline">p=0.5</span> and approaches 0 as <span class="math inline">p \to 0</span> or <span class="math inline">p \to 1</span>.</p>
</div>
</div>
</div></li>
<li><p><strong>Odds Ratio to Probability</strong>: Baseline probability is <span class="math inline">p_0 = 0.20</span>. A predictor has an odds ratio <span class="math inline">\text{OR} = 2.4</span>. What is the new probability?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-35-contents" aria-controls="callout-35" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-35" class="callout-35-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Convert baseline to odds: <span class="math inline">o_0 = \frac{p_0}{1-p_0} = \frac{0.20}{0.80} = 0.25</span>. Multiply by OR to get <span class="math inline">o_1 = 2.4 \times 0.25 = 0.6</span>. Convert back: <span class="math inline">p_1 = \frac{o_1}{1+o_1} = \frac{0.6}{1.6} = 0.375</span>.</p>
</div>
</div>
</div></li>
<li><p><strong>Interpreting an Interaction</strong>: In the model <span class="math inline">Y = \beta_0 + \beta_1 \cdot \text{FRW} + \beta_2 \cdot \text{SEX} + \beta_3 \cdot (\text{FRW} \times \text{SEX}) + \epsilon</span> with <span class="math inline">\text{SEX}=0</span> (female) and <span class="math inline">\text{SEX}=1</span> (male), what is the effect (slope) of FRW on <span class="math inline">Y</span> for females vs males? What does <span class="math inline">\beta_2</span> represent?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-36-contents" aria-controls="callout-36" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-36" class="callout-36-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Plug in <span class="math inline">\text{SEX}=0</span> and <span class="math inline">\text{SEX}=1</span>. The FRW slope is <span class="math inline">\beta_1</span> for females and <span class="math inline">\beta_1 + \beta_3</span> for males. <span class="math inline">\beta_2</span> is the male-female intercept difference when <span class="math inline">\text{FRW}=0</span>.</p>
</div>
</div>
</div></li>
</ol>
</section>
<section id="python-and-r-reference" class="level3" data-number="9.6.6">
<h3 data-number="9.6.6" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">9.6.6</span> Python and R Reference</h3>
<p>This section provides a quick reference for the main functions used in linear and logistic regression.</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255612-961-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-961-1" role="tab" aria-controls="tabset-1757255612-961-1" aria-selected="true" href="">Python (statsmodels)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255612-961-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255612-961-2" role="tab" aria-controls="tabset-1757255612-961-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255612-961-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255612-961-1-tab"><p><strong>Linear Regression</strong></p><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Using formula interface (R-style)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(<span class="st">'Y ~ X1 + X2 + X3'</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Using arrays interface</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)  <span class="co"># Add intercept column</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> sm.OLS(Y, X).fit()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Key methods</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>results.params           <span class="co"># Coefficients</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>results.bse             <span class="co"># Standard errors</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>results.conf_int()      <span class="co"># Confidence intervals</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>results.pvalues         <span class="co"># P-values</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>results.rsquared        <span class="co"># R-squared</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>results.fittedvalues    <span class="co"># Predicted values</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>results.resid           <span class="co"># Residuals</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>results.predict(new_df) <span class="co"># Predictions for new data (DataFrame with same columns)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><p><strong>Logistic Regression</strong></p><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using formula interface</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.logit(<span class="st">'Y ~ X1 + X2 + X3'</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Using arrays interface (must add intercept!)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)  <span class="co"># Don't forget the intercept</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> sm.Logit(Y, X).fit()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Key methods (similar to linear)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>results.params          <span class="co"># Log-odds coefficients</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>np.exp(results.params)  <span class="co"># Odds ratios</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>results.predict()       <span class="co"># Predicted probabilities</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>results.pred_table()    <span class="co"># Classification table (2x2 contingency)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Model selection</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Check multicollinearity (compute on X with constant, skip reporting intercept VIF)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>X_with_const <span class="op">=</span> sm.add_constant(X_features)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>vif <span class="op">=</span> pd.DataFrame()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>vif[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(X_with_const.values, i) </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>              <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, X_with_const.shape[<span class="dv">1</span>])]  <span class="co"># Skip intercept column</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><p><strong>Diagnostic Plots</strong></p><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.graphics.api <span class="im">as</span> smg</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic plots</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>smg.plot_fit(results, <span class="st">'X1'</span>)  <span class="co"># Observed vs fitted for predictor X1</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>smg.qqplot(results.resid, line<span class="op">=</span><span class="st">'45'</span>)  <span class="co"># Q-Q plot with 45° line</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Residuals vs fitted (custom)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(results.fittedvalues, results.resid)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Fitted values'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial regression plots</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.regressionplots <span class="im">import</span> plot_partregress_grid</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plot_partregress_grid(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255612-961-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255612-961-2-tab"><p><strong>Linear Regression</strong></p><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data =</span> df)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Key functions</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(model)         <span class="co"># Coefficients</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model)      <span class="co"># Confidence intervals</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model)      <span class="co"># Predictions</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">residuals</span>(model)    <span class="co"># Residuals</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">fitted</span>(model)       <span class="co"># Fitted values</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(model)         <span class="co"># Variance-covariance matrix</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(model)        <span class="co"># ANOVA table</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostic plots</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model)         <span class="co"># Standard diagnostic plots</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial regression plots</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">avPlots</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><p><strong>Logistic Regression</strong></p><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data =</span> df, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">"logit"</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Odds ratios</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(model))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(model))  <span class="co"># Profile likelihood CIs (more accurate)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># For faster Wald CIs: exp(confint.default(model))</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model, <span class="at">type =</span> <span class="st">"response"</span>)  <span class="co"># Probabilities</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model, <span class="at">type =</span> <span class="st">"link"</span>)      <span class="co"># Log-odds</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Model selection</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">stepAIC</span>(model)  <span class="co"># Stepwise selection using AIC</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Check multicollinearity</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><p><strong>Model Selection and Comparison</strong></p><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare models</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(model1, model2, model3)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(model1, model2, model3)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for linear regression</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>train_control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"cv"</span>, <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>model_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(Y <span class="sc">~</span> ., <span class="at">data =</span> df, <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                  <span class="at">trControl =</span> train_control)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># For logistic regression with caret:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># train(Y ~ ., data = df, method = "glm", </span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#       family = binomial, trControl = train_control)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="9.6.7">
<h3 data-number="9.6.7" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">9.6.7</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-37-contents" aria-controls="callout-37" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to Course Materials
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-37" class="callout-37-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<colgroup>
<col style="width: 47%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding Source(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction: Why Linear Models Still Matter</strong></td>
<td style="text-align: left;">Lecture 9 slides intro on interpretable ML and LIME</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Power of Interpretability</td>
<td style="text-align: left;">Expanded from lecture motivation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Linear Models as Building Blocks</td>
<td style="text-align: left;">New material connecting to GLMs, mixed models, LIME</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Simple Linear Regression</strong></td>
<td style="text-align: left;">AoS §13.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Regression Models</td>
<td style="text-align: left;">AoS §13.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Simple Linear Regression Model</td>
<td style="text-align: left;">AoS Definition 13.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Estimating Parameters: Method of Least Squares</td>
<td style="text-align: left;">AoS §13.1, Theorem 13.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Connection to Maximum Likelihood</td>
<td style="text-align: left;">AoS §13.2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Properties of the Least Squares Estimators</td>
<td style="text-align: left;">AoS §13.3, Theorems 13.8-13.9</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Simple Linear Regression in Practice</td>
<td style="text-align: left;">New Framingham example from lecture slides, applying concepts from AoS §13.1-13.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Multiple Linear Regression</strong></td>
<td style="text-align: left;">AoS §13.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Extending the Model to Multiple Predictors</td>
<td style="text-align: left;">AoS §13.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Least Squares in Matrix Form</td>
<td style="text-align: left;">AoS Theorem 13.13</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Multiple Regression in Practice</td>
<td style="text-align: left;">New Framingham example from lecture slides, applying concepts from AoS §13.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Model Selection: Choosing the Right Predictors</strong></td>
<td style="text-align: left;">AoS §13.6</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Scoring Models: The Bias-Variance Trade-off</td>
<td style="text-align: left;">AoS §13.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Mallow’s Cp, AIC, BIC, Cross-Validation</td>
<td style="text-align: left;">AoS §13.6</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Search Strategies</td>
<td style="text-align: left;">AoS §13.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Comparing Predictor Importance</td>
<td style="text-align: left;">Lecture 9 slides + Gelman &amp; Hill</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Controlling for Background Variables</td>
<td style="text-align: left;">Lecture 9 slides</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Regression Assumptions and Diagnostics</strong></td>
<td style="text-align: left;">Lecture 9 slides (sourcing Gelman &amp; Hill)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Five Assumptions</td>
<td style="text-align: left;">Lecture 9 slides (sourcing Gelman &amp; Hill)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Checking Assumptions: Residual Plots</td>
<td style="text-align: left;">Lecture 9 slides + expanded examples</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">AoS §13.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Modeling Binary Outcomes</td>
<td style="text-align: left;">AoS §13.7</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Logistic Regression Model</td>
<td style="text-align: left;">AoS §13.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Logistic Regression in Practice</td>
<td style="text-align: left;">New Framingham example from lecture slides, applying concepts from AoS §13.7</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Chapter Summary and Connections</strong></td>
<td style="text-align: left;">New comprehensive summary</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Python and R Reference</strong></td>
<td style="text-align: left;">New - added Python alongside R implementations</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-materials" class="level3" data-number="9.6.8">
<h3 data-number="9.6.8" class="anchored" data-anchor-id="further-materials"><span class="header-section-number">9.6.8</span> Further Materials</h3>
<ul>
<li><span class="citation" data-cites="gelman2007data">Gelman and Hill (<a href="../references.html#ref-gelman2007data" role="doc-biblioref">2007</a>)</span> - Practical regression guidance with excellent intuition and real-world advice</li>
<li><span class="citation" data-cites="ribeiro2016lime">Ribeiro, Singh, and Guestrin (<a href="../references.html#ref-ribeiro2016lime" role="doc-biblioref">2016</a>)</span> - The LIME paper demonstrating how local linear models can explain any classifier’s predictions (<a href="https://github.com/marcotcr/lime">GitHub repo</a>)</li>
</ul>
<hr>
<p><em>Remember: Start with linear regression – many problems that get a neural network thrown at them could be solved with these simple models. Always establish a linear/logistic baseline first: if your complex deep network only improves accuracy by 2%, is the added complexity, computation, and loss of interpretability really worth it? Master these fundamental methods – they’re used daily by practitioners worldwide and remain indispensable as baselines, interpretable models, and building blocks for more complex systems.</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-gelman2007data" class="csl-entry" role="listitem">
Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-ribeiro2016lime" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. <span>“Why Should i Trust You?: Explaining the Predictions of Any Classifier.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1135–44. ACM. <a href="https://doi.org/10.1145/2939672.2939778">https://doi.org/10.1145/2939672.2939778</a>.
</div>
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note that the distribution of <span class="math inline">\epsilon</span> may depend on <span class="math inline">X</span>, though its mean is always zero.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Though for very large datasets, iterative methods like stochastic gradient descent may still be used for computational efficiency, even when closed-form solutions exist. Also, when we add regularization (Ridge, LASSO) or have constraints, we lose the closed-form solution and must use iterative methods.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Customer churn refers to when customers stop doing business with a company or cancel their subscription to a service. For example, a mobile phone customer “churns” when they switch to a different provider or cancel their contract. Predicting churn helps companies identify at-risk customers and take preventive action.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/08-bayesian-inference-decision-theory.html" class="pagination-link" aria-label="Bayesian Inference and Statistical Decision Theory">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Linear and Logistic Regression</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Build and interpret linear regression models** to understand relationships between variables, connecting the geometric intuition of least squares with the statistical framework of maximum likelihood estimation.</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Evaluate regression coefficients meaningfully**, including their practical interpretation, statistical significance via confidence intervals and hypothesis tests, and the crucial distinction between association and causation.</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Diagnose and address model inadequacies** using residual plots to check assumptions, applying transformations when relationships are nonlinear, and selecting predictors using principled criteria (AIC, BIC, cross-validation).</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extend regression to binary outcomes** through logistic regression, understanding how the logit link enables probability modeling and interpreting coefficients as odds ratios.</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply regression for both prediction and explanation**, recognizing when linear models excel (interpretability needs, moderate sample sizes) versus when more complex methods are warranted.</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>This chapter introduces the two most fundamental and widely used models in statistics: linear and logistic regression. While modern machine learning offers powerful black-box predictors, linear models remain essential for their interpretability and foundational role in statistical inference. The material is adapted from Chapter 13 of @wasserman2013all, supplemented with modern perspectives on model interpretability and practical examples.</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction: Why Linear Models Still Matter</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Power of Interpretability</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>In an age dominated by complex machine learning models -- neural networks with millions of parameters, random forests with thousands of trees, gradient boosting machines with intricate interactions -- one might wonder: why dedicate an entire chapter to something as simple as linear regression?</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>The answer lies in a fundamental trade-off in statistical modeling: **complexity versus interpretability**. While a deep neural network might achieve better prediction accuracy on a complex dataset, it operates as a "black box". We feed in inputs, we get outputs, but the mechanism connecting them remains opaque. This opacity becomes problematic when we need to:</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain predictions to stakeholders**: "Why was this loan application denied?"</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Identify which features matter most**: "Which factors most strongly predict patient readmission?"</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ensure fairness and avoid bias**: "Is our model discriminating based on protected attributes?"</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Debug unexpected behavior**: "Why did the model fail on this particular case?"</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>Linear models excel at all these tasks. Every coefficient has a clear interpretation: it tells us exactly how a one-unit change in a predictor affects the outcome, holding all else constant. This interpretability has made linear models the reference tools in fields where understanding relationships is as important as making predictions -- from economics to medicine to social sciences.</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Are Neural Networks Black Boxes?</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>The story we presented above of neural networks being completely black boxes is a simplification nowadays. The field of *interpretability research* has developed sophisticated methods to understand neural networks, including techniques like mechanistic interpretability, activation analysis, and circuit discovery. These approaches have yielded notable insights, for example for <span class="co">[</span><span class="ot">large language models</span><span class="co">](https://www.anthropic.com/research/tracing-thoughts-language-model)</span>.</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>Still, there's a crucial distinction: linear models are transparent by construction -- each coefficient directly tells us how a unit change in input affects output. Neural networks must be **reverse-engineered** through complex analysis requiring specialized tools and expertise. Think of it as the difference between reading a recipe versus doing forensic analysis on a finished cake.</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>In short, while interpretability research continues to advance, linear models remain uniquely valuable when interpretability is a primary requirement rather than an afterthought -- and linear models can be used as tools to understand more complex models, as seen in the next paragraph.</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear Models as Building Blocks</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>Beyond their direct application, linear models serve as the foundation for building and understanding more complex methods:</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Generalized Linear Models (GLMs)** extend linear regression to handle non-normal outcomes</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mixed Effects Models** add random effects to account for hierarchical data structures</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regularized Regression** (Ridge, LASSO, Elastic Net) adds penalties to handle high-dimensional data</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Local Linear Methods** like LOESS use linear regression in small neighborhoods for flexible curve fitting</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>Even in the realm of "black-box" machine learning, linear models play a crucial role in model interpretation. Consider **LIME** (Local Interpretable Model-Agnostic Explanations) <span class="co">[</span><span class="ot">@ribeiro2016lime</span><span class="co">]</span>, a popular technique for explaining individual predictions from any complex model. LIME works by:</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Taking a prediction from a complex model (e.g., "This image is 92% likely to contain a cat")</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generating perturbed samples around the input of interest</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Getting predictions from the complex model for these perturbed samples</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Fitting a simple linear model** to approximate the complex model's behavior locally</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Using the linear model's coefficients to explain **which features drove the prediction**</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>In essence, LIME uses the interpretability of linear models to shed light on the darkness of black-box predictions. When we can't understand the global behavior of a complex model, we can at least understand its local behavior through the lens of linear approximation.</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### This Chapter's Goal</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>Our goal in this chapter is to master both the theory and practice of linear and logistic regression. We'll develop the mathematical framework, explore the connections to maximum likelihood estimation, and learn how to implement and interpret these models in practice. Along the way, we'll address critical questions like:</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How do we estimate regression coefficients and quantify our uncertainty about them?</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What assumptions underlie linear regression, and what happens when they're violated?</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How do we choose which predictors to include when we have many candidates?</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How do we extend the framework to binary outcomes?</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>By the end of this chapter, you'll have a thorough understanding of these fundamental models -- knowledge that will serve you whether you're building interpretable models for scientific research or using LIME to explain neural network predictions.</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="fu">## Historical Note: The Origins of "Regression"</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>The term "regression" might seem odd for a method that predicts one variable from others. Its origin lies in the work of <span class="co">[</span><span class="ot">Sir Francis Galton</span><span class="co">](https://en.wikipedia.org/wiki/Francis_Galton)</span> (1822-1911), who studied the relationship between parents' heights and their children's heights. Galton observed that while tall parents tended to have tall children and short parents short children, the children's heights tended to be closer to the population mean than their parents' heights.</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>He called this phenomenon "regression towards mediocrity" (later "regression to the mean"). Though the name stuck, modern regression analysis is far more general than Galton's original application -- it's a comprehensive framework for modeling relationships between variables.</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here's a concise reference table of key terms in this chapter:</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>| Regression | Regressio | General statistical method |</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>| Linear regression | Lineaarinen regressio | Predicting continuous outcomes |</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>| Simple linear regression | Yhden selittäjän lineaarinen regressio | One predictor variable |</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>| Multiple regression | Usean selittäjän lineaarinen regressio | Multiple predictor variables |</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>| Logistic regression | Logistinen regressio | Predicting binary outcomes |</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>| Regression function | Regressiofunktio | $r(x) = \mathbb{E}(Y \mid X=x)$ |</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>| Response variable | Vastemuuttuja | Dependent variable |</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>| Predictor variable | Selittävä muuttuja (myös: kovariaatti, piirre) | Independent variable |</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>| Least squares | Pienimmän neliösumman menetelmä | Parameter estimation method |</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>| Residual Sum of Squares (RSS) | Jäännösneliösumma | Fit criterion minimized by OLS |</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>| Residual | Residuaali, jäännös | Difference between observed and predicted |</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>| Fitted value | Sovitettu arvo (myös: sovite; ennustettu arvo) | Model prediction for a data point |</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>| Fitted line | Sovitettu suora | Line of best fit |</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>| Coefficient | Kerroin | Parameter in regression equation |</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>| Intercept | Vakiotermi | Constant term in regression |</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>| Slope | Kulmakerroin | Rate of change parameter |</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>| Standard error | Keskivirhe | Uncertainty of an estimate |</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>| Conditional likelihood | Ehdollinen uskottavuus | Likelihood given covariates |</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>| R-squared | Selitysaste ($R^2$) | Proportion of variance explained |</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>| Training error | Opetusvirhe | In-sample error |</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>| Overfitting | Ylisovitus (tai: liikasovitus) | Model too complex for data |</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>| Underfitting | Vajaasovitus | Model too simple for data |</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>| AIC | Akaiken informaatiokriteeri | Model selection criterion |</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>| BIC | Bayes-informaatiokriteeri | Model selection criterion |</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>| Forward search/selection | Etenevä haku (myös: eteenpäin valinta) | Greedy model search |</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>| Statistical control | Vakiointi | Including background variables |</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>| Cross-validation | Ristiinvalidointi | Model evaluation technique |</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>| Leave-one-out cross-validation | Yksi-pois-ristiinvalidointi (myös: yksittäisristiinvalidointi) | CV with one held-out point |</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>| p-value | p-arvo | Significance measure for tests |</span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>| Confidence interval | Luottamusväli | Uncertainty quantification |</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>| Odds ratio | Vetosuhde | Effect measure in logistic regression |</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>| Logit (log-odds) | Logit-muunnos (logaritminen vetosuhde) | Link in logistic regression |</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>| Interaction term | Vuorovaikutustermi (interaktiotermi) | Effect modification between predictors |</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>| Homoscedasticity / Heteroscedasticity | Homo-/heteroskedastisuus | (Non-)constant error variance |</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>| Multicollinearity | Multikollineaarisuus | Correlation among predictors |</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simple Linear Regression</span></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Models</span></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>**Regression** is a method for studying the relationship between a **response variable** $Y$ and a **covariate** $X$. The response variable (also called the dependent variable) is what we're trying to understand or predict -- exam scores, blood pressure, sales revenue. The covariate is also called a **predictor variable** or **feature** -- these are the variables we use to explain or predict the response, such as study hours, weight, or advertising spend.</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>When we observe pairs of these variables, we naturally ask: how does $Y$ tend to change as $X$ varies? The answer lies in the **regression function**:</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>$$r(x) = \mathbb{E}(Y \mid X=x) = \int y f(y \mid x) \, dy$$</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>This function tells us the expected (average) value of $Y$ for any given value of $X$. Think of it as the systematic part of the relationship -- the signal beneath the noise. If we knew $r(x)$ perfectly, we could say "when $X = x$, we expect $Y$ to be around $r(x)$, though individual observations will vary."</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>Our goal is to estimate the regression function $r(x)$ from data of the form:</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>$$(X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n) \sim F_{X,Y}$$</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>where each pair represents one observation drawn from some joint distribution. In this chapter, we take a parametric approach and assume that $r$ has a specific functional form -- namely, that it's linear.</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Notation</span></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>Before diving into linear regression specifically, let's establish a basic notation. *Any* regression model can be written as:</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>$$Y = \underbrace{r(X)}_{\text{signal}} + \underbrace{\epsilon}_{\text{noise}}$$</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>where $\mathbb{E}(\epsilon | X) = 0$. This decomposition is fundamental: the observed response equals the **signal** (the systematic component we can predict from $X$) plus **noise** or **error** (the random variation we cannot predict).^<span class="co">[</span><span class="ot">Note that the distribution of $\epsilon$ may depend on $X$, though its mean is always zero.</span><span class="co">]</span></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why can we always write it this way?</span></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>The proof is simple. Define $\epsilon = Y - r(X)$, then:</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>$$Y = Y + r(X) - r(X) = r(X) + \epsilon.$$</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>Moreover, since $r(X) = \mathbb{E}(Y \mid X)$ by definition, we have:</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(\epsilon \mid X) = \mathbb{E}(Y - r(X) \mid X) = \mathbb{E}(Y \mid X) - r(X) = 0$$</span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>This shows that the decomposition isn't just a notational convenience -- it's a mathematical fact that any joint distribution of $(X, Y)$ can be decomposed into a predictable part (the conditional expectation) and an unpredictable part (the zero-mean error).</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>This notation separates what's predictable (the regression function) from what's unpredictable (the error term). The art and science of regression lies in finding good estimates for $r(x)$ from finite data.</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Simple Linear Regression Model</span></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>In **simple linear regression**, we assume $r(x)$ is a linear function of one-dimensional $X$:</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>$$r(x) = \beta_0 + \beta_1 x$$</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>This defines the simple linear regression model:</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>**Simple Linear Regression Model**</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>where $\mathbb{E}(\epsilon_i \mid X_i) = 0$ and $\mathbb{V}(\epsilon_i \mid X_i) = \sigma^2$.</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>The parameters have specific interpretations:</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_0$ is the **intercept**: the expected value of $Y$ when $X = 0$</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_1$ is the **slope**: the expected change in $Y$ for a one-unit increase in $X$</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma^2$ is the **error variance**: how much individual observations vary around the line</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>This model makes a bold claim: the relationship between $X$ and $Y$ can be adequately captured by a straight line, with all deviations from this line being random noise with constant variance. The assumption $\mathbb{E}(\epsilon_i \mid X_i) = 0$ ensures the line goes through the "middle" of the data at each value of $X$, while $\mathbb{V}(\epsilon_i \mid X_i) = \sigma^2$ (homoscedasticity) means the scatter around the line is equally variable across all values of $X$.</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a><span class="fu">## When is Linearity Reasonable?</span></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>More often than you might think! Many relationships are approximately linear, at least over the range of observed data. Even when the true relationship is nonlinear, linear regression often provides a useful first-order approximation -- much like how we can approximate curves with tangent lines in calculus.</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>Consider height and weight, income and spending, or temperature and ice cream sales. While none of these relationships are perfectly linear across all possible values, they're often reasonably linear within the range we observe. And sometimes that's all we need for useful predictions and insights.</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>Once we've estimated the parameters -- which we'll discuss next --, we can make predictions and assess our model:</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>**Regression Terminology**</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>Let $\hat{\beta}_0$ and $\hat{\beta}_1$ denote estimates of $\beta_0$ and $\beta_1$. Then:</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **fitted line** is: $\hat{r}(x) = \hat{\beta}_0 + \hat{\beta}_1 x$</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **predicted values** or **fitted values** are: $\hat{Y}_i = \hat{r}(X_i) = \hat{\beta}_0 + \hat{\beta}_1 X_i$</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **residuals** are: $\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X_i)$</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>The residuals are crucial -- they represent what our model doesn't explain. Small residuals mean our line fits well; large residuals suggest either a poor fit or inherent variability in the relationship.</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a><span class="fu">### Estimating Parameters: The Method of Least Squares</span></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>Now comes the central question: given our data, how do we find the "best" line? What values should we choose for $\hat{\beta}_0$ and $\hat{\beta}_1$?</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>The most common answer is the **method of least squares**. The idea is to find the line that minimizes the **Residual Sum of Squares (RSS)**:</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a>$$\text{RSS} = \sum_{i=1}^{n} \hat{\epsilon}_i^2 = \sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)^2$$</span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>The RSS measures how well the line fits the data -- it's the sum of squared vertical distances from the points to the line. The least squares estimates are the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that make RSS as small as possible.</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>Imagine you're trying to draw a line through a cloud of points on a scatter plot. You want the line to be "close" to all the points simultaneously. But what does "close" mean?</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>For each point, we have an error: how far the point lies above the line (positive error) or below the line (negative error). We need to aggregate these errors into a single measure of fit. We have several options:</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Sum of errors**: Won't work -- positive and negative errors cancel out. A terrible line far above half the points and far below the others could have zero total error!</span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Sum of absolute errors**: This works (no cancellation), but absolute values are mathematically inconvenient -- they're not differentiable at zero, making optimization harder.</span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Sum of squared errors**: This is the winner! Squaring prevents cancellation, penalizes large errors more than small ones (outliers matter), and is mathematically convenient allowing closed-form solutions.</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>The least squares line is the one that minimizes this sum of squared errors. It's the line that best "threads through" the cloud of points in the squared-error sense.</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>To find the least squares estimates, we minimize RSS with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$. Taking partial derivatives and setting them to zero:</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \text{RSS}}{\partial \hat{\beta}_0} = -2\sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0$$</span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \text{RSS}}{\partial \hat{\beta}_1} = -2\sum_{i=1}^{n} X_i(Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0$$</span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>These are called the **normal equations**. From the first equation:</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^{n} Y_i = n\hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^{n} X_i$$</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a>Dividing by $n$ gives: $\bar{Y}_n = \hat{\beta}_0 + \hat{\beta}_1 \bar{X}_n$, which shows the fitted line passes through $(\bar{X}_n, \bar{Y}_n)$.</span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a>From the second equation and some algebra (expanding the products and using the first equation), we get:</span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n} X_i Y_i - n\bar{X}_n\bar{Y}_n}{\sum_{i=1}^{n} X_i^2 - n\bar{X}_n^2} = \frac{\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n)}{\sum_{i=1}^{n}(X_i - \bar{X}_n)^2}$$</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a>This is the sample covariance of $X$ and $Y$ divided by the sample variance of $X$.</span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>Let's see least squares in action. We'll generate some synthetic data with a known linear relationship plus noise, then find the least squares line. The visualization will show two key perspectives:</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**The fitted line with residuals**: How well the line fits the data and what the residuals look like</span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**The optimization landscape**: How RSS changes as we vary the slope, showing that our formula finds the minimum</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code for visualization"</span></span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some example data</span></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, n)</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a>true_beta0, true_beta1 <span class="op">=</span> <span class="dv">2</span>, <span class="fl">1.5</span></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> true_beta0 <span class="op">+</span> true_beta1 <span class="op">*</span> X <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">2</span>, n)</span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate least squares estimates</span></span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a>X_mean <span class="op">=</span> np.mean(X)</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a>Y_mean <span class="op">=</span> np.mean(Y)</span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a>beta1_hat <span class="op">=</span> np.<span class="bu">sum</span>((X <span class="op">-</span> X_mean) <span class="op">*</span> (Y <span class="op">-</span> Y_mean)) <span class="op">/</span> np.<span class="bu">sum</span>((X <span class="op">-</span> X_mean)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>beta0_hat <span class="op">=</span> Y_mean <span class="op">-</span> beta1_hat <span class="op">*</span> X_mean</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization with stacked subplots</span></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a><span class="co"># Top panel: Show the residuals</span></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>ax1.scatter(X, Y, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>Y_plot <span class="op">=</span> beta0_hat <span class="op">+</span> beta1_hat <span class="op">*</span> X_plot</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>ax1.plot(X_plot, Y_plot, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Fitted line: Y = </span><span class="sc">{</span>beta0_hat<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>beta1_hat<span class="sc">:.2f}</span><span class="ss">X'</span>)</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw residuals</span></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a>    Y_pred <span class="op">=</span> beta0_hat <span class="op">+</span> beta1_hat <span class="op">*</span> X[i]</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a>    ax1.plot([X[i], X[i]], [Y[i], Y_pred], <span class="st">'g--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Y'</span>)</span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Least Squares Minimizes Squared Residuals'</span>)</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a><span class="co"># Bottom panel: Show RSS as a function of slope</span></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a>slopes <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="fl">2.5</span>, <span class="dv">100</span>)</span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>rss_values <span class="op">=</span> []</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> slope <span class="kw">in</span> slopes:</span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>    intercept <span class="op">=</span> Y_mean <span class="op">-</span> slope <span class="op">*</span> X_mean  <span class="co"># Best intercept given slope</span></span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> Y <span class="op">-</span> (intercept <span class="op">+</span> slope <span class="op">*</span> X)</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a>    rss <span class="op">=</span> np.<span class="bu">sum</span>(residuals<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a>    rss_values.append(rss)</span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>ax2.plot(slopes, rss_values, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>ax2.axvline(beta1_hat, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Optimal slope = </span><span class="sc">{</span>beta1_hat<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a>ax2.scatter([beta1_hat], [<span class="bu">min</span>(rss_values)], color<span class="op">=</span><span class="st">'r'</span>, s<span class="op">=</span><span class="dv">100</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Slope (β₁)'</span>)</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Residual Sum of Squares'</span>)</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'RSS as a Function of Slope'</span>)</span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Least squares estimates: β₀ = </span><span class="sc">{</span>beta0_hat<span class="sc">:.3f}</span><span class="ss">, β₁ = </span><span class="sc">{</span>beta1_hat<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimum RSS = </span><span class="sc">{</span><span class="bu">min</span>(rss_values)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>The top panel shows the fitted line and the residuals (green dashed lines). Notice how the line "threads through" the cloud of points, balancing the errors above and below. The residuals visualize what we're minimizing -- we want these vertical distances (squared) to be as small as possible in total.</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a>The bottom panel reveals the optimization landscape. As we vary the slope $\beta_1$ (while adjusting $\beta_0$ optimally for each slope to maintain the constraint that the line passes through $(\bar{X}, \bar{Y})$), the RSS forms a parabola with a clear minimum. The red dashed line marks where our formula places us -- exactly at the minimum! This confirms that the least squares formula genuinely finds the best fit in terms of minimizing squared errors.</span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Least Squares Estimates"}</span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a>The values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the RSS are:</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n)}{\sum_{i=1}^{n}(X_i - \bar{X}_n)^2} \quad \text{(slope)}$$</span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta}_0 = \bar{Y}_n - \hat{\beta}_1 \bar{X}_n \quad \text{(intercept)}$$</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a>where $\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$ and $\bar{Y}_n = \frac{1}{n}\sum_{i=1}^{n} Y_i$ are the sample means.</span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a>These formulas have intuitive interpretations:</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The slope $\hat{\beta}_1$ is essentially the sample covariance between $X$ and $Y$ divided by the sample variance of $X$</span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The intercept $\hat{\beta}_0$ is chosen so the fitted line passes through the point $(\bar{X}_n, \bar{Y}_n)$ -- the center of the data</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a>Least squares is very convenient because it has a closed-form solution. We don't need iterative algorithms or numerical optimization^<span class="co">[</span><span class="ot">Though for very large datasets, iterative methods like stochastic gradient descent may still be used for computational efficiency, even when closed-form solutions exist. Also, when we add regularization (Ridge, LASSO) or have constraints, we lose the closed-form solution and must use iterative methods.</span><span class="co">]</span> -- just plug the data into our formulas and we get the optimal answer.</span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimating Error Variance</span></span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a>We also need to estimate the error variance $\sigma^2$. An unbiased estimator is:</span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a>$$\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^{n} \hat{\epsilon}_i^2$$</span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a>Why divide by $n-2$ instead of $n$? We've estimated two parameters ($\beta_0$ and $\beta_1$), which costs us two degrees of freedom. This adjustment ensures our variance estimate is unbiased.</span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connection to Maximum Likelihood Estimation</span></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a>So far, we've motivated least squares geometrically -- it finds the line that minimizes squared distances. But there's a deeper connection to the likelihood principle we studied in previous chapters.</span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a>**Adding the Normality Assumption**</span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a>Suppose we strengthen our assumptions by specifying that the errors are normally distributed:</span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>$$\epsilon_i \mid X_i \sim \mathcal{N}(0, \sigma^2)$$</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a>This implies that:</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>$$Y_i \mid X_i \sim \mathcal{N}(\beta_0 + \beta_1 X_i, \sigma^2)$$</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>Each observation follows a normal distribution centered at the regression line, with constant variance $\sigma^2$.</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a>**The Likelihood Function**</span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>Under these assumptions, we can write the likelihood function. The joint density of all observations is:</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a>$$\prod_{i=1}^{n} f(X_i, Y_i) = \prod_{i=1}^{n} f_X(X_i) \times \prod_{i=1}^{n} f_{Y|X}(Y_i | X_i)$$</span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>The first term doesn't involve our parameters $\beta_0$ and $\beta_1$, so we focus on the second term -- the **conditional likelihood**:</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L}(\beta_0, \beta_1, \sigma) = \prod_{i=1}^{n} f_{Y|X}(Y_i | X_i) \propto \sigma^{-n} \exp\left<span class="sc">\{</span>-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2\right<span class="sc">\}</span>$$</span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a>The conditional log-likelihood is:</span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a>$$\ell(\beta_0, \beta_1, \sigma) = -n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2$$</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Key Insight</span></span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>To maximize the log-likelihood with respect to $\beta_0$ and $\beta_1$, we must minimize the sum:</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2$$</span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a>But this is exactly the RSS! Therefore, under the normality assumption, **the least squares estimators are also the maximum likelihood estimators**.</span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a>This is a profound connection: the simple geometric idea of minimizing squared distances coincides with the principled statistical approach of maximum likelihood, at least when errors are normal.</span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a><span class="fu">## What about estimating σ²?</span></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a>From the conditional log-likelihood above we can also derive the MLE for the error variance:</span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>$$\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^{n} \hat{\epsilon}_i^2$$</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>However, this estimator is biased -- it systematically underestimates the true variance. In practice, we use the unbiased estimator:</span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a>$$\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^{n} \hat{\epsilon}_i^2$$</span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a>The difference is small for large $n$, but the unbiased version provides more accurate confidence intervals and hypothesis tests, which is why it's standard in linear regression.</span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of the Least Squares Estimators</span></span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>Understanding the statistical properties of our estimates is crucial for inference. How accurate are they? How does accuracy improve with more data? Can we quantify our uncertainty?</span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>**Finite Sample Properties**</span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>Let $\hat{\beta} = (\hat{\beta}_0, \hat{\beta}_1)^T$ denote the least squares estimators. Assume that:</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbb{E}(\epsilon_i \mid X^n) = 0$ for all $i$</span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\mathbb{V}(\epsilon_i \mid X^n) = \sigma^2$ for all $i$ (homoscedasticity)</span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\text{Cov}(\epsilon_i, \epsilon_j \mid X^n) = 0$ for $i \neq j$ (uncorrelated errors)</span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a>Then, conditional on $X^n = (X_1, \ldots, X_n)$:</span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(\hat{\beta} \mid X^n) = \begin{pmatrix} \beta_0 <span class="sc">\\</span> \beta_1 \end{pmatrix}$$</span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(\hat{\beta} \mid X^n) = \frac{\sigma^2}{n s_X^2} \begin{pmatrix} \frac{1}{n}\sum_{i=1}^{n} X_i^2 &amp; -\bar{X}_n <span class="sc">\\</span> -\bar{X}_n &amp; 1 \end{pmatrix}$$</span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>where $s_X^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2$ is the sample variance of $X$.</span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>This theorem tells us that the least squares estimators are **unbiased** -- on average, they hit the true values. The variance formula allows us to compute standard errors:</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}}(\hat{\beta}_0) = \frac{\hat{\sigma}}{s_X \sqrt{n}} \sqrt{\frac{\sum_{i=1}^{n} X_i^2}{n}} \quad \text{and} \quad \widehat{\text{se}}(\hat{\beta}_1) = \frac{\hat{\sigma}}{s_X \sqrt{n}}$$</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>These standard errors quantify the uncertainty in our estimates. Notice that both decrease with $\sqrt{n}$ -- more data means more precision.</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced: Derivation of Standard Errors</span></span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a>The variance formula comes from the fact that $\hat{\beta}_1$ is a linear combination of the $Y_i$'s:</span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta}_1 = \sum_{i=1}^{n} w_i Y_i$$</span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a>where $w_i = \frac{X_i - \bar{X}_n}{\sum_{j=1}^{n}(X_j - \bar{X}_n)^2}$.</span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>Since the $Y_i$'s are independent with variance $\sigma^2$:</span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(\hat{\beta}_1 \mid X^n) = \sigma^2 \sum_{i=1}^{n} w_i^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(X_i - \bar{X}_n)^2} = \frac{\sigma^2}{n s_X^2}$$</span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a>The derivation for $\hat{\beta}_0$ is similar but more involved due to its dependence on both $\bar{Y}_n$ and $\hat{\beta}_1$.</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a>**Asymptotic Properties and Inference**</span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a>Under appropriate regularity conditions, as $n \to \infty$:</span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Consistency**: $\hat{\beta}_0 \xrightarrow{P} \beta_0$ and $\hat{\beta}_1 \xrightarrow{P} \beta_1$</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Asymptotic Normality**: </span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>   $$\frac{\hat{\beta}_0 - \beta_0}{\widehat{\text{se}}(\hat{\beta}_0)} \rightsquigarrow \mathcal{N}(0, 1) \quad \text{and} \quad \frac{\hat{\beta}_1 - \beta_1}{\widehat{\text{se}}(\hat{\beta}_1)} \rightsquigarrow \mathcal{N}(0, 1)$$</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Confidence Intervals**: An approximate $(1-\alpha)$ confidence interval for $\beta_j$ is:</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a>   $$\hat{\beta}_j \pm z_{\alpha/2} \cdot \widehat{\text{se}}(\hat{\beta}_j)$$</span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Hypothesis Testing**: The Wald test for $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$ is:</span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a>   reject $H_0$ if $|W| &gt; z_{\alpha/2}$ where $W = \hat{\beta}_1 / \widehat{\text{se}}(\hat{\beta}_1)$</span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finite Sample Refinement</span></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a>For finite $n$, a more accurate test uses the Student's $t$-distribution with $n-2$ degrees of freedom rather than the normal distribution. This accounts for the additional uncertainty from estimating $\sigma^2$. Most statistical software uses the $t$-distribution by default for regression inference.</span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple Linear Regression in Practice</span></span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a>Let's see how these concepts work with real data. We'll use the classic Framingham Heart Study dataset to explore the relationship between weight and blood pressure, building our understanding step by step.</span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: The Framingham Heart Study</span></span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a>The Framingham Heart Study is a long-term cardiovascular study that began in 1948 in Framingham, Massachusetts. This landmark study has provided crucial insights into cardiovascular disease risk factors. We'll examine the relationship between **relative weight** (FRW - a normalized weight measure where 100 represents median weight for height) and **systolic blood pressure** (SBP - the pressure when the heart beats).</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 1: Loading and Exploring the Data</span></span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a>First, let's load the data and understand what we're working with:</span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Framingham data</span></span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a>fram <span class="op">=</span> pd.read_csv(<span class="st">'../data/fram.txt'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first few rows to understand the structure</span></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 6 rows of the Framingham data:"</span>)</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fram.head(<span class="dv">6</span>))</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Dataset shape: </span><span class="sc">{</span>fram<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Columns: </span><span class="sc">{</span><span class="bu">list</span>(fram.columns)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a><span class="co"># Focus on our variables of interest</span></span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Summary statistics for key variables:"</span>)</span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fram[[<span class="st">'FRW'</span>, <span class="st">'SBP'</span>]].describe())</span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a>fram <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'../data/fram.txt'</span>, <span class="at">sep=</span><span class="st">'</span><span class="sc">\t</span><span class="st">'</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first few rows to understand the structure</span></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"First 6 rows of the Framingham data:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(fram)</span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Dataset dimensions:"</span>, <span class="fu">dim</span>(fram), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Column names:"</span>, <span class="fu">names</span>(fram), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary statistics for key variables</span></span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fram[<span class="fu">c</span>(<span class="st">"FRW"</span>, <span class="st">"SBP"</span>)])</span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understanding the Variables</span></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The dataset contains 1,394 observations with 13 variables including demographics, behaviors, and health outcomes.</span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FRW (Framingham Relative Weight)**: A normalized measure where 100 represents the median weight for a given height. Values above 100 indicate above-median weight.</span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SBP (Systolic Blood Pressure)**: Measured in mmHg, normal range is typically below 120. Values ≥140 indicate hypertension.</span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 2: Initial Visualization</span></span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a>Before fitting any model, let's visualize the relationship between weight and blood pressure:</span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scatter plot to explore the relationship</span></span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'FRW'</span>], fram[<span class="st">'SBP'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Relative Weight (FRW)'</span>)</span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Systolic Blood Pressure (SBP)'</span>)</span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Relationship between Relative Weight and Blood Pressure'</span>)</span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scatter plot to explore the relationship</span></span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fram<span class="sc">$</span>FRW, fram<span class="sc">$</span>SBP, </span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Relative Weight (FRW)"</span>,</span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Systolic Blood Pressure (SBP)"</span>,</span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Relationship between Relative Weight and Blood Pressure"</span>,</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>))</span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a><span class="fu">## What the Visualization Tells Us</span></span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a>Looking at this scatter plot, we can observe:</span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There appears to be a **positive relationship**: As relative weight increases, blood pressure tends to increase.</span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There's **substantial variation**: The wide spread of points suggests that weight alone won't perfectly predict blood pressure - other factors must also be important.</span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The pattern looks **compatible with the linearity hypothesis**, in the sense that there's no obvious curve or nonlinear pattern that would suggest a straight line is inappropriate.</span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a>This visualization motivates our next step: fitting a linear model to quantify this relationship.</span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 3: Fitting the Linear Regression Model</span></span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a>Now let's fit the simple linear regression model: $\text{SBP} = \beta_0 + \beta_1 \cdot \text{FRW} + \epsilon$. </span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a>Linear regression with a quadratic loss is also called "ordinary least squares" (OLS), a term which can be found in statistical software like in Python's <span class="in">`statsmodel`</span> package.</span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear regression model using statsmodels</span></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW'</span>, data<span class="op">=</span>fram)</span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> model.fit()</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the fitted equation</span></span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fitted equation: SBP = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">'Intercept'</span>]<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss"> * FRW"</span>)</span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the linear regression model</span></span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW, <span class="at">data =</span> fram)</span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the fitted equation</span></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Fitted equation: SBP = %.2f + %.3f * FRW</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a>            <span class="fu">coef</span>(fit)[<span class="dv">1</span>], <span class="fu">coef</span>(fit)[<span class="dv">2</span>]))</span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a>We've now fitted our linear regression model. The fitted equation shows the estimated relationship between relative weight and blood pressure. In the following steps, we'll use this model to make predictions, visualize the fit, check our assumptions, and interpret the statistical significance of our findings.</span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 4: Making Predictions</span></span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a>Let's use our model to make predictions for specific weight values:</span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions for specific FRW values</span></span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a>test_weights <span class="op">=</span> pd.DataFrame({<span class="st">'FRW'</span>: [<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>]})</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> results.predict(test_weights)</span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predictions for new data points:"</span>)</span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">40</span>)</span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> frw, sbp <span class="kw">in</span> <span class="bu">zip</span>(test_weights[<span class="st">'FRW'</span>], predictions):</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"FRW = </span><span class="sc">{</span>frw<span class="sc">:3d}</span><span class="ss"> → Predicted SBP = </span><span class="sc">{</span>sbp<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual calculation to show the mechanics</span></span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Manual calculation (verifying our understanding):"</span>)</span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>beta0 <span class="op">=</span> results.params[<span class="st">'Intercept'</span>]</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> results.params[<span class="st">'FRW'</span>]</span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> frw <span class="kw">in</span> [<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>]:</span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a>    manual_pred <span class="op">=</span> beta0 <span class="op">+</span> beta1 <span class="op">*</span> frw</span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"FRW = </span><span class="sc">{</span>frw<span class="sc">:3d}</span><span class="ss"> → </span><span class="sc">{</span>beta0<span class="sc">:.3f}</span><span class="ss"> + </span><span class="sc">{</span>beta1<span class="sc">:.3f}</span><span class="ss"> × </span><span class="sc">{</span>frw<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>manual_pred<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions for specific FRW values</span></span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a>test_weights <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">FRW =</span> <span class="fu">c</span>(<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>))</span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> test_weights)</span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Predictions for new data points:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="fu">rep</span>(<span class="st">"="</span>, <span class="dv">40</span>), <span class="at">collapse=</span><span class="st">""</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(test_weights)) {</span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"FRW = %3d → Predicted SBP = %.1f</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a>                test_weights<span class="sc">$</span>FRW[i], predictions[i]))</span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual calculation</span></span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)[<span class="dv">1</span>]</span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>]</span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Manual calculation (verifying our understanding):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(frw <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">120</span>)) {</span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a>    manual_pred <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> frw</span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"FRW = %3d → %.3f + %.3f × %d = %.1f</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a>                frw, beta0, beta1, frw, manual_pred))</span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 5: Visualizing the Fitted Model</span></span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a>Let's plot the fitted model:</span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic fitted line visualization</span></span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'FRW'</span>], fram[<span class="st">'SBP'</span>], alpha<span class="op">=</span><span class="fl">0.4</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(fram[<span class="st">'FRW'</span>].<span class="bu">min</span>(), fram[<span class="st">'FRW'</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> results.params[<span class="st">'Intercept'</span>] <span class="op">+</span> results.params[<span class="st">'FRW'</span>] <span class="op">*</span> x_range</span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, y_pred, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted line'</span>)</span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Relative Weight (FRW)'</span>)</span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Systolic Blood Pressure (SBP)'</span>)</span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Fitted Regression Line'</span>)</span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a><span class="co"># Add equation to the plot</span></span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a>equation <span class="op">=</span> <span class="ss">f'SBP = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"Intercept"</span>]<span class="sc">:.1f}</span><span class="ss"> + </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"FRW"</span>]<span class="sc">:.3f}</span><span class="ss"> × FRW'</span></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, equation, transform<span class="op">=</span>plt.gca().transAxes, fontsize<span class="op">=</span><span class="dv">11</span>,</span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a>         verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic fitted line visualization</span></span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fram<span class="sc">$</span>FRW, fram<span class="sc">$</span>SBP, </span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Relative Weight (FRW)"</span>,</span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Systolic Blood Pressure (SBP)"</span>,</span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Fitted Regression Line"</span>,</span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.4</span>))</span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a><span class="co"># Add equation to the plot</span></span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a>equation <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"SBP ="</span>, <span class="fu">round</span>(<span class="fu">coef</span>(fit)[<span class="dv">1</span>], <span class="dv">1</span>), <span class="st">"+"</span>, </span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">round</span>(<span class="fu">coef</span>(fit)[<span class="dv">2</span>], <span class="dv">3</span>), <span class="st">"× FRW"</span>)</span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> equation, </span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a>       <span class="at">bty =</span> <span class="st">"n"</span>, <span class="at">cex =</span> <span class="fl">1.1</span>,</span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.col =</span> <span class="st">"black"</span>, <span class="at">bg =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="fl">0.96</span>, <span class="fl">0.8</span>, <span class="fl">0.8</span>))</span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a>The plot above shows our fitted regression line. The equation in the box gives us the specific relationship: for each unit increase in relative weight (FRW), systolic blood pressure increases by approximately half a mmHg on average (see the exact coefficient in the equation above).</span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a>The plot below shows the geometric interpretation of intercept ($\beta_0$) and slope ($\beta_1$) parameters:</span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code"</span></span>
<span id="cb2-755"><a href="#cb2-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-756"><a href="#cb2-756" aria-hidden="true" tabindex="-1"></a><span class="co"># Geometric interpretation of intercept and slope</span></span>
<span id="cb2-757"><a href="#cb2-757" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb2-758"><a href="#cb2-758" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'FRW'</span>], fram[<span class="st">'SBP'</span>], alpha<span class="op">=</span><span class="fl">0.4</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-759"><a href="#cb2-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-760"><a href="#cb2-760" aria-hidden="true" tabindex="-1"></a><span class="co"># Extend x range to show intercept</span></span>
<span id="cb2-761"><a href="#cb2-761" aria-hidden="true" tabindex="-1"></a>x_extended <span class="op">=</span> np.linspace(<span class="dv">0</span>, fram[<span class="st">'FRW'</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb2-762"><a href="#cb2-762" aria-hidden="true" tabindex="-1"></a>y_extended <span class="op">=</span> results.params[<span class="st">'Intercept'</span>] <span class="op">+</span> results.params[<span class="st">'FRW'</span>] <span class="op">*</span> x_extended</span>
<span id="cb2-763"><a href="#cb2-763" aria-hidden="true" tabindex="-1"></a>plt.plot(x_extended, y_extended, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-764"><a href="#cb2-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-765"><a href="#cb2-765" aria-hidden="true" tabindex="-1"></a><span class="co"># Show intercept (β₀) at x=0</span></span>
<span id="cb2-766"><a href="#cb2-766" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, results.params[<span class="st">'Intercept'</span>]], <span class="st">'green'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-767"><a href="#cb2-767" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">0</span>, results.params[<span class="st">'Intercept'</span>], <span class="st">'go'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb2-768"><a href="#cb2-768" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="vs">r'$\beta_0$'</span> <span class="op">+</span> <span class="ss">f' = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"Intercept"</span>]<span class="sc">:.1f}</span><span class="ss">'</span>, </span>
<span id="cb2-769"><a href="#cb2-769" aria-hidden="true" tabindex="-1"></a>            xy<span class="op">=</span>(<span class="dv">0</span>, results.params[<span class="st">'Intercept'</span>]), xytext<span class="op">=</span>(<span class="dv">10</span>, results.params[<span class="st">'Intercept'</span>] <span class="op">-</span> <span class="dv">10</span>),</span>
<span id="cb2-770"><a href="#cb2-770" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">'green'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb2-771"><a href="#cb2-771" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb2-772"><a href="#cb2-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-773"><a href="#cb2-773" aria-hidden="true" tabindex="-1"></a><span class="co"># Show slope (β₁) interpretation</span></span>
<span id="cb2-774"><a href="#cb2-774" aria-hidden="true" tabindex="-1"></a>x_demo <span class="op">=</span> [<span class="dv">40</span>, <span class="dv">60</span>]</span>
<span id="cb2-775"><a href="#cb2-775" aria-hidden="true" tabindex="-1"></a>y_demo <span class="op">=</span> [results.params[<span class="st">'Intercept'</span>] <span class="op">+</span> results.params[<span class="st">'FRW'</span>] <span class="op">*</span> x <span class="cf">for</span> x <span class="kw">in</span> x_demo]</span>
<span id="cb2-776"><a href="#cb2-776" aria-hidden="true" tabindex="-1"></a>plt.plot(x_demo, [y_demo[<span class="dv">0</span>], y_demo[<span class="dv">0</span>]], <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-777"><a href="#cb2-777" aria-hidden="true" tabindex="-1"></a>plt.plot([x_demo[<span class="dv">1</span>], x_demo[<span class="dv">1</span>]], y_demo, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-778"><a href="#cb2-778" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Δx = 20'</span>, xy<span class="op">=</span>(<span class="dv">50</span>, y_demo[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">15</span>), fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'blue'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb2-779"><a href="#cb2-779" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Δy = </span><span class="sc">{</span><span class="dv">20</span> <span class="op">*</span> results<span class="sc">.</span>params[<span class="st">"FRW"</span>]<span class="sc">:.1f}</span><span class="ss">'</span>, xy<span class="op">=</span>(x_demo[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">2</span>, np.mean(y_demo)), </span>
<span id="cb2-780"><a href="#cb2-780" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'blue'</span>, rotation<span class="op">=</span><span class="dv">90</span>, va<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb2-781"><a href="#cb2-781" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="dv">20</span>, y_demo[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">25</span>, <span class="vs">r'$\beta_1$'</span> <span class="op">+</span> <span class="ss">f' = </span><span class="sc">{</span>results<span class="sc">.</span>params[<span class="st">"FRW"</span>]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb2-782"><a href="#cb2-782" aria-hidden="true" tabindex="-1"></a>         fontsize<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">'blue'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb2-783"><a href="#cb2-783" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>))</span>
<span id="cb2-784"><a href="#cb2-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-785"><a href="#cb2-785" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">5</span>, fram[<span class="st">'FRW'</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">5</span>)</span>
<span id="cb2-786"><a href="#cb2-786" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, fram[<span class="st">'SBP'</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb2-787"><a href="#cb2-787" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Relative Weight (FRW)'</span>)</span>
<span id="cb2-788"><a href="#cb2-788" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Systolic Blood Pressure (SBP)'</span>)</span>
<span id="cb2-789"><a href="#cb2-789" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r'Geometric Interpretation: $\beta_0$ (intercept) and $\beta_1$ (slope)'</span>)</span>
<span id="cb2-790"><a href="#cb2-790" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-791"><a href="#cb2-791" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-792"><a href="#cb2-792" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-793"><a href="#cb2-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-794"><a href="#cb2-794" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 6: Model Diagnostics {#step-6-model-diagnostics}</span></span>
<span id="cb2-795"><a href="#cb2-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-796"><a href="#cb2-796" aria-hidden="true" tabindex="-1"></a>Before interpreting our regression results, let's check if our model assumptions are satisfied using diagnostic plots:</span>
<span id="cb2-797"><a href="#cb2-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-798"><a href="#cb2-798" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-799"><a href="#cb2-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-800"><a href="#cb2-800" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-801"><a href="#cb2-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-804"><a href="#cb2-804" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-805"><a href="#cb2-805" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-806"><a href="#cb2-806" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb2-807"><a href="#cb2-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-808"><a href="#cb2-808" aria-hidden="true" tabindex="-1"></a><span class="co"># Create diagnostic plots</span></span>
<span id="cb2-809"><a href="#cb2-809" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">8</span>))</span>
<span id="cb2-810"><a href="#cb2-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-811"><a href="#cb2-811" aria-hidden="true" tabindex="-1"></a><span class="co"># Get residuals and fitted values</span></span>
<span id="cb2-812"><a href="#cb2-812" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> results.resid</span>
<span id="cb2-813"><a href="#cb2-813" aria-hidden="true" tabindex="-1"></a>fitted <span class="op">=</span> results.fittedvalues</span>
<span id="cb2-814"><a href="#cb2-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-815"><a href="#cb2-815" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Residuals vs Fitted Values</span></span>
<span id="cb2-816"><a href="#cb2-816" aria-hidden="true" tabindex="-1"></a>ax1.scatter(fitted, residuals, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-817"><a href="#cb2-817" aria-hidden="true" tabindex="-1"></a>ax1.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-818"><a href="#cb2-818" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb2-819"><a href="#cb2-819" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb2-820"><a href="#cb2-820" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Residuals vs Fitted Values'</span>)</span>
<span id="cb2-821"><a href="#cb2-821" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-822"><a href="#cb2-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-823"><a href="#cb2-823" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Q-Q plot for normality check</span></span>
<span id="cb2-824"><a href="#cb2-824" aria-hidden="true" tabindex="-1"></a>stats.probplot(residuals, dist<span class="op">=</span><span class="st">"norm"</span>, plot<span class="op">=</span>ax2)</span>
<span id="cb2-825"><a href="#cb2-825" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Normal Q-Q Plot'</span>)</span>
<span id="cb2-826"><a href="#cb2-826" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-827"><a href="#cb2-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-828"><a href="#cb2-828" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-829"><a href="#cb2-829" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-830"><a href="#cb2-830" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-831"><a href="#cb2-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-832"><a href="#cb2-832" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-833"><a href="#cb2-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-834"><a href="#cb2-834" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-835"><a href="#cb2-835" aria-hidden="true" tabindex="-1"></a><span class="co"># Create diagnostic plots</span></span>
<span id="cb2-836"><a href="#cb2-836" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb2-837"><a href="#cb2-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-838"><a href="#cb2-838" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Residuals vs Fitted</span></span>
<span id="cb2-839"><a href="#cb2-839" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb2-840"><a href="#cb2-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-841"><a href="#cb2-841" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Q-Q plot</span></span>
<span id="cb2-842"><a href="#cb2-842" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">which =</span> <span class="dv">2</span>)</span>
<span id="cb2-843"><a href="#cb2-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-844"><a href="#cb2-844" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset plot layout</span></span>
<span id="cb2-845"><a href="#cb2-845" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-846"><a href="#cb2-846" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-847"><a href="#cb2-847" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-848"><a href="#cb2-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-849"><a href="#cb2-849" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-850"><a href="#cb2-850" aria-hidden="true" tabindex="-1"></a><span class="fu">## What the Diagnostic Plots Tell Us</span></span>
<span id="cb2-851"><a href="#cb2-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-852"><a href="#cb2-852" aria-hidden="true" tabindex="-1"></a>**Residuals vs Fitted**: The residuals are approximately centered around zero with reasonably random scatter, though we can see some evidence of skew (asymmetry) in the distribution.</span>
<span id="cb2-853"><a href="#cb2-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-854"><a href="#cb2-854" aria-hidden="true" tabindex="-1"></a>**Q-Q Plot (Quantile-Quantile Plot)**: This plot compares the distribution of our residuals to a normal distribution. If residuals were perfectly normal, all points would fall exactly on the diagonal line. Here we see the points roughly follow the line but with some deviation at the extremes (the tails), confirming that the residuals are not perfectly normally distributed.</span>
<span id="cb2-855"><a href="#cb2-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-856"><a href="#cb2-856" aria-hidden="true" tabindex="-1"></a>**Important**: These minor violations of assumptions are common with real data and not necessarily dealbreakers. Linear regression is quite robust to modest departures from normality, especially with large samples like ours (n=1,394). However, we should keep these limitations in mind when interpreting results and consider transformations or robust methods if violations become severe.</span>
<span id="cb2-857"><a href="#cb2-857" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-858"><a href="#cb2-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-859"><a href="#cb2-859" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 7: Model Interpretation</span></span>
<span id="cb2-860"><a href="#cb2-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-861"><a href="#cb2-861" aria-hidden="true" tabindex="-1"></a>Now let's examine and interpret the full regression output:</span>
<span id="cb2-862"><a href="#cb2-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-863"><a href="#cb2-863" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-864"><a href="#cb2-864" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-865"><a href="#cb2-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-868"><a href="#cb2-868" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-869"><a href="#cb2-869" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-870"><a href="#cb2-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-871"><a href="#cb2-871" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the full regression summary</span></span>
<span id="cb2-872"><a href="#cb2-872" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span>
<span id="cb2-873"><a href="#cb2-873" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-874"><a href="#cb2-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-875"><a href="#cb2-875" aria-hidden="true" tabindex="-1"></a>**Note:** Python's statsmodels output is more verbose than R's. Focus on these key sections:</span>
<span id="cb2-876"><a href="#cb2-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-877"><a href="#cb2-877" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Coefficients table** (middle): Shows estimates, standard errors, t-statistics, and p-values</span>
<span id="cb2-878"><a href="#cb2-878" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**R-squared** (top-right): Proportion of variance explained</span>
<span id="cb2-879"><a href="#cb2-879" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**F-statistic** (top-right): Tests overall model significance</span>
<span id="cb2-880"><a href="#cb2-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-881"><a href="#cb2-881" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-882"><a href="#cb2-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-883"><a href="#cb2-883" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-884"><a href="#cb2-884" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the regression summary</span></span>
<span id="cb2-885"><a href="#cb2-885" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb2-886"><a href="#cb2-886" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-887"><a href="#cb2-887" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-888"><a href="#cb2-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-889"><a href="#cb2-889" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-890"><a href="#cb2-890" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpreting the Regression Output</span></span>
<span id="cb2-891"><a href="#cb2-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-892"><a href="#cb2-892" aria-hidden="true" tabindex="-1"></a>**Key Statistical Tests:**</span>
<span id="cb2-893"><a href="#cb2-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-894"><a href="#cb2-894" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The reported p-values for individual coefficients use the **Wald Test**:</span>
<span id="cb2-895"><a href="#cb2-895" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$H_0: \beta_j = 0$ (coefficient equals zero, no effect)</span>
<span id="cb2-896"><a href="#cb2-896" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Test statistic: $t = \hat{\beta}_j / \text{SE}(\hat{\beta}_j)$</span>
<span id="cb2-897"><a href="#cb2-897" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Our result: FRW p-value &lt; 0.001 → reject $H_0$ → weight significantly affects blood pressure</span>
<span id="cb2-898"><a href="#cb2-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-899"><a href="#cb2-899" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**F-Test** is used for overall model significance</span>
<span id="cb2-900"><a href="#cb2-900" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$H_0: \beta_1 = \beta_2 = \ldots = \beta_k = 0$ (no predictors have any effect)</span>
<span id="cb2-901"><a href="#cb2-901" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Tests if **any** coefficient is non-zero</span>
<span id="cb2-902"><a href="#cb2-902" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Our result: F = 172.5, p &lt; 0.001 → reject $H_0$ → model is useful</span>
<span id="cb2-903"><a href="#cb2-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-904"><a href="#cb2-904" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**R-Squared** ($R^2$, coefficient of determination)</span>
<span id="cb2-905"><a href="#cb2-905" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Proportion of variance in $Y$ explained by the model (typically between 0 and 1)</span>
<span id="cb2-906"><a href="#cb2-906" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Measures accuracy on training data</span>
<span id="cb2-907"><a href="#cb2-907" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>In our example: model explains roughly 11% of blood pressure variation (see exact value in output above)</span>
<span id="cb2-908"><a href="#cb2-908" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Low $R^2$ common when many unmeasured factors affect outcome, which happens for many real-world data</span>
<span id="cb2-909"><a href="#cb2-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-910"><a href="#cb2-910" aria-hidden="true" tabindex="-1"></a>**Reading the Coefficients Table:**</span>
<span id="cb2-911"><a href="#cb2-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-912"><a href="#cb2-912" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Intercept ($\beta_0$)**: Expected SBP when FRW = 0 (extrapolation - not meaningful here)</span>
<span id="cb2-913"><a href="#cb2-913" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FRW coefficient ($\beta_1$)**: Each unit increase in relative weight increases SBP by approximately 0.5 mmHg on average (see exact value in output)</span>
<span id="cb2-914"><a href="#cb2-914" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Standard errors**: Quantify uncertainty in estimates</span>
<span id="cb2-915"><a href="#cb2-915" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**95% CI for $\beta_1$**: Check the confidence interval in the output - it quantifies our uncertainty about the true slope</span>
<span id="cb2-916"><a href="#cb2-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-917"><a href="#cb2-917" aria-hidden="true" tabindex="-1"></a>**Model Quality Assessment:**</span>
<span id="cb2-918"><a href="#cb2-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-919"><a href="#cb2-919" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual Standard Error**: Typical prediction error (see output for exact value)</span>
<span id="cb2-920"><a href="#cb2-920" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistical vs. Practical Significance**: While statistically significant (p &lt; 0.001), the effect size is modest</span>
<span id="cb2-921"><a href="#cb2-921" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Predictive Power**: The relatively low $R^2$ indicates that weight alone is not a strong predictor of blood pressure</span>
<span id="cb2-922"><a href="#cb2-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-923"><a href="#cb2-923" aria-hidden="true" tabindex="-1"></a>**Key Takeaway**: The regression confirms a statistically significant positive relationship between weight and blood pressure. However, the modest $R^2$ reminds us that blood pressure is multifactorial -- diet, exercise, genetics, stress, and many other factors play important roles.</span>
<span id="cb2-924"><a href="#cb2-924" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-925"><a href="#cb2-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-926"><a href="#cb2-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-927"><a href="#cb2-927" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Summary: What We've Learned</span></span>
<span id="cb2-928"><a href="#cb2-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-929"><a href="#cb2-929" aria-hidden="true" tabindex="-1"></a>Through this comprehensive example, we've followed a complete regression workflow:</span>
<span id="cb2-930"><a href="#cb2-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-931"><a href="#cb2-931" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Data Exploration**: Understood our variables and their context</span>
<span id="cb2-932"><a href="#cb2-932" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Initial Visualization**: Examined the relationship visually before modeling</span>
<span id="cb2-933"><a href="#cb2-933" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Model Fitting**: Applied least squares to find the best-fitting line</span>
<span id="cb2-934"><a href="#cb2-934" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Making Predictions**: Used the model to predict new values</span>
<span id="cb2-935"><a href="#cb2-935" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Visualizing the Fit**: Showed the fitted line</span>
<span id="cb2-936"><a href="#cb2-936" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Model Diagnostics**: Checked assumptions through residual plots</span>
<span id="cb2-937"><a href="#cb2-937" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Model Interpretation**: Understood the statistical tests and what they tell us</span>
<span id="cb2-938"><a href="#cb2-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-939"><a href="#cb2-939" aria-hidden="true" tabindex="-1"></a>The Framingham data reveals an important finding: weight has a statistically significant effect on blood pressure (p &lt; 0.001), with each unit increase in relative weight associated with approximately a 0.5 mmHg increase in systolic blood pressure (see the exact coefficient in the regression output above). </span>
<span id="cb2-940"><a href="#cb2-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-941"><a href="#cb2-941" aria-hidden="true" tabindex="-1"></a>The modest $R^2$ (around 11% of variance explained) doesn't diminish the medical importance of this relationship. Rather, it reminds us that:</span>
<span id="cb2-942"><a href="#cb2-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-943"><a href="#cb2-943" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Blood pressure is multifactorial - weight is one important factor among many (age, diet, exercise, genetics, stress)</span>
<span id="cb2-944"><a href="#cb2-944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Even when individual predictors explain modest variance, they can still be clinically meaningful</span>
<span id="cb2-945"><a href="#cb2-945" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple linear regression effectively quantifies real-world relationships and their uncertainty</span>
<span id="cb2-946"><a href="#cb2-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-947"><a href="#cb2-947" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple Linear Regression</span></span>
<span id="cb2-948"><a href="#cb2-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-949"><a href="#cb2-949" aria-hidden="true" tabindex="-1"></a><span class="fu">### Extending the Model to Multiple Predictors</span></span>
<span id="cb2-950"><a href="#cb2-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-951"><a href="#cb2-951" aria-hidden="true" tabindex="-1"></a>Real-world phenomena rarely depend on just one predictor. A person's blood pressure isn't determined solely by their weight -- age, diet, exercise, genetics, and countless other factors play roles. Multiple linear regression extends our framework to handle multiple predictors simultaneously.</span>
<span id="cb2-952"><a href="#cb2-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-953"><a href="#cb2-953" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Model</span></span>
<span id="cb2-954"><a href="#cb2-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-955"><a href="#cb2-955" aria-hidden="true" tabindex="-1"></a>With $k$ predictors (covariates), the multiple linear regression model becomes:</span>
<span id="cb2-956"><a href="#cb2-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-957"><a href="#cb2-957" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \epsilon_i, \quad i = 1, \ldots, n$$</span>
<span id="cb2-958"><a href="#cb2-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-959"><a href="#cb2-959" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb2-960"><a href="#cb2-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-961"><a href="#cb2-961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Y_i$ is the response for observation $i$</span>
<span id="cb2-962"><a href="#cb2-962" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{ij}$ is the value of the $j$-th covariate for observation $i$</span>
<span id="cb2-963"><a href="#cb2-963" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_0$ is the intercept</span>
<span id="cb2-964"><a href="#cb2-964" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_j$ is the coefficient for the $j$-th covariate (for $j = 1, \ldots, k$)</span>
<span id="cb2-965"><a href="#cb2-965" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\epsilon_i$ is the error term with $\mathbb{E}(\epsilon_i \mid X_i) = 0$ and $\mathbb{V}(\epsilon_i \mid X_i) = \sigma^2$</span>
<span id="cb2-966"><a href="#cb2-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-967"><a href="#cb2-967" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb2-968"><a href="#cb2-968" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convention: Incorporating the Intercept</span></span>
<span id="cb2-969"><a href="#cb2-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-970"><a href="#cb2-970" aria-hidden="true" tabindex="-1"></a>The intercept $\beta_0$ is often incorporated directly into the covariate notation by defining $X_{i0} = 1$ for all observations $i = 1, \ldots, n$. This allows us to write the model more compactly as:</span>
<span id="cb2-971"><a href="#cb2-971" aria-hidden="true" tabindex="-1"></a>$$Y_i = \sum_{j=0}^{k} \beta_j X_{ij} + \epsilon_i$$</span>
<span id="cb2-972"><a href="#cb2-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-973"><a href="#cb2-973" aria-hidden="true" tabindex="-1"></a>This convention simplifies matrix notation and many derivations. When you see design matrices or covariate vectors, check whether the intercept is handled explicitly (with a column of ones) or implicitly (assumed but not shown).</span>
<span id="cb2-974"><a href="#cb2-974" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-975"><a href="#cb2-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-976"><a href="#cb2-976" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-977"><a href="#cb2-977" aria-hidden="true" tabindex="-1"></a><span class="fu">## Indexing Confusion: Starting from 0 vs 1</span></span>
<span id="cb2-978"><a href="#cb2-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-979"><a href="#cb2-979" aria-hidden="true" tabindex="-1"></a>You'll encounter two indexing conventions in the literature:</span>
<span id="cb2-980"><a href="#cb2-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-981"><a href="#cb2-981" aria-hidden="true" tabindex="-1"></a>**Convention 1 (0-indexed, used here in the lecture notes):**</span>
<span id="cb2-982"><a href="#cb2-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-983"><a href="#cb2-983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i0} = 1$ for the intercept</span>
<span id="cb2-984"><a href="#cb2-984" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i1}, X_{i2}, \ldots, X_{ik}$ for the $k$ actual covariates  </span>
<span id="cb2-985"><a href="#cb2-985" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Design matrix $\mathbf{X}$ is $n \times (k+1)$</span>
<span id="cb2-986"><a href="#cb2-986" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model: $Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_k X_{ik} + \epsilon_i$</span>
<span id="cb2-987"><a href="#cb2-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-988"><a href="#cb2-988" aria-hidden="true" tabindex="-1"></a>**Convention 2 (1-indexed, common in some texts):**</span>
<span id="cb2-989"><a href="#cb2-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-990"><a href="#cb2-990" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i1} = 1$ for the intercept</span>
<span id="cb2-991"><a href="#cb2-991" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i2}, X_{i3}, \ldots, X_{i,k+1}$ for the $k$ actual covariates</span>
<span id="cb2-992"><a href="#cb2-992" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Design matrix $\mathbf{X}$ is still $n \times (k+1)$ </span>
<span id="cb2-993"><a href="#cb2-993" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model: $Y_i = \beta_1 + \beta_2 X_{i2} + \cdots + \beta_{k+1} X_{i,k+1} + \epsilon_i$</span>
<span id="cb2-994"><a href="#cb2-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-995"><a href="#cb2-995" aria-hidden="true" tabindex="-1"></a>Both are correct! The key is consistency within a given analysis. Software typically handles this transparently -- R's <span class="in">`lm()`</span> and Python's <span class="in">`statsmodels`</span> automatically add the intercept column regardless of your indexing preference.</span>
<span id="cb2-996"><a href="#cb2-996" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-997"><a href="#cb2-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-998"><a href="#cb2-998" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb2-999"><a href="#cb2-999" aria-hidden="true" tabindex="-1"></a>**Multiple Linear Regression in Matrix Form**</span>
<span id="cb2-1000"><a href="#cb2-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1001"><a href="#cb2-1001" aria-hidden="true" tabindex="-1"></a>The model is more elegantly expressed using matrix notation:</span>
<span id="cb2-1002"><a href="#cb2-1002" aria-hidden="true" tabindex="-1"></a>$$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$</span>
<span id="cb2-1003"><a href="#cb2-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1004"><a href="#cb2-1004" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb2-1005"><a href="#cb2-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1006"><a href="#cb2-1006" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{Y}$ is an $n \times 1$ vector of responses</span>
<span id="cb2-1007"><a href="#cb2-1007" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{X}$ is an $n \times (k+1)$ design matrix (often written as $n \times k$ when the intercept is implicit)</span>
<span id="cb2-1008"><a href="#cb2-1008" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{\beta}$ is a $(k+1) \times 1$ vector of coefficients (or $k \times 1$ when intercept is implicit)</span>
<span id="cb2-1009"><a href="#cb2-1009" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{\epsilon}$ is an $n \times 1$ vector of errors</span>
<span id="cb2-1010"><a href="#cb2-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1011"><a href="#cb2-1011" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1012"><a href="#cb2-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1013"><a href="#cb2-1013" aria-hidden="true" tabindex="-1"></a>Explicitly (showing the column of 1s separately from the actual covariates):</span>
<span id="cb2-1014"><a href="#cb2-1014" aria-hidden="true" tabindex="-1"></a>$$\begin{pmatrix} Y_1 <span class="sc">\\</span> Y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> Y_n \end{pmatrix} = \begin{pmatrix} 1 &amp; X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1k} <span class="sc">\\</span> 1 &amp; X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2k} <span class="sc">\\</span> \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span> 1 &amp; X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{nk} \end{pmatrix} \begin{pmatrix} \beta_0 <span class="sc">\\</span> \beta_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \beta_k \end{pmatrix} + \begin{pmatrix} \epsilon_1 <span class="sc">\\</span> \epsilon_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \epsilon_n \end{pmatrix}$$</span>
<span id="cb2-1015"><a href="#cb2-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1016"><a href="#cb2-1016" aria-hidden="true" tabindex="-1"></a>Here:</span>
<span id="cb2-1017"><a href="#cb2-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1018"><a href="#cb2-1018" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The first column of 1s corresponds to what we defined as $X_{i0} = 1$ for the intercept</span>
<span id="cb2-1019"><a href="#cb2-1019" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i1}, X_{i2}, \ldots, X_{ik}$ are the values of the $k$ actual covariates for observation $i$</span>
<span id="cb2-1020"><a href="#cb2-1020" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $n$ rows correspond to different observations</span>
<span id="cb2-1021"><a href="#cb2-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $k+1$ columns correspond to the intercept and $k$ covariates</span>
<span id="cb2-1022"><a href="#cb2-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1023"><a href="#cb2-1023" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Least Squares in Matrix Form</span></span>
<span id="cb2-1024"><a href="#cb2-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1025"><a href="#cb2-1025" aria-hidden="true" tabindex="-1"></a>The least squares criterion remains the same -- minimize RSS -- but the solution is now expressed in matrix form:</span>
<span id="cb2-1026"><a href="#cb2-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1027"><a href="#cb2-1027" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Least Squares Estimate (Matrix Form)"}</span>
<span id="cb2-1028"><a href="#cb2-1028" aria-hidden="true" tabindex="-1"></a>Assuming $\mathbf{X}^T\mathbf{X}$ is invertible, the least squares estimate is:</span>
<span id="cb2-1029"><a href="#cb2-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1030"><a href="#cb2-1030" aria-hidden="true" tabindex="-1"></a>$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$</span>
<span id="cb2-1031"><a href="#cb2-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1032"><a href="#cb2-1032" aria-hidden="true" tabindex="-1"></a>with variance-covariance matrix:</span>
<span id="cb2-1033"><a href="#cb2-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1034"><a href="#cb2-1034" aria-hidden="true" tabindex="-1"></a>$$\mathbb{V}(\hat{\boldsymbol{\beta}} \mid \mathbf{X}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$$</span>
<span id="cb2-1035"><a href="#cb2-1035" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1036"><a href="#cb2-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1037"><a href="#cb2-1037" aria-hidden="true" tabindex="-1"></a>**Key Results from the Least Squares Solution:**</span>
<span id="cb2-1038"><a href="#cb2-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1039"><a href="#cb2-1039" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**The estimated regression function** is:</span>
<span id="cb2-1040"><a href="#cb2-1040" aria-hidden="true" tabindex="-1"></a>   $$\hat{r}(x) = \hat{\beta}_0 + \sum_{j=1}^{k} \hat{\beta}_j x_j$$</span>
<span id="cb2-1041"><a href="#cb2-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1042"><a href="#cb2-1042" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**An unbiased estimate of the error variance** $\sigma^2$ is:</span>
<span id="cb2-1043"><a href="#cb2-1043" aria-hidden="true" tabindex="-1"></a>   $$\hat{\sigma}^2 = \frac{1}{n-k-1} \sum_{i=1}^{n} \hat{\epsilon}_i^2 = \frac{1}{n-k-1} ||\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}||^2$$</span>
<span id="cb2-1044"><a href="#cb2-1044" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb2-1045"><a href="#cb2-1045" aria-hidden="true" tabindex="-1"></a>   We divide by $n-k-1$ because we've estimated $k+1$ parameters (including the intercept).</span>
<span id="cb2-1046"><a href="#cb2-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1047"><a href="#cb2-1047" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Confidence intervals** for individual coefficients:</span>
<span id="cb2-1048"><a href="#cb2-1048" aria-hidden="true" tabindex="-1"></a>   $$\hat{\beta}_j \pm z_{\alpha/2} \cdot \widehat{\text{se}}(\hat{\beta}_j)$$</span>
<span id="cb2-1049"><a href="#cb2-1049" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb2-1050"><a href="#cb2-1050" aria-hidden="true" tabindex="-1"></a>   where $\widehat{\text{se}}^2(\hat{\beta}_j)$ is the $j$-th diagonal element of $\hat{\sigma}^2(\mathbf{X}^T\mathbf{X})^{-1}$.</span>
<span id="cb2-1051"><a href="#cb2-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1052"><a href="#cb2-1052" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb2-1053"><a href="#cb2-1053" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Meaning of a Coefficient</span></span>
<span id="cb2-1054"><a href="#cb2-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1055"><a href="#cb2-1055" aria-hidden="true" tabindex="-1"></a>In multiple regression, the interpretation of coefficients becomes more subtle. The coefficient $\beta_j$ represents the expected change in $Y$ for a one-unit change in $X_j$, **holding all other covariates constant**.</span>
<span id="cb2-1056"><a href="#cb2-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1057"><a href="#cb2-1057" aria-hidden="true" tabindex="-1"></a>This is the crucial concept of **statistical control** or **adjustment**. We're estimating the partial effect of each predictor, after accounting for the linear effects of all other predictors in the model.</span>
<span id="cb2-1058"><a href="#cb2-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1059"><a href="#cb2-1059" aria-hidden="true" tabindex="-1"></a>This interpretation assumes:</span>
<span id="cb2-1060"><a href="#cb2-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1061"><a href="#cb2-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The other variables can actually be held constant (may not be realistic)</span>
<span id="cb2-1062"><a href="#cb2-1062" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The relationship is truly linear</span>
<span id="cb2-1063"><a href="#cb2-1063" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>No important interactions exist between predictors</span>
<span id="cb2-1064"><a href="#cb2-1064" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1065"><a href="#cb2-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1066"><a href="#cb2-1066" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiple Regression in Practice</span></span>
<span id="cb2-1067"><a href="#cb2-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1068"><a href="#cb2-1068" aria-hidden="true" tabindex="-1"></a>We'll now add two biologically relevant predictors to our weight-only model:</span>
<span id="cb2-1069"><a href="#cb2-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1070"><a href="#cb2-1070" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SEX**: Categorical variable ("female"/"male" in the data)</span>
<span id="cb2-1071"><a href="#cb2-1071" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>R and Python automatically encode with female = 0 (reference), male = 1</span>
<span id="cb2-1072"><a href="#cb2-1072" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Output shows as "SEX<span class="co">[</span><span class="ot">T.male</span><span class="co">]</span>" (Python) or "SEXmale" (R)</span>
<span id="cb2-1073"><a href="#cb2-1073" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Coefficient interpretation: difference in mean SBP for males vs females</span>
<span id="cb2-1074"><a href="#cb2-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1075"><a href="#cb2-1075" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CHOL**: Total serum cholesterol in mg/dL (typical range: 150-250 mg/dL)</span>
<span id="cb2-1076"><a href="#cb2-1076" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>A known cardiovascular risk factor</span>
<span id="cb2-1077"><a href="#cb2-1077" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Coefficient interpretation: change in SBP per 1 mg/dL increase in cholesterol</span>
<span id="cb2-1078"><a href="#cb2-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1079"><a href="#cb2-1079" aria-hidden="true" tabindex="-1"></a>We'll build three models:</span>
<span id="cb2-1080"><a href="#cb2-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1081"><a href="#cb2-1081" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Model 1**: <span class="in">`SBP ~ FRW`</span> (baseline simple regression)</span>
<span id="cb2-1082"><a href="#cb2-1082" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Model 2**: <span class="in">`SBP ~ FRW + SEX + CHOL`</span> (multiple regression)</span>
<span id="cb2-1083"><a href="#cb2-1083" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Adds <span class="in">`SEX`</span> and <span class="in">`CHOL`</span> predictors</span>
<span id="cb2-1084"><a href="#cb2-1084" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Model 3**: <span class="in">`SBP ~ FRW + SEX + CHOL + FRW:SEX`</span> (with interaction/cross term)</span>
<span id="cb2-1085"><a href="#cb2-1085" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="in">`FRW:SEX`</span> is an **interaction term** - that is the product FRW × SEX (weight × 0/1 for female/male)</span>
<span id="cb2-1086"><a href="#cb2-1086" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>This tests: "Does weight affect blood pressure differently for males vs females?"</span>
<span id="cb2-1087"><a href="#cb2-1087" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>For females (<span class="in">`SEX = 0`</span>): Effect of weight = $\beta_{\text{FRW}}$</span>
<span id="cb2-1088"><a href="#cb2-1088" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>For males (<span class="in">`SEX = 1`</span>): Effect of weight = $\beta_{\text{FRW}} + \beta_{\text{FRW:SEX}}$</span>
<span id="cb2-1089"><a href="#cb2-1089" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>If $\beta_{\text{FRW:SEX}} &gt; 0$: weight increases BP more for males than females</span>
<span id="cb2-1090"><a href="#cb2-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1091"><a href="#cb2-1091" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb2-1092"><a href="#cb2-1092" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: From Simple to Multiple Regression</span></span>
<span id="cb2-1093"><a href="#cb2-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1094"><a href="#cb2-1094" aria-hidden="true" tabindex="-1"></a>Let's build on our simple regression model by adding sex and cholesterol as predictors:</span>
<span id="cb2-1095"><a href="#cb2-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1096"><a href="#cb2-1096" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-1097"><a href="#cb2-1097" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-1098"><a href="#cb2-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1101"><a href="#cb2-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-1102"><a href="#cb2-1102" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-1103"><a href="#cb2-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1104"><a href="#cb2-1104" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the same Framingham data</span></span>
<span id="cb2-1105"><a href="#cb2-1105" aria-hidden="true" tabindex="-1"></a>fram <span class="op">=</span> pd.read_csv(<span class="st">'../data/fram.txt'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-1106"><a href="#cb2-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1107"><a href="#cb2-1107" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 1: Simple regression (for comparison)</span></span>
<span id="cb2-1108"><a href="#cb2-1108" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW'</span>, data<span class="op">=</span>fram).fit()</span>
<span id="cb2-1109"><a href="#cb2-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1110"><a href="#cb2-1110" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 2: Multiple regression</span></span>
<span id="cb2-1111"><a href="#cb2-1111" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW + SEX + CHOL'</span>, data<span class="op">=</span>fram).fit()</span>
<span id="cb2-1112"><a href="#cb2-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1113"><a href="#cb2-1113" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 3: With interaction (does weight affect BP differently for males/females?)</span></span>
<span id="cb2-1114"><a href="#cb2-1114" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> smf.ols(<span class="st">'SBP ~ FRW + SEX + CHOL + FRW:SEX'</span>, data<span class="op">=</span>fram).fit()</span>
<span id="cb2-1115"><a href="#cb2-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1116"><a href="#cb2-1116" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Simple Regression (Model 1):"</span>)</span>
<span id="cb2-1117"><a href="#cb2-1117" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  FRW coefficient: </span><span class="sc">{</span>model1<span class="sc">.</span>params[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss"> (SE=</span><span class="sc">{</span>model1<span class="sc">.</span>bse[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb2-1118"><a href="#cb2-1118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  R-squared: </span><span class="sc">{</span>model1<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb2-1119"><a href="#cb2-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1120"><a href="#cb2-1120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Multiple Regression (Model 2):"</span>)</span>
<span id="cb2-1121"><a href="#cb2-1121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  FRW coefficient: </span><span class="sc">{</span>model2<span class="sc">.</span>params[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss"> (SE=</span><span class="sc">{</span>model2<span class="sc">.</span>bse[<span class="st">'FRW'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb2-1122"><a href="#cb2-1122" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  SEX[T.male] coefficient: </span><span class="sc">{</span>model2<span class="sc">.</span>params[<span class="st">'SEX[T.male]'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb2-1123"><a href="#cb2-1123" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  CHOL coefficient: </span><span class="sc">{</span>model2<span class="sc">.</span>params[<span class="st">'CHOL'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb2-1124"><a href="#cb2-1124" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  R-squared: </span><span class="sc">{</span>model2<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb2-1125"><a href="#cb2-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1126"><a href="#cb2-1126" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">With Interaction (Model 3):"</span>)</span>
<span id="cb2-1127"><a href="#cb2-1127" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'FRW:SEX[T.male]'</span> <span class="kw">in</span> model3.params:</span>
<span id="cb2-1128"><a href="#cb2-1128" aria-hidden="true" tabindex="-1"></a>    interaction_coef <span class="op">=</span> model3.params[<span class="st">'FRW:SEX[T.male]'</span>]</span>
<span id="cb2-1129"><a href="#cb2-1129" aria-hidden="true" tabindex="-1"></a>    interaction_pval <span class="op">=</span> model3.pvalues[<span class="st">'FRW:SEX[T.male]'</span>]</span>
<span id="cb2-1130"><a href="#cb2-1130" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  FRW:SEX interaction: </span><span class="sc">{</span>interaction_coef<span class="sc">:.3f}</span><span class="ss"> (p=</span><span class="sc">{</span>interaction_pval<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb2-1131"><a href="#cb2-1131" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> interaction_pval <span class="op">&gt;</span> <span class="fl">0.05</span>:</span>
<span id="cb2-1132"><a href="#cb2-1132" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Interpretation: No significant interaction - weight affects BP similarly for both sexes"</span>)</span>
<span id="cb2-1133"><a href="#cb2-1133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-1134"><a href="#cb2-1134" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Interpretation: Weight has </span><span class="sc">{</span><span class="st">'stronger'</span> <span class="cf">if</span> interaction_coef <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'weaker'</span><span class="sc">}</span><span class="ss"> effect for males"</span>)</span>
<span id="cb2-1135"><a href="#cb2-1135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1136"><a href="#cb2-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1137"><a href="#cb2-1137" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-1138"><a href="#cb2-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1139"><a href="#cb2-1139" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-1140"><a href="#cb2-1140" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb2-1141"><a href="#cb2-1141" aria-hidden="true" tabindex="-1"></a>fram <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'../data/fram.txt'</span>, <span class="at">sep=</span><span class="st">'</span><span class="sc">\t</span><span class="st">'</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb2-1142"><a href="#cb2-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1143"><a href="#cb2-1143" aria-hidden="true" tabindex="-1"></a><span class="co"># Build models</span></span>
<span id="cb2-1144"><a href="#cb2-1144" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW, <span class="at">data =</span> fram)</span>
<span id="cb2-1145"><a href="#cb2-1145" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW <span class="sc">+</span> SEX <span class="sc">+</span> CHOL, <span class="at">data =</span> fram)</span>
<span id="cb2-1146"><a href="#cb2-1146" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(SBP <span class="sc">~</span> FRW <span class="sc">+</span> SEX <span class="sc">+</span> CHOL <span class="sc">+</span> FRW<span class="sc">:</span>SEX, <span class="at">data =</span> fram)</span>
<span id="cb2-1147"><a href="#cb2-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1148"><a href="#cb2-1148" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare coefficients</span></span>
<span id="cb2-1149"><a href="#cb2-1149" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Simple Regression (Model 1):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-1150"><a href="#cb2-1150" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  FRW coefficient: %.3f (SE=%.3f)</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-1151"><a href="#cb2-1151" aria-hidden="true" tabindex="-1"></a>            <span class="fu">coef</span>(model1)[<span class="st">"FRW"</span>], <span class="fu">summary</span>(model1)<span class="sc">$</span>coef[<span class="st">"FRW"</span>, <span class="st">"Std. Error"</span>]))</span>
<span id="cb2-1152"><a href="#cb2-1152" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  R-squared: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">summary</span>(model1)<span class="sc">$</span>r.squared))</span>
<span id="cb2-1153"><a href="#cb2-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1154"><a href="#cb2-1154" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Multiple Regression (Model 2):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-1155"><a href="#cb2-1155" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  FRW coefficient: %.3f (SE=%.3f)</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-1156"><a href="#cb2-1156" aria-hidden="true" tabindex="-1"></a>            <span class="fu">coef</span>(model2)[<span class="st">"FRW"</span>], <span class="fu">summary</span>(model2)<span class="sc">$</span>coef[<span class="st">"FRW"</span>, <span class="st">"Std. Error"</span>]))</span>
<span id="cb2-1157"><a href="#cb2-1157" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  SEXmale coefficient: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">coef</span>(model2)[<span class="st">"SEXmale"</span>]))</span>
<span id="cb2-1158"><a href="#cb2-1158" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  CHOL coefficient: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">coef</span>(model2)[<span class="st">"CHOL"</span>]))</span>
<span id="cb2-1159"><a href="#cb2-1159" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  R-squared: %.3f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">summary</span>(model2)<span class="sc">$</span>r.squared))</span>
<span id="cb2-1160"><a href="#cb2-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1161"><a href="#cb2-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">With Interaction (Model 3):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-1162"><a href="#cb2-1162" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(<span class="st">"FRW:SEXmale"</span> <span class="sc">%in%</span> <span class="fu">names</span>(<span class="fu">coef</span>(model3))) {</span>
<span id="cb2-1163"><a href="#cb2-1163" aria-hidden="true" tabindex="-1"></a>  interaction_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(model3)[<span class="st">"FRW:SEXmale"</span>]</span>
<span id="cb2-1164"><a href="#cb2-1164" aria-hidden="true" tabindex="-1"></a>  interaction_pval <span class="ot">&lt;-</span> <span class="fu">summary</span>(model3)<span class="sc">$</span>coef[<span class="st">"FRW:SEXmale"</span>, <span class="st">"Pr(&gt;|t|)"</span>]</span>
<span id="cb2-1165"><a href="#cb2-1165" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  FRW:SEX interaction: %.3f (p=%.3f)</span><span class="sc">\n</span><span class="st">"</span>, interaction_coef, interaction_pval))</span>
<span id="cb2-1166"><a href="#cb2-1166" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(interaction_pval <span class="sc">&gt;</span> <span class="fl">0.05</span>) {</span>
<span id="cb2-1167"><a href="#cb2-1167" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">"  Interpretation: No significant interaction - weight affects BP similarly for both sexes</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-1168"><a href="#cb2-1168" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb2-1169"><a href="#cb2-1169" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Interpretation: Weight has %s effect for males</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-1170"><a href="#cb2-1170" aria-hidden="true" tabindex="-1"></a>                <span class="fu">ifelse</span>(interaction_coef <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">"stronger"</span>, <span class="st">"weaker"</span>)))</span>
<span id="cb2-1171"><a href="#cb2-1171" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-1172"><a href="#cb2-1172" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-1173"><a href="#cb2-1173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1174"><a href="#cb2-1174" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1175"><a href="#cb2-1175" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1176"><a href="#cb2-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1177"><a href="#cb2-1177" aria-hidden="true" tabindex="-1"></a>**Key Observations from the Multiple Regression Results:**</span>
<span id="cb2-1178"><a href="#cb2-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1179"><a href="#cb2-1179" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Minimal coefficient change**: The FRW coefficient changes only slightly when adding other predictors (compare Model 1 vs Model 2 outputs above). This stability suggests weight has a robust relationship with blood pressure that isn't confounded by sex or cholesterol.</span>
<span id="cb2-1180"><a href="#cb2-1180" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Sex effect in this cohort**: In this Framingham cohort, the model shows males have somewhat lower blood pressure than females (see the SEX coefficient in Model 2 output). This pattern in the 1950s-60s data (mean age ≈ 52 years) may reflect post-menopausal effects in women.</span>
<span id="cb2-1181"><a href="#cb2-1181" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Modest R-squared improvement**: Adding sex and cholesterol only marginally improves $R^2$ (compare the R-squared values between Model 1 and Model 2). This teaches us that more predictors don't always mean much better predictions.</span>
<span id="cb2-1182"><a href="#cb2-1182" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**No meaningful interaction**: The interaction term in Model 3 is near zero (see FRW:SEX coefficient), suggesting weight affects blood pressure similarly for both sexes. Not all hypothesized interactions turn out to be important!</span>
<span id="cb2-1183"><a href="#cb2-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1184"><a href="#cb2-1184" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-1185"><a href="#cb2-1185" aria-hidden="true" tabindex="-1"></a><span class="fu">## Real Data Lessons</span></span>
<span id="cb2-1186"><a href="#cb2-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1187"><a href="#cb2-1187" aria-hidden="true" tabindex="-1"></a>These Framingham results illustrate important realities of data analysis:</span>
<span id="cb2-1188"><a href="#cb2-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1189"><a href="#cb2-1189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Effects can be counterintuitive**: Women having higher blood pressure in this 1950s-60s cohort (mean age 52) surprises many, but it's consistent across all age groups in the data. Historical context and demographics matter!</span>
<span id="cb2-1190"><a href="#cb2-1190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**More predictors ≠ much better fit**: Despite adding two predictors and an interaction, we only explained an additional 1.5% of variance.</span>
<span id="cb2-1191"><a href="#cb2-1191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Most interactions are null**: We often hypothesize interactions that don't materialize. That's fine -- testing and rejecting hypotheses is part of science.</span>
<span id="cb2-1192"><a href="#cb2-1192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Biological systems are complex**: Even our best model explains only 12.5% of blood pressure variation. The remaining 87.5% comes from genetics, lifestyle, measurement error, and countless other factors.</span>
<span id="cb2-1193"><a href="#cb2-1193" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1194"><a href="#cb2-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1195"><a href="#cb2-1195" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model Selection: Choosing the Right Predictors</span></span>
<span id="cb2-1196"><a href="#cb2-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1197"><a href="#cb2-1197" aria-hidden="true" tabindex="-1"></a>With many potential predictors, a critical question arises: which ones should we include? Including too few predictors (**underfitting**) leads to bias; including too many (**overfitting**) increases variance and reduces interpretability. **Model selection** seeks the sweet spot.</span>
<span id="cb2-1198"><a href="#cb2-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1199"><a href="#cb2-1199" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Core Problem</span></span>
<span id="cb2-1200"><a href="#cb2-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1201"><a href="#cb2-1201" aria-hidden="true" tabindex="-1"></a>When we have $k$ potential predictors, there are $2^k$ possible models (each predictor is either in or out). With just 10 predictors, that's 1,024 models; with 20 predictors, over a million! We need:</span>
<span id="cb2-1202"><a href="#cb2-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1203"><a href="#cb2-1203" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A way to score each model's quality</span>
<span id="cb2-1204"><a href="#cb2-1204" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>An efficient search strategy to find the best model</span>
<span id="cb2-1205"><a href="#cb2-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1206"><a href="#cb2-1206" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Scoring Models: The Bias-Variance Trade-off</span></span>
<span id="cb2-1207"><a href="#cb2-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1208"><a href="#cb2-1208" aria-hidden="true" tabindex="-1"></a>The fundamental challenge is that training error -- how well the model fits the data used to build it -- is a bad guide to how well it will predict new data. Complex models always fit training data better, but they may perform poorly on new data. This is a manifestation of the bias-variance tradeoff we studied in Chapter 3: as we add more predictors to a regression, bias decreases (better fit to the true relationship) but variance increases (more sensitivity to the particular sample). Too few predictors leads to **underfitting** (high bias), while too many leads to **overfitting** (high variance).</span>
<span id="cb2-1209"><a href="#cb2-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1210"><a href="#cb2-1210" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb2-1211"><a href="#cb2-1211" aria-hidden="true" tabindex="-1"></a>**Prediction Risk (Quadratic Loss)**</span>
<span id="cb2-1212"><a href="#cb2-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1213"><a href="#cb2-1213" aria-hidden="true" tabindex="-1"></a>For a model $S$ with predictors $\mathcal{X}_S$, the prediction risk under quadratic loss is:</span>
<span id="cb2-1214"><a href="#cb2-1214" aria-hidden="true" tabindex="-1"></a>$$R(S) = \sum_{i=1}^{n} \mathbb{E}<span class="co">[</span><span class="ot">(\hat{Y}_i(S) - Y_i^*)^2</span><span class="co">]</span>$$</span>
<span id="cb2-1215"><a href="#cb2-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1216"><a href="#cb2-1216" aria-hidden="true" tabindex="-1"></a>where $Y_i^*$ is a future observation at covariate value $X_i$, and $\hat{Y}_i(S)$ is the prediction from model $S$.</span>
<span id="cb2-1217"><a href="#cb2-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1218"><a href="#cb2-1218" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1219"><a href="#cb2-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1220"><a href="#cb2-1220" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-1221"><a href="#cb2-1221" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Squared Error Loss?</span></span>
<span id="cb2-1222"><a href="#cb2-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1223"><a href="#cb2-1223" aria-hidden="true" tabindex="-1"></a>We use quadratic loss throughout model selection because:</span>
<span id="cb2-1224"><a href="#cb2-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1225"><a href="#cb2-1225" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>It matches our least squares estimation method (consistency across model fitting and model evaluation)</span>
<span id="cb2-1226"><a href="#cb2-1226" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>It leads to tractable bias-variance decompositions  </span>
<span id="cb2-1227"><a href="#cb2-1227" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>It penalizes large errors more than small ones (often desirable in practice)</span>
<span id="cb2-1228"><a href="#cb2-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1229"><a href="#cb2-1229" aria-hidden="true" tabindex="-1"></a>Other loss functions (absolute error, 0-1 loss) are valid but lead to different optimal models.</span>
<span id="cb2-1230"><a href="#cb2-1230" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1231"><a href="#cb2-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1232"><a href="#cb2-1232" aria-hidden="true" tabindex="-1"></a>Since we can't directly compute prediction risk (we don't have future data!), we need estimates. Many model selection criteria follow a similar form which we want to maximize:</span>
<span id="cb2-1233"><a href="#cb2-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1234"><a href="#cb2-1234" aria-hidden="true" tabindex="-1"></a>$$\text{Model Score} = \underbrace{\text{Goodness of Fit}}_{\text{how well model fits data}} - \underbrace{\text{Complexity Penalty}}_{\text{penalty for too many parameters}}$$</span>
<span id="cb2-1235"><a href="#cb2-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1236"><a href="#cb2-1236" aria-hidden="true" tabindex="-1"></a>Equivalently, some model selection metrics aim to minimize:</span>
<span id="cb2-1237"><a href="#cb2-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1238"><a href="#cb2-1238" aria-hidden="true" tabindex="-1"></a>$$\text{Model Score Loss} = \text{Training Error} + \text{Complexity Penalty}$$</span>
<span id="cb2-1239"><a href="#cb2-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1240"><a href="#cb2-1240" aria-hidden="true" tabindex="-1"></a>This fundamental trade-off appears in different guises across the methods we'll examine. The key insight is that we must balance how well we fit the current data against the danger of overfitting.</span>
<span id="cb2-1241"><a href="#cb2-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1242"><a href="#cb2-1242" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-1243"><a href="#cb2-1243" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mallow's Cp</span></span>
<span id="cb2-1244"><a href="#cb2-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1245"><a href="#cb2-1245" aria-hidden="true" tabindex="-1"></a>**Mallow's $C_p$ Statistic** provides an estimate of prediction risk:</span>
<span id="cb2-1246"><a href="#cb2-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1247"><a href="#cb2-1247" aria-hidden="true" tabindex="-1"></a>$$\hat{R}(S) = \text{RSS}(S) + 2|S|\hat{\sigma}^2$$</span>
<span id="cb2-1248"><a href="#cb2-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1249"><a href="#cb2-1249" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb2-1250"><a href="#cb2-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1251"><a href="#cb2-1251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{RSS}(S)$ = residual sum of squares (training error)</span>
<span id="cb2-1252"><a href="#cb2-1252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$|S|$ = number of parameters in model $S$</span>
<span id="cb2-1253"><a href="#cb2-1253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\sigma}^2$ = error variance estimate from the full model</span>
<span id="cb2-1254"><a href="#cb2-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1255"><a href="#cb2-1255" aria-hidden="true" tabindex="-1"></a>**Interpretation**: The first term measures lack of fit, the second penalizes complexity. Named after statistician <span class="co">[</span><span class="ot">Colin Mallows</span><span class="co">](https://en.wikipedia.org/wiki/Colin_Lingwood_Mallows)</span> who developed it.</span>
<span id="cb2-1256"><a href="#cb2-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1257"><a href="#cb2-1257" aria-hidden="true" tabindex="-1"></a>**When to use**: Linear regression with normal errors when you want an unbiased estimate of prediction risk.</span>
<span id="cb2-1258"><a href="#cb2-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1259"><a href="#cb2-1259" aria-hidden="true" tabindex="-1"></a><span class="fu">## AIC</span></span>
<span id="cb2-1260"><a href="#cb2-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1261"><a href="#cb2-1261" aria-hidden="true" tabindex="-1"></a>**AIC (Akaike Information Criterion)** takes an information-theoretic approach. The standard definition used in statistical software is:</span>
<span id="cb2-1262"><a href="#cb2-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1263"><a href="#cb2-1263" aria-hidden="true" tabindex="-1"></a>$$\text{AIC}(S) = -2\ell_S + 2|S|$$</span>
<span id="cb2-1264"><a href="#cb2-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1265"><a href="#cb2-1265" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb2-1266"><a href="#cb2-1266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\ell_S$ is the log-likelihood at the MLE</span>
<span id="cb2-1267"><a href="#cb2-1267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$|S|$ is the number of parameters in model $S$ (including the intercept)</span>
<span id="cb2-1268"><a href="#cb2-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1269"><a href="#cb2-1269" aria-hidden="true" tabindex="-1"></a>We **minimize** AIC to select the best model. Conceptually, this is equivalent to maximizing "goodness of fit minus complexity penalty" since minimizing $-2\ell_S + 2|S|$ is the same as maximizing $\ell_S - |S|$.</span>
<span id="cb2-1270"><a href="#cb2-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1271"><a href="#cb2-1271" aria-hidden="true" tabindex="-1"></a>**Key insight**: For linear regression with normal errors, AIC is equivalent to Mallow's $C_p$ (they select the same model).</span>
<span id="cb2-1272"><a href="#cb2-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1273"><a href="#cb2-1273" aria-hidden="true" tabindex="-1"></a>**Philosophy**: AIC was developed by statistician <span class="co">[</span><span class="ot">Hirotugu Akaike</span><span class="co">](https://en.wikipedia.org/wiki/Hirotugu_Akaike)</span> to approximate the Kullback-Leibler divergence between the true and fitted models. It aims to minimize prediction error, not find the "true" model -- recognizing that the true model might be too complex to express mathematically.</span>
<span id="cb2-1274"><a href="#cb2-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1275"><a href="#cb2-1275" aria-hidden="true" tabindex="-1"></a>**When to use**: When prediction accuracy is the primary goal and you believe the true model may be complex.</span>
<span id="cb2-1276"><a href="#cb2-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1277"><a href="#cb2-1277" aria-hidden="true" tabindex="-1"></a><span class="fu">## BIC</span></span>
<span id="cb2-1278"><a href="#cb2-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1279"><a href="#cb2-1279" aria-hidden="true" tabindex="-1"></a>**BIC (Bayesian Information Criterion)** adds a stronger complexity penalty. The standard definition is:</span>
<span id="cb2-1280"><a href="#cb2-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1281"><a href="#cb2-1281" aria-hidden="true" tabindex="-1"></a>$$\text{BIC}(S) = -2\ell_S + |S|\log n$$</span>
<span id="cb2-1282"><a href="#cb2-1282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1283"><a href="#cb2-1283" aria-hidden="true" tabindex="-1"></a>where $n$ is the sample size. We **minimize** BIC to select the best model.</span>
<span id="cb2-1284"><a href="#cb2-1284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1285"><a href="#cb2-1285" aria-hidden="true" tabindex="-1"></a>Note that the penalty $|S|\log n$ grows with sample size, making BIC more conservative than AIC (which has a fixed penalty of $2|S|$). For $n &gt; e^2 \approx 7.4$, BIC penalizes complexity more heavily than AIC.</span>
<span id="cb2-1286"><a href="#cb2-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1287"><a href="#cb2-1287" aria-hidden="true" tabindex="-1"></a>**Philosophy**: BIC, developed by statistician Gideon E. Schwarz, has a Bayesian interpretation -- it approximates the log posterior probability of the model. As $n \to \infty$, BIC selects the true model with probability 1 (consistency), *if the true model belongs to the candidate model set*.</span>
<span id="cb2-1288"><a href="#cb2-1288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1289"><a href="#cb2-1289" aria-hidden="true" tabindex="-1"></a>**Key difference from AIC**: Stronger penalty leads to simpler models. BIC assumes a true, relatively simple model exists among the candidates.</span>
<span id="cb2-1290"><a href="#cb2-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1291"><a href="#cb2-1291" aria-hidden="true" tabindex="-1"></a>**When to use**: When you believe a relatively simple true model exists and want consistency.</span>
<span id="cb2-1292"><a href="#cb2-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1293"><a href="#cb2-1293" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cross-Validation</span></span>
<span id="cb2-1294"><a href="#cb2-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1295"><a href="#cb2-1295" aria-hidden="true" tabindex="-1"></a>Alternatively, we can approximate prediction error by training our model on a **subset** of the data, and testing on the remaining (held-out) set.</span>
<span id="cb2-1296"><a href="#cb2-1296" aria-hidden="true" tabindex="-1"></a>Repeating this procedure multiple times for different partitions of the data is called **cross-validation** (CV).</span>
<span id="cb2-1297"><a href="#cb2-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1298"><a href="#cb2-1298" aria-hidden="true" tabindex="-1"></a>**Leave-one-out Cross-Validation (LOO-CV)** directly estimates prediction error on one held-out data point at a time:</span>
<span id="cb2-1299"><a href="#cb2-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1300"><a href="#cb2-1300" aria-hidden="true" tabindex="-1"></a>$$\hat{R}_{CV}(S) = \sum_{i=1}^{n} (Y_i - \hat{Y}_{(i)})^2$$</span>
<span id="cb2-1301"><a href="#cb2-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1302"><a href="#cb2-1302" aria-hidden="true" tabindex="-1"></a>where $\hat{Y}_{(i)}$ is the prediction for observation $i$ from a model fit without observation $i$.</span>
<span id="cb2-1303"><a href="#cb2-1303" aria-hidden="true" tabindex="-1"></a>LOO-CV can be very expensive (requires refitting the model $n$ times!) but for linear models it can be computed efficiently.</span>
<span id="cb2-1304"><a href="#cb2-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1305"><a href="#cb2-1305" aria-hidden="true" tabindex="-1"></a>**$k$-fold CV**: Divide data into $k$ groups called "folds" (often $k=5$ or $k=10$), train on $k-1$ folds, test on the held-out fold, repeat and average. This only requires retraining the model $k$ times.</span>
<span id="cb2-1306"><a href="#cb2-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1307"><a href="#cb2-1307" aria-hidden="true" tabindex="-1"></a>**When to use**: When you want a direct, model-agnostic estimate of prediction performance. Essential for complex models where AIC/BIC aren't available. CV is a common evaluation technique in machine learning.</span>
<span id="cb2-1308"><a href="#cb2-1308" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1309"><a href="#cb2-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1310"><a href="#cb2-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1311"><a href="#cb2-1311" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Search Strategies</span></span>
<span id="cb2-1312"><a href="#cb2-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1313"><a href="#cb2-1313" aria-hidden="true" tabindex="-1"></a>Even with a scoring criterion, we can't check all $2^k$ models when $k$ is large. Common search strategies include:</span>
<span id="cb2-1314"><a href="#cb2-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1315"><a href="#cb2-1315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Forward Stepwise Selection**: Start with no predictors, add the best one at each step</span>
<span id="cb2-1316"><a href="#cb2-1316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Backward Stepwise Selection**: Start with all predictors, remove the worst one at each step</span>
<span id="cb2-1317"><a href="#cb2-1317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Best Subset Selection**: Check all models of each size (computationally intensive)</span>
<span id="cb2-1318"><a href="#cb2-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1319"><a href="#cb2-1319" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution}</span>
<span id="cb2-1320"><a href="#cb2-1320" aria-hidden="true" tabindex="-1"></a><span class="fu">## Greedy Search Limitations</span></span>
<span id="cb2-1321"><a href="#cb2-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1322"><a href="#cb2-1322" aria-hidden="true" tabindex="-1"></a>Stepwise methods are greedy algorithms -- they make locally optimal choices without considering the global picture. They may miss the best model. For example, two predictors might be useless alone but powerful together due to interaction effects.</span>
<span id="cb2-1323"><a href="#cb2-1323" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1324"><a href="#cb2-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1325"><a href="#cb2-1325" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Comparing Predictor Importance</span></span>
<span id="cb2-1326"><a href="#cb2-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1327"><a href="#cb2-1327" aria-hidden="true" tabindex="-1"></a>Once we've selected a model, we often want to know: which predictors have the most impact? The raw regression coefficients can be misleading because predictors are on different scales. A predictor measured in millimeters will have a much smaller coefficient than one measured in kilometers, even if they have the same actual importance.</span>
<span id="cb2-1328"><a href="#cb2-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1329"><a href="#cb2-1329" aria-hidden="true" tabindex="-1"></a>The solution is to **standardize predictors before comparing coefficients**. After standardization:</span>
<span id="cb2-1330"><a href="#cb2-1330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1331"><a href="#cb2-1331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>All predictors have mean 0 and the same spread</span>
<span id="cb2-1332"><a href="#cb2-1332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coefficients become directly comparable</span>
<span id="cb2-1333"><a href="#cb2-1333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The coefficient magnitude indicates relative importance</span>
<span id="cb2-1334"><a href="#cb2-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1335"><a href="#cb2-1335" aria-hidden="true" tabindex="-1"></a>**Gelman &amp; Hill's recommendation**: @gelman2007data recommend dividing continuous predictors by 2 standard deviations (not 1). This makes binary and continuous predictors more comparable, since a binary predictor's standard deviation is at most 0.5, so dividing by 2 SDs puts it on a similar scale.</span>
<span id="cb2-1336"><a href="#cb2-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1337"><a href="#cb2-1337" aria-hidden="true" tabindex="-1"></a>After standardization, predictors with larger coefficient magnitudes have stronger effects on the outcome (in standard deviation units). However, when predictors are correlated, standardized coefficients don't directly measure the unique variance explained by each predictor. For that, consider partial $R^2$ or other variance decomposition methods.</span>
<span id="cb2-1338"><a href="#cb2-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1339"><a href="#cb2-1339" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb2-1340"><a href="#cb2-1340" aria-hidden="true" tabindex="-1"></a><span class="fu">## Heuristics for Model Selection</span></span>
<span id="cb2-1341"><a href="#cb2-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1342"><a href="#cb2-1342" aria-hidden="true" tabindex="-1"></a>Beyond automated criteria, @gelman2007data suggest these practical guidelines:</span>
<span id="cb2-1343"><a href="#cb2-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1344"><a href="#cb2-1344" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Include predictors you expect to be important** based on subject knowledge</span>
<span id="cb2-1345"><a href="#cb2-1345" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Consider creating composite predictors**: Not all related variables need separate inclusion -- you can combine multiple covariates into meaningful composites (e.g., a socioeconomic index from income, education, and occupation)</span>
<span id="cb2-1346"><a href="#cb2-1346" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Add interaction terms for strong predictors**: When predictors have large effects, their interactions often matter too</span>
<span id="cb2-1347"><a href="#cb2-1347" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Use statistical significance and sign to guide decisions**:</span>
<span id="cb2-1348"><a href="#cb2-1348" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Significant with expected sign**: Keep these predictors</span>
<span id="cb2-1349"><a href="#cb2-1349" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Significant with unexpected sign**: Investigate further -- may indicate model misspecification, confounding, or data issues</span>
<span id="cb2-1350"><a href="#cb2-1350" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Non-significant with expected sign**: Often worth keeping if theoretically important</span>
<span id="cb2-1351"><a href="#cb2-1351" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Non-significant with unexpected sign**: Generally drop these</span>
<span id="cb2-1352"><a href="#cb2-1352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1353"><a href="#cb2-1353" aria-hidden="true" tabindex="-1"></a>Remember: statistical significance isn't everything. A predictor's theoretical importance and practical significance matter too.</span>
<span id="cb2-1354"><a href="#cb2-1354" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1355"><a href="#cb2-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1356"><a href="#cb2-1356" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Controlling for Background Variables</span></span>
<span id="cb2-1357"><a href="#cb2-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1358"><a href="#cb2-1358" aria-hidden="true" tabindex="-1"></a>Many studies **control** for background variables (age, sex, education, socioeconomic status, etc.). This simply means **including these variables as predictors** in the model to "remove their impact" on the relationship of interest.</span>
<span id="cb2-1359"><a href="#cb2-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1360"><a href="#cb2-1360" aria-hidden="true" tabindex="-1"></a>For example, in our <span class="co">[</span><span class="ot">earlier Framingham analysis</span><span class="co">](#multiple-regression-in-practice)</span>, Model 2 controlled for sex and cholesterol when examining the weight-blood pressure relationship:</span>
<span id="cb2-1361"><a href="#cb2-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1362"><a href="#cb2-1362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model 1: <span class="in">`SBP ~ FRW`</span> (simple regression with weight only)  </span>
<span id="cb2-1363"><a href="#cb2-1363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model 2: <span class="in">`SBP ~ FRW + SEX + CHOL`</span> (controlling for sex and cholesterol)</span>
<span id="cb2-1364"><a href="#cb2-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1365"><a href="#cb2-1365" aria-hidden="true" tabindex="-1"></a>The weight coefficient changed only slightly between models (see the outputs above), suggesting the relationship isn't confounded by these variables.</span>
<span id="cb2-1366"><a href="#cb2-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1367"><a href="#cb2-1367" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb2-1368"><a href="#cb2-1368" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limitation of Statistical Control</span></span>
<span id="cb2-1369"><a href="#cb2-1369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1370"><a href="#cb2-1370" aria-hidden="true" tabindex="-1"></a>Controlling only captures **linear effects** of the control variables. If age has a nonlinear effect on the outcome (e.g., quadratic), simply including age as a linear term won't fully control for it. Background variables can still affect inferences if their effects are nonlinear.</span>
<span id="cb2-1371"><a href="#cb2-1371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1372"><a href="#cb2-1372" aria-hidden="true" tabindex="-1"></a>Consider including polynomial terms or splines for control variables when you suspect nonlinear relationships.</span>
<span id="cb2-1373"><a href="#cb2-1373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1374"><a href="#cb2-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1375"><a href="#cb2-1375" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Assumptions and Diagnostics</span></span>
<span id="cb2-1376"><a href="#cb2-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1377"><a href="#cb2-1377" aria-hidden="true" tabindex="-1"></a>Linear regression makes strong assumptions. When violated, our inferences may be invalid.</span>
<span id="cb2-1378"><a href="#cb2-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1379"><a href="#cb2-1379" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-1380"><a href="#cb2-1380" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Five Assumptions of Linear Regression</span></span>
<span id="cb2-1381"><a href="#cb2-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1382"><a href="#cb2-1382" aria-hidden="true" tabindex="-1"></a>In decreasing order of importance <span class="co">[</span><span class="ot">per @gelman2007data</span><span class="co">]</span>:</span>
<span id="cb2-1383"><a href="#cb2-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1384"><a href="#cb2-1384" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Validity**: Are the data relevant to your research question? Does the outcome measure what you think it measures? Are all important predictors included? Missing key variables can invalidate all conclusions.</span>
<span id="cb2-1385"><a href="#cb2-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1386"><a href="#cb2-1386" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Additivity and Linearity**: The model assumes $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...$. If relationships are nonlinear, consider transformations ($\log$, square root), polynomial terms, or interaction terms.</span>
<span id="cb2-1387"><a href="#cb2-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1388"><a href="#cb2-1388" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Independence of Errors**: Each observation's error should be independent of others. Watch out for time series (temporal correlation), spatial data (geographic clustering), or grouped data (students within schools).</span>
<span id="cb2-1389"><a href="#cb2-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1390"><a href="#cb2-1390" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Equal Variance (Homoscedasticity)**: Error variance should be constant across all predictor values. Violation makes standard errors and confidence intervals unreliable.</span>
<span id="cb2-1391"><a href="#cb2-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1392"><a href="#cb2-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Normality of Errors**: Errors should follow a normal distribution. This is the least important -- with large samples, the Central Limit Theorem ensures valid inference even with non-normal errors. Outliers matter more than the exact distribution shape.</span>
<span id="cb2-1393"><a href="#cb2-1393" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1394"><a href="#cb2-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1395"><a href="#cb2-1395" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Checking Assumptions: Residual Plots</span></span>
<span id="cb2-1396"><a href="#cb2-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1397"><a href="#cb2-1397" aria-hidden="true" tabindex="-1"></a>Since the statistical assumptions of linear regression focus on the errors $\epsilon_i = Y_i - \hat{Y}_i$, visualizing the residuals provides our primary diagnostic tool. The most important plot is **residuals versus fitted values**, which can reveal multiple assumption violations at once.</span>
<span id="cb2-1398"><a href="#cb2-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1399"><a href="#cb2-1399" aria-hidden="true" tabindex="-1"></a>We demonstrated this in <span class="co">[</span><span class="ot">Step 6 of our Framingham analysis</span><span class="co">](#step-6-model-diagnostics)</span>, where we created two key diagnostic plots:</span>
<span id="cb2-1400"><a href="#cb2-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1401"><a href="#cb2-1401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residuals vs Fitted**: Reveals problems with linearity and constant variance assumptions.</span>
<span id="cb2-1402"><a href="#cb2-1402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Q-Q Plot**: Checks whether residuals follow a normal distribution.</span>
<span id="cb2-1403"><a href="#cb2-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1404"><a href="#cb2-1404" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-1405"><a href="#cb2-1405" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpreting Residual Patterns</span></span>
<span id="cb2-1406"><a href="#cb2-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1407"><a href="#cb2-1407" aria-hidden="true" tabindex="-1"></a>Good residuals look like **random noise** around zero -- no patterns, just scatter. Specific patterns reveal specific problems:</span>
<span id="cb2-1408"><a href="#cb2-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1409"><a href="#cb2-1409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Curved patterns** (U-shape, waves): Nonlinearity detected. The true relationship isn't straight. Try transformations or polynomial terms.</span>
<span id="cb2-1410"><a href="#cb2-1410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Funnel shape** (variance changes with fitted values): Heteroscedasticity. Errors have unequal variance. Consider log-transforming $Y$ or weighted least squares.</span>
<span id="cb2-1411"><a href="#cb2-1411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Outliers or extreme points**: Can dominate the entire regression. Check if they're data errors or reveal model limitations.</span>
<span id="cb2-1412"><a href="#cb2-1412" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1413"><a href="#cb2-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1414"><a href="#cb2-1414" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning collapse="true"}</span>
<span id="cb2-1415"><a href="#cb2-1415" aria-hidden="true" tabindex="-1"></a><span class="fu">## Correlation vs. Causation</span></span>
<span id="cb2-1416"><a href="#cb2-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1417"><a href="#cb2-1417" aria-hidden="true" tabindex="-1"></a>A significant regression coefficient does **not** imply causation! Regression finds associations, not causal relationships. For example, ice cream sales and swimming pool drownings are positively correlated (both increase in summer), but ice cream doesn't cause drowning.</span>
<span id="cb2-1418"><a href="#cb2-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1419"><a href="#cb2-1419" aria-hidden="true" tabindex="-1"></a>Establishing causation requires theoretical justification, temporal precedence, ruling out confounders, and ideally randomized experiments. We'll explore causal inference in detail in Chapter 11.</span>
<span id="cb2-1420"><a href="#cb2-1420" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1421"><a href="#cb2-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1422"><a href="#cb2-1422" aria-hidden="true" tabindex="-1"></a><span class="fu">## Logistic Regression</span></span>
<span id="cb2-1423"><a href="#cb2-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1424"><a href="#cb2-1424" aria-hidden="true" tabindex="-1"></a><span class="fu">### Modeling Binary Outcomes</span></span>
<span id="cb2-1425"><a href="#cb2-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1426"><a href="#cb2-1426" aria-hidden="true" tabindex="-1"></a>So far, we've assumed the response variable $Y$ is continuous. But what if $Y$ is binary? Consider:</span>
<span id="cb2-1427"><a href="#cb2-1427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1428"><a href="#cb2-1428" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Does a patient have the disease? (Yes/No)</span>
<span id="cb2-1429"><a href="#cb2-1429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Will a customer churn? (Yes/No)^<span class="co">[</span><span class="ot">Customer churn refers to when customers stop doing business with a company or cancel their subscription to a service. For example, a mobile phone customer "churns" when they switch to a different provider or cancel their contract. Predicting churn helps companies identify at-risk customers and take preventive action.</span><span class="co">]</span></span>
<span id="cb2-1430"><a href="#cb2-1430" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Did the email get clicked? (Yes/No)</span>
<span id="cb2-1431"><a href="#cb2-1431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Will a loan default? (Yes/No)</span>
<span id="cb2-1432"><a href="#cb2-1432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1433"><a href="#cb2-1433" aria-hidden="true" tabindex="-1"></a>For these binary outcomes, we need **logistic regression**.</span>
<span id="cb2-1434"><a href="#cb2-1434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1435"><a href="#cb2-1435" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Logistic Regression Model</span></span>
<span id="cb2-1436"><a href="#cb2-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1437"><a href="#cb2-1437" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb2-1438"><a href="#cb2-1438" aria-hidden="true" tabindex="-1"></a>**The Logistic Regression Model**</span>
<span id="cb2-1439"><a href="#cb2-1439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1440"><a href="#cb2-1440" aria-hidden="true" tabindex="-1"></a>For a binary outcome $Y_i \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ and predictors $X_i$, the logistic regression model specifies:</span>
<span id="cb2-1441"><a href="#cb2-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1442"><a href="#cb2-1442" aria-hidden="true" tabindex="-1"></a>$$p_i \equiv \mathbb{P}(Y_i = 1 \mid X_i) = \frac{e^{\beta_0 + \sum_{j=1}^k \beta_j X_{ij}}}{1 + e^{\beta_0 + \sum_{j=1}^k \beta_j X_{ij}}}$$</span>
<span id="cb2-1443"><a href="#cb2-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1444"><a href="#cb2-1444" aria-hidden="true" tabindex="-1"></a>Equivalently, using the **logit** (log-odds) transformation:</span>
<span id="cb2-1445"><a href="#cb2-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1446"><a href="#cb2-1446" aria-hidden="true" tabindex="-1"></a>$$\text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \sum_{j=1}^k \beta_j X_{ij}$$</span>
<span id="cb2-1447"><a href="#cb2-1447" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1448"><a href="#cb2-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1449"><a href="#cb2-1449" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-1450"><a href="#cb2-1450" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb2-1451"><a href="#cb2-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1452"><a href="#cb2-1452" aria-hidden="true" tabindex="-1"></a>**Where does this formula come from? Why "logistic" regression?**</span>
<span id="cb2-1453"><a href="#cb2-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1454"><a href="#cb2-1454" aria-hidden="true" tabindex="-1"></a>Point is, when $Y$ is binary, linear regression produces continuous predictions -- not the 0s and 1s we observe with binary data. </span>
<span id="cb2-1455"><a href="#cb2-1455" aria-hidden="true" tabindex="-1"></a>The natural approach is to model the probability $p = \mathbb{P}(Y = 1 \mid X)$ instead of $Y$. </span>
<span id="cb2-1456"><a href="#cb2-1456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1457"><a href="#cb2-1457" aria-hidden="true" tabindex="-1"></a>We could try to model the probability $p$ with a linear model such as $p = \beta_0 + \beta_1 X$. However, we would immediately hit two problems: </span>
<span id="cb2-1458"><a href="#cb2-1458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1459"><a href="#cb2-1459" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Linear functions are unbounded: when $X$ is large, $\beta_0 + \beta_1 X$ can exceed 1; when $X$ is small, it can fall below 0. Both give impossible "probabilities." </span>
<span id="cb2-1460"><a href="#cb2-1460" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Binary data strongly violates the homoscedasticity assumption -- a Bernoulli (i.e., binary) variable with probability $p$ has variance $p(1-p)$, which depends on $X$.</span>
<span id="cb2-1461"><a href="#cb2-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1462"><a href="#cb2-1462" aria-hidden="true" tabindex="-1"></a>The trick is that instead of modelling directly $p$, we model the **log-odds** (logarithm of the probability ratio), that is $\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$, which is an unbounded quantity that lives in $(-\infty, \infty)$.</span>
<span id="cb2-1463"><a href="#cb2-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1464"><a href="#cb2-1464" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">logistic function</span><span class="co">](https://en.wikipedia.org/wiki/Logistic_function)</span> is then the function that allows us to map the logit values back to $(0,1)$, ensuring valid probabilities regardless of predictor values.</span>
<span id="cb2-1465"><a href="#cb2-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1466"><a href="#cb2-1466" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb2-1467"><a href="#cb2-1467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1468"><a href="#cb2-1468" aria-hidden="true" tabindex="-1"></a>The logistic function emerges naturally from maximum likelihood with Bernoulli data. Since $Y_i \sim \text{Bernoulli}(p_i)$, the likelihood is:</span>
<span id="cb2-1469"><a href="#cb2-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1470"><a href="#cb2-1470" aria-hidden="true" tabindex="-1"></a>$$\mathcal{L} = \prod_{i=1}^n p_i^{Y_i}(1-p_i)^{1-Y_i}$$</span>
<span id="cb2-1471"><a href="#cb2-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1472"><a href="#cb2-1472" aria-hidden="true" tabindex="-1"></a>The log-likelihood becomes:</span>
<span id="cb2-1473"><a href="#cb2-1473" aria-hidden="true" tabindex="-1"></a>$$\ell = \sum_{i=1}^n \left<span class="co">[</span><span class="ot">Y_i \log p_i + (1-Y_i)\log(1-p_i)\right</span><span class="co">]</span>$$</span>
<span id="cb2-1474"><a href="#cb2-1474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1475"><a href="#cb2-1475" aria-hidden="true" tabindex="-1"></a>We need to connect $p_i$ to the predictors $X_i$. We could try various transformations, but the **logit** turns out to be special -- it's the "canonical link" for Bernoulli distributions, meaning it makes the mathematics particularly elegant. Specifically, if we set:</span>
<span id="cb2-1476"><a href="#cb2-1476" aria-hidden="true" tabindex="-1"></a>$$\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 X_i$$</span>
<span id="cb2-1477"><a href="#cb2-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1478"><a href="#cb2-1478" aria-hidden="true" tabindex="-1"></a>then the log-likelihood becomes **concave** in the parameters $\beta_0, \beta_1$. This is crucial: a concave function has a unique maximum, so we're guaranteed to find the best fit without worrying about local maxima. Solving the logit equation for $p_i$ gives us the logistic function.</span>
<span id="cb2-1479"><a href="#cb2-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1480"><a href="#cb2-1480" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb2-1481"><a href="#cb2-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1482"><a href="#cb2-1482" aria-hidden="true" tabindex="-1"></a>Let's visualize the logistic function to build intuition for how it transforms linear predictors into probabilities:</span>
<span id="cb2-1483"><a href="#cb2-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1486"><a href="#cb2-1486" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-1487"><a href="#cb2-1487" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-1488"><a href="#cb2-1488" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb2-1489"><a href="#cb2-1489" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show visualization code"</span></span>
<span id="cb2-1490"><a href="#cb2-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1491"><a href="#cb2-1491" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-1492"><a href="#cb2-1492" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-1493"><a href="#cb2-1493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1494"><a href="#cb2-1494" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">200</span>)</span>
<span id="cb2-1495"><a href="#cb2-1495" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb2-1496"><a href="#cb2-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1497"><a href="#cb2-1497" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb2-1498"><a href="#cb2-1498" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb2-1499"><a href="#cb2-1499" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-1500"><a href="#cb2-1500" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-1501"><a href="#cb2-1501" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Linear predictor: β₀ + β₁X'</span>)</span>
<span id="cb2-1502"><a href="#cb2-1502" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability: P(Y=1|X)'</span>)</span>
<span id="cb2-1503"><a href="#cb2-1503" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'The Logistic Function'</span>)</span>
<span id="cb2-1504"><a href="#cb2-1504" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-1505"><a href="#cb2-1505" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>)</span>
<span id="cb2-1506"><a href="#cb2-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1507"><a href="#cb2-1507" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark key point</span></span>
<span id="cb2-1508"><a href="#cb2-1508" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb2-1509"><a href="#cb2-1509" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'When β₀ + β₁X = 0,</span><span class="ch">\n</span><span class="st">probability = 0.5'</span>, </span>
<span id="cb2-1510"><a href="#cb2-1510" aria-hidden="true" tabindex="-1"></a>            xy<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), xytext<span class="op">=</span>(<span class="dv">2</span>, <span class="fl">0.3</span>),</span>
<span id="cb2-1511"><a href="#cb2-1511" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'red'</span>))</span>
<span id="cb2-1512"><a href="#cb2-1512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1513"><a href="#cb2-1513" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-1514"><a href="#cb2-1514" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-1515"><a href="#cb2-1515" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1516"><a href="#cb2-1516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1517"><a href="#cb2-1517" aria-hidden="true" tabindex="-1"></a>The S-shaped curve is the logistic function in action. Notice three critical features:</span>
<span id="cb2-1518"><a href="#cb2-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1519"><a href="#cb2-1519" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bounded output**: No matter how extreme the input (β₀ + β₁X), the output stays strictly between 0 and 1</span>
<span id="cb2-1520"><a href="#cb2-1520" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Decision boundary**: When the linear predictor equals 0, the probability equals 0.5 -- this is the natural decision threshold</span>
<span id="cb2-1521"><a href="#cb2-1521" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Smooth transitions**: Unlike a hard step function, the logistic provides gradual probability changes, reflecting uncertainty near the boundary</span>
<span id="cb2-1522"><a href="#cb2-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1523"><a href="#cb2-1523" aria-hidden="true" tabindex="-1"></a>This smooth mapping from $(-\infty, \infty) \to (0,1)$ is what makes logistic regression both mathematically tractable and practically interpretable.</span>
<span id="cb2-1524"><a href="#cb2-1524" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1525"><a href="#cb2-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1526"><a href="#cb2-1526" aria-hidden="true" tabindex="-1"></a>Unlike linear regression, logistic regression has no closed-form solution. The parameters are estimated via maximum likelihood using numerical optimization algorithms, as implemented in common statistical packages.</span>
<span id="cb2-1527"><a href="#cb2-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1528"><a href="#cb2-1528" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-1529"><a href="#cb2-1529" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpreting Coefficients: Odds Ratios</span></span>
<span id="cb2-1530"><a href="#cb2-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1531"><a href="#cb2-1531" aria-hidden="true" tabindex="-1"></a>In logistic regression, coefficients have a specific interpretation:</span>
<span id="cb2-1532"><a href="#cb2-1532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1533"><a href="#cb2-1533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_j$ is the change in **log-odds** for a one-unit increase in $X_j$, holding other variables constant</span>
<span id="cb2-1534"><a href="#cb2-1534" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$e^{\beta_j}$ is the **odds ratio**: the factor by which odds are multiplied for a one-unit increase in $X_j$</span>
<span id="cb2-1535"><a href="#cb2-1535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1536"><a href="#cb2-1536" aria-hidden="true" tabindex="-1"></a>For example, if $\beta_{\text{age}} = 0.05$, then:</span>
<span id="cb2-1537"><a href="#cb2-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1538"><a href="#cb2-1538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each additional year of age increases log-odds by 0.05</span>
<span id="cb2-1539"><a href="#cb2-1539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each additional year multiplies odds by $e^{0.05} \approx 1.051$ (5.1% increase)</span>
<span id="cb2-1540"><a href="#cb2-1540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1541"><a href="#cb2-1541" aria-hidden="true" tabindex="-1"></a>Remember: odds = $p/(1-p)$. If $p = 0.2$, odds = 0.25. If $p = 0.8$, odds = 4.</span>
<span id="cb2-1542"><a href="#cb2-1542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1543"><a href="#cb2-1543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1544"><a href="#cb2-1544" aria-hidden="true" tabindex="-1"></a><span class="fu">### Logistic Regression in Practice</span></span>
<span id="cb2-1545"><a href="#cb2-1545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1546"><a href="#cb2-1546" aria-hidden="true" tabindex="-1"></a>Let's apply logistic regression to the Framingham data to predict high blood pressure.</span>
<span id="cb2-1547"><a href="#cb2-1547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1548"><a href="#cb2-1548" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb2-1549"><a href="#cb2-1549" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Predicting High Blood Pressure</span></span>
<span id="cb2-1550"><a href="#cb2-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1551"><a href="#cb2-1551" aria-hidden="true" tabindex="-1"></a>We'll create a **binary outcome** for high blood pressure using the clinical definition: systolic blood pressure (<span class="in">`SBP`</span>) ≥ 140 mmHg or diastolic blood pressure (<span class="in">`DBP`</span>) ≥ 90 mmHg. This is the standard threshold used in medical practice.</span>
<span id="cb2-1552"><a href="#cb2-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1553"><a href="#cb2-1553" aria-hidden="true" tabindex="-1"></a>To model the probability of high blood pressure, we'll:</span>
<span id="cb2-1554"><a href="#cb2-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1555"><a href="#cb2-1555" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Standardize continuous predictors (weight, age, cholesterol) by dividing by 2 standard deviations -- this makes coefficients comparable across predictors.</span>
<span id="cb2-1556"><a href="#cb2-1556" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Fit logistic regression models, starting with a single predictor then adding multiple variables.</span>
<span id="cb2-1557"><a href="#cb2-1557" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Interpret the results through odds ratios.</span>
<span id="cb2-1558"><a href="#cb2-1558" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1559"><a href="#cb2-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1560"><a href="#cb2-1560" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-1561"><a href="#cb2-1561" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-1562"><a href="#cb2-1562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1565"><a href="#cb2-1565" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-1566"><a href="#cb2-1566" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-1567"><a href="#cb2-1567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1568"><a href="#cb2-1568" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-1569"><a href="#cb2-1569" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-1570"><a href="#cb2-1570" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb2-1571"><a href="#cb2-1571" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-1572"><a href="#cb2-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1573"><a href="#cb2-1573" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Framingham data</span></span>
<span id="cb2-1574"><a href="#cb2-1574" aria-hidden="true" tabindex="-1"></a>fram <span class="op">=</span> pd.read_csv(<span class="st">'../data/fram.txt'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-1575"><a href="#cb2-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1576"><a href="#cb2-1576" aria-hidden="true" tabindex="-1"></a><span class="co"># Define high blood pressure (standard clinical threshold)</span></span>
<span id="cb2-1577"><a href="#cb2-1577" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'HIGH_BP'</span>] <span class="op">=</span> ((fram[<span class="st">'SBP'</span>] <span class="op">&gt;=</span> <span class="dv">140</span>) <span class="op">|</span> (fram[<span class="st">'DBP'</span>] <span class="op">&gt;=</span> <span class="dv">90</span>)).astype(<span class="bu">int</span>)</span>
<span id="cb2-1578"><a href="#cb2-1578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1579"><a href="#cb2-1579" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prevalence of high BP: </span><span class="sc">{</span>fram[<span class="st">'HIGH_BP'</span>]<span class="sc">.</span>mean()<span class="sc">:.1%}</span><span class="ss"> (</span><span class="sc">{</span>fram[<span class="st">'HIGH_BP'</span>]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> of </span><span class="sc">{</span><span class="bu">len</span>(fram)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb2-1580"><a href="#cb2-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1581"><a href="#cb2-1581" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize predictors (following Gelman &amp; Hill's recommendation)</span></span>
<span id="cb2-1582"><a href="#cb2-1582" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividing by 2*SD makes binary and continuous predictors comparable</span></span>
<span id="cb2-1583"><a href="#cb2-1583" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardize(x):</span>
<span id="cb2-1584"><a href="#cb2-1584" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">-</span> x.mean()) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> x.std())</span>
<span id="cb2-1585"><a href="#cb2-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1586"><a href="#cb2-1586" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'sFRW'</span>] <span class="op">=</span> standardize(fram[<span class="st">'FRW'</span>])</span>
<span id="cb2-1587"><a href="#cb2-1587" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'sAGE'</span>] <span class="op">=</span> standardize(fram[<span class="st">'AGE'</span>])</span>
<span id="cb2-1588"><a href="#cb2-1588" aria-hidden="true" tabindex="-1"></a>fram[<span class="st">'sCHOL'</span>] <span class="op">=</span> standardize(fram[<span class="st">'CHOL'</span>])</span>
<span id="cb2-1589"><a href="#cb2-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1590"><a href="#cb2-1590" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a simple logistic regression with standardized weight</span></span>
<span id="cb2-1591"><a href="#cb2-1591" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.logit(<span class="st">'HIGH_BP ~ sFRW'</span>, data<span class="op">=</span>fram).fit(disp<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-1592"><a href="#cb2-1592" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-1593"><a href="#cb2-1593" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression: HIGH_BP ~ sFRW"</span>)</span>
<span id="cb2-1594"><a href="#cb2-1594" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-1595"><a href="#cb2-1595" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.summary2().tables[<span class="dv">1</span>])</span>
<span id="cb2-1596"><a href="#cb2-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1597"><a href="#cb2-1597" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fitted model</span></span>
<span id="cb2-1598"><a href="#cb2-1598" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb2-1599"><a href="#cb2-1599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1600"><a href="#cb2-1600" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points (with slight jitter for visibility)</span></span>
<span id="cb2-1601"><a href="#cb2-1601" aria-hidden="true" tabindex="-1"></a>y_jitter <span class="op">=</span> fram[<span class="st">'HIGH_BP'</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>, <span class="bu">len</span>(fram))</span>
<span id="cb2-1602"><a href="#cb2-1602" aria-hidden="true" tabindex="-1"></a>plt.scatter(fram[<span class="st">'sFRW'</span>], y_jitter, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-1603"><a href="#cb2-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1604"><a href="#cb2-1604" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the fitted probability curve</span></span>
<span id="cb2-1605"><a href="#cb2-1605" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(fram[<span class="st">'sFRW'</span>].<span class="bu">min</span>(), fram[<span class="st">'sFRW'</span>].<span class="bu">max</span>(), <span class="dv">200</span>)</span>
<span id="cb2-1606"><a href="#cb2-1606" aria-hidden="true" tabindex="-1"></a>X_pred <span class="op">=</span> pd.DataFrame({<span class="st">'sFRW'</span>: x_range})</span>
<span id="cb2-1607"><a href="#cb2-1607" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_pred)</span>
<span id="cb2-1608"><a href="#cb2-1608" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, y_pred, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted probability'</span>)</span>
<span id="cb2-1609"><a href="#cb2-1609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1610"><a href="#cb2-1610" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Standardized Weight (sFRW)'</span>)</span>
<span id="cb2-1611"><a href="#cb2-1611" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(High BP = 1)'</span>)</span>
<span id="cb2-1612"><a href="#cb2-1612" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Probability of High Blood Pressure vs Weight'</span>)</span>
<span id="cb2-1613"><a href="#cb2-1613" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>)</span>
<span id="cb2-1614"><a href="#cb2-1614" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-1615"><a href="#cb2-1615" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-1616"><a href="#cb2-1616" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-1617"><a href="#cb2-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1618"><a href="#cb2-1618" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpret the coefficient as odds ratio</span></span>
<span id="cb2-1619"><a href="#cb2-1619" aria-hidden="true" tabindex="-1"></a>beta_sfrw <span class="op">=</span> model.params[<span class="st">'sFRW'</span>]</span>
<span id="cb2-1620"><a href="#cb2-1620" aria-hidden="true" tabindex="-1"></a>or_sfrw <span class="op">=</span> np.exp(beta_sfrw)</span>
<span id="cb2-1621"><a href="#cb2-1621" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Coefficient for sFRW: </span><span class="sc">{</span>beta_sfrw<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-1622"><a href="#cb2-1622" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Odds ratio: </span><span class="sc">{</span>or_sfrw<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-1623"><a href="#cb2-1623" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Interpretation: A 2-SD increase in weight multiplies odds of high BP by </span><span class="sc">{</span>or_sfrw<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-1624"><a href="#cb2-1624" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1625"><a href="#cb2-1625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1626"><a href="#cb2-1626" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-1627"><a href="#cb2-1627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1628"><a href="#cb2-1628" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-1629"><a href="#cb2-1629" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)  <span class="co"># For rescale and invlogit functions</span></span>
<span id="cb2-1630"><a href="#cb2-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1631"><a href="#cb2-1631" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb2-1632"><a href="#cb2-1632" aria-hidden="true" tabindex="-1"></a>fram <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'../data/fram.txt'</span>, <span class="at">sep=</span><span class="st">'</span><span class="sc">\t</span><span class="st">'</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb2-1633"><a href="#cb2-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1634"><a href="#cb2-1634" aria-hidden="true" tabindex="-1"></a><span class="co"># Define high blood pressure</span></span>
<span id="cb2-1635"><a href="#cb2-1635" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>HIGH_BP <span class="ot">&lt;-</span> (fram<span class="sc">$</span>SBP <span class="sc">&gt;=</span> <span class="dv">140</span>) <span class="sc">|</span> (fram<span class="sc">$</span>DBP <span class="sc">&gt;=</span> <span class="dv">90</span>)</span>
<span id="cb2-1636"><a href="#cb2-1636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1637"><a href="#cb2-1637" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Prevalence of high BP: %.1f%% (%d of %d)</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb2-1638"><a href="#cb2-1638" aria-hidden="true" tabindex="-1"></a>            <span class="fu">mean</span>(fram<span class="sc">$</span>HIGH_BP)<span class="sc">*</span><span class="dv">100</span>, <span class="fu">sum</span>(fram<span class="sc">$</span>HIGH_BP), <span class="fu">nrow</span>(fram)))</span>
<span id="cb2-1639"><a href="#cb2-1639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1640"><a href="#cb2-1640" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize predictors (rescale divides by 2*SD as recommended by Gelman &amp; Hill)</span></span>
<span id="cb2-1641"><a href="#cb2-1641" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>sFRW <span class="ot">&lt;-</span> <span class="fu">rescale</span>(fram<span class="sc">$</span>FRW)</span>
<span id="cb2-1642"><a href="#cb2-1642" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>sAGE <span class="ot">&lt;-</span> <span class="fu">rescale</span>(fram<span class="sc">$</span>AGE)</span>
<span id="cb2-1643"><a href="#cb2-1643" aria-hidden="true" tabindex="-1"></a>fram<span class="sc">$</span>sCHOL <span class="ot">&lt;-</span> <span class="fu">rescale</span>(fram<span class="sc">$</span>CHOL)</span>
<span id="cb2-1644"><a href="#cb2-1644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1645"><a href="#cb2-1645" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit logistic regression with standardized weight</span></span>
<span id="cb2-1646"><a href="#cb2-1646" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(HIGH_BP <span class="sc">~</span> sFRW, <span class="at">data =</span> fram, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">'logit'</span>))</span>
<span id="cb2-1647"><a href="#cb2-1647" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb2-1648"><a href="#cb2-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1649"><a href="#cb2-1649" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fitted model</span></span>
<span id="cb2-1650"><a href="#cb2-1650" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fram<span class="sc">$</span>sFRW, <span class="fu">jitter</span>(<span class="fu">as.numeric</span>(fram<span class="sc">$</span>HIGH_BP), <span class="fl">0.05</span>), </span>
<span id="cb2-1651"><a href="#cb2-1651" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Standardized Weight (sFRW)"</span>, </span>
<span id="cb2-1652"><a href="#cb2-1652" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"P(High BP = 1)"</span>,</span>
<span id="cb2-1653"><a href="#cb2-1653" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Probability of High Blood Pressure vs Weight"</span>,</span>
<span id="cb2-1654"><a href="#cb2-1654" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>))</span>
<span id="cb2-1655"><a href="#cb2-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1656"><a href="#cb2-1656" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted curve</span></span>
<span id="cb2-1657"><a href="#cb2-1657" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">invlogit</span>(<span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>]<span class="sc">*</span>x), </span>
<span id="cb2-1658"><a href="#cb2-1658" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-1659"><a href="#cb2-1659" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="st">"Fitted probability"</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-1660"><a href="#cb2-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1661"><a href="#cb2-1661" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpret as odds ratio</span></span>
<span id="cb2-1662"><a href="#cb2-1662" aria-hidden="true" tabindex="-1"></a>beta_sfrw <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)[<span class="st">"sFRW"</span>]</span>
<span id="cb2-1663"><a href="#cb2-1663" aria-hidden="true" tabindex="-1"></a>or_sfrw <span class="ot">&lt;-</span> <span class="fu">exp</span>(beta_sfrw)</span>
<span id="cb2-1664"><a href="#cb2-1664" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Coefficient for sFRW: %.4f</span><span class="sc">\n</span><span class="st">"</span>, beta_sfrw))</span>
<span id="cb2-1665"><a href="#cb2-1665" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Odds ratio: %.4f</span><span class="sc">\n</span><span class="st">"</span>, or_sfrw))</span>
<span id="cb2-1666"><a href="#cb2-1666" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Interpretation: A 2-SD increase in weight multiplies odds of high BP by %.4f</span><span class="sc">\n</span><span class="st">"</span>, or_sfrw))</span>
<span id="cb2-1667"><a href="#cb2-1667" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1668"><a href="#cb2-1668" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1669"><a href="#cb2-1669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1670"><a href="#cb2-1670" aria-hidden="true" tabindex="-1"></a>The fitted logistic curve shows how the probability of high blood pressure increases with weight. Unlike linear regression, the relationship is nonlinear -- the effect of weight on probability is strongest in the middle range where probabilities are near 0.5.</span>
<span id="cb2-1671"><a href="#cb2-1671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1672"><a href="#cb2-1672" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adding Multiple Predictors</span></span>
<span id="cb2-1673"><a href="#cb2-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1674"><a href="#cb2-1674" aria-hidden="true" tabindex="-1"></a>Now let's include age and sex to improve our model:</span>
<span id="cb2-1675"><a href="#cb2-1675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1676"><a href="#cb2-1676" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-1677"><a href="#cb2-1677" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb2-1678"><a href="#cb2-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1681"><a href="#cb2-1681" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-1682"><a href="#cb2-1682" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-1683"><a href="#cb2-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1684"><a href="#cb2-1684" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with multiple predictors (using standardized variables)</span></span>
<span id="cb2-1685"><a href="#cb2-1685" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> smf.logit(<span class="st">'HIGH_BP ~ sFRW + sAGE + SEX'</span>, data<span class="op">=</span>fram).fit(disp<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-1686"><a href="#cb2-1686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1687"><a href="#cb2-1687" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-1688"><a href="#cb2-1688" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multiple Logistic Regression"</span>)</span>
<span id="cb2-1689"><a href="#cb2-1689" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-1690"><a href="#cb2-1690" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model2.summary2().tables[<span class="dv">1</span>])</span>
<span id="cb2-1691"><a href="#cb2-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1692"><a href="#cb2-1692" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and display odds ratios</span></span>
<span id="cb2-1693"><a href="#cb2-1693" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-1694"><a href="#cb2-1694" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ODDS RATIOS"</span>)</span>
<span id="cb2-1695"><a href="#cb2-1695" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-1696"><a href="#cb2-1696" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> model2.params.index:</span>
<span id="cb2-1697"><a href="#cb2-1697" aria-hidden="true" tabindex="-1"></a>    or_val <span class="op">=</span> np.exp(model2.params[var])</span>
<span id="cb2-1698"><a href="#cb2-1698" aria-hidden="true" tabindex="-1"></a>    ci <span class="op">=</span> model2.conf_int().loc[var]</span>
<span id="cb2-1699"><a href="#cb2-1699" aria-hidden="true" tabindex="-1"></a>    ci_low, ci_high <span class="op">=</span> np.exp(ci[<span class="dv">0</span>]), np.exp(ci[<span class="dv">1</span>])</span>
<span id="cb2-1700"><a href="#cb2-1700" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> var <span class="op">!=</span> <span class="st">'Intercept'</span>:</span>
<span id="cb2-1701"><a href="#cb2-1701" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>var<span class="sc">:10s}</span><span class="ss">: OR = </span><span class="sc">{</span>or_val<span class="sc">:5.3f}</span><span class="ss"> (95% CI: </span><span class="sc">{</span>ci_low<span class="sc">:5.3f}</span><span class="ss">-</span><span class="sc">{</span>ci_high<span class="sc">:5.3f}</span><span class="ss">)"</span>)</span>
<span id="cb2-1702"><a href="#cb2-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1703"><a href="#cb2-1703" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the effect of sex on the weight-BP relationship</span></span>
<span id="cb2-1704"><a href="#cb2-1704" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb2-1705"><a href="#cb2-1705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1706"><a href="#cb2-1706" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the actual data points with jitter, separated by sex</span></span>
<span id="cb2-1707"><a href="#cb2-1707" aria-hidden="true" tabindex="-1"></a>females <span class="op">=</span> fram[fram[<span class="st">'SEX'</span>] <span class="op">==</span> <span class="st">'female'</span>]</span>
<span id="cb2-1708"><a href="#cb2-1708" aria-hidden="true" tabindex="-1"></a>males <span class="op">=</span> fram[fram[<span class="st">'SEX'</span>] <span class="op">==</span> <span class="st">'male'</span>]</span>
<span id="cb2-1709"><a href="#cb2-1709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1710"><a href="#cb2-1710" aria-hidden="true" tabindex="-1"></a><span class="co"># Add jitter to binary outcome for visibility</span></span>
<span id="cb2-1711"><a href="#cb2-1711" aria-hidden="true" tabindex="-1"></a>jitter_f <span class="op">=</span> females[<span class="st">'HIGH_BP'</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>, <span class="bu">len</span>(females))</span>
<span id="cb2-1712"><a href="#cb2-1712" aria-hidden="true" tabindex="-1"></a>jitter_m <span class="op">=</span> males[<span class="st">'HIGH_BP'</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.02</span>, <span class="bu">len</span>(males))</span>
<span id="cb2-1713"><a href="#cb2-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1714"><a href="#cb2-1714" aria-hidden="true" tabindex="-1"></a>plt.scatter(females[<span class="st">'sFRW'</span>], jitter_f, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Female (data)'</span>)</span>
<span id="cb2-1715"><a href="#cb2-1715" aria-hidden="true" tabindex="-1"></a>plt.scatter(males[<span class="st">'sFRW'</span>], jitter_m, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Male (data)'</span>)</span>
<span id="cb2-1716"><a href="#cb2-1716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1717"><a href="#cb2-1717" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction data: vary weight, hold age at mean (0 for standardized)</span></span>
<span id="cb2-1718"><a href="#cb2-1718" aria-hidden="true" tabindex="-1"></a>weight_range <span class="op">=</span> np.linspace(fram[<span class="st">'sFRW'</span>].<span class="bu">min</span>(), fram[<span class="st">'sFRW'</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb2-1719"><a href="#cb2-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1720"><a href="#cb2-1720" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions for females</span></span>
<span id="cb2-1721"><a href="#cb2-1721" aria-hidden="true" tabindex="-1"></a>pred_data_f <span class="op">=</span> pd.DataFrame({<span class="st">'sFRW'</span>: weight_range, <span class="st">'sAGE'</span>: <span class="dv">0</span>, <span class="st">'SEX'</span>: <span class="st">'female'</span>})</span>
<span id="cb2-1722"><a href="#cb2-1722" aria-hidden="true" tabindex="-1"></a>pred_f <span class="op">=</span> model2.predict(pred_data_f)</span>
<span id="cb2-1723"><a href="#cb2-1723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1724"><a href="#cb2-1724" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions for males  </span></span>
<span id="cb2-1725"><a href="#cb2-1725" aria-hidden="true" tabindex="-1"></a>pred_data_m <span class="op">=</span> pd.DataFrame({<span class="st">'sFRW'</span>: weight_range, <span class="st">'sAGE'</span>: <span class="dv">0</span>, <span class="st">'SEX'</span>: <span class="st">'male'</span>})</span>
<span id="cb2-1726"><a href="#cb2-1726" aria-hidden="true" tabindex="-1"></a>pred_m <span class="op">=</span> model2.predict(pred_data_m)</span>
<span id="cb2-1727"><a href="#cb2-1727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1728"><a href="#cb2-1728" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the fitted curves</span></span>
<span id="cb2-1729"><a href="#cb2-1729" aria-hidden="true" tabindex="-1"></a>plt.plot(weight_range, pred_f, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Female (fitted)'</span>)</span>
<span id="cb2-1730"><a href="#cb2-1730" aria-hidden="true" tabindex="-1"></a>plt.plot(weight_range, pred_m, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Male (fitted)'</span>)</span>
<span id="cb2-1731"><a href="#cb2-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1732"><a href="#cb2-1732" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Standardized Weight (sFRW)'</span>)</span>
<span id="cb2-1733"><a href="#cb2-1733" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(High BP = 1)'</span>)</span>
<span id="cb2-1734"><a href="#cb2-1734" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Probability of High BP by Weight and Sex</span><span class="ch">\n</span><span class="st">(Age held at mean)'</span>)</span>
<span id="cb2-1735"><a href="#cb2-1735" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-1736"><a href="#cb2-1736" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-1737"><a href="#cb2-1737" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb2-1738"><a href="#cb2-1738" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-1739"><a href="#cb2-1739" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1740"><a href="#cb2-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1741"><a href="#cb2-1741" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-1742"><a href="#cb2-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1743"><a href="#cb2-1743" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-1744"><a href="#cb2-1744" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with multiple predictors (using standardized variables)</span></span>
<span id="cb2-1745"><a href="#cb2-1745" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(HIGH_BP <span class="sc">~</span> sFRW <span class="sc">+</span> sAGE <span class="sc">+</span> SEX, <span class="at">data =</span> fram, </span>
<span id="cb2-1746"><a href="#cb2-1746" aria-hidden="true" tabindex="-1"></a>            <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">'logit'</span>))</span>
<span id="cb2-1747"><a href="#cb2-1747" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span>
<span id="cb2-1748"><a href="#cb2-1748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1749"><a href="#cb2-1749" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate odds ratios with confidence intervals</span></span>
<span id="cb2-1750"><a href="#cb2-1750" aria-hidden="true" tabindex="-1"></a>or_table <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(fit2), <span class="fu">confint</span>(fit2)))</span>
<span id="cb2-1751"><a href="#cb2-1751" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(or_table)</span>
<span id="cb2-1752"><a href="#cb2-1752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1753"><a href="#cb2-1753" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the effect of sex on the weight-BP relationship</span></span>
<span id="cb2-1754"><a href="#cb2-1754" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the actual data points with jitter, separated by sex</span></span>
<span id="cb2-1755"><a href="#cb2-1755" aria-hidden="true" tabindex="-1"></a>females <span class="ot">&lt;-</span> fram[fram<span class="sc">$</span>SEX <span class="sc">==</span> <span class="st">"female"</span>, ]</span>
<span id="cb2-1756"><a href="#cb2-1756" aria-hidden="true" tabindex="-1"></a>males <span class="ot">&lt;-</span> fram[fram<span class="sc">$</span>SEX <span class="sc">==</span> <span class="st">"male"</span>, ]</span>
<span id="cb2-1757"><a href="#cb2-1757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1758"><a href="#cb2-1758" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(females<span class="sc">$</span>sFRW, <span class="fu">jitter</span>(<span class="fu">as.numeric</span>(females<span class="sc">$</span>HIGH_BP), <span class="fl">0.05</span>), </span>
<span id="cb2-1759"><a href="#cb2-1759" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>), <span class="at">pch =</span> <span class="dv">16</span>, </span>
<span id="cb2-1760"><a href="#cb2-1760" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Standardized Weight (sFRW)"</span>, </span>
<span id="cb2-1761"><a href="#cb2-1761" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"P(High BP = 1)"</span>,</span>
<span id="cb2-1762"><a href="#cb2-1762" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Probability of High BP by Weight and Sex</span><span class="sc">\n</span><span class="st">(Age held at mean)"</span>,</span>
<span id="cb2-1763"><a href="#cb2-1763" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>))</span>
<span id="cb2-1764"><a href="#cb2-1764" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(males<span class="sc">$</span>sFRW, <span class="fu">jitter</span>(<span class="fu">as.numeric</span>(males<span class="sc">$</span>HIGH_BP), <span class="fl">0.05</span>), </span>
<span id="cb2-1765"><a href="#cb2-1765" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.3</span>), <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb2-1766"><a href="#cb2-1766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1767"><a href="#cb2-1767" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted curves for females and males (age at mean = 0 for standardized)</span></span>
<span id="cb2-1768"><a href="#cb2-1768" aria-hidden="true" tabindex="-1"></a><span class="co"># Female curve</span></span>
<span id="cb2-1769"><a href="#cb2-1769" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">invlogit</span>(<span class="fu">coef</span>(fit2)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sFRW"</span>]<span class="sc">*</span>x <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sAGE"</span>]<span class="sc">*</span><span class="dv">0</span>), </span>
<span id="cb2-1770"><a href="#cb2-1770" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-1771"><a href="#cb2-1771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1772"><a href="#cb2-1772" aria-hidden="true" tabindex="-1"></a><span class="co"># Male curve  </span></span>
<span id="cb2-1773"><a href="#cb2-1773" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">invlogit</span>(<span class="fu">coef</span>(fit2)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"SEXmale"</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sFRW"</span>]<span class="sc">*</span>x <span class="sc">+</span> <span class="fu">coef</span>(fit2)[<span class="st">"sAGE"</span>]<span class="sc">*</span><span class="dv">0</span>),</span>
<span id="cb2-1774"><a href="#cb2-1774" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-1775"><a href="#cb2-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1776"><a href="#cb2-1776" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb2-1777"><a href="#cb2-1777" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">"Female (data)"</span>, <span class="st">"Male (data)"</span>, <span class="st">"Female (fitted)"</span>, <span class="st">"Male (fitted)"</span>), </span>
<span id="cb2-1778"><a href="#cb2-1778" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>), <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.5</span>), <span class="st">"red"</span>, <span class="st">"blue"</span>), </span>
<span id="cb2-1779"><a href="#cb2-1779" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="cn">NA</span>, <span class="cn">NA</span>),</span>
<span id="cb2-1780"><a href="#cb2-1780" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fl">2.5</span>, <span class="fl">2.5</span>))</span>
<span id="cb2-1781"><a href="#cb2-1781" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb2-1782"><a href="#cb2-1782" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1783"><a href="#cb2-1783" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1784"><a href="#cb2-1784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1785"><a href="#cb2-1785" aria-hidden="true" tabindex="-1"></a>**Interpretation of Results:**</span>
<span id="cb2-1786"><a href="#cb2-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1787"><a href="#cb2-1787" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**sFRW (Weight)**: A 2-SD increase in weight multiplies the odds of high BP by 3.17 (95% CI: 2.41-4.18) -- a very strong effect</span>
<span id="cb2-1788"><a href="#cb2-1788" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**sAGE (Age)**: A 2-SD increase in age multiplies the odds of high BP by 1.45 (95% CI: 1.16-1.83) -- significant but weaker than weight</span>
<span id="cb2-1789"><a href="#cb2-1789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SEX**: Being male decreases the odds by about 18% (OR = 0.82), but this is not statistically significant (95% CI: 0.65-1.03 includes 1)</span>
<span id="cb2-1790"><a href="#cb2-1790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1791"><a href="#cb2-1791" aria-hidden="true" tabindex="-1"></a>The visualization shows how the probability curves are parallel on the logit scale -- males have consistently lower probability across all weight values (the blue curve sits below the red curve). The standardization allows direct comparison: weight has the strongest association with high blood pressure in this model.</span>
<span id="cb2-1792"><a href="#cb2-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1793"><a href="#cb2-1793" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-1794"><a href="#cb2-1794" aria-hidden="true" tabindex="-1"></a><span class="fu">## Remarks About Logistic Regression</span></span>
<span id="cb2-1795"><a href="#cb2-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1796"><a href="#cb2-1796" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**No R-squared**: The usual $R^2$ doesn't apply. Pseudo-$R^2$ measures exist but are less interpretable.</span>
<span id="cb2-1797"><a href="#cb2-1797" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Classification vs. Probability**: Logistic regression estimates probabilities. Classification (yes/no) requires choosing a threshold (often 0.5, but domain-specific considerations matter).</span>
<span id="cb2-1798"><a href="#cb2-1798" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Separation Problem**: If predictors perfectly separate the classes, the MLE doesn't exist (coefficients go to ±∞). Regularization or Bayesian methods can help.</span>
<span id="cb2-1799"><a href="#cb2-1799" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Sample Size Requirements**: Need more data than linear regression. Rule of thumb: 10-20 events per predictor for the less common outcome.</span>
<span id="cb2-1800"><a href="#cb2-1800" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Link Functions**: The logit is just one choice, another choice is for example the <span class="co">[</span><span class="ot">probit</span><span class="co">](https://en.wikipedia.org/wiki/Probit)</span> (normal CDF).</span>
<span id="cb2-1801"><a href="#cb2-1801" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1802"><a href="#cb2-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1803"><a href="#cb2-1803" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary and Connections</span></span>
<span id="cb2-1804"><a href="#cb2-1804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1805"><a href="#cb2-1805" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb2-1806"><a href="#cb2-1806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1807"><a href="#cb2-1807" aria-hidden="true" tabindex="-1"></a>We've covered the two fundamental models in statistical learning:</span>
<span id="cb2-1808"><a href="#cb2-1808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1809"><a href="#cb2-1809" aria-hidden="true" tabindex="-1"></a>**Linear Regression**:</span>
<span id="cb2-1810"><a href="#cb2-1810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1811"><a href="#cb2-1811" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models the expected value of a continuous response as a linear function of predictors</span>
<span id="cb2-1812"><a href="#cb2-1812" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimated via least squares, which coincides with MLE under normality</span>
<span id="cb2-1813"><a href="#cb2-1813" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provides interpretable coefficients with well-understood inference procedures</span>
<span id="cb2-1814"><a href="#cb2-1814" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extends naturally to multiple predictors, with matrix formulation</span>
<span id="cb2-1815"><a href="#cb2-1815" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires careful attention to assumptions and model selection</span>
<span id="cb2-1816"><a href="#cb2-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1817"><a href="#cb2-1817" aria-hidden="true" tabindex="-1"></a>**Logistic Regression**:</span>
<span id="cb2-1818"><a href="#cb2-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1819"><a href="#cb2-1819" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extends the linear framework to binary outcomes via the logit link</span>
<span id="cb2-1820"><a href="#cb2-1820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models probabilities, not the outcomes directly</span>
<span id="cb2-1821"><a href="#cb2-1821" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coefficients represent changes in log-odds, interpretable as odds ratios</span>
<span id="cb2-1822"><a href="#cb2-1822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimated via maximum likelihood with iterative algorithms</span>
<span id="cb2-1823"><a href="#cb2-1823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shares the interpretability advantages of linear models</span>
<span id="cb2-1824"><a href="#cb2-1824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1825"><a href="#cb2-1825" aria-hidden="true" tabindex="-1"></a>**Key Connections**:</span>
<span id="cb2-1826"><a href="#cb2-1826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1827"><a href="#cb2-1827" aria-hidden="true" tabindex="-1"></a>Both models exemplify the fundamental statistical modeling workflow:</span>
<span id="cb2-1828"><a href="#cb2-1828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1829"><a href="#cb2-1829" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Specify a model (assumptions about the data-generating process)</span>
<span id="cb2-1830"><a href="#cb2-1830" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Estimate parameters (least squares or maximum likelihood)</span>
<span id="cb2-1831"><a href="#cb2-1831" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Quantify uncertainty (standard errors, confidence intervals)</span>
<span id="cb2-1832"><a href="#cb2-1832" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Check assumptions (diagnostic plots)</span>
<span id="cb2-1833"><a href="#cb2-1833" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Make predictions and interpret results</span>
<span id="cb2-1834"><a href="#cb2-1834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1835"><a href="#cb2-1835" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Big Picture</span></span>
<span id="cb2-1836"><a href="#cb2-1836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1837"><a href="#cb2-1837" aria-hidden="true" tabindex="-1"></a>This chapter has taken you through the fundamentals of linear and logistic regression, from basic concepts to advanced applications. These models exemplify the core statistical modeling workflow: specify a model, estimate parameters, quantify uncertainty, check assumptions, and make predictions.</span>
<span id="cb2-1838"><a href="#cb2-1838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1839"><a href="#cb2-1839" aria-hidden="true" tabindex="-1"></a>But why do these simple linear models remain so important in the era of deep learning? The answer lies in their unique ability to explain complex predictions. **LIME (Local Interpretable Model-Agnostic Explanations)** <span class="co">[</span><span class="ot">@ribeiro2016lime</span><span class="co">]</span> demonstrates this perfectly: it explains any complex model's predictions by fitting simple linear models locally around points of interest.</span>
<span id="cb2-1840"><a href="#cb2-1840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1841"><a href="#cb2-1841" aria-hidden="true" tabindex="-1"></a><span class="al">![LIME: Local linear models explain complex predictions by approximating them in small neighborhoods. Figure reproduced from @ribeiro2016lime.](../figures/lime.png)</span>{#fig-lime width="60%"}</span>
<span id="cb2-1842"><a href="#cb2-1842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1843"><a href="#cb2-1843" aria-hidden="true" tabindex="-1"></a>This principle -- that complex functions are locally linear -- makes linear models useful for understanding predictions from any model, no matter how complex. The techniques you've learned in this chapter (least squares, coefficient interpretation, diagnostics) aren't just historical artifacts; they're the foundation for both classical statistical analysis and modern interpretable machine learning.</span>
<span id="cb2-1844"><a href="#cb2-1844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1845"><a href="#cb2-1845" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb2-1846"><a href="#cb2-1846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1847"><a href="#cb2-1847" aria-hidden="true" tabindex="-1"></a>When working with linear and logistic regression, watch out for these critical mistakes:</span>
<span id="cb2-1848"><a href="#cb2-1848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1849"><a href="#cb2-1849" aria-hidden="true" tabindex="-1"></a>**1. Interpretation Errors**</span>
<span id="cb2-1850"><a href="#cb2-1850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1851"><a href="#cb2-1851" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Correlation ≠ Causation**: A significant coefficient shows association, not causation (that's Chapter 11's topic!)</span>
<span id="cb2-1852"><a href="#cb2-1852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistical ≠ Practical significance**: With n=10,000, even tiny effects become "significant"</span>
<span id="cb2-1853"><a href="#cb2-1853" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Logistic coefficients are log-odds**: A coefficient of 0.5 doesn't mean "50% increase in probability"</span>
<span id="cb2-1854"><a href="#cb2-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1855"><a href="#cb2-1855" aria-hidden="true" tabindex="-1"></a>**2. Model Selection Traps**</span>
<span id="cb2-1856"><a href="#cb2-1856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1857"><a href="#cb2-1857" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Overfitting**: Using training error to select models guarantees disappointment on new data</span>
<span id="cb2-1858"><a href="#cb2-1858" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automation without thinking**: If your model says ice cream sales *decrease* temperatures, something's wrong</span>
<span id="cb2-1859"><a href="#cb2-1859" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ignoring validation**: Always hold out data -- a perfect training fit often means terrible generalization</span>
<span id="cb2-1860"><a href="#cb2-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1861"><a href="#cb2-1861" aria-hidden="true" tabindex="-1"></a>**3. Technical Violations**</span>
<span id="cb2-1862"><a href="#cb2-1862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1863"><a href="#cb2-1863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ignoring diagnostic plots**: That funnel-shaped residual plot? Your model needs help</span>
<span id="cb2-1864"><a href="#cb2-1864" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multicollinearity chaos**: When predictors correlate highly, coefficients become unstable and standard errors explode</span>
<span id="cb2-1865"><a href="#cb2-1865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1866"><a href="#cb2-1866" aria-hidden="true" tabindex="-1"></a>**4. The Big One: Context Blindness**</span>
<span id="cb2-1867"><a href="#cb2-1867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1868"><a href="#cb2-1868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extrapolation**: Linear trends rarely continue forever (no, humans won't be 20 feet tall in year 3000)</span>
<span id="cb2-1869"><a href="#cb2-1869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Domain knowledge matters**: Statistical criteria (AIC/BIC) are guides, not gospel</span>
<span id="cb2-1870"><a href="#cb2-1870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1871"><a href="#cb2-1871" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb2-1872"><a href="#cb2-1872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1873"><a href="#cb2-1873" aria-hidden="true" tabindex="-1"></a>**Previous (Chapters 5-8):**</span>
<span id="cb2-1874"><a href="#cb2-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1875"><a href="#cb2-1875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapters 5-6 introduced parametric inference -- linear regression is the quintessential parametric model, with least squares achieving the Cramér-Rao bound under normality</span>
<span id="cb2-1876"><a href="#cb2-1876" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 7's hypothesis testing framework directly applies to testing regression coefficients via Wald tests and F-tests</span>
<span id="cb2-1877"><a href="#cb2-1877" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 8's Bayesian paradigm offers an alternative: Bayesian linear regression incorporates prior knowledge about parameters</span>
<span id="cb2-1878"><a href="#cb2-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1879"><a href="#cb2-1879" aria-hidden="true" tabindex="-1"></a>**This Chapter:**</span>
<span id="cb2-1880"><a href="#cb2-1880" aria-hidden="true" tabindex="-1"></a>Introduced the two workhorses of statistical modeling: linear and logistic regression. We saw how least squares connects to maximum likelihood, how to handle multiple predictors, select models, and extend to binary outcomes. The focus on interpretability makes these models essential even in the era of complex machine learning.</span>
<span id="cb2-1881"><a href="#cb2-1881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1882"><a href="#cb2-1882" aria-hidden="true" tabindex="-1"></a>**Next (Ch. 10-11):**</span>
<span id="cb2-1883"><a href="#cb2-1883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1884"><a href="#cb2-1884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 10 will show Bayesian regression in practice using probabilistic programming languages, with complex priors and hierarchical structures</span>
<span id="cb2-1885"><a href="#cb2-1885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 11 will distinguish association from causation -- regression finds associations, not causal effects</span>
<span id="cb2-1886"><a href="#cb2-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1887"><a href="#cb2-1887" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb2-1888"><a href="#cb2-1888" aria-hidden="true" tabindex="-1"></a>Linear models remain indispensable across fields: economics (modeling market relationships), medicine (risk factor analysis), social sciences (understanding social determinants), machine learning (LIME and interpretability), and A/B testing (variance reduction through regression adjustment).</span>
<span id="cb2-1889"><a href="#cb2-1889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1890"><a href="#cb2-1890" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb2-1891"><a href="#cb2-1891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1892"><a href="#cb2-1892" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Centroid Property**: Show that the least squares regression line always passes through the point $(\bar{X}, \bar{Y})$.</span>
<span id="cb2-1893"><a href="#cb2-1893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1894"><a href="#cb2-1894" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb2-1895"><a href="#cb2-1895" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb2-1896"><a href="#cb2-1896" aria-hidden="true" tabindex="-1"></a>   Use the normal equations: from $\frac{\partial \text{RSS}}{\partial \hat{\beta}_0} = 0$ you get $\sum_i(Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0$, which gives $\bar{Y} = \hat{\beta}_0 + \hat{\beta}_1\bar{X}$.</span>
<span id="cb2-1897"><a href="#cb2-1897" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb2-1898"><a href="#cb2-1898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1899"><a href="#cb2-1899" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Multicollinearity Inflates Standard Errors**: Explain why high correlation between $X_j$ and the other predictors increases the standard error of $\hat{\beta}_j$.</span>
<span id="cb2-1900"><a href="#cb2-1900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1901"><a href="#cb2-1901" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb2-1902"><a href="#cb2-1902" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb2-1903"><a href="#cb2-1903" aria-hidden="true" tabindex="-1"></a>   In multiple regression, $\text{Var}(\hat{\beta}_j) \propto (1-R_j^2)^{-1}$, where $R_j^2$ is from regressing $X_j$ on the other $X$'s. As $R_j^2 \to 1$, the variance (and SE) blows up.</span>
<span id="cb2-1904"><a href="#cb2-1904" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb2-1905"><a href="#cb2-1905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1906"><a href="#cb2-1906" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Binary Outcomes and Heteroscedasticity**: For $Y \sim \text{Bernoulli}(p)$, compute $\text{Var}(Y)$ and explain why applying linear regression to binary $Y$ violates the constant-variance assumption.</span>
<span id="cb2-1907"><a href="#cb2-1907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1908"><a href="#cb2-1908" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb2-1909"><a href="#cb2-1909" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb2-1910"><a href="#cb2-1910" aria-hidden="true" tabindex="-1"></a>   $\text{Var}(Y) = p(1-p)$, which depends on $X$ if $p = \mathbb{P}(Y=1 \mid X)$ changes with $X$. The variance is maximized at $p=0.5$ and approaches 0 as $p \to 0$ or $p \to 1$.</span>
<span id="cb2-1911"><a href="#cb2-1911" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb2-1912"><a href="#cb2-1912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1913"><a href="#cb2-1913" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Odds Ratio to Probability**: Baseline probability is $p_0 = 0.20$. A predictor has an odds ratio $\text{OR} = 2.4$. What is the new probability?</span>
<span id="cb2-1914"><a href="#cb2-1914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1915"><a href="#cb2-1915" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb2-1916"><a href="#cb2-1916" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb2-1917"><a href="#cb2-1917" aria-hidden="true" tabindex="-1"></a>   Convert baseline to odds: $o_0 = \frac{p_0}{1-p_0} = \frac{0.20}{0.80} = 0.25$. Multiply by OR to get $o_1 = 2.4 \times 0.25 = 0.6$. Convert back: $p_1 = \frac{o_1}{1+o_1} = \frac{0.6}{1.6} = 0.375$.</span>
<span id="cb2-1918"><a href="#cb2-1918" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb2-1919"><a href="#cb2-1919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1920"><a href="#cb2-1920" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Interpreting an Interaction**: In the model $Y = \beta_0 + \beta_1 \cdot \text{FRW} + \beta_2 \cdot \text{SEX} + \beta_3 \cdot (\text{FRW} \times \text{SEX}) + \epsilon$ with $\text{SEX}=0$ (female) and $\text{SEX}=1$ (male), what is the effect (slope) of FRW on $Y$ for females vs males? What does $\beta_2$ represent?</span>
<span id="cb2-1921"><a href="#cb2-1921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1922"><a href="#cb2-1922" aria-hidden="true" tabindex="-1"></a>   ::: {.callout-note collapse="true"}</span>
<span id="cb2-1923"><a href="#cb2-1923" aria-hidden="true" tabindex="-1"></a>   ## Solution Hint</span>
<span id="cb2-1924"><a href="#cb2-1924" aria-hidden="true" tabindex="-1"></a>   Plug in $\text{SEX}=0$ and $\text{SEX}=1$. The FRW slope is $\beta_1$ for females and $\beta_1 + \beta_3$ for males. $\beta_2$ is the male-female intercept difference when $\text{FRW}=0$.</span>
<span id="cb2-1925"><a href="#cb2-1925" aria-hidden="true" tabindex="-1"></a>   :::</span>
<span id="cb2-1926"><a href="#cb2-1926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1927"><a href="#cb2-1927" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb2-1928"><a href="#cb2-1928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1929"><a href="#cb2-1929" aria-hidden="true" tabindex="-1"></a>This section provides a quick reference for the main functions used in linear and logistic regression.</span>
<span id="cb2-1930"><a href="#cb2-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1931"><a href="#cb2-1931" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb2-1932"><a href="#cb2-1932" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb2-1933"><a href="#cb2-1933" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python (statsmodels)</span></span>
<span id="cb2-1934"><a href="#cb2-1934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1935"><a href="#cb2-1935" aria-hidden="true" tabindex="-1"></a>**Linear Regression**</span>
<span id="cb2-1936"><a href="#cb2-1936" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb2-1937"><a href="#cb2-1937" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb2-1938"><a href="#cb2-1938" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-1939"><a href="#cb2-1939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1940"><a href="#cb2-1940" aria-hidden="true" tabindex="-1"></a><span class="co"># Using formula interface (R-style)</span></span>
<span id="cb2-1941"><a href="#cb2-1941" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(<span class="st">'Y ~ X1 + X2 + X3'</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb2-1942"><a href="#cb2-1942" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span>
<span id="cb2-1943"><a href="#cb2-1943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1944"><a href="#cb2-1944" aria-hidden="true" tabindex="-1"></a><span class="co"># Using arrays interface</span></span>
<span id="cb2-1945"><a href="#cb2-1945" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)  <span class="co"># Add intercept column</span></span>
<span id="cb2-1946"><a href="#cb2-1946" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> sm.OLS(Y, X).fit()</span>
<span id="cb2-1947"><a href="#cb2-1947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1948"><a href="#cb2-1948" aria-hidden="true" tabindex="-1"></a><span class="co"># Key methods</span></span>
<span id="cb2-1949"><a href="#cb2-1949" aria-hidden="true" tabindex="-1"></a>results.params           <span class="co"># Coefficients</span></span>
<span id="cb2-1950"><a href="#cb2-1950" aria-hidden="true" tabindex="-1"></a>results.bse             <span class="co"># Standard errors</span></span>
<span id="cb2-1951"><a href="#cb2-1951" aria-hidden="true" tabindex="-1"></a>results.conf_int()      <span class="co"># Confidence intervals</span></span>
<span id="cb2-1952"><a href="#cb2-1952" aria-hidden="true" tabindex="-1"></a>results.pvalues         <span class="co"># P-values</span></span>
<span id="cb2-1953"><a href="#cb2-1953" aria-hidden="true" tabindex="-1"></a>results.rsquared        <span class="co"># R-squared</span></span>
<span id="cb2-1954"><a href="#cb2-1954" aria-hidden="true" tabindex="-1"></a>results.fittedvalues    <span class="co"># Predicted values</span></span>
<span id="cb2-1955"><a href="#cb2-1955" aria-hidden="true" tabindex="-1"></a>results.resid           <span class="co"># Residuals</span></span>
<span id="cb2-1956"><a href="#cb2-1956" aria-hidden="true" tabindex="-1"></a>results.predict(new_df) <span class="co"># Predictions for new data (DataFrame with same columns)</span></span>
<span id="cb2-1957"><a href="#cb2-1957" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1958"><a href="#cb2-1958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1959"><a href="#cb2-1959" aria-hidden="true" tabindex="-1"></a>**Logistic Regression**</span>
<span id="cb2-1960"><a href="#cb2-1960" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb2-1961"><a href="#cb2-1961" aria-hidden="true" tabindex="-1"></a><span class="co"># Using formula interface</span></span>
<span id="cb2-1962"><a href="#cb2-1962" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.logit(<span class="st">'Y ~ X1 + X2 + X3'</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb2-1963"><a href="#cb2-1963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1964"><a href="#cb2-1964" aria-hidden="true" tabindex="-1"></a><span class="co"># Using arrays interface (must add intercept!)</span></span>
<span id="cb2-1965"><a href="#cb2-1965" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)  <span class="co"># Don't forget the intercept</span></span>
<span id="cb2-1966"><a href="#cb2-1966" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> sm.Logit(Y, X).fit()</span>
<span id="cb2-1967"><a href="#cb2-1967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1968"><a href="#cb2-1968" aria-hidden="true" tabindex="-1"></a><span class="co"># Key methods (similar to linear)</span></span>
<span id="cb2-1969"><a href="#cb2-1969" aria-hidden="true" tabindex="-1"></a>results.params          <span class="co"># Log-odds coefficients</span></span>
<span id="cb2-1970"><a href="#cb2-1970" aria-hidden="true" tabindex="-1"></a>np.exp(results.params)  <span class="co"># Odds ratios</span></span>
<span id="cb2-1971"><a href="#cb2-1971" aria-hidden="true" tabindex="-1"></a>results.predict()       <span class="co"># Predicted probabilities</span></span>
<span id="cb2-1972"><a href="#cb2-1972" aria-hidden="true" tabindex="-1"></a>results.pred_table()    <span class="co"># Classification table (2x2 contingency)</span></span>
<span id="cb2-1973"><a href="#cb2-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1974"><a href="#cb2-1974" aria-hidden="true" tabindex="-1"></a><span class="co"># Model selection</span></span>
<span id="cb2-1975"><a href="#cb2-1975" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb2-1976"><a href="#cb2-1976" aria-hidden="true" tabindex="-1"></a><span class="co"># Check multicollinearity (compute on X with constant, skip reporting intercept VIF)</span></span>
<span id="cb2-1977"><a href="#cb2-1977" aria-hidden="true" tabindex="-1"></a>X_with_const <span class="op">=</span> sm.add_constant(X_features)</span>
<span id="cb2-1978"><a href="#cb2-1978" aria-hidden="true" tabindex="-1"></a>vif <span class="op">=</span> pd.DataFrame()</span>
<span id="cb2-1979"><a href="#cb2-1979" aria-hidden="true" tabindex="-1"></a>vif[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(X_with_const.values, i) </span>
<span id="cb2-1980"><a href="#cb2-1980" aria-hidden="true" tabindex="-1"></a>              <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, X_with_const.shape[<span class="dv">1</span>])]  <span class="co"># Skip intercept column</span></span>
<span id="cb2-1981"><a href="#cb2-1981" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1982"><a href="#cb2-1982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1983"><a href="#cb2-1983" aria-hidden="true" tabindex="-1"></a>**Diagnostic Plots**</span>
<span id="cb2-1984"><a href="#cb2-1984" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb2-1985"><a href="#cb2-1985" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.graphics.api <span class="im">as</span> smg</span>
<span id="cb2-1986"><a href="#cb2-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1987"><a href="#cb2-1987" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic plots</span></span>
<span id="cb2-1988"><a href="#cb2-1988" aria-hidden="true" tabindex="-1"></a>smg.plot_fit(results, <span class="st">'X1'</span>)  <span class="co"># Observed vs fitted for predictor X1</span></span>
<span id="cb2-1989"><a href="#cb2-1989" aria-hidden="true" tabindex="-1"></a>smg.qqplot(results.resid, line<span class="op">=</span><span class="st">'45'</span>)  <span class="co"># Q-Q plot with 45° line</span></span>
<span id="cb2-1990"><a href="#cb2-1990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1991"><a href="#cb2-1991" aria-hidden="true" tabindex="-1"></a><span class="co"># Residuals vs fitted (custom)</span></span>
<span id="cb2-1992"><a href="#cb2-1992" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-1993"><a href="#cb2-1993" aria-hidden="true" tabindex="-1"></a>plt.scatter(results.fittedvalues, results.resid)</span>
<span id="cb2-1994"><a href="#cb2-1994" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb2-1995"><a href="#cb2-1995" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Fitted values'</span>)</span>
<span id="cb2-1996"><a href="#cb2-1996" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb2-1997"><a href="#cb2-1997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1998"><a href="#cb2-1998" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial regression plots</span></span>
<span id="cb2-1999"><a href="#cb2-1999" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.regressionplots <span class="im">import</span> plot_partregress_grid</span>
<span id="cb2-2000"><a href="#cb2-2000" aria-hidden="true" tabindex="-1"></a>plot_partregress_grid(results)</span>
<span id="cb2-2001"><a href="#cb2-2001" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-2002"><a href="#cb2-2002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2003"><a href="#cb2-2003" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb2-2004"><a href="#cb2-2004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2005"><a href="#cb2-2005" aria-hidden="true" tabindex="-1"></a>**Linear Regression**</span>
<span id="cb2-2006"><a href="#cb2-2006" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-2007"><a href="#cb2-2007" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb2-2008"><a href="#cb2-2008" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data =</span> df)</span>
<span id="cb2-2009"><a href="#cb2-2009" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb2-2010"><a href="#cb2-2010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2011"><a href="#cb2-2011" aria-hidden="true" tabindex="-1"></a><span class="co"># Key functions</span></span>
<span id="cb2-2012"><a href="#cb2-2012" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(model)         <span class="co"># Coefficients</span></span>
<span id="cb2-2013"><a href="#cb2-2013" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model)      <span class="co"># Confidence intervals</span></span>
<span id="cb2-2014"><a href="#cb2-2014" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model)      <span class="co"># Predictions</span></span>
<span id="cb2-2015"><a href="#cb2-2015" aria-hidden="true" tabindex="-1"></a><span class="fu">residuals</span>(model)    <span class="co"># Residuals</span></span>
<span id="cb2-2016"><a href="#cb2-2016" aria-hidden="true" tabindex="-1"></a><span class="fu">fitted</span>(model)       <span class="co"># Fitted values</span></span>
<span id="cb2-2017"><a href="#cb2-2017" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(model)         <span class="co"># Variance-covariance matrix</span></span>
<span id="cb2-2018"><a href="#cb2-2018" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(model)        <span class="co"># ANOVA table</span></span>
<span id="cb2-2019"><a href="#cb2-2019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2020"><a href="#cb2-2020" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostic plots</span></span>
<span id="cb2-2021"><a href="#cb2-2021" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb2-2022"><a href="#cb2-2022" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model)         <span class="co"># Standard diagnostic plots</span></span>
<span id="cb2-2023"><a href="#cb2-2023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2024"><a href="#cb2-2024" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial regression plots</span></span>
<span id="cb2-2025"><a href="#cb2-2025" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb2-2026"><a href="#cb2-2026" aria-hidden="true" tabindex="-1"></a><span class="fu">avPlots</span>(model)</span>
<span id="cb2-2027"><a href="#cb2-2027" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-2028"><a href="#cb2-2028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2029"><a href="#cb2-2029" aria-hidden="true" tabindex="-1"></a>**Logistic Regression**</span>
<span id="cb2-2030"><a href="#cb2-2030" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-2031"><a href="#cb2-2031" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb2-2032"><a href="#cb2-2032" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data =</span> df, </span>
<span id="cb2-2033"><a href="#cb2-2033" aria-hidden="true" tabindex="-1"></a>             <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">"logit"</span>))</span>
<span id="cb2-2034"><a href="#cb2-2034" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb2-2035"><a href="#cb2-2035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2036"><a href="#cb2-2036" aria-hidden="true" tabindex="-1"></a><span class="co"># Odds ratios</span></span>
<span id="cb2-2037"><a href="#cb2-2037" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(model))</span>
<span id="cb2-2038"><a href="#cb2-2038" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(model))  <span class="co"># Profile likelihood CIs (more accurate)</span></span>
<span id="cb2-2039"><a href="#cb2-2039" aria-hidden="true" tabindex="-1"></a><span class="co"># For faster Wald CIs: exp(confint.default(model))</span></span>
<span id="cb2-2040"><a href="#cb2-2040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2041"><a href="#cb2-2041" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb2-2042"><a href="#cb2-2042" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model, <span class="at">type =</span> <span class="st">"response"</span>)  <span class="co"># Probabilities</span></span>
<span id="cb2-2043"><a href="#cb2-2043" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model, <span class="at">type =</span> <span class="st">"link"</span>)      <span class="co"># Log-odds</span></span>
<span id="cb2-2044"><a href="#cb2-2044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2045"><a href="#cb2-2045" aria-hidden="true" tabindex="-1"></a><span class="co"># Model selection</span></span>
<span id="cb2-2046"><a href="#cb2-2046" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb2-2047"><a href="#cb2-2047" aria-hidden="true" tabindex="-1"></a><span class="fu">stepAIC</span>(model)  <span class="co"># Stepwise selection using AIC</span></span>
<span id="cb2-2048"><a href="#cb2-2048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2049"><a href="#cb2-2049" aria-hidden="true" tabindex="-1"></a><span class="co"># Check multicollinearity</span></span>
<span id="cb2-2050"><a href="#cb2-2050" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb2-2051"><a href="#cb2-2051" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(model)</span>
<span id="cb2-2052"><a href="#cb2-2052" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-2053"><a href="#cb2-2053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2054"><a href="#cb2-2054" aria-hidden="true" tabindex="-1"></a>**Model Selection and Comparison**</span>
<span id="cb2-2055"><a href="#cb2-2055" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb2-2056"><a href="#cb2-2056" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare models</span></span>
<span id="cb2-2057"><a href="#cb2-2057" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(model1, model2, model3)</span>
<span id="cb2-2058"><a href="#cb2-2058" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(model1, model2, model3)</span>
<span id="cb2-2059"><a href="#cb2-2059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2060"><a href="#cb2-2060" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for linear regression</span></span>
<span id="cb2-2061"><a href="#cb2-2061" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb2-2062"><a href="#cb2-2062" aria-hidden="true" tabindex="-1"></a>train_control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"cv"</span>, <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb2-2063"><a href="#cb2-2063" aria-hidden="true" tabindex="-1"></a>model_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(Y <span class="sc">~</span> ., <span class="at">data =</span> df, <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb2-2064"><a href="#cb2-2064" aria-hidden="true" tabindex="-1"></a>                  <span class="at">trControl =</span> train_control)</span>
<span id="cb2-2065"><a href="#cb2-2065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2066"><a href="#cb2-2066" aria-hidden="true" tabindex="-1"></a><span class="co"># For logistic regression with caret:</span></span>
<span id="cb2-2067"><a href="#cb2-2067" aria-hidden="true" tabindex="-1"></a><span class="co"># train(Y ~ ., data = df, method = "glm", </span></span>
<span id="cb2-2068"><a href="#cb2-2068" aria-hidden="true" tabindex="-1"></a><span class="co">#       family = binomial, trControl = train_control)</span></span>
<span id="cb2-2069"><a href="#cb2-2069" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-2070"><a href="#cb2-2070" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-2071"><a href="#cb2-2071" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-2072"><a href="#cb2-2072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2073"><a href="#cb2-2073" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb2-2074"><a href="#cb2-2074" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-2075"><a href="#cb2-2075" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb2-2076"><a href="#cb2-2076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2077"><a href="#cb2-2077" aria-hidden="true" tabindex="-1"></a>Python and R code examples for this chapter can be found in the HTML version of these notes.</span>
<span id="cb2-2078"><a href="#cb2-2078" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-2079"><a href="#cb2-2079" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-2080"><a href="#cb2-2080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2081"><a href="#cb2-2081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2082"><a href="#cb2-2082" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb2-2083"><a href="#cb2-2083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2084"><a href="#cb2-2084" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb2-2085"><a href="#cb2-2085" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to Course Materials</span></span>
<span id="cb2-2086"><a href="#cb2-2086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2087"><a href="#cb2-2087" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding Source(s) |</span>
<span id="cb2-2088"><a href="#cb2-2088" aria-hidden="true" tabindex="-1"></a>|:---------------------|:-----------------------|</span>
<span id="cb2-2089"><a href="#cb2-2089" aria-hidden="true" tabindex="-1"></a>| **Introduction: Why Linear Models Still Matter** | Lecture 9 slides intro on interpretable ML and LIME |</span>
<span id="cb2-2090"><a href="#cb2-2090" aria-hidden="true" tabindex="-1"></a>| ↳ The Power of Interpretability | Expanded from lecture motivation |</span>
<span id="cb2-2091"><a href="#cb2-2091" aria-hidden="true" tabindex="-1"></a>| ↳ Linear Models as Building Blocks | New material connecting to GLMs, mixed models, LIME |</span>
<span id="cb2-2092"><a href="#cb2-2092" aria-hidden="true" tabindex="-1"></a>| **Simple Linear Regression** | AoS §13.1 |</span>
<span id="cb2-2093"><a href="#cb2-2093" aria-hidden="true" tabindex="-1"></a>| ↳ Regression Models | AoS §13.1 |</span>
<span id="cb2-2094"><a href="#cb2-2094" aria-hidden="true" tabindex="-1"></a>| ↳ The Simple Linear Regression Model | AoS Definition 13.1 |</span>
<span id="cb2-2095"><a href="#cb2-2095" aria-hidden="true" tabindex="-1"></a>| ↳ Estimating Parameters: Method of Least Squares | AoS §13.1, Theorem 13.4 |</span>
<span id="cb2-2096"><a href="#cb2-2096" aria-hidden="true" tabindex="-1"></a>| ↳ Connection to Maximum Likelihood | AoS §13.2 |</span>
<span id="cb2-2097"><a href="#cb2-2097" aria-hidden="true" tabindex="-1"></a>| ↳ Properties of the Least Squares Estimators | AoS §13.3, Theorems 13.8-13.9 |</span>
<span id="cb2-2098"><a href="#cb2-2098" aria-hidden="true" tabindex="-1"></a>| ↳ Simple Linear Regression in Practice | New Framingham example from lecture slides, applying concepts from AoS §13.1-13.3 |</span>
<span id="cb2-2099"><a href="#cb2-2099" aria-hidden="true" tabindex="-1"></a>| **Multiple Linear Regression** | AoS §13.5 |</span>
<span id="cb2-2100"><a href="#cb2-2100" aria-hidden="true" tabindex="-1"></a>| ↳ Extending the Model to Multiple Predictors | AoS §13.5 |</span>
<span id="cb2-2101"><a href="#cb2-2101" aria-hidden="true" tabindex="-1"></a>| ↳ Least Squares in Matrix Form | AoS Theorem 13.13 |</span>
<span id="cb2-2102"><a href="#cb2-2102" aria-hidden="true" tabindex="-1"></a>| ↳ Multiple Regression in Practice | New Framingham example from lecture slides, applying concepts from AoS §13.5 |</span>
<span id="cb2-2103"><a href="#cb2-2103" aria-hidden="true" tabindex="-1"></a>| **Model Selection: Choosing the Right Predictors** | AoS §13.6 |</span>
<span id="cb2-2104"><a href="#cb2-2104" aria-hidden="true" tabindex="-1"></a>| ↳ Scoring Models: The Bias-Variance Trade-off | AoS §13.6 |</span>
<span id="cb2-2105"><a href="#cb2-2105" aria-hidden="true" tabindex="-1"></a>| ↳ Mallow's Cp, AIC, BIC, Cross-Validation | AoS §13.6 |</span>
<span id="cb2-2106"><a href="#cb2-2106" aria-hidden="true" tabindex="-1"></a>| ↳ Search Strategies | AoS §13.6 |</span>
<span id="cb2-2107"><a href="#cb2-2107" aria-hidden="true" tabindex="-1"></a>| ↳ Comparing Predictor Importance | Lecture 9 slides + Gelman &amp; Hill |</span>
<span id="cb2-2108"><a href="#cb2-2108" aria-hidden="true" tabindex="-1"></a>| ↳ Controlling for Background Variables | Lecture 9 slides |</span>
<span id="cb2-2109"><a href="#cb2-2109" aria-hidden="true" tabindex="-1"></a>| **Regression Assumptions and Diagnostics** | Lecture 9 slides (sourcing Gelman &amp; Hill) |</span>
<span id="cb2-2110"><a href="#cb2-2110" aria-hidden="true" tabindex="-1"></a>| ↳ The Five Assumptions | Lecture 9 slides (sourcing Gelman &amp; Hill) |</span>
<span id="cb2-2111"><a href="#cb2-2111" aria-hidden="true" tabindex="-1"></a>| ↳ Checking Assumptions: Residual Plots | Lecture 9 slides + expanded examples |</span>
<span id="cb2-2112"><a href="#cb2-2112" aria-hidden="true" tabindex="-1"></a>| **Logistic Regression** | AoS §13.7 |</span>
<span id="cb2-2113"><a href="#cb2-2113" aria-hidden="true" tabindex="-1"></a>| ↳ Modeling Binary Outcomes | AoS §13.7 |</span>
<span id="cb2-2114"><a href="#cb2-2114" aria-hidden="true" tabindex="-1"></a>| ↳ The Logistic Regression Model | AoS §13.7 |</span>
<span id="cb2-2115"><a href="#cb2-2115" aria-hidden="true" tabindex="-1"></a>| ↳ Logistic Regression in Practice | New Framingham example from lecture slides, applying concepts from AoS §13.7 |</span>
<span id="cb2-2116"><a href="#cb2-2116" aria-hidden="true" tabindex="-1"></a>| **Chapter Summary and Connections** | New comprehensive summary |</span>
<span id="cb2-2117"><a href="#cb2-2117" aria-hidden="true" tabindex="-1"></a>| **Python and R Reference** | New - added Python alongside R implementations |</span>
<span id="cb2-2118"><a href="#cb2-2118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2119"><a href="#cb2-2119" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-2120"><a href="#cb2-2120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2121"><a href="#cb2-2121" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Materials</span></span>
<span id="cb2-2122"><a href="#cb2-2122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2123"><a href="#cb2-2123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@gelman2007data - Practical regression guidance with excellent intuition and real-world advice</span>
<span id="cb2-2124"><a href="#cb2-2124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@ribeiro2016lime - The LIME paper demonstrating how local linear models can explain any classifier's predictions (<span class="co">[</span><span class="ot">GitHub repo</span><span class="co">](https://github.com/marcotcr/lime)</span>)</span>
<span id="cb2-2125"><a href="#cb2-2125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2126"><a href="#cb2-2126" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-2127"><a href="#cb2-2127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2128"><a href="#cb2-2128" aria-hidden="true" tabindex="-1"></a>*Remember: Start with linear regression -- many problems that get a neural network thrown at them could be solved with these simple models. Always establish a linear/logistic baseline first: if your complex deep network only improves accuracy by 2%, is the added complexity, computation, and loss of interpretability really worth it? Master these fundamental methods -- they're used daily by practitioners worldwide and remain indispensable as baselines, interpretable models, and building blocks for more complex systems.*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>