<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-07">

<title>Statistics for Data Science: Lecture Notes - 7&nbsp; Hypothesis Testing and p-values</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/08-bayesian-inference-decision-theory.html" rel="next">
<link href="../chapters/06-parametric-inference-II.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/07-hypothesis-testing.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for Data Science: Lecture Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-convergence-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Convergence and The Basics of Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-nonparametric-bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Estimation and The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-parametric-inference-I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parametric Inference I: Finding Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-parametric-inference-II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-hypothesis-testing.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-bayesian-inference-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-linear-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear and Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pdf-download.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Download Complete PDF</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">7.1</span> Learning Objectives</a></li>
  <li><a href="#introduction-is-an-observed-effect-real-or-just-random-chance" id="toc-introduction-is-an-observed-effect-real-or-just-random-chance" class="nav-link" data-scroll-target="#introduction-is-an-observed-effect-real-or-just-random-chance"><span class="header-section-number">7.2</span> Introduction: Is an Observed Effect Real or Just Random Chance?</a></li>
  <li><a href="#the-framework-of-hypothesis-testing" id="toc-the-framework-of-hypothesis-testing" class="nav-link" data-scroll-target="#the-framework-of-hypothesis-testing"><span class="header-section-number">7.3</span> The Framework of Hypothesis Testing</a>
  <ul class="collapse">
  <li><a href="#null-and-alternative-hypotheses" id="toc-null-and-alternative-hypotheses" class="nav-link" data-scroll-target="#null-and-alternative-hypotheses"><span class="header-section-number">7.3.1</span> Null and Alternative Hypotheses</a></li>
  <li><a href="#the-machinery-of-a-test" id="toc-the-machinery-of-a-test" class="nav-link" data-scroll-target="#the-machinery-of-a-test"><span class="header-section-number">7.3.2</span> The Machinery of a Test</a></li>
  <li><a href="#two-ways-to-be-wrong-type-i-and-type-ii-errors" id="toc-two-ways-to-be-wrong-type-i-and-type-ii-errors" class="nav-link" data-scroll-target="#two-ways-to-be-wrong-type-i-and-type-ii-errors"><span class="header-section-number">7.3.3</span> Two Ways to Be Wrong: Type I and Type II Errors</a></li>
  <li><a href="#the-wald-test" id="toc-the-wald-test" class="nav-link" data-scroll-target="#the-wald-test"><span class="header-section-number">7.3.4</span> The Wald Test</a></li>
  <li><a href="#statistical-and-scientific-significance" id="toc-statistical-and-scientific-significance" class="nav-link" data-scroll-target="#statistical-and-scientific-significance"><span class="header-section-number">7.3.5</span> Statistical and Scientific Significance</a></li>
  </ul></li>
  <li><a href="#the-p-value-a-continuous-measure-of-evidence" id="toc-the-p-value-a-continuous-measure-of-evidence" class="nav-link" data-scroll-target="#the-p-value-a-continuous-measure-of-evidence"><span class="header-section-number">7.4</span> The p-value: A Continuous Measure of Evidence</a>
  <ul class="collapse">
  <li><a href="#understanding-the-p-value" id="toc-understanding-the-p-value" class="nav-link" data-scroll-target="#understanding-the-p-value"><span class="header-section-number">7.4.1</span> Understanding the p-value</a></li>
  <li><a href="#how-to-interpret-p-values-and-how-not-to" id="toc-how-to-interpret-p-values-and-how-not-to" class="nav-link" data-scroll-target="#how-to-interpret-p-values-and-how-not-to"><span class="header-section-number">7.4.2</span> How to Interpret p-values (and How Not To)</a></li>
  <li><a href="#applying-p-values-classification-algorithm-comparison" id="toc-applying-p-values-classification-algorithm-comparison" class="nav-link" data-scroll-target="#applying-p-values-classification-algorithm-comparison"><span class="header-section-number">7.4.3</span> Applying p-values: Classification Algorithm Comparison</a></li>
  </ul></li>
  <li><a href="#constructing-statistical-tests" id="toc-constructing-statistical-tests" class="nav-link" data-scroll-target="#constructing-statistical-tests"><span class="header-section-number">7.5</span> Constructing Statistical Tests</a>
  <ul class="collapse">
  <li><a href="#the-permutation-test-a-simulation-approach" id="toc-the-permutation-test-a-simulation-approach" class="nav-link" data-scroll-target="#the-permutation-test-a-simulation-approach"><span class="header-section-number">7.5.1</span> The Permutation Test: A Simulation Approach</a></li>
  <li><a href="#fishers-exact-test-an-exact-approach" id="toc-fishers-exact-test-an-exact-approach" class="nav-link" data-scroll-target="#fishers-exact-test-an-exact-approach"><span class="header-section-number">7.5.2</span> Fisher’s Exact Test: An Exact Approach</a></li>
  <li><a href="#the-likelihood-ratio-test-a-general-asymptotic-approach" id="toc-the-likelihood-ratio-test-a-general-asymptotic-approach" class="nav-link" data-scroll-target="#the-likelihood-ratio-test-a-general-asymptotic-approach"><span class="header-section-number">7.5.3</span> The Likelihood Ratio Test: A General Asymptotic Approach</a></li>
  </ul></li>
  <li><a href="#the-multiple-testing-problem-the-peril-of-many-tests" id="toc-the-multiple-testing-problem-the-peril-of-many-tests" class="nav-link" data-scroll-target="#the-multiple-testing-problem-the-peril-of-many-tests"><span class="header-section-number">7.6</span> The Multiple Testing Problem: The Peril of Many Tests</a>
  <ul class="collapse">
  <li><a href="#the-problem" id="toc-the-problem" class="nav-link" data-scroll-target="#the-problem"><span class="header-section-number">7.6.1</span> The Problem</a></li>
  <li><a href="#key-concepts-for-multiple-testing" id="toc-key-concepts-for-multiple-testing" class="nav-link" data-scroll-target="#key-concepts-for-multiple-testing"><span class="header-section-number">7.6.2</span> Key Concepts for Multiple Testing</a></li>
  <li><a href="#the-bonferroni-method-controlling-fwer" id="toc-the-bonferroni-method-controlling-fwer" class="nav-link" data-scroll-target="#the-bonferroni-method-controlling-fwer"><span class="header-section-number">7.6.3</span> The Bonferroni Method: Controlling FWER</a></li>
  <li><a href="#the-benjamini-hochberg-method-controlling-fdr" id="toc-the-benjamini-hochberg-method-controlling-fdr" class="nav-link" data-scroll-target="#the-benjamini-hochberg-method-controlling-fdr"><span class="header-section-number">7.6.4</span> The Benjamini-Hochberg Method: Controlling FDR</a></li>
  <li><a href="#practical-recommendations" id="toc-practical-recommendations" class="nav-link" data-scroll-target="#practical-recommendations"><span class="header-section-number">7.6.5</span> Practical Recommendations</a></li>
  </ul></li>
  <li><a href="#nhst-in-practice-a-critical-view" id="toc-nhst-in-practice-a-critical-view" class="nav-link" data-scroll-target="#nhst-in-practice-a-critical-view"><span class="header-section-number">7.7</span> NHST in Practice: A Critical View</a>
  <ul class="collapse">
  <li><a href="#fundamental-problems-with-nhst" id="toc-fundamental-problems-with-nhst" class="nav-link" data-scroll-target="#fundamental-problems-with-nhst"><span class="header-section-number">7.7.1</span> Fundamental Problems with NHST</a></li>
  <li><a href="#the-low-power-trap" id="toc-the-low-power-trap" class="nav-link" data-scroll-target="#the-low-power-trap"><span class="header-section-number">7.7.2</span> The Low Power Trap</a></li>
  <li><a href="#common-misuses" id="toc-common-misuses" class="nav-link" data-scroll-target="#common-misuses"><span class="header-section-number">7.7.3</span> Common Misuses</a></li>
  <li><a href="#moving-forward-better-practices" id="toc-moving-forward-better-practices" class="nav-link" data-scroll-target="#moving-forward-better-practices"><span class="header-section-number">7.7.4</span> Moving Forward: Better Practices</a></li>
  </ul></li>
  <li><a href="#chapter-summary" id="toc-chapter-summary" class="nav-link" data-scroll-target="#chapter-summary"><span class="header-section-number">7.8</span> Chapter Summary</a>
  <ul class="collapse">
  <li><a href="#key-concepts-review" id="toc-key-concepts-review" class="nav-link" data-scroll-target="#key-concepts-review"><span class="header-section-number">7.8.1</span> Key Concepts Review</a></li>
  <li><a href="#common-pitfalls-to-avoid" id="toc-common-pitfalls-to-avoid" class="nav-link" data-scroll-target="#common-pitfalls-to-avoid"><span class="header-section-number">7.8.2</span> Common Pitfalls to Avoid</a></li>
  <li><a href="#chapter-connections" id="toc-chapter-connections" class="nav-link" data-scroll-target="#chapter-connections"><span class="header-section-number">7.8.3</span> Chapter Connections</a></li>
  <li><a href="#self-test-problems" id="toc-self-test-problems" class="nav-link" data-scroll-target="#self-test-problems"><span class="header-section-number">7.8.4</span> Self-Test Problems</a></li>
  <li><a href="#python-and-r-reference" id="toc-python-and-r-reference" class="nav-link" data-scroll-target="#python-and-r-reference"><span class="header-section-number">7.8.5</span> Python and R Reference</a></li>
  <li><a href="#connections-to-source-material" id="toc-connections-to-source-material" class="nav-link" data-scroll-target="#connections-to-source-material"><span class="header-section-number">7.8.6</span> Connections to Source Material</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">7.8.7</span> Further Reading</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing and p-values</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">7.1</span> Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li><strong>Explain the core framework of null-hypothesis significance testing (NHST)</strong>, including the roles of null/alternative hypotheses, Type I/II errors, and statistical power.</li>
<li><strong>Define the p-value and correctly interpret its meaning</strong>, recognizing its limitations and common misinterpretations.</li>
<li><strong>Apply and interpret common hypothesis tests</strong>, such as the Wald test and permutation test, and understand the connection between tests and confidence intervals.</li>
<li><strong>Explain the multiple testing problem</strong> and apply standard correction methods like the Bonferroni and Benjamini-Hochberg procedures.</li>
<li><strong>Critically evaluate the use of NHST and p-values</strong> in data analysis and scientific research.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter covers null-hypothesis significance testing (NHST) and p-values, fundamental concepts in statistical inference. The material is adapted from Chapter 10 of <span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span>, supplemented with computational examples and critical perspectives on the use of NHST in modern data science.</p>
</div>
</div>
</section>
<section id="introduction-is-an-observed-effect-real-or-just-random-chance" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="introduction-is-an-observed-effect-real-or-just-random-chance"><span class="header-section-number">7.2</span> Introduction: Is an Observed Effect Real or Just Random Chance?</h2>
<p>A drug company is running a clinical trial of a new drug. Patients are randomly assigned to either a treatment group that receives the new drug (100 subjects), or a control group that receives a placebo (other 100 subjects). After a suitable period of observation, the results are tallied:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Better</th>
<th style="text-align: center;">Not Better</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Treated</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
</tr>
<tr class="even">
<td style="text-align: left;">Control</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">60</td>
</tr>
</tbody>
</table>
<p>Looking at these numbers, we see that 50% of the treated patients improved, compared to only 40% of the control patients. But is this 10 percentage point difference large enough to represent a <em>real</em> effect of the drug, or could it simply be due to random chance in how we happened to assign individuals to groups?</p>
<p>Similarly, an online retailer might run an A/B test comparing two different user interfaces. Users are randomly shown either the old system or a new design, and we track their purchase behavior:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Purchase</th>
<th style="text-align: center;">No Purchase</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">New System</td>
<td style="text-align: center;">850</td>
<td style="text-align: center;">150</td>
</tr>
<tr class="even">
<td style="text-align: left;">Old System</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">200</td>
</tr>
</tbody>
</table>
<p>The new system appears to have a higher conversion rate (85% vs 80%), but again we face the fundamental question: is this difference statistically meaningful, or just random variation?</p>
<p>These questions lie at the heart of <strong>Null-Hypothesis Significance Testing (NHST)</strong>, one of the most widely used – and most debated<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> – frameworks in statistics. The core idea is to start by assuming there is <em>no effect</em> (the “null hypothesis”) and then ask how surprising our observed data would be under that assumption. If the data would be very surprising under the null, we have evidence against it.</p>
<p>This chapter builds the NHST framework from the ground up, introducing key concepts like p-values, statistical power, and the critical issue of multiple testing. We’ll see how these tools help us distinguish real effects from random noise, while also understanding their limitations and why they’re often misused in practice.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finnish Terminology Reference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For Finnish-speaking students, here’s a reference table of key terms in this chapter:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>English</th>
<th>Finnish</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null Hypothesis</td>
<td>Nollahypoteesi</td>
<td>The default assumption of no effect</td>
</tr>
<tr class="even">
<td>Alternative Hypothesis</td>
<td>Vastahypoteesi, vaihtoehtoinen hypoteesi</td>
<td>What we hope to find evidence for</td>
</tr>
<tr class="odd">
<td>Simple Hypothesis</td>
<td>Yksinkertainen hypoteesi, pistehypoteesi</td>
<td>Hypothesis that completely specifies the distribution</td>
</tr>
<tr class="even">
<td>Composite Hypothesis</td>
<td>Yhdistetty hypoteesi</td>
<td>Hypothesis that specifies a range of values</td>
</tr>
<tr class="odd">
<td>Two-sided Test</td>
<td>Kaksisuuntainen testi</td>
<td>Test detecting differences in either direction</td>
</tr>
<tr class="even">
<td>One-sided Test</td>
<td>Yksitahoinen testi, yksisuuntainen testi</td>
<td>Test detecting differences in one direction</td>
</tr>
<tr class="odd">
<td>Rejection Region</td>
<td>Hylkäysalue</td>
<td>Set of outcomes leading to rejection</td>
</tr>
<tr class="even">
<td>Test Statistic</td>
<td>Testisuure</td>
<td>Summary of evidence against null</td>
</tr>
<tr class="odd">
<td>Critical Value</td>
<td>Kriittinen arvo</td>
<td>Threshold for rejection</td>
</tr>
<tr class="even">
<td>Type I Error</td>
<td>Hylkäysvirhe</td>
<td>False positive rejection</td>
</tr>
<tr class="odd">
<td>Type II Error</td>
<td>Hyväksymisvirhe</td>
<td>False negative (failure to detect)</td>
</tr>
<tr class="even">
<td>Power</td>
<td>Voima</td>
<td>Probability of detecting true effect</td>
</tr>
<tr class="odd">
<td>Power function</td>
<td>Voimafunktio</td>
<td>Power as function of parameter</td>
</tr>
<tr class="even">
<td>Size of a test</td>
<td>Testin koko</td>
<td>Maximum Type I error rate</td>
</tr>
<tr class="odd">
<td>Statistically significant</td>
<td>Tilastollisesti merkitsevä</td>
<td>Result unlikely under null</td>
</tr>
<tr class="even">
<td>Wald Test</td>
<td>Waldin testi</td>
<td>Test based on asymptotic normality</td>
</tr>
<tr class="odd">
<td>Paired test</td>
<td>Parittainen testi</td>
<td>Test for dependent samples</td>
</tr>
<tr class="even">
<td>Permutation test</td>
<td>Permutaatiotesti, satunnaistamistesti</td>
<td>Non-parametric test</td>
</tr>
<tr class="odd">
<td>Likelihood ratio statistic</td>
<td>Uskottavuusosamääräsuure</td>
<td>Ratio of likelihoods</td>
</tr>
<tr class="even">
<td>Multiple testing</td>
<td>Monitestaus</td>
<td>Running many tests simultaneously</td>
</tr>
<tr class="odd">
<td>False discovery rate (FDR)</td>
<td>Väärien löydösten osuus</td>
<td>Expected proportion of false positives</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="the-framework-of-hypothesis-testing" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="the-framework-of-hypothesis-testing"><span class="header-section-number">7.3</span> The Framework of Hypothesis Testing</h2>
<section id="null-and-alternative-hypotheses" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="null-and-alternative-hypotheses"><span class="header-section-number">7.3.1</span> Null and Alternative Hypotheses</h3>
<p>When we observe a difference between two groups or a pattern in data, we need a systematic way to determine whether this observation represents a genuine phenomenon or could simply be due to chance. Hypothesis testing provides this framework by setting up two competing explanations and evaluating the evidence against one of them.</p>
<div class="definition">
<p><strong>Null Hypothesis (<span class="math inline">H_0</span>)</strong>: A statement of “no effect” or “no difference.” It’s the default assumption we seek to find evidence against.</p>
<p><strong>Alternative Hypothesis (<span class="math inline">H_1</span>)</strong>: The statement we hope to find evidence for, typically representing the presence of an effect or difference.</p>
</div>
<p>For example, in our drug trial:</p>
<ul>
<li><span class="math inline">H_0</span>: The drug has the same efficacy as the placebo (no effect)</li>
<li><span class="math inline">H_1</span>: The drug’s efficacy differs from the placebo</li>
</ul>
<p>In the parametric framework we studied in Chapters 5-6, we can often formalize this by partitioning the parameter space <span class="math inline">\Theta</span> into two disjoint sets <span class="math inline">\Theta_0</span> and <span class="math inline">\Theta_1</span>, and testing:</p>
<p><span class="math display">H_0: \theta \in \Theta_0 \quad \text{versus} \quad H_1: \theta \in \Theta_1</span></p>
<p>The nature of the hypotheses determines the type of test:</p>
<div class="definition">
<p><strong>Simple hypothesis</strong>: A hypothesis that completely specifies the distribution, such as <span class="math inline">\theta = \theta_0</span>.</p>
<p><strong>Composite hypothesis</strong>: A hypothesis that specifies a range of values, such as <span class="math inline">\theta &gt; \theta_0</span> or <span class="math inline">\theta &lt; \theta_0</span>.</p>
<p><strong>Two-sided test</strong>: Tests <span class="math inline">H_0: \theta = \theta_0</span> versus <span class="math inline">H_1: \theta \neq \theta_0</span> (detects differences in either direction).</p>
<p><strong>One-sided test</strong>: Tests <span class="math inline">H_0: \theta \leq \theta_0</span> versus <span class="math inline">H_1: \theta &gt; \theta_0</span> (or the reverse), detecting differences in a specific direction.</p>
</div>
<p>Most scientific applications use two-sided tests, as we’re typically interested in detecting any difference from the null, not just differences in a predetermined direction.</p>
</section>
<section id="the-machinery-of-a-test" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="the-machinery-of-a-test"><span class="header-section-number">7.3.2</span> The Machinery of a Test</h3>
<p>Once we’ve specified our hypotheses, we need a systematic procedure for deciding between them based on the observed data. This involves defining what outcomes would lead us to reject the null hypothesis.</p>
<div class="definition">
<p>Let <span class="math inline">X</span> be a random variable with range <span class="math inline">\mathcal{X}</span>.</p>
<p><strong>Rejection Region</strong> (<span class="math inline">R \subset \mathcal{X}</span>): The subset of outcomes for which we will reject <span class="math inline">H_0</span>. If <span class="math inline">X \in R</span>, we reject the null hypothesis; otherwise, we retain it.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><strong>Test Statistic</strong> (<span class="math inline">T(X^n)</span>): A function of the data that summarizes the evidence against <span class="math inline">H_0</span>. Common examples include differences in sample means, ratios of variances, or correlation coefficients.</p>
<p><strong>Critical Value</strong> (<span class="math inline">c</span>): A threshold used to define the rejection region, often in terms of a test statistic, such as</p>
<p><span class="math display"> R = \{x: T(x) &gt; c\}  \quad \text{ or } \quad  R = \{x: |T(x)| &gt; c\} </span></p>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Testing Equality of Means
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose we have samples <span class="math inline">X_1, \ldots, X_n \sim F_X</span> and <span class="math inline">Y_1, \ldots, Y_m \sim F_Y</span> and want to test:</p>
<p><span class="math display">H_0: \mathbb{E}(X) = \mathbb{E}(Y) \quad \text{versus} \quad H_1: \mathbb{E}(X) \neq \mathbb{E}(Y)</span></p>
<p>The test statistic might be (a scaled version of) the difference in sample means: <span class="math display">T = \bar{X}_n - \bar{Y}_m</span></p>
<p>If <span class="math inline">|T|</span> is large, we have evidence against <span class="math inline">H_0</span>. The rejection region would be <span class="math inline">R = \{(x_1,\ldots,x_n,y_1,\ldots,y_m): |T| &gt; c\}</span> for some critical value <span class="math inline">c</span> chosen to control the error rates, as explained in the next section.</p>
</div>
</div>
</section>
<section id="two-ways-to-be-wrong-type-i-and-type-ii-errors" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="two-ways-to-be-wrong-type-i-and-type-ii-errors"><span class="header-section-number">7.3.3</span> Two Ways to Be Wrong: Type I and Type II Errors</h3>
<p>When we make a decision based on data, we can make two types of errors. Understanding these errors is crucial for properly designing and interpreting hypothesis tests.</p>
<div class="definition">
<p><strong>Type I Error</strong>: Rejecting <span class="math inline">H_0</span> when <span class="math inline">H_0</span> is true (false positive). The probability of Type I error is denoted <span class="math inline">\alpha</span>.</p>
<p><strong>Type II Error</strong>: Failing to reject <span class="math inline">H_0</span> when <span class="math inline">H_1</span> is true (false negative). The probability of Type II error is denoted <span class="math inline">\beta</span>.</p>
</div>
<p>The possible outcomes of a hypothesis test can be summarized as:</p>
<table class="table">
<colgroup>
<col style="width: 28%">
<col style="width: 35%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><span class="math inline">H_0</span> True</th>
<th style="text-align: center;"><span class="math inline">H_0</span> False</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">H_0</span> Retained</strong></td>
<td style="text-align: center;">✓ Correct (True Negative)</td>
<td style="text-align: center;">✗ Type II Error (False Negative)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">H_0</span> Rejected</strong></td>
<td style="text-align: center;">✗ Type I Error (False Positive)</td>
<td style="text-align: center;">✓ Correct (True Positive)</td>
</tr>
</tbody>
</table>
<p>Key quantities for characterizing test performance:</p>
<div class="definition">
<p><strong>Power Function</strong>: For a test with rejection region <span class="math inline">R</span>, the power function is: <span class="math display">\beta(\theta) = \mathbb{P}_\theta(X \in R)</span></p>
<p><strong>Size of a Test</strong>: The maximum probability of Type I error: <span class="math display">\alpha = \sup_{\theta \in \Theta_0} \beta(\theta)</span></p>
<p><strong>Level of a Test</strong>: A test has level <span class="math inline">\alpha</span> if its size is at most <span class="math inline">\alpha</span>.</p>
<p><strong>Power of a Test</strong>: The probability of correctly rejecting <span class="math inline">H_0</span> when it’s false: <span class="math display">\text{Power} = 1 - \beta = \mathbb{P}_\theta(\text{Reject } H_0 \mid \theta \in \Theta_1)</span></p>
</div>
<p>There’s an inherent trade-off between Type I and Type II errors: making it harder to commit a Type I error (lowering <span class="math inline">\alpha</span>) makes it easier to commit a Type II error (increasing <span class="math inline">\beta</span>), thus reducing power. Standard practice is to fix the Type I error rate at a conventional level (typically <span class="math inline">\alpha = 0.05</span>) and design the study to have adequate power (typically 80% or higher).</p>
<p>The relationship between these errors explains why we use asymmetric language: we “reject” or “fail to reject” <span class="math inline">H_0</span>, never “accept” it. Failing to reject doesn’t prove <span class="math inline">H_0</span> is true – we might simply lack power to detect the effect!</p>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255603-11-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-11-1" role="tab" aria-controls="tabset-1757255603-11-1" aria-selected="true" href="" aria-current="page">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-11-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-11-2" role="tab" aria-controls="tabset-1757255603-11-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-11-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-11-3" role="tab" aria-controls="tabset-1757255603-11-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255603-11-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255603-11-1-tab"><p>Think of hypothesis testing like a criminal trial. The null
hypothesis is “the defendant is innocent” – our default assumption until
proven otherwise. We only reject this assumption (convict) if the
evidence against innocence is very strong.</p><p>Just as a trial can go wrong in two ways, so can a hypothesis
test:</p><ul>
<li><strong>Type I Error</strong>: Convicting an innocent person (false
positive)</li>
<li><strong>Type II Error</strong>: Acquitting a guilty person (false
negative)</li>
</ul><p>A common illustration uses the <a href="https://en.wikipedia.org/wiki/Blade_Runner#Voight-Kampff_machine">Voight-Kampff
test</a>, which detects replicants (human-looking androids):</p><figure class="figure">
<img src="../images/07_errors_blade_runner.png" style="width:70.0%" alt="Type I and Type II Errors illustrated with replicant detection. Credits: gpt-4o." class="figure-img">
<figcaption aria-hidden="true">Type I and Type II Errors illustrated
with replicant detection. Credits: <a href="https://openai.com/index/introducing-4o-image-generation/">gpt-4o</a>.</figcaption>
</figure><p>With <span class="math inline">\(H_0\)</span>: “The subject is
human”:</p><ul>
<li><strong>Type I Error</strong>: Test says <code>REPLICANT</code> but
subject is actually human (wrongly flagged as android).</li>
<li><strong>Type II Error</strong>: Test says <code>HUMAN</code> but
subject is actually an android (missed detection).</li>
</ul></div><div id="tabset-1757255603-11-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-11-2-tab"><p>Let’s derive the power function for a concrete example. Consider
testing the mean of a normal distribution:</p><p><strong>Setup</strong>: We have
<span class="math inline">\(X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)\)</span>
with known variance <span class="math inline">\(\sigma^2\)</span>,
testing:
<span class="math display">\[H_0: \mu = 0 \quad \text{versus} \quad H_1: \mu \neq 0\]</span></p><p><strong>Test statistic</strong>: The sample mean
<span class="math inline">\(\bar{X} \sim \mathcal{N}(\mu, \sigma^2/n)\)</span>.
Under <span class="math inline">\(H_0\)</span> where
<span class="math inline">\(\mu = 0\)</span>:
<span class="math display">\[Z = \frac{\sqrt{n}\bar{X}}{\sigma} \sim \mathcal{N}(0, 1)\]</span></p><p><strong>Decision rule</strong>: For a level
<span class="math inline">\(\alpha\)</span> test, we reject
<span class="math inline">\(H_0\)</span> when
<span class="math inline">\(|Z| &gt; z_{\alpha/2}\)</span>, where
<span class="math inline">\(z_{\alpha/2}\)</span> is the
<span class="math inline">\((1-\alpha/2)\)</span> quantile of the
standard normal distribution.</p><p><strong>Power function</strong>: For any true value
<span class="math inline">\(\mu\)</span>, the power is:
<span class="math display">\[\beta(\mu) = \mathbb{P}_\mu(\text{Reject } H_0) = \mathbb{P}_\mu(|Z| &gt; z_{\alpha/2})\]</span></p><p>Under the true <span class="math inline">\(\mu\)</span>, the test
statistic follows
<span class="math inline">\(Z \sim \mathcal{N}(\sqrt{n}\mu/\sigma, 1)\)</span>.
Therefore:
<span class="math display">\[\beta(\mu) = \mathbb{P}\left(\left|\mathcal{N}\left(\frac{\sqrt{n}\mu}{\sigma}, 1\right)\right| &gt; z_{\alpha/2}\right)\]</span></p><p>This probability depends on three key factors:</p><ul>
<li><strong>Effect size</strong>:
<span class="math inline">\(\delta = \mu/\sigma\)</span> (standardized
distance from null)<br>
</li>
<li><strong>Sample size</strong>: Larger
<span class="math inline">\(n\)</span> → higher power</li>
<li><strong>Significance level</strong>: Larger
<span class="math inline">\(\alpha\)</span> → higher power (but more
Type I errors)</li>
</ul><p>Let’s visualize this power function for
<span class="math inline">\(H_0: \mu = 0\)</span>,
<span class="math inline">\(\sigma = 1\)</span>,
<span class="math inline">\(n = 25\)</span>,
<span class="math inline">\(\alpha = 0.05\)</span>:</p><div id="b20fb40f" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="1">
<div class="cell-output cell-output-display">
<p><img src="07-hypothesis-testing_files/figure-html/cell-2-output-1.png"></p>
</div>
</div><p>The mirrored S-shaped curve demonstrates that power is minimized at
the boundary of <span class="math inline">\(H_0\)</span> (here,
<span class="math inline">\(\mu = 0\)</span>) and increases
monotonically as the true parameter moves away from the null value,
making detecting the difference easier.</p></div><div id="tabset-1757255603-11-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-11-3-tab"><p>Let’s simulate hypothesis testing to see Type I and Type II errors in
action. We’ll test <span class="math inline">\(H_0: \mu = 0\)</span>
using one-sample t-tests on data from normal distributions. By
generating many datasets under different true means, we can observe the
empirical error rates:</p><div id="4122c9f7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" data-code-fold="true" data-code-summary="Show simulation code"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate multiple hypothesis tests</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n_tests <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Scenario 1: H0 is true (μ = 0)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># We should reject H0 about 5% of the time (Type I error rate)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>type_i_errors <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_tests):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n_samples)  <span class="co"># H0 true: μ = 0</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    t_stat, p_value <span class="op">=</span> stats.ttest_1samp(sample, <span class="dv">0</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p_value <span class="op">&lt;</span> alpha:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        type_i_errors <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type I Error Rate (H₀ true, μ=0):"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Theoretical: </span><span class="sc">{</span>alpha<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Observed:    </span><span class="sc">{</span>type_i_errors<span class="op">/</span>n_tests<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Scenario 2: H0 is false (μ = 0.5)</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Power = probability of correctly rejecting H0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>true_mu <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>rejections <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_tests):</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> np.random.normal(true_mu, <span class="dv">1</span>, n_samples)  <span class="co"># H0 false: μ = 0.5</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    t_stat, p_value <span class="op">=</span> stats.ttest_1samp(sample, <span class="dv">0</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p_value <span class="op">&lt;</span> alpha:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        rejections <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>power <span class="op">=</span> rejections <span class="op">/</span> n_tests</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>type_ii_error_rate <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> power</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">When H₀ is false (true μ=</span><span class="sc">{</span>true_mu<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Power (correct rejection):     </span><span class="sc">{</span>power<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Type II Error Rate (miss):     </span><span class="sc">{</span>type_ii_error_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Type I Error Rate (H₀ true, μ=0):
  Theoretical: 0.050
  Observed:    0.056

When H₀ is false (true μ=0.5):
  Power (correct rejection):     0.750
  Type II Error Rate (miss):     0.250</code></pre>
</div>
</div><p>Factors affecting power:</p><ul>
<li><strong>Effect size</strong>: Larger true differences from
<span class="math inline">\(H_0\)</span> → higher power</li>
<li><strong>Sample size</strong>: More data → higher power</li>
<li><strong>Variability</strong>: Lower variance → higher power</li>
<li><strong>Significance level</strong>: Higher
<span class="math inline">\(\alpha\)</span> → higher power (but more
Type I errors)</li>
</ul></div></div></div>
</section>
<section id="the-wald-test" class="level3" data-number="7.3.4">
<h3 data-number="7.3.4" class="anchored" data-anchor-id="the-wald-test"><span class="header-section-number">7.3.4</span> The Wald Test</h3>
<p>The Wald test is one of the most widely used hypothesis tests in statistics. It leverages the asymptotic normality of estimators that we studied in Chapters 5-6, providing a general framework for testing hypotheses about parameters.</p>
<p>The key insight: If our estimator <span class="math inline">\hat{\theta}</span> is approximately normal (which many estimators are for large samples), we can use this normality to construct a test statistic with a known distribution under the null hypothesis.</p>
<div class="definition">
<p><strong>The Wald Test</strong>: For testing <span class="math inline">H_0: \theta = \theta_0</span> versus <span class="math inline">H_1: \theta \neq \theta_0</span>:</p>
<p>Assume that <span class="math inline">\hat{\theta}</span> is asymptotically normal: <span class="math display">\frac{(\hat{\theta} - \theta_0)}{\widehat{\text{se}}} \rightsquigarrow \mathcal{N}(0,1)</span></p>
<p>The size <span class="math inline">\alpha</span> Wald test rejects <span class="math inline">H_0</span> when <span class="math inline">|W| &gt; z_{\alpha/2}</span>, where: <span class="math display">W = \frac{\hat{\theta} - \theta_0}{\widehat{\text{se}}}</span></p>
</div>
<p><strong>Key Properties of the Wald Test:</strong></p>
<ol type="1">
<li><p><strong>Correct error rate</strong>: For large samples, the test has (approximately) the desired Type I error rate <span class="math inline">\alpha</span></p></li>
<li><p><strong>Power increases with</strong>:</p>
<ul>
<li>Larger effect size (bigger difference from <span class="math inline">\theta_0</span>)</li>
<li>Larger sample size (more data)</li>
<li>Smaller variance (less noise)</li>
</ul></li>
<li><p><strong>Duality with confidence intervals</strong>: The Wald test rejects <span class="math inline">H_0: \theta = \theta_0</span> if and only if <span class="math inline">\theta_0</span> is outside the <span class="math inline">(1-\alpha)</span> confidence interval</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="theorem" name="Formal Properties of the Wald Test">
<ol type="1">
<li><p><strong>Asymptotic size</strong>: As <span class="math inline">n \to \infty</span>, <span class="math inline">\mathbb{P}_{\theta_0}(|W| &gt; z_{\alpha/2}) \to \alpha</span></p></li>
<li><p><strong>Power function</strong>: When the true parameter is <span class="math inline">\theta_* \neq \theta_0</span>, the power is: <span class="math display">\text{Power}(\theta_*) = 1 - \Phi\left(\frac{\theta_0 - \theta_*}{\widehat{\text{se}}} + z_{\alpha/2}\right) + \Phi\left(\frac{\theta_0 - \theta_*}{\widehat{\text{se}}} - z_{\alpha/2}\right)</span> where <span class="math inline">\Phi</span> is the standard normal CDF.</p></li>
<li><p><strong>Confidence interval duality</strong>: Reject <span class="math inline">H_0</span> if and only if <span class="math inline">\theta_0 \notin [\hat{\theta} - z_{\alpha/2} \cdot \widehat{\text{se}}, \hat{\theta} + z_{\alpha/2} \cdot \widehat{\text{se}}]</span></p></li>
</ol>
</div>
</div>
</div>
</div>
<p>This last property provides a powerful duality: every confidence interval can be inverted to give a hypothesis test, and vice versa. This is why confidence intervals are often more informative than p-values alone – they show all parameter values that wouldn’t be rejected.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Comparing Two Means
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Wald test extends naturally to comparing means from two populations. For independent samples <span class="math inline">X_1, \ldots, X_m</span> and <span class="math inline">Y_1, \ldots, Y_n</span> with means <span class="math inline">\mu_1</span> and <span class="math inline">\mu_2</span>:</p>
<p>Testing <span class="math inline">H_0: \mu_1 = \mu_2</span> (or <span class="math inline">\delta = \mu_1 - \mu_2 = 0</span>):</p>
<ul>
<li>Estimator: <span class="math inline">\hat{\delta} = \bar{X} - \bar{Y}</span></li>
<li>Standard error: <span class="math inline">\widehat{\text{se}} = \sqrt{s_1^2/m + s_2^2/n}</span></li>
<li>Test statistic: <span class="math inline">W = \hat{\delta} / \widehat{\text{se}}</span></li>
</ul>
<p>This is the basis for the famous two-sample t-test (which uses a t-distribution for small samples instead of the normal approximation).</p>
</div>
</div>
</section>
<section id="statistical-and-scientific-significance" class="level3" data-number="7.3.5">
<h3 data-number="7.3.5" class="anchored" data-anchor-id="statistical-and-scientific-significance"><span class="header-section-number">7.3.5</span> Statistical and Scientific Significance</h3>
<p>When <span class="math inline">H_0</span> is rejected, the result is called <strong>statistically significant</strong>. This does <strong>not</strong> mean that the result has any practical or scientific relevance! The effect found could be very tiny and negligible for all practical purpose.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Statistical vs.&nbsp;Scientific Significance
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Statistical significance</strong> ≠ <strong>Scientific significance</strong></p>
<p>Let’s assume <span class="math inline">\alpha = 0.05</span> and <span class="math inline">\theta_0 = 0</span>. In this case, we reject the null if the 95 % confidence interval (CI) excludes 0. A result can be:</p>
<ul>
<li><strong>Statistically significant but scientifically trivial</strong>: The CI excludes 0 but is very close to it (where “very close” depends on the domain).</li>
<li><strong>Statistically non-significant but scientifically important</strong>: The CI includes 0 but also includes large, meaningful effects.</li>
</ul>
<p>Always report confidence intervals alongside p-values (described in the next section) to show both statistical and practical significance!</p>
</div>
</div>
<p>Let’s visualize this distinction:</p>
<div id="0777e79a" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="3">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Two example confidence intervals</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 1: Statistically significant but small effect</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>theta0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>ci1_lower, ci1_upper <span class="op">=</span> <span class="fl">0.001</span>, <span class="fl">0.003</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>ci1_center <span class="op">=</span> (ci1_lower <span class="op">+</span> ci1_upper) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>ax1.axvline(theta0, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'H₀: θ = 0'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>ax1.barh(<span class="dv">0</span>, ci1_upper <span class="op">-</span> ci1_lower, left<span class="op">=</span>ci1_lower, height<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>ax1.plot(ci1_center, <span class="dv">0</span>, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'Estimate'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Statistically Significant</span><span class="ch">\n</span><span class="st">but Tiny Effect'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>ax1.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ax1.set_yticks([])</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 2: Not significant but large potential effect</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>ci2_lower, ci2_upper <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>ci2_center <span class="op">=</span> (ci2_lower <span class="op">+</span> ci2_upper) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax2.axvline(theta0, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'H₀: θ = 0'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax2.barh(<span class="dv">0</span>, ci2_upper <span class="op">-</span> ci2_lower, left<span class="op">=</span>ci2_lower, height<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>ax2.plot(ci2_center, <span class="dv">0</span>, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'Estimate'</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax2.set_xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Not Statistically Significant</span><span class="ch">\n</span><span class="st">but Large Potential Effect'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax2.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>ax2.set_yticks([])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="07-hypothesis-testing_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The visualization demonstrates the CI-test duality: using significance level <span class="math inline">\alpha = 0.05</span>, we construct a 95% CI (since <span class="math inline">1 - \alpha = 0.95</span>). We reject <span class="math inline">H_0: \theta = 0</span> if and only if 0 lies outside this interval.</p>
<p><strong>Left panel</strong>: The 95% CI excludes 0, so we reject <span class="math inline">H_0</span> at the 5% level. But the effect is tiny (0.001 to 0.003) – statistically significant but practically negligible.</p>
<p><strong>Right panel</strong>: The 95% CI includes 0, so we fail to reject <span class="math inline">H_0</span>. However, the interval also includes large values (up to 2.5) – we simply lack precision to determine if there’s an effect.</p>
</section>
</section>
<section id="the-p-value-a-continuous-measure-of-evidence" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="the-p-value-a-continuous-measure-of-evidence"><span class="header-section-number">7.4</span> The p-value: A Continuous Measure of Evidence</h2>
<section id="understanding-the-p-value" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="understanding-the-p-value"><span class="header-section-number">7.4.1</span> Understanding the p-value</h3>
<p>Simply reporting “reject <span class="math inline">H_0</span>” or “retain <span class="math inline">H_0</span>” isn’t very informative – the result depends entirely on the chosen significance level <span class="math inline">\alpha</span>. What if we could provide a continuous measure of the strength of evidence against the null hypothesis? This is exactly what the p-value provides.</p>
<div class="definition">
<p>The <strong>p-value</strong> is the probability, calculated under <span class="math inline">H_0</span>, of observing a test statistic value at least as extreme as the one actually observed:</p>
<p><span class="math display">\text{p-value} = \mathbb{P}_{H_0}(T \text{ as extreme or more extreme than } T_\text{obs})</span></p>
<p>Equivalently, it’s the smallest significance level <span class="math inline">\alpha</span> at which we would reject <span class="math inline">H_0</span>: <span class="math display">\text{p-value} = \inf\{\alpha : T(X^n) \in R_\alpha\}</span></p>
<p>where <span class="math inline">R_\alpha</span> is the rejection region for a size <span class="math inline">\alpha</span> test.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why are these two definitions equivalent?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The rejection region <span class="math inline">R_\alpha</span> contains the most extreme <span class="math inline">\alpha</span> proportion of possible test statistics. If our observed statistic is “more extreme than 2.1% of possible values” (p = 0.021), then:</p>
<ul>
<li>We’d reject for any <span class="math inline">\alpha \geq 0.021</span> (our observation is extreme enough)</li>
<li>We’d fail to reject for any <span class="math inline">\alpha &lt; 0.021</span> (our observation isn’t extreme enough)<br>
</li>
<li>Thus 0.021 is exactly the boundary – the smallest <span class="math inline">\alpha</span> for which we’d reject</li>
</ul>
<p>The p-value acts as a threshold: it tells us both how extreme our observation is AND the critical significance level where our decision switches.</p>
</div>
</div>
</div>
<p><strong>Key properties of p-values:</strong></p>
<ol type="1">
<li><strong>Range</strong>: p-values lie between 0 and 1</li>
<li><strong>Interpretation</strong>: Small p-value = strong evidence against <span class="math inline">H_0</span></li>
<li><strong>Decision rule</strong>: Reject <span class="math inline">H_0</span> if p-value <span class="math inline">&lt; \alpha</span></li>
<li><strong>Under <span class="math inline">H_0</span></strong>: The p-value follows a <span class="math inline">\text{Uniform}(0,1)</span> distribution</li>
</ol>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: p-value for the Wald Test
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the Wald test we studied earlier, if <span class="math inline">w = (\hat{\theta} - \theta_0)/\widehat{\text{se}}</span> is the observed Wald statistic, then: <span class="math display">\text{p-value} = \mathbb{P}(|Z| &gt; |w|) = 2\Phi(-|w|)</span> where <span class="math inline">Z \sim \mathcal{N}(0,1)</span> and <span class="math inline">\Phi</span> is the standard normal CDF.</p>
<p>This formula works because under <span class="math inline">H_0</span>, the Wald statistic <span class="math inline">W \sim \mathcal{N}(0,1)</span> asymptotically, so we calculate the probability of seeing a value as extreme as our observed <span class="math inline">w</span> from a standard normal distribution.</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255603-499-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-499-1" role="tab" aria-controls="tabset-1757255603-499-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-499-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-499-2" role="tab" aria-controls="tabset-1757255603-499-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-499-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-499-3" role="tab" aria-controls="tabset-1757255603-499-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255603-499-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255603-499-1-tab"><p>The p-value is often called a “surprise index.” It answers the
question: “If there were truly no effect (if
<span class="math inline">\(H_0\)</span> were true), how likely would we
be to see a result at least as extreme as the one we actually
observed?”</p><p>Think of it this way: Imagine you suspect a coin is biased. You flip
it 10 times and get 9 heads. The p-value asks: “If this were actually a
fair coin, what’s the probability of getting 9 or more heads in 10
flips?” If that probability is very small, you have strong evidence the
coin isn’t fair.</p><p>A small p-value means our data would be very surprising under the
null hypothesis, providing evidence against it. A large p-value means
our data is consistent with the null hypothesis (though this doesn’t
prove the null is true! Maybe we simply didn’t collect enough data).</p></div><div id="tabset-1757255603-499-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-499-2-tab"><p>Let <span class="math inline">\(T(X^n)\)</span> be the test statistic
and <span class="math inline">\(t_\text{obs} = T(x^n)\)</span> be its
observed value from the data. The p-value formalizes the “surprise
index” under the null hypothesis
<span class="math inline">\(H_0\)</span>.</p><p><strong>1. Simple Null Hypothesis
(<span class="math inline">\(H_0: \theta = \theta_0\)</span>)</strong></p><p>For a simple null, the p-value is the probability of observing a test
statistic at least as extreme as
<span class="math inline">\(t_\text{obs}\)</span>. The definition of
“extreme” depends on the alternative hypothesis
<span class="math inline">\(H_1\)</span>:</p><ul>
<li><strong>Right-tailed test</strong>
(<span class="math inline">\(H_1: \theta &gt; \theta_0\)</span>): Extreme
means large values of <span class="math inline">\(T\)</span>.
<span class="math display">\[ \text{p-value} = \mathbb{P}_{\theta_0}(T(X^n) \geq t_\text{obs}) \]</span></li>
<li><strong>Left-tailed test</strong>
(<span class="math inline">\(H_1: \theta &lt; \theta_0\)</span>): Extreme
means small values of <span class="math inline">\(T\)</span>.
<span class="math display">\[ \text{p-value} = \mathbb{P}_{\theta_0}(T(X^n) \leq t_\text{obs}) \]</span></li>
<li><strong>Two-tailed test</strong>
(<span class="math inline">\(H_1: \theta \neq \theta_0\)</span>):
Extreme means large values of <span class="math inline">\(|T|\)</span>
(or similar symmetric measure).
<span class="math display">\[ \text{p-value} = \mathbb{P}_{\theta_0}(|T(X^n)| \geq |t_\text{obs}|) \]</span></li>
</ul><p><strong>2. Composite Null Hypothesis
(<span class="math inline">\(H_0: \theta \in \Theta_0\)</span>)</strong></p><p>When the null hypothesis is composite (e.g.,
<span class="math inline">\(H_0: \mu \le 0\)</span>), there isn’t a
single distribution under <span class="math inline">\(H_0\)</span>. We
need to find the probability of an extreme result under the “worst-case”
scenario within <span class="math inline">\(\Theta_0\)</span> – the one
that makes our data look least surprising. This is achieved by taking
the supremum (least upper bound) of the probability over all possible
parameter values in <span class="math inline">\(\Theta_0\)</span>.</p><p>For a right-tailed test, the formula is:
<span class="math display">\[ \text{p-value} = \sup_{\theta \in \Theta_0} \mathbb{P}_\theta(T(X^n) \geq t_\text{obs}) \]</span></p><p>This ensures that if we reject when p-value
<span class="math inline">\(&lt; \alpha\)</span>, the Type I error rate is
controlled and does not exceed
<span class="math inline">\(\alpha\)</span> for any
<span class="math inline">\(\theta \in \Theta_0\)</span>. For many
standard tests, this supremum occurs at the boundary of
<span class="math inline">\(\Theta_0\)</span> (e.g., at
<span class="math inline">\(\mu=0\)</span> for
<span class="math inline">\(H_0: \mu \le 0\)</span>).</p></div><div id="tabset-1757255603-499-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-499-3-tab"><p>Let’s visualize how the p-value is calculated for a two-sided test.
We’ll use the Wald test as our example, where the test statistic follows
a standard normal distribution under
<span class="math inline">\(H_0\)</span>.</p><p>For a two-sided test (e.g.,
<span class="math inline">\(H_0: \theta = \theta_0\)</span> vs
<span class="math inline">\(H_1: \theta \neq \theta_0\)</span>),
“extreme” means far from zero in either direction. The p-value is the
total probability in both tails beyond our observed test statistic:</p><div id="c260951e" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Observed test statistic value</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>w_obs <span class="op">=</span> <span class="fl">2.3</span>  <span class="co"># Our observed Wald statistic</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the standard normal distribution</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>z_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">300</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>pdf_values <span class="op">=</span> stats.norm.pdf(z_values)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate p-value</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(w_obs))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, pdf_values, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Standard Normal'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade the rejection regions (tails)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>tail_right <span class="op">=</span> z_values[z_values <span class="op">&gt;=</span> <span class="bu">abs</span>(w_obs)]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>tail_left <span class="op">=</span> z_values[z_values <span class="op">&lt;=</span> <span class="op">-</span><span class="bu">abs</span>(w_obs)]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tail_right, <span class="dv">0</span>, stats.norm.pdf(tail_right), </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                  color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="ss">f'Right tail'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tail_left, <span class="dv">0</span>, stats.norm.pdf(tail_left), </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>                  color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="ss">f'Left tail'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the observed values</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.axvline(w_obs, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Observed: w = </span><span class="sc">{</span>w_obs<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="op">-</span>w_obs, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Add annotations</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.text(w_obs <span class="op">+</span> <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="ss">f'α/2 = </span><span class="sc">{</span>p_value<span class="op">/</span><span class="dv">2</span><span class="sc">:.4f}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="op">-</span>w_obs <span class="op">-</span> <span class="fl">0.8</span>, <span class="fl">0.05</span>, <span class="ss">f'α/2 = </span><span class="sc">{</span>p_value<span class="op">/</span><span class="dv">2</span><span class="sc">:.4f}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Test Statistic Value'</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'p-value Calculation: p = </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.45</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For observed test statistic w = </span><span class="sc">{</span>w_obs<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value = 2 * P(Z &gt; |</span><span class="sc">{</span>w_obs<span class="sc">}</span><span class="ss">|) = </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation: If H₀ were true, we'd see a test statistic"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"this extreme or more extreme only </span><span class="sc">{</span>p_value<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% of the time."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="07-hypothesis-testing_files/figure-html/cell-5-output-1.png"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>For observed test statistic w = 2.3:
p-value = 2 * P(Z &gt; |2.3|) = 0.0214

Interpretation: If H₀ were true, we'd see a test statistic
this extreme or more extreme only 2.14% of the time.</code></pre>
</div>
</div></div></div></div>
</section>
<section id="how-to-interpret-p-values-and-how-not-to" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="how-to-interpret-p-values-and-how-not-to"><span class="header-section-number">7.4.2</span> How to Interpret p-values (and How Not To)</h3>
<p>The p-value is one of the most misunderstood concepts in statistics. Let’s clarify what it is and isn’t.</p>
<p><strong>What the p-value IS:</strong></p>
<ul>
<li><strong>The p-value IS the probability</strong>, computed under <span class="math inline">H_0</span>, of observing data as extreme or more extreme than what we actually observed</li>
<li><strong>The p-value IS a measure of evidence</strong> against <span class="math inline">H_0</span> – smaller values indicate stronger evidence</li>
<li><strong>The p-value IS the answer</strong> to: “If <span class="math inline">H_0</span> were true, how surprising would our data be?”</li>
<li><strong>The p-value IS useful</strong> for deciding whether to reject <span class="math inline">H_0</span> at any given significance level</li>
</ul>
<p>A p-value of 0.02 means: If the null hypothesis were true, we’d see data this extreme or more extreme only 2% of the time. That’s fairly surprising, suggesting the null might be false.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Common p-value Misinterpretations
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>A p-value is NOT the probability that the null hypothesis is true.</strong></p>
<ul>
<li>Wrong: “p = 0.03 means there’s a 3% chance the null hypothesis is true”</li>
<li>The p-value is <span class="math inline">\mathbb{P}(\text{data} | H_0)</span>, not <span class="math inline">\mathbb{P}(H_0 | \text{data})</span></li>
<li>Computing <span class="math inline">\mathbb{P}(H_0 | \text{data})</span> requires Bayesian methods (Chapter 8)</li>
</ul>
<p><strong>A large p-value is NOT strong evidence that the null hypothesis is true.</strong></p>
<ul>
<li>A large p-value could mean:
<ol type="1">
<li><span class="math inline">H_0</span> is true, or</li>
<li><span class="math inline">H_0</span> is false but our test has low power to detect the effect</li>
</ol></li>
<li>Never conclude “we accept <span class="math inline">H_0</span>” based on a large p-value</li>
</ul>
<p><strong>Statistical significance is NOT the same as practical significance.</strong></p>
<ul>
<li>With enough data, tiny meaningless effects can become “statistically significant”</li>
<li>Always examine the effect size (e.g., via confidence intervals) to judge practical importance</li>
<li>Example: A drug that lowers blood pressure by 0.1 mmHg might be statistically significant with n=10,000 but clinically meaningless</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Standard p-value Interpretation Scales
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The interpretation of p-values varies significantly by field and context. Here’s a common scale used in many fields:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">p-value</th>
<th style="text-align: left;">Evidence against <span class="math inline">H_0</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">&lt; 0.01</td>
<td style="text-align: left;">Strong evidence</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.01 - 0.05</td>
<td style="text-align: left;">Positive evidence</td>
</tr>
<tr class="odd">
<td style="text-align: left;">&gt; 0.05</td>
<td style="text-align: left;">Little or no evidence</td>
</tr>
</tbody>
</table>
<p><strong>Field-specific standards:</strong></p>
<ul>
<li><strong>Medicine/Psychology</strong>: Often use <span class="math inline">\alpha = 0.05</span> as the standard.</li>
<li><strong>Genomics</strong>: Use much stricter thresholds (e.g., <span class="math inline">5 × 10^{-8}</span>) due to multiple testing, as we will see below.</li>
<li><strong>Particle Physics</strong>: Extremely strict standards for discoveries:
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Higgs_boson#Discovery_of_candidate_boson_at_CERN">Higgs boson discovery</a> required a “5-sigma” result (<span class="math inline">p &lt; 3 × 10^{-7}</span>).</li>
<li>This corresponds to less than 1 in 3.5 million chance of a false positive.</li>
</ul></li>
</ul>
<p>These thresholds are conventions, not laws of nature. The appropriate threshold depends on the consequences of Type I and Type II errors in your specific context.</p>
</div>
</div>
</div>
<div class="theorem">
<p>If the test statistic has a continuous distribution, then under <span class="math inline">H_0: \theta = \theta_0</span>, the p-value has a Uniform(0,1) distribution. Therefore, if we reject <span class="math inline">H_0</span> when the p-value is less than <span class="math inline">\alpha</span>, the probability of a Type I error is exactly <span class="math inline">\alpha</span>.</p>
</div>
<p>This property means that p-values “work correctly” – under the null hypothesis, you’ll get a p-value less than 0.05 exactly 5% of the time, a p-value less than 0.01 exactly 1% of the time, and so on.</p>
</section>
<section id="applying-p-values-classification-algorithm-comparison" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="applying-p-values-classification-algorithm-comparison"><span class="header-section-number">7.4.3</span> Applying p-values: Classification Algorithm Comparison</h3>
<p>Let’s apply the Wald test and p-value concepts to a practical problem that appears frequently in machine learning and data science: comparing the performance of two classification algorithms. This example (from AoS Example 10.7) illustrates both independent and paired testing scenarios.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Comparing Algorithms with Independent Test Sets
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose we test two classification algorithms on different, independent test sets:</p>
<ul>
<li>Algorithm 1: Test set of size <span class="math inline">m</span>, makes <span class="math inline">X</span> errors</li>
<li>Algorithm 2: Test set of size <span class="math inline">n</span>, makes <span class="math inline">Y</span> errors</li>
</ul>
<p>Then <span class="math inline">X \sim \text{Binomial}(m, p_1)</span> and <span class="math inline">Y \sim \text{Binomial}(n, p_2)</span>, where <span class="math inline">p_1</span> and <span class="math inline">p_2</span> are the true error rates.</p>
<p>We want to test: <span class="math display">H_0: p_1 = p_2 \quad \text{versus} \quad H_1: p_1 \neq p_2</span></p>
<p>Or equivalently, <span class="math inline">H_0: \delta = 0</span> versus <span class="math inline">H_1: \delta \neq 0</span>, where <span class="math inline">\delta = p_1 - p_2</span>.</p>
<p><strong>The Wald Test Approach:</strong></p>
<p>The MLE is <span class="math inline">\hat{\delta} = \hat{p}_1 - \hat{p}_2</span> where <span class="math inline">\hat{p}_1 = X/m</span> and <span class="math inline">\hat{p}_2 = Y/n</span>.</p>
<p>The estimated standard error is: <span class="math display">\widehat{\text{se}} = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{m} + \frac{\hat{p}_2(1-\hat{p}_2)}{n}}</span></p>
<p>The Wald test statistic is: <span class="math display">W = \frac{\hat{\delta} - 0}{\widehat{\text{se}}} = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{m} + \frac{\hat{p}_2(1-\hat{p}_2)}{n}}}</span></p>
<p>For a size <span class="math inline">\alpha</span> test, we reject <span class="math inline">H_0</span> when <span class="math inline">|W| &gt; z_{\alpha/2}</span>, and the p-value is <span class="math inline">2\Phi(-|W|)</span>.</p>
<p><strong>Numerical Example:</strong> Let <span class="math inline">m = n = 500</span>, with Algorithm 1 making 75 errors and Algorithm 2 making 100 errors:</p>
<div id="af51ebfd" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent test sets</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>m, n <span class="op">=</span> <span class="dv">500</span>, <span class="dv">500</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>p1_hat, p2_hat <span class="op">=</span> <span class="fl">0.15</span>, <span class="fl">0.20</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard error</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p1_hat<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p1_hat)<span class="op">/</span>m <span class="op">+</span> p2_hat<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p2_hat)<span class="op">/</span>n)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test statistic</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> (p1_hat <span class="op">-</span> p2_hat) <span class="op">/</span> se</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># p-value (two-sided)</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Error rates: Algorithm 1 = </span><span class="sc">{</span>p1_hat<span class="sc">:.2%}</span><span class="ss">, Algorithm 2 = </span><span class="sc">{</span>p2_hat<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Difference: </span><span class="sc">{</span>p1_hat <span class="op">-</span> p2_hat<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard error: </span><span class="sc">{</span>se<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wald statistic: </span><span class="sc">{</span>W<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Conclusion: </span><span class="sc">{</span><span class="st">'Significant difference'</span> <span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="st">'No significant difference'</span><span class="sc">}</span><span class="ss"> at α = 0.05"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Error rates: Algorithm 1 = 15.00%, Algorithm 2 = 20.00%
Difference: -5.00%
Standard error: 0.0240
Wald statistic: -2.085
p-value: 0.0371

Conclusion: Significant difference at α = 0.05</code></pre>
</div>
</div>
<p>This 5 percentage point difference is statistically significant at α = 0.05. But the significance depends critically on sample size. As a numerical illustration:</p>
<ul>
<li>With <span class="math inline">m = n = 100</span>: Same 5% difference gives <span class="math inline">|W| \approx 0.93</span>, p-value ≈ 0.35 (not significant)</li>
<li>With <span class="math inline">m = n = 500</span>: <span class="math inline">|W| \approx 2.09</span>, p-value ≈ 0.037 (significant)<br>
</li>
<li>With <span class="math inline">m = n = 1000</span>: <span class="math inline">|W| \approx 2.95</span>, p-value ≈ 0.003 (highly significant)</li>
</ul>
<p>Let’s visualize how sample size affects our ability to detect this difference:</p>
<div id="1ccad61b" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="07-hypothesis-testing_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Key Insight:</strong> The same 5% difference in error rates can be:</p>
<ul>
<li>Non-significant with small samples (low power)</li>
<li>Highly significant with large samples (high power)</li>
</ul>
<p>This illustrates why reporting effect sizes (the actual difference) alongside p-values is crucial!</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Comparing Algorithms with Paired Test Sets
</div>
</div>
<div class="callout-body-container callout-body">
<p>Often we test both algorithms on the <strong>same</strong> test set. This is more efficient but requires a different analysis because the results are no longer independent.</p>
<p><strong>Data Structure:</strong> For each test instance <span class="math inline">i = 1, \ldots, n</span>:</p>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Test Case</th>
<th style="text-align: center;">Algorithm 1 (<span class="math inline">X_i</span>)</th>
<th style="text-align: center;">Algorithm 2 (<span class="math inline">Y_i</span>)</th>
<th style="text-align: center;">Difference (<span class="math inline">D_i = X_i - Y_i</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1 (correct)</td>
<td style="text-align: center;">0 (incorrect)</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1 (correct)</td>
<td style="text-align: center;">1 (correct)</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0 (incorrect)</td>
<td style="text-align: center;">1 (correct)</td>
<td style="text-align: center;">-1</td>
</tr>
<tr class="even">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
<tr class="odd">
<td style="text-align: center;">n</td>
<td style="text-align: center;"><span class="math inline">X_n</span></td>
<td style="text-align: center;"><span class="math inline">Y_n</span></td>
<td style="text-align: center;"><span class="math inline">D_n</span></td>
</tr>
</tbody>
</table>
<p>The key insight: We can no longer treat <span class="math inline">X</span> and <span class="math inline">Y</span> as independent because they’re tested on the same instances.</p>
<p><strong>The Paired Test Approach:</strong></p>
<p>Define <span class="math inline">D_i = X_i - Y_i</span> for each test instance. Then: <span class="math display">\delta = \mathbb{E}(D_i) = \mathbb{E}(X_i) - \mathbb{E}(Y_i) = \mathbb{P}(X_i = 1) - \mathbb{P}(Y_i = 1)</span></p>
<p>We test <span class="math inline">H_0: \delta = 0</span> versus <span class="math inline">H_1: \delta \neq 0</span>.</p>
<p>The nonparametric plug-in estimate is <span class="math inline">\hat{\delta} = \bar{D} = \frac{1}{n}\sum_{i=1}^n D_i</span>.</p>
<p>The standard error is <span class="math inline">\widehat{\text{se}}(\hat{\delta}) = S/\sqrt{n}</span>, where <span class="math inline">S^2 = \frac{1}{n}\sum_{i=1}^n (D_i - \bar{D})^2</span>.</p>
<p>The Wald test statistic is <span class="math inline">W = \hat{\delta}/\widehat{\text{se}}</span> and we reject <span class="math inline">H_0</span> if <span class="math inline">|W| &gt; z_{\alpha/2}</span>.</p>
<p>This is called a <strong>paired comparison</strong> or <strong>paired test</strong>.</p>
<div id="8c90c1ec" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate paired comparison</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># test set size</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># True probabilities of correct classification</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>p1_true <span class="op">=</span> <span class="fl">0.85</span>  <span class="co"># Algorithm 1</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>p2_true <span class="op">=</span> <span class="fl">0.80</span>  <span class="co"># Algorithm 2</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate correlated outcomes (algorithms often agree)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>correlation <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate correlated binary outcomes</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, correlation], [correlation, <span class="dv">1</span>]]</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>latent <span class="op">=</span> multivariate_normal.rvs([<span class="dv">0</span>, <span class="dv">0</span>], cov, size<span class="op">=</span>n)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> (latent[:, <span class="dv">0</span>] <span class="op">&lt;</span> stats.norm.ppf(p1_true)).astype(<span class="bu">int</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> (latent[:, <span class="dv">1</span>] <span class="op">&lt;</span> stats.norm.ppf(p2_true)).astype(<span class="bu">int</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute differences</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> X <span class="op">-</span> Y</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>D_bar <span class="op">=</span> np.mean(D)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.std(D, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> S <span class="op">/</span> np.sqrt(n)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> D_bar <span class="op">/</span> se</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample mean difference: </span><span class="sc">{</span>D_bar<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard error: </span><span class="sc">{</span>se<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wald statistic: </span><span class="sc">{</span>W<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Show contingency table</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> crosstab, DataFrame</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>ct <span class="op">=</span> crosstab(X, Y, rownames<span class="op">=</span>[<span class="st">'Alg1'</span>], colnames<span class="op">=</span>[<span class="st">'Alg2'</span>])</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Contingency table:"</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ct)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Algorithms agree on </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(X <span class="op">==</span> Y)<span class="op">/</span>n<span class="sc">:.1%}</span><span class="ss"> of instances"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample mean difference: 0.0580
Standard error: 0.0125
Wald statistic: 4.632
p-value: 0.0000

Contingency table:
Alg2    0    1
Alg1          
0      74   51
1     109  766

Algorithms agree on 84.0% of instances</code></pre>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Paired vs Independent Tests
</div>
</div>
<div class="callout-body-container callout-body">
<p>Paired tests are generally <strong>more powerful</strong> than independent tests when:</p>
<ol type="1">
<li>The same subjects/instances are measured twice</li>
<li>There’s positive correlation between measurements</li>
</ol>
<p>The paired test removes the between-subject variability, focusing only on within-subject differences.</p>
</div>
</div>
</section>
</section>
<section id="constructing-statistical-tests" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="constructing-statistical-tests"><span class="header-section-number">7.5</span> Constructing Statistical Tests</h2>
<p>Now that we understand p-values and have seen the Wald test in action, let’s step back and consider the general problem: How do we construct a statistical test?</p>
<p>Given a test statistic <span class="math inline">T(X^n)</span> and null hypothesis <span class="math inline">H_0</span>, we need to determine the distribution of <span class="math inline">T</span> under <span class="math inline">H_0</span> to calculate p-values and make decisions. There are three main approaches:</p>
<ol type="1">
<li><strong>Exact Distribution</strong>: Sometimes we can calculate the exact distribution of <span class="math inline">T</span> under <span class="math inline">H_0</span>
<ul>
<li>Example: Fisher’s exact test for 2×2 tables</li>
<li>Advantage: Exact p-values, valid for any sample size</li>
<li>Disadvantage: Only possible for simple cases</li>
</ul></li>
<li><strong>Asymptotic Approximation</strong>: Use limiting distributions as <span class="math inline">n \to \infty</span>
<ul>
<li>Examples: Wald test (normal), likelihood ratio test (chi-squared)</li>
<li>Advantage: Widely applicable, computationally simple</li>
<li>Disadvantage: May be inaccurate for small samples</li>
</ul></li>
<li><strong>Simulation/Resampling</strong>: Simulate the distribution by resampling
<ul>
<li>Examples: Permutation test, bootstrap test</li>
<li>Advantage: Minimal assumptions, works for any test statistic</li>
<li>Disadvantage: Computationally intensive</li>
</ul></li>
</ol>
<p>The choice depends on:</p>
<ul>
<li>Sample size (small samples → avoid asymptotics)</li>
<li>Distributional assumptions (violated → use resampling)</li>
<li>Test statistic complexity (complex → simulation may be only option)</li>
<li>Computational resources (limited → prefer analytical methods)</li>
</ul>
<p>Let’s now explore specific tests that exemplify each approach: the permutation test (simulation), Fisher’s exact test (exact distribution), and the likelihood ratio test (asymptotic).</p>
<section id="the-permutation-test-a-simulation-approach" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="the-permutation-test-a-simulation-approach"><span class="header-section-number">7.5.1</span> The Permutation Test: A Simulation Approach</h3>
<p>The permutation test is a powerful non-parametric method for testing whether two distributions are the same. It exemplifies the simulation approach to test construction, making minimal assumptions – only requiring that the distributions are identical under the null hypothesis.</p>
<div class="definition">
<p><strong>Permutation Test</strong>: A nonparametric method for testing whether two distributions are the same.</p>
<p>Let <span class="math inline">X_1, \ldots, X_m \sim F_X</span> and <span class="math inline">Y_1, \ldots, Y_n \sim F_Y</span> be independent samples.</p>
<p><strong>Hypotheses</strong>: <span class="math inline">H_0: F_X = F_Y</span> versus <span class="math inline">H_1: F_X \neq F_Y</span></p>
<p><strong>Test statistic</strong>: Choose any statistic <span class="math inline">T(x_1, \ldots, x_m, y_1, \ldots, y_n)</span>, such as <span class="math display">T = |\bar{X}_m - \bar{Y}_n|</span></p>
<p><strong>Key principle</strong>: Under <span class="math inline">H_0</span>, all <span class="math inline">N! = (m+n)!</span> permutations of the combined data are equally likely.</p>
<p><strong>Permutation distribution</strong>: The distribution that puts mass <span class="math inline">1/N!</span> on each value <span class="math inline">T_j</span> obtained from the <span class="math inline">N!</span> permutations.</p>
<p><strong>Permutation p-value</strong>: <span class="math display">\text{p-value} = \mathbb{P}_0(T \geq t_\text{obs}) = \frac{1}{N!} \sum_{j=1}^{N!} I(T_j \geq t_\text{obs})</span></p>
<p>where <span class="math inline">t_\text{obs}</span> is the observed test statistic and <span class="math inline">T_j</span> is the statistic for permutation <span class="math inline">j</span>.</p>
</div>
<p>The key insight: If the null hypothesis is true (both groups come from the same distribution), then the specific assignment of observations to groups was just one random possibility among many. We can simulate the null distribution by considering all possible reassignments.</p>
<p><strong>When to use permutation tests:</strong></p>
<ul>
<li>Small sample sizes where asymptotic approximations may fail</li>
<li>Non-standard or unknown distributions</li>
<li>Complex test statistics without known distributions</li>
<li>When exact p-values are needed for critical decisions</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Permutation Test Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since evaluating all <span class="math inline">(m+n)!</span> permutations is computationally infeasible for realistic sample sizes, we use Monte Carlo sampling:</p>
<ol type="1">
<li><p><strong>Compute observed test statistic</strong>: <span class="math inline">t_\text{obs} = T(X_1, \ldots, X_m, Y_1, \ldots, Y_n)</span></p></li>
<li><p><strong>Generate permutation distribution</strong>: For <span class="math inline">B</span> iterations (typically 10,000+):</p>
<ul>
<li>Randomly permute the combined data</li>
<li>Split into groups of original sizes <span class="math inline">m</span> and <span class="math inline">n</span></li>
<li>Recompute the test statistic <span class="math inline">T_i</span></li>
</ul></li>
<li><p><strong>Calculate p-value</strong>: <span class="math display">\text{p-value} \approx \frac{1 + \sum_{i=1}^B I(T_i \geq t_\text{obs})}{B + 1}</span></p></li>
</ol>
<p>The “+1” corrections in both numerator and denominator:</p>
<ul>
<li>Prevent p-values of exactly 0 (which would be misleading)</li>
<li>Make the estimate slightly conservative but consistent (as <span class="math inline">B \rightarrow \infty</span>, it converges to the true p-value)</li>
<li>Can be interpreted as treating the observed data arrangement as one of B+1 total permutations (since it’s a valid permutation that always has <span class="math inline">T = t_\text{obs}</span>)</li>
</ul>
</div>
</div>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255603-133-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-133-1" role="tab" aria-controls="tabset-1757255603-133-1" aria-selected="true" href="">Intuitive</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-133-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-133-2" role="tab" aria-controls="tabset-1757255603-133-2" aria-selected="false" href="">Mathematical</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-133-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-133-3" role="tab" aria-controls="tabset-1757255603-133-3" aria-selected="false" href="">Computational</a></li></ul><div class="tab-content"><div id="tabset-1757255603-133-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255603-133-1-tab"><p>The core idea of the permutation test is quite simple: if the group
labels (e.g., “Treatment” vs “Control”) don’t actually matter – that is,
if the null hypothesis is true – then shuffling these labels randomly
shouldn’t change the distribution of our test statistic.</p><p>Here’s the logic:</p><ol type="1">
<li>Under <span class="math inline">\(H_0\)</span>, the treatment and
control groups come from the same distribution</li>
<li>So the specific assignment of units to groups was just one random
possibility</li>
<li>We can simulate other equally likely assignments by shuffling the
labels</li>
<li>If our observed difference is unusual compared to these shuffled
differences, we have evidence against
<span class="math inline">\(H_0\)</span></li>
</ol></div><div id="tabset-1757255603-133-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-133-2-tab"><p>The mathematical foundation of the permutation test is the principle
of <strong>exchangeability</strong>. Under the null hypothesis
<span class="math inline">\(H_0: F_X = F_Y\)</span>, all observations
<span class="math inline">\((X_1, \ldots, X_m, Y_1, \ldots, Y_n)\)</span>
are independent and identically distributed (IID) from the same
underlying distribution. This implies that the group labels
(“Treatment”, “Control”) are arbitrary; any permutation of the combined
data is equally likely to have occurred.</p><p>The test leverages this property to construct an <strong>exact,
non-parametric reference distribution</strong> for a chosen test
statistic <span class="math inline">\(T\)</span> under
<span class="math inline">\(H_0\)</span>:</p><ol type="1">
<li><p><strong>The Permutation Distribution</strong>: Conceptually, we
consider the set of all <span class="math inline">\(N! = (m+n)!\)</span>
possible permutations of the combined data. For each permutation, we
compute the test statistic. The set of these
<span class="math inline">\(N!\)</span> values forms the <em>exact</em>
distribution of <span class="math inline">\(T\)</span> under the null
hypothesis, conditional on the observed data values.</p></li>
<li><p><strong>Exactness</strong>: Because this distribution is derived
directly from the data without asymptotic approximations, the
permutation test is an <strong>exact test</strong>. This means that for
a chosen significance level <span class="math inline">\(\alpha\)</span>,
the Type I error rate is controlled at exactly
<span class="math inline">\(\alpha\)</span> (with proper handling of
discrete data). This is a significant advantage over asymptotic tests
like the Wald test, which are only guaranteed to have the correct size
as <span class="math inline">\(n \to \infty\)</span>.</p></li>
<li><p><strong>Formal p-value</strong>: The exact p-value is the
proportion of permutations that yield a test statistic value as extreme
or more extreme than the one observed with the original data labelling:
<span class="math display">\[ \text{p-value} = \frac{\#\{\text{permutations } \pi : T(\pi(\text{data})) \geq t_\text{obs}\}}{N!} \]</span></p></li>
<li><p><strong>Monte Carlo Approximation</strong>: Since calculating all
<span class="math inline">\(N!\)</span> statistics is computationally
infeasible, the algorithm in the “Computational” tab uses Monte Carlo
sampling to approximate this exact p-value. By drawing a large number of
random permutations (<span class="math inline">\(B\)</span>), we create
an empirical distribution that converges to the true permutation
distribution as
<span class="math inline">\(B \to \infty\)</span>.</p></li>
</ol></div><div id="tabset-1757255603-133-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-133-3-tab"><p>Let’s implement a permutation test and see it in action. We’ll test
whether two groups have different means, comparing our permutation test
results with the standard parametric t-test. This demonstrates both how
the test works and when it differs from classical approaches.</p><p>Since evaluating all <span class="math inline">\(N!\)</span>
permutations is computationally infeasible for realistic sample sizes
(recall that 20! ≈ 2.4 × 10^18), we’ll use Monte Carlo sampling to
approximate the permutation distribution with
<span class="math inline">\(B = 10,000\)</span> random permutations.</p><div id="187c5610" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> permutation_test(x, y, n_permutations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform a two-sample permutation test.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    H0: The two samples come from the same distribution.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Test statistic: Absolute difference in means.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Observed test statistic</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    t_obs <span class="op">=</span> <span class="bu">abs</span>(np.mean(x) <span class="op">-</span> np.mean(y))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine the data</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    combined <span class="op">=</span> np.concatenate([x, y])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    n_x <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate permutations and compute test statistics</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    t_perm <span class="op">=</span> []</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">42</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_permutations):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Randomly permute the combined data</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        permuted <span class="op">=</span> np.random.permutation(combined)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split into two groups of original sizes</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        x_perm <span class="op">=</span> permuted[:n_x]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        y_perm <span class="op">=</span> permuted[n_x:]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute test statistic for this permutation</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="bu">abs</span>(np.mean(x_perm) <span class="op">-</span> np.mean(y_perm))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        t_perm.append(t)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate p-value (with +1 correction)</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding 1 to numerator and denominator for unbiased estimate</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    p_value <span class="op">=</span> (np.<span class="bu">sum</span>(np.array(t_perm) <span class="op">&gt;=</span> t_obs) <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (n_permutations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t_obs, t_perm, p_value</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Compare two small samples</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>group1 <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">15</span>, <span class="dv">12</span>)  <span class="co"># Mean 100</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>group2 <span class="op">=</span> np.random.normal(<span class="dv">110</span>, <span class="dv">15</span>, <span class="dv">10</span>)  <span class="co"># Mean 110</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>t_obs, t_perm, p_value <span class="op">=</span> permutation_test(group1, group2)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Observed difference in means: </span><span class="sc">{</span>t_obs<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Permutation p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># For comparison, also run parametric t-test</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>t_stat, p_parametric <span class="op">=</span> stats.ttest_ind(group1, group2)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parametric t-test p-value: </span><span class="sc">{</span>p_parametric<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Observed difference in means: 26.31
Permutation p-value: 0.0016
Parametric t-test p-value: 0.0017</code></pre>
</div>
</div><p>Let’s visualize the permutation distribution to see what we’ve
created:</p><div id="b5323082" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="9">
<div class="sourceCode cell-code" id="cb1" data-code-fold="true" data-code-summary="Show visualization code"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>plt.hist(t_perm, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'blue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">'Permutation distribution'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(t_obs, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Observed statistic = </span><span class="sc">{</span>t_obs<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Add text annotation for p-value</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.text(t_obs <span class="op">+</span> <span class="dv">1</span>, plt.ylim()[<span class="dv">1</span>] <span class="op">*</span> <span class="fl">0.8</span>, </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f'p-value = </span><span class="sc">{</span>p_value<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Test Statistic (|difference in means|)'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Permutation Test: Distribution Under H₀'</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="07-hypothesis-testing_files/figure-html/cell-10-output-1.png"></p>
</div>
</div><p><strong>What this demonstration shows:</strong></p><ol type="1">
<li><p><strong>The permutation distribution shows typical chance
differences</strong>: Under <span class="math inline">\(H_0\)</span>, we
see what absolute differences in means we’d expect purely by chance when
randomly assigning labels. Since we use the absolute difference, all
values are positive, with most falling between 0 and 20.</p></li>
<li><p><strong>Our observed statistic falls in the far right
tail</strong>: The red dashed line shows our actual observed difference
is larger than what we’d typically see by chance. The p-value annotation
shows that only about 0.2% of permutations produce a difference this
large or larger, providing evidence against
<span class="math inline">\(H_0\)</span>.</p></li>
<li><p><strong>Agreement with the parametric test</strong>: Both the
permutation test (p ≈ 0.016) and the t-test (p ≈ 0.017) reach similar
conclusions. This is reassuring when assumptions hold, but the
permutation test would still be valid even if normality assumptions were
violated.</p></li>
<li><p><strong>The Monte Carlo approximation works well</strong>: With
10,000 permutations, we get a smooth estimate of the true permutation
distribution, making the test both computationally feasible and
statistically reliable. The distribution has the characteristic shape
for an absolute difference statistic – starting near zero and skewing
right – but the permutation test makes no assumptions about this
shape.</p></li>
</ol></div></div></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Use <span class="math inline">\geq</span> not <span class="math inline">&gt;</span> for discrete distributions in permutation tests
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="wasserman2013all">Wasserman (<a href="../references.html#ref-wasserman2013all" role="doc-biblioref">2013</a>)</span> uses <span class="math inline">I(T_j &gt; t_\text{obs})</span> but this is incorrect for discrete distributions. The definition of p-value includes equality: the probability of observing a value “as extreme or more extreme.” For continuous distributions this makes no difference, but for discrete cases (including permutation tests) we must use <span class="math inline">\geq</span>.</p>
<p><strong>Example illustrating why this matters:</strong></p>
<p>Consider a tiny dataset: <span class="math inline">X = (1, 9)</span> and <span class="math inline">Y = (3)</span>. We want to test if the means are different using <span class="math inline">T = |\bar{X} - \bar{Y}|</span> as our test statistic.</p>
<ul>
<li>Observed: <span class="math inline">\bar{X} = 5</span>, <span class="math inline">\bar{Y} = 3</span>, so <span class="math inline">t_\text{obs} = |5 - 3| = 2</span></li>
</ul>
<p>Now let’s enumerate all 6 possible permutations:</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Permutation</th>
<th style="text-align: center;">X values</th>
<th style="text-align: center;">Y value</th>
<th style="text-align: center;"><span class="math inline">\bar{X}</span></th>
<th style="text-align: center;"><span class="math inline">\bar{Y}</span></th>
<th style="text-align: center;"><span class="math inline">T = |\bar{X} - \bar{Y}|</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Original</td>
<td style="text-align: center;">(1, 9)</td>
<td style="text-align: center;">(3)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">(9, 1)</td>
<td style="text-align: center;">(3)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">(1, 3)</td>
<td style="text-align: center;">(9)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">(3, 1)</td>
<td style="text-align: center;">(9)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">(3, 9)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">(9, 3)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Computing the p-value:</p>
<ul>
<li><strong>Correct</strong> (using <span class="math inline">\geq</span>): <span class="math inline">\mathbb{P}(T \geq 2) = 6/6 = 1.0</span> (all permutations have <span class="math inline">T \geq 2</span>)</li>
<li><strong>Incorrect</strong> (using <span class="math inline">&gt;</span>): <span class="math inline">\mathbb{P}(T &gt; 2) = 4/6 = 0.67</span> (only 4 permutations have <span class="math inline">T &gt; 2</span>)</li>
</ul>
<p>The correct p-value of 1.0 tells us our observed difference is the <strong>smallest possible</strong> – not evidence against <span class="math inline">H_0</span> at all! The incorrect formula would suggest some evidence against the null, which is completely wrong.</p>
<p>This example shows why the correct formula must use <span class="math inline">I(T_j \geq t_\text{obs})</span>.</p>
</div>
</div>
</section>
<section id="fishers-exact-test-an-exact-approach" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="fishers-exact-test-an-exact-approach"><span class="header-section-number">7.5.2</span> Fisher’s Exact Test: An Exact Approach</h3>
<p>Fisher’s exact test exemplifies the exact distribution approach. It provides the exact probability of observing data as extreme or more extreme than what we observed, given fixed marginal totals in a 2×2 contingency table. This is particularly valuable for small sample sizes where asymptotic approximations may fail.</p>
<p>To illustrate, let’s return to the motivating drug trial problem from the chapter introduction:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Application: The Drug Trial
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall our 2×2 table of outcomes:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Better</th>
<th style="text-align: center;">Not Better</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Treated</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
</tr>
<tr class="even">
<td style="text-align: left;">Control</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">60</td>
</tr>
</tbody>
</table>
<p>Many different statistical tests could be applied in this setting – we could use a two-sample test of proportions (Wald test), a chi-squared test, or a permutation test. However, for 2×2 contingency tables with modest sample sizes, <strong>Fisher’s Exact Test</strong> is one of the more attractive alternatives. It calculates the exact probability of observing a table as extreme or more extreme than this, given fixed marginal totals.</p>
<div id="cca34d9e" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the contingency table</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> [[<span class="dv">50</span>, <span class="dv">50</span>],  <span class="co"># Treated: 50 better, 50 not better</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>         [<span class="dv">40</span>, <span class="dv">60</span>]]  <span class="co"># Control: 40 better, 60 not better</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># alternative="greater" tests if treatment has higher "better" rate</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>odds_ratio, p_value <span class="op">=</span> stats.fisher_exact(table, alternative<span class="op">=</span><span class="st">"greater"</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Contingency table:"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"         Better  Not Better"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Treated:   50        50"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Control:   40        60"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Odds ratio: </span><span class="sc">{</span>odds_ratio<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"One-sided p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Also try two-sided test</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>_, p_two_sided <span class="op">=</span> stats.fisher_exact(table, alternative<span class="op">=</span><span class="st">"two-sided"</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Two-sided p-value: </span><span class="sc">{</span>p_two_sided<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conclusion: Significant evidence of treatment effect (p &lt; 0.05)"</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conclusion: No significant evidence of treatment effect at α = 0.05"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Contingency table:
         Better  Not Better
Treated:   50        50
Control:   40        60

Odds ratio: 1.500
One-sided p-value: 0.1004
Two-sided p-value: 0.2007

Conclusion: No significant evidence of treatment effect at α = 0.05</code></pre>
</div>
</div>
<p>With p = 0.10, we don’t have strong evidence to reject the null hypothesis at the conventional α = 0.05 level. The observed 10 percentage point difference could plausibly arise by chance.</p>
<p>This example illustrates a key point: even seemingly large differences (50% vs 40% success rate) may not be statistically significant with modest sample sizes. Power analysis before conducting studies is crucial!</p>
<p><strong>Many other applications share this same 2×2 structure:</strong></p>
<ul>
<li><strong>A/B testing</strong>: Comparing conversion rates between website designs</li>
<li><strong>Demographics</strong>: Testing differences in proportions between groups<br>
</li>
<li><strong>Medical screening</strong>: Comparing test accuracy between methods</li>
<li><strong>Quality control</strong>: Comparing defect rates between processes</li>
</ul>
<p>Whenever you’re comparing binary outcomes between two groups, you face the same statistical question: is the observed difference real or just chance?</p>
</div>
</div>
</section>
<section id="the-likelihood-ratio-test-a-general-asymptotic-approach" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="the-likelihood-ratio-test-a-general-asymptotic-approach"><span class="header-section-number">7.5.3</span> The Likelihood Ratio Test: A General Asymptotic Approach</h3>
<p>The Wald test is useful for testing a scalar parameter. The likelihood ratio test is more general and can be used for testing a vector-valued parameter, making it one of the most important tools in statistical inference.</p>
<div class="definition">
<p><strong>Likelihood Ratio Test</strong>: For testing <span class="math inline">H_0: \theta \in \Theta_0</span> versus <span class="math inline">H_1: \theta \notin \Theta_0</span>:</p>
<p>The <strong>likelihood ratio statistic</strong> is: <span class="math display">\lambda = 2 \log \left( \frac{\sup_{\theta \in \Theta} \mathcal{L}(\theta)}{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)} \right) = 2[\ell(\hat{\theta}) - \ell(\hat{\theta}_0)]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\hat{\theta}</span> is the MLE over the entire parameter space <span class="math inline">\Theta</span></li>
<li><span class="math inline">\hat{\theta}_0</span> is the MLE under the constraint <span class="math inline">\theta \in \Theta_0</span></li>
</ul>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why use <span class="math inline">\Theta</span> instead of <span class="math inline">\Theta_0^c</span> in the numerator?
</div>
</div>
<div class="callout-body-container callout-body">
<p>You might expect to maximize over <span class="math inline">\Theta_0^c</span> (the alternative hypothesis space) in the numerator. However:</p>
<ul>
<li>Using <span class="math inline">\Theta</span> has little practical effect on the test statistic</li>
<li>The theoretical properties are much simpler with this definition</li>
<li>It ensures the statistic is always non-negative</li>
</ul>
</div>
</div>
<p>The LRT is most useful when <span class="math inline">\Theta_0</span> is defined by constraining some parameters to fixed values. For example, if <span class="math inline">\theta = (\theta_1, \ldots, \theta_r)</span> and we want to test that the last <span class="math inline">r-q</span> components equal specific values: <span class="math display">\Theta_0 = \{\theta: (\theta_{q+1}, \ldots, \theta_r) = (\theta_{0,q+1}, \ldots, \theta_{0,r})\}</span></p>
<div class="theorem">
<p>Under <span class="math inline">H_0: \theta \in \Theta_0</span>, the likelihood ratio statistic has an asymptotic <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a>: <span class="math display">\lambda \rightsquigarrow \chi^2_{r-q}</span></p>
<p>where <span class="math inline">r-q</span> is the difference in dimensionality between <span class="math inline">\Theta</span> and <span class="math inline">\Theta_0</span> (the number of constraints imposed by <span class="math inline">H_0</span>).</p>
<p><strong>The p-value</strong> is: <span class="math inline">\mathbb{P}(\chi^2_{r-q} \geq \lambda)</span></p>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Counting Degrees of Freedom
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\theta = (\theta_1, \theta_2, \theta_3, \theta_4, \theta_5)</span> and we test: <span class="math display">H_0: \theta_4 = \theta_5 = 0</span></p>
<p>Then:</p>
<ul>
<li>Dimension of <span class="math inline">\Theta</span> = 5 (all parameters free)</li>
<li>Dimension of <span class="math inline">\Theta_0</span> = 3 (only <span class="math inline">\theta_1, \theta_2, \theta_3</span> free)</li>
<li>Degrees of freedom = 5 - 3 = 2</li>
</ul>
<p>The test statistic <span class="math inline">\lambda \rightsquigarrow \chi^2_2</span> under <span class="math inline">H_0</span>.</p>
</div>
</div>
</section>
</section>
<section id="the-multiple-testing-problem-the-peril-of-many-tests" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="the-multiple-testing-problem-the-peril-of-many-tests"><span class="header-section-number">7.6</span> The Multiple Testing Problem: The Peril of Many Tests</h2>
<section id="the-problem" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="the-problem"><span class="header-section-number">7.6.1</span> The Problem</h3>
<p>Modern data science often involves testing many hypotheses simultaneously. Consider these scenarios:</p>
<ul>
<li><strong>Genomics</strong>: Testing thousands of genes for association with disease</li>
<li><strong>A/B testing</strong>: Running dozens of experiments across a website</li>
<li><strong>Neuroscience</strong>: Testing brain activity at thousands of voxels</li>
<li><strong>Feature selection</strong>: Testing which of hundreds of features predict an outcome</li>
</ul>
<p>The problem is simple but severe: If you perform many tests, you’re virtually guaranteed to get false positives by chance alone.</p>
<p>Let’s quantify this:</p>
<ul>
<li><strong>One test</strong> at α = 0.05: 5% chance of a Type I error</li>
<li><strong>20 independent tests</strong> at α = 0.05 each:
<ul>
<li>Probability of at least one Type I error = <span class="math inline">1 - (0.95)^{20} \approx 0.64</span></li>
<li>That’s a 64% chance of at least one false positive!</li>
</ul></li>
<li><strong>1000 tests</strong>: Virtually certain to get many false positives</li>
</ul>
<div id="c13263b8" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="11">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the multiple testing problem</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>n_tests <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate probability of at least one false positive</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>prob_no_false_positive <span class="op">=</span> [(<span class="dv">1</span> <span class="op">-</span> alpha)<span class="op">**</span>m <span class="cf">for</span> m <span class="kw">in</span> n_tests]</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>prob_at_least_one_fp <span class="op">=</span> [<span class="dv">1</span> <span class="op">-</span> p <span class="cf">for</span> p <span class="kw">in</span> prob_no_false_positive]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected number of false positives</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>expected_fp <span class="op">=</span> [m <span class="op">*</span> alpha <span class="cf">for</span> m <span class="kw">in</span> n_tests]</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.plot(n_tests, prob_at_least_one_fp, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>alpha, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="ss">f'α = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Tests'</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(At least one false positive)'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Probability of False Discoveries'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.plot(n_tests, expected_fp, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">'s'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Tests'</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Expected # of False Positives'</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Expected False Positives'</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="07-hypothesis-testing_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="key-concepts-for-multiple-testing" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="key-concepts-for-multiple-testing"><span class="header-section-number">7.6.2</span> Key Concepts for Multiple Testing</h3>
<p>When conducting <span class="math inline">m</span> hypothesis tests, we need to understand what types of errors we want to control. Consider this classification table of test outcomes:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><span class="math inline">H_0</span> Not Rejected</th>
<th style="text-align: center;"><span class="math inline">H_0</span> Rejected</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">H_0</span> True</td>
<td style="text-align: center;"><span class="math inline">U</span></td>
<td style="text-align: center;"><span class="math inline">V</span></td>
<td style="text-align: center;"><span class="math inline">m_0</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">H_0</span> False</td>
<td style="text-align: center;"><span class="math inline">T</span></td>
<td style="text-align: center;"><span class="math inline">S</span></td>
<td style="text-align: center;"><span class="math inline">m_1</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: center;"><span class="math inline">m-R</span></td>
<td style="text-align: center;"><span class="math inline">R</span></td>
<td style="text-align: center;"><span class="math inline">m</span></td>
</tr>
</tbody>
</table>
<p>Where:</p>
<ul>
<li><span class="math inline">V</span> = Number of false positives (Type I errors)<br>
</li>
<li><span class="math inline">S</span> = Number of true positives (correct rejections)</li>
<li><span class="math inline">R = V + S</span> = Total rejections</li>
<li><span class="math inline">m_0</span> = Number of true null hypotheses</li>
</ul>
<div class="definition">
<p><strong>Family-Wise Error Rate (FWER)</strong>: The probability of making at least one Type I error among all tests: <span class="math display">\text{FWER} = \mathbb{P}(V \geq 1)</span></p>
<p><strong>False Discovery Proportion (FDP)</strong>: The proportion of rejections that are false: <span class="math display">\text{FDP} = \begin{cases}
\frac{V}{R} &amp; \text{if } R &gt; 0 \\
0 &amp; \text{if } R = 0
\end{cases}</span></p>
<p><strong>False Discovery Rate (FDR)</strong>: The expected false discovery proportion: <span class="math display">\text{FDR} = \mathbb{E}[\text{FDP}]</span></p>
</div>
<p>These quantities represent different philosophies for error control:</p>
<ul>
<li><strong>FWER control</strong> is very conservative, aiming to avoid any false positives.</li>
<li><strong>FDR control</strong> is more liberal and often more sensible in practical applications, accepting some false positives but controlling their overall proportion.</li>
</ul>
</section>
<section id="the-bonferroni-method-controlling-fwer" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="the-bonferroni-method-controlling-fwer"><span class="header-section-number">7.6.3</span> The Bonferroni Method: Controlling FWER</h3>
<p>The Bonferroni correction provides the simplest and most conservative approach to multiple testing.</p>
<div class="definition">
<p><strong>Bonferroni Method</strong>: Given p-values <span class="math inline">P_1, \ldots, P_m</span>, reject null hypothesis <span class="math inline">H_{0i}</span> if: <span class="math display">P_i &lt; \frac{\alpha}{m}</span></p>
</div>
<p><strong>Intuition</strong>: We divide our total error budget <span class="math inline">\alpha</span> equally among all <span class="math inline">m</span> tests. If each test has Type I error rate <span class="math inline">\alpha/m</span>, the total error rate can’t exceed <span class="math inline">\alpha</span>.</p>
<div class="theorem">
<p>Using the Bonferroni method, the probability of falsely rejecting any null hypothesis is at most <span class="math inline">\alpha</span>: <span class="math display">\text{FWER} \leq \alpha</span></p>
<p><strong>Proof</strong>: Let <span class="math inline">A_i</span> be the event that test <span class="math inline">i</span> rejects when <span class="math inline">H_{0i}</span> is true. Then: <span class="math display">\text{FWER} = \mathbb{P}(\bigcup_{i \in I_0} A_i) \leq \sum_{i \in I_0} \mathbb{P}(A_i) \leq \sum_{i \in I_0} \frac{\alpha}{m} \leq \alpha</span> where <span class="math inline">I_0</span> is the set of true null hypotheses.</p>
</div>
<p><strong>Pros</strong>:</p>
<ul>
<li>Simple to implement<br>
</li>
<li>Provides strong control of Type I errors</li>
<li>Works for any dependency structure among tests</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><strong>Extremely conservative</strong>: Dramatically reduces power</li>
<li>Often fails to detect true effects<br>
</li>
<li>Becomes nearly useless with thousands of tests</li>
</ul>
</section>
<section id="the-benjamini-hochberg-method-controlling-fdr" class="level3" data-number="7.6.4">
<h3 data-number="7.6.4" class="anchored" data-anchor-id="the-benjamini-hochberg-method-controlling-fdr"><span class="header-section-number">7.6.4</span> The Benjamini-Hochberg Method: Controlling FDR</h3>
<p>The Benjamini-Hochberg (BH) procedure offers a more powerful alternative by controlling the proportion of false discoveries rather than the probability of any false discovery.</p>
<div class="definition">
<p><strong>Benjamini-Hochberg (BH) Procedure</strong>:</p>
<ol type="1">
<li>Let <span class="math inline">P_{(1)} \leq P_{(2)} \leq \ldots \leq P_{(m)}</span> denote the ordered p-values</li>
<li>Define: <span class="math display">\ell_i = \frac{i \alpha}{C_m m} \quad \text{and} \quad R = \max\{i: P_{(i)} \leq \ell_i\}</span> where <span class="math inline">C_m</span> is defined as:
<ul>
<li><span class="math inline">C_m = 1</span> if the p-values are independent</li>
<li><span class="math inline">C_m = \sum_{i=1}^m \frac{1}{i}</span> otherwise (for dependent tests)</li>
</ul></li>
<li>Let <span class="math inline">T = P_{(R)}</span> be the <strong>BH rejection threshold</strong></li>
<li>Reject all null hypotheses <span class="math inline">H_{0i}</span> for which <span class="math inline">P_i \leq T</span></li>
</ol>
</div>
<p><strong>Intuition</strong>: The procedure finds the largest set of rejections such that the expected proportion of false discoveries is controlled. In practice, <span class="math inline">C_m = 1</span> is almost always used (assuming independence), so the threshold for the <span class="math inline">i</span>-th smallest p-value is <span class="math inline">\ell_i = i\alpha/m</span>. The procedure looks for the rightmost p-value that falls below this sloped line.</p>
<p>The procedure can be visualized by plotting ordered p-values against their threshold line:</p>
<div id="62e2a59e" class="cell" data-fig-height="4" data-fig-width="7" data-execution_count="12">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize BH procedure with a clearer example</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">20</span>  <span class="co"># Number of tests</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>alpha_fdr <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate p-values designed to show BH advantage</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># With α=0.05 and m=20: Bonferroni threshold = 0.0025, BH thresholds = i*0.0025</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.array([</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.0008</span>, <span class="fl">0.0045</span>, <span class="fl">0.0068</span>,  <span class="co"># BH will reject these 3 (below 0.0025, 0.005, 0.0075)</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.012</span>, <span class="fl">0.024</span>, <span class="fl">0.041</span>, <span class="fl">0.063</span>, <span class="fl">0.089</span>,  <span class="co"># Won't be rejected</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.115</span>, <span class="fl">0.181</span>, <span class="fl">0.224</span>, <span class="fl">0.301</span>,  <span class="co"># Medium p-values</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.412</span>, <span class="fl">0.501</span>, <span class="fl">0.578</span>, <span class="fl">0.656</span>,  <span class="co"># Larger p-values  </span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.721</span>, <span class="fl">0.812</span>, <span class="fl">0.888</span>, <span class="fl">0.951</span>  <span class="co"># Very large p-values</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>p_sorted <span class="op">=</span> np.sort(p_values)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># BH threshold line (using C_m = 1 for independent tests)</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.arange(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>bh_threshold <span class="op">=</span> k_values <span class="op">*</span> alpha_fdr <span class="op">/</span> m  <span class="co"># This is ℓ_i with C_m = 1</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Find BH cutoff using the standard definition</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co"># R = max{i: P_(i) ≤ ℓ_i}</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>bh_check <span class="op">=</span> p_sorted <span class="op">&lt;=</span> bh_threshold</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> np.<span class="bu">any</span>(bh_check):</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.<span class="bu">max</span>(np.where(bh_check)[<span class="dv">0</span>]) <span class="op">+</span> <span class="dv">1</span>  <span class="co"># +1 because Python uses 0-indexing</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> p_sorted[R<span class="op">-</span><span class="dv">1</span>]  <span class="co"># BH rejection threshold</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot p-values and thresholds</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>plt.scatter(k_values, p_sorted, color<span class="op">=</span><span class="st">'blue'</span>, s<span class="op">=</span><span class="dv">60</span>, label<span class="op">=</span><span class="st">'Sorted p-values'</span>, zorder<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, bh_threshold, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'BH threshold (α=</span><span class="sc">{</span>alpha_fdr<span class="sc">}</span><span class="ss">)'</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, [alpha_fdr<span class="op">/</span>m]<span class="op">*</span>m, <span class="st">'g--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="ss">f'Bonferroni threshold'</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> R <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Highlight rejected p-values</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    plt.scatter(k_values[:R], p_sorted[:R], </span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'Rejected'</span>, zorder<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark the cutoff point R</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    plt.axvline(R <span class="op">+</span> <span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    plt.text(R <span class="op">+</span> <span class="fl">0.7</span>, <span class="fl">0.15</span>, <span class="ss">f'R = </span><span class="sc">{</span>R<span class="sc">}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Formatting</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Rank i'</span>)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'p-value (log scale)'</span>)</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Benjamini-Hochberg Procedure'</span>)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>, framealpha<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>, which<span class="op">=</span><span class="st">'both'</span>)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix axes</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>, m<span class="op">+</span><span class="dv">1</span>, <span class="dv">5</span>))  <span class="co"># Show every 5th rank</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, m<span class="op">+</span><span class="dv">1</span>)  <span class="co"># Proper bounds for discrete ranks</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Use log scale for y-axis</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.0005</span>, <span class="fl">1.0</span>)  <span class="co"># Start just below smallest p-value</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> R <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"BH procedure:"</span>)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  R = </span><span class="sc">{</span>R<span class="sc">}</span><span class="ss"> (largest i where P_(i) &lt; i·α/m)"</span>)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Rejection threshold T = P_(</span><span class="sc">{</span>R<span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span>T<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Rejects </span><span class="sc">{</span>R<span class="sc">}</span><span class="ss"> hypotheses (p-values ≤ </span><span class="sc">{</span>T<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Bonferroni would reject </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(p_sorted <span class="op">&lt;</span> alpha_fdr<span class="op">/</span>m)<span class="sc">}</span><span class="ss"> hypotheses"</span>)</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  (only those with p &lt; </span><span class="sc">{</span>alpha_fdr<span class="op">/</span>m<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"No hypotheses rejected by either method"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="07-hypothesis-testing_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>BH procedure:
  R = 3 (largest i where P_(i) &lt; i·α/m)
  Rejection threshold T = P_(3) = 0.0068
  Rejects 3 hypotheses (p-values ≤ 0.0068)

Bonferroni would reject 1 hypotheses
  (only those with p &lt; 0.0025)</code></pre>
</div>
</div>
<p>The visualization shows the key difference between methods:</p>
<ul>
<li><strong>Bonferroni</strong> (flat green line): Same strict threshold for all tests</li>
<li><strong>BH</strong> (sloped red line): More lenient threshold for higher-ranked p-values</li>
<li>The BH procedure finds the rightmost crossing of p-values below the sloped line</li>
</ul>
<div class="theorem" name="Benjamini-Hochberg Theorem">
<p>When the BH procedure is applied with the appropriate <span class="math inline">C_m</span>: <span class="math display">\text{FDR} = \mathbb{E}[\text{FDP}] \leq \frac{m_0}{m}\alpha \leq \alpha</span></p>
<p>where <span class="math inline">m_0</span> is the number of true null hypotheses.</p>
<p>This guarantee holds:</p>
<ul>
<li>With <span class="math inline">C_m = 1</span> when the test statistics are independent</li>
<li>With <span class="math inline">C_m = \sum_{i=1}^m \frac{1}{i}</span> for arbitrary dependence structures</li>
</ul>
<p>In practice, <span class="math inline">C_m = 1</span> is almost always used as the procedure is remarkably robust to many forms of dependence.</p>
</div>
<p><strong>Comparison of Methods</strong>:</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 14%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Controls</th>
<th style="text-align: left;">Decision Rule</th>
<th style="text-align: left;">Power</th>
<th style="text-align: left;">Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bonferroni</td>
<td style="text-align: left;">FWER <span class="math inline">\leq \alpha</span></td>
<td style="text-align: left;">Reject if <span class="math inline">P_i &lt; \alpha/m</span></td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Critical applications</td>
</tr>
<tr class="even">
<td style="text-align: left;">Benjamini-Hochberg</td>
<td style="text-align: left;">FDR <span class="math inline">\leq \alpha</span></td>
<td style="text-align: left;">Reject if <span class="math inline">P_i \leq T = P_{(R)}</span>*</td>
<td style="text-align: left;">Higher</td>
<td style="text-align: left;">Large-scale testing</td>
</tr>
</tbody>
</table>
<p>*Where <span class="math inline">R = \max\{i: P_{(i)} \leq i\alpha/(C_m m)\}</span> with <span class="math inline">C_m = 1</span> for independent tests</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Adjusted p-values: An Alternative Presentation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The BH procedure can also be presented using <strong>adjusted p-values</strong>, which is how many software packages (including <code>scipy.stats.false_discovery_control</code>) implement it. This might seem like a completely different method, but it’s just a reformulation of the same procedure.</p>
<p><strong>Adjusted p-values</strong>: For the BH method, the adjusted p-value is computed in two steps:</p>
<ol type="1">
<li><p>First, scale each p-value: <span class="math inline">\tilde{P}'_{(i)} = \min\left(1, \frac{m}{i} P_{(i)}\right)</span></p></li>
<li><p>Then enforce monotonicity (working from largest to smallest): <span class="math display">\tilde{P}_{(i)} = \min_{j \geq i} \tilde{P}'_{(j)}</span></p></li>
</ol>
<p>This ensures adjusted p-values are non-decreasing: <span class="math inline">\tilde{P}_{(1)} \leq \tilde{P}_{(2)} \leq \ldots \leq \tilde{P}_{(m)}</span>.</p>
<p>After computing adjusted p-values, reject all hypotheses where <span class="math inline">\tilde{P}_i \leq \alpha</span>.</p>
<p>This is <strong>mathematically equivalent</strong> to the threshold approach but ensures the adjusted p-values maintain proper ordering.</p>
<div id="1f9704f9" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example p-values</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>pvals <span class="op">=</span> [<span class="fl">0.003</span>, <span class="fl">0.02</span>, <span class="fl">0.04</span>, <span class="fl">0.08</span>, <span class="fl">0.15</span>, <span class="fl">0.25</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(pvals)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Using scipy's function (returns adjusted p-values)</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>adjusted <span class="op">=</span> stats.false_discovery_control(pvals, method<span class="op">=</span><span class="st">'bh'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original vs Adjusted p-values:"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (p, adj) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(pvals, adjusted)):</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Scale the p-value</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    scaled <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, (m<span class="op">/</span>(i<span class="op">+</span><span class="dv">1</span>)) <span class="op">*</span> p)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"p[</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">] = </span><span class="sc">{</span>p<span class="sc">:.3f}</span><span class="ss"> → scaled = </span><span class="sc">{</span>scaled<span class="sc">:.3f}</span><span class="ss"> → adjusted = </span><span class="sc">{</span>adj<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Note: adjusted values enforce monotonicity"</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"(each adjusted p-value ≥ previous one)"</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">At α = 0.05:"</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reject hypotheses where adjusted p-value ≤ 0.05"</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Result: Reject first </span><span class="sc">{</span><span class="bu">sum</span>(adjusted <span class="op">&lt;=</span> <span class="fl">0.05</span>)<span class="sc">}</span><span class="ss"> hypotheses"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original vs Adjusted p-values:
----------------------------------------
p[1] = 0.003 → scaled = 0.018 → adjusted = 0.018
p[2] = 0.020 → scaled = 0.060 → adjusted = 0.060
p[3] = 0.040 → scaled = 0.080 → adjusted = 0.080
p[4] = 0.080 → scaled = 0.120 → adjusted = 0.120
p[5] = 0.150 → scaled = 0.180 → adjusted = 0.180
p[6] = 0.250 → scaled = 0.250 → adjusted = 0.250

Note: adjusted values enforce monotonicity
(each adjusted p-value ≥ previous one)

At α = 0.05:
Reject hypotheses where adjusted p-value ≤ 0.05
Result: Reject first 1 hypotheses</code></pre>
</div>
</div>
<p>The adjusted p-values tell you: “This is the smallest α level at which this hypothesis would be rejected by the BH procedure.”</p>
</div>
</div>
</div>
</section>
<section id="practical-recommendations" class="level3" data-number="7.6.5">
<h3 data-number="7.6.5" class="anchored" data-anchor-id="practical-recommendations"><span class="header-section-number">7.6.5</span> Practical Recommendations</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When to Use Which Method
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Use Bonferroni when</strong>:</p>
<ul>
<li>You have a small number of tests (&lt; 20)</li>
<li>The cost of any false positive is very high</li>
<li>You need to convince skeptical reviewers</li>
</ul>
<p><strong>Use Benjamini-Hochberg when</strong>:</p>
<ul>
<li>You have many tests (genomics, neuroimaging, etc.)</li>
<li>Some false positives are acceptable</li>
<li>You want to generate hypotheses for follow-up</li>
</ul>
<p><strong>Use no correction when</strong>:</p>
<ul>
<li>Tests are explicitly exploratory</li>
<li>You’re doing hypothesis generation, not confirmation</li>
<li>But always report that no correction was applied!</li>
</ul>
</div>
</div>
</section>
</section>
<section id="nhst-in-practice-a-critical-view" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="nhst-in-practice-a-critical-view"><span class="header-section-number">7.7</span> NHST in Practice: A Critical View</h2>
<p>Despite its ubiquity in scientific research, NHST has fundamental limitations that have contributed to what many call the <strong>replication crisis</strong> – the inability to reproduce many published scientific findings.</p>
<section id="fundamental-problems-with-nhst" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="fundamental-problems-with-nhst"><span class="header-section-number">7.7.1</span> Fundamental Problems with NHST</h3>
<p><strong>The Focus on the Null</strong>: NHST tells us whether to reject <span class="math inline">H_0</span>, but doesn’t require us to specify a realistic alternative. We test against “no effect” without having to say what effect we actually expect. This leads to vague research questions and post-hoc rationalization of any significant finding.</p>
<p><strong>Ignoring Prior Plausibility</strong>: NHST treats all hypotheses equally – the same p &lt; 0.05 is required whether testing a well-established biological mechanism or claiming telepathy exists. As the saying goes, “extraordinary claims require extraordinary evidence,” but NHST doesn’t account for this. This is a key limitation addressed by Bayesian methods (Chapter 8).</p>
<p><strong>The Dichotomy Problem</strong>: The division into “significant” and “non-significant” at α = 0.05 is entirely arbitrary. As statistician Andrew Gelman notes:</p>
<blockquote class="blockquote">
<p>“The difference between ‘significant’ and ‘not significant’ is not itself statistically significant!”</p>
</blockquote>
<p>A result with p = 0.049 is treated fundamentally differently from p = 0.051, despite being practically identical.</p>
</section>
<section id="the-low-power-trap" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="the-low-power-trap"><span class="header-section-number">7.7.2</span> The Low Power Trap</h3>
<p>Low-powered studies create a vicious cycle:</p>
<ul>
<li>They often fail to detect true effects (high Type II error)</li>
<li>When they do find “significant” results, these are more likely to be false positives</li>
<li><strong>Crucially</strong>: In low-power settings, most published “significant” findings are false positives</li>
</ul>
<p>This counterintuitive result occurs because with low power, the few significant results that emerge are disproportionately likely to be statistical flukes rather than real effects.</p>
</section>
<section id="common-misuses" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="common-misuses"><span class="header-section-number">7.7.3</span> Common Misuses</h3>
<p><strong>p-hacking (Data Dredging)</strong>:</p>
<ul>
<li>Testing multiple hypotheses until finding p &lt; 0.05</li>
<li>Stopping data collection when significance is reached</li>
<li>Trying different analyses until one “works”</li>
<li>Excluding “outliers” post-hoc to achieve significance</li>
</ul>
<p><strong>HARKing</strong> (Hypothesizing After Results are Known): Looking at the data first, then pretending the observed pattern was the hypothesis all along.</p>
<p><strong>Publication Bias</strong>: Journals preferentially publish “significant” results, creating a distorted scientific literature where negative results disappear. This file-drawer problem means the published record overestimates effect sizes and underestimates uncertainty.</p>
</section>
<section id="moving-forward-better-practices" class="level3" data-number="7.7.4">
<h3 data-number="7.7.4" class="anchored" data-anchor-id="moving-forward-better-practices"><span class="header-section-number">7.7.4</span> Moving Forward: Better Practices</h3>
<p><strong>Practical Recommendations</strong>:</p>
<ul>
<li><strong>Always report effect sizes and confidence intervals</strong>, not just p-values</li>
<li><strong>Conduct power analyses</strong> before collecting data</li>
<li><strong>Pre-register hypotheses and analysis plans</strong> to prevent p-hacking</li>
<li><strong>Consider Bayesian methods</strong> when prior information exists</li>
<li><strong>Use appropriate multiple testing corrections</strong> when testing many hypotheses</li>
<li><strong>Report all analyses attempted</strong>, not just the “significant” ones</li>
</ul>
<p><strong>Alternative Approaches</strong>:</p>
<ul>
<li><strong>Bayesian inference</strong> (Chapter 8): Incorporates prior knowledge and provides probability statements about hypotheses</li>
<li><strong>Estimation-focused analysis</strong>: Emphasize effect sizes and uncertainty rather than binary decisions</li>
<li><strong>Replication studies</strong>: The ultimate test of a finding’s validity</li>
</ul>
</section>
</section>
<section id="chapter-summary" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="chapter-summary"><span class="header-section-number">7.8</span> Chapter Summary</h2>
<section id="key-concepts-review" class="level3" data-number="7.8.1">
<h3 data-number="7.8.1" class="anchored" data-anchor-id="key-concepts-review"><span class="header-section-number">7.8.1</span> Key Concepts Review</h3>
<p>We’ve explored the foundations of null-hypothesis significance testing (NHST):</p>
<p><strong>The Framework</strong>:</p>
<ul>
<li><strong>Null and alternative hypotheses</strong>: <span class="math inline">H_0</span> (no effect) vs <span class="math inline">H_1</span> (effect exists)</li>
<li><strong>Type I and Type II errors</strong>: False positives vs false negatives</li>
<li><strong>Power and size</strong>: Probability of detecting true effects vs controlling false positives</li>
<li><strong>Test statistic and rejection region</strong>: Summarizing evidence and decision rules</li>
</ul>
<p><strong>The p-value</strong>:</p>
<ul>
<li>Measures how surprising data would be under <span class="math inline">H_0</span></li>
<li>NOT the probability that <span class="math inline">H_0</span> is true</li>
<li>Small p-value = evidence against <span class="math inline">H_0</span>, not proof</li>
<li>Statistical significance ≠ practical significance</li>
</ul>
<p><strong>Key Tests</strong>:</p>
<ul>
<li><strong>Wald test</strong>: Uses asymptotic normality of estimators</li>
<li><strong>Permutation test</strong>: Non-parametric alternative requiring minimal assumptions</li>
<li><strong>Fisher’s exact test</strong>: Exact test for contingency tables</li>
<li><strong>Likelihood ratio test</strong>: General framework for testing constraints on parameters</li>
</ul>
<p><strong>Multiple Testing</strong>:</p>
<ul>
<li>Running many tests inflates Type I error rate</li>
<li><strong>Bonferroni</strong>: Controls FWER (conservative but safe)</li>
<li><strong>Benjamini-Hochberg</strong>: Controls FDR (more powerful, modern standard)</li>
</ul>
</section>
<section id="common-pitfalls-to-avoid" class="level3" data-number="7.8.2">
<h3 data-number="7.8.2" class="anchored" data-anchor-id="common-pitfalls-to-avoid"><span class="header-section-number">7.8.2</span> Common Pitfalls to Avoid</h3>
<ol type="1">
<li><strong>Misinterpreting p-values</strong>: Remember, p-value ≠ P(<span class="math inline">H_0</span> is true)</li>
<li><strong>Multiple testing without correction</strong>: Always consider how many tests you’re running</li>
<li><strong>Confusing statistical and practical significance</strong>: A tiny effect can be “significant” with enough data</li>
<li><strong>Ignoring assumptions</strong>: Tests like the Wald test require large samples</li>
<li><strong>Post-hoc hypothesis formulation</strong>: Don’t look at data, then formulate hypotheses to test</li>
</ol>
</section>
<section id="chapter-connections" class="level3" data-number="7.8.3">
<h3 data-number="7.8.3" class="anchored" data-anchor-id="chapter-connections"><span class="header-section-number">7.8.3</span> Chapter Connections</h3>
<ul>
<li><strong>Previous chapters</strong>:
<ul>
<li>Chapters 5-6 gave us estimators and their properties; now we test hypotheses about them</li>
<li>Chapter 4 (Bootstrap) provides an alternative to asymptotic tests</li>
</ul></li>
<li><strong>This chapter</strong>: Core framework for statistical inference and decision-making</li>
<li><strong>Next chapter</strong>: Bayesian inference offers an alternative paradigm that:
<ul>
<li>Provides probabilities for hypotheses</li>
<li>Incorporates prior information</li>
<li>Avoids some NHST pitfalls</li>
</ul></li>
</ul>
</section>
<section id="self-test-problems" class="level3" data-number="7.8.4">
<h3 data-number="7.8.4" class="anchored" data-anchor-id="self-test-problems"><span class="header-section-number">7.8.4</span> Self-Test Problems</h3>
<ol type="1">
<li><p><strong>Understanding Type I and Type II Errors</strong></p>
<p>A medical test for a disease has the following properties:</p>
<ul>
<li>If a person has the disease, the test is positive 95% of the time</li>
<li>If a person doesn’t have the disease, the test is negative 98% of the time</li>
</ul>
<p>In hypothesis testing terms (where <span class="math inline">H_0</span>: person is healthy):</p>
<ol type="a">
<li><p>What is the Type I error rate?</p></li>
<li><p>What is the Type II error rate?</p></li>
<li><p>What is the power of the test?</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li>Type I error = rejecting <span class="math inline">H_0</span> when true = false positive = 1 - 0.98 = 0.02</li>
<li>Type II error = failing to reject <span class="math inline">H_0</span> when false = false negative = 1 - 0.95 = 0.05</li>
<li>Power = 1 - Type II error = 0.95</li>
</ol>
</div>
</div>
</div>
<ol start="2" type="1">
<li><p><strong>Wald Test Calculation</strong></p>
<p>Death times around Passover (from AoS Exercise 10.6): Of 1919 deaths, 922 occurred the week before Passover and 997 the week after. Test <span class="math inline">H_0: p = 0.5</span> where <span class="math inline">p</span> is the probability of death in the week before.</p>
<p>Calculate the Wald test statistic and p-value.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="6e952cde" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1919</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">922</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> x <span class="op">/</span> n</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>p_0 <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test statistic</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p_0 <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_0) <span class="op">/</span> n)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> (p_hat <span class="op">-</span> p_0) <span class="op">/</span> se</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># p-value (two-sided)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample proportion: </span><span class="sc">{</span>p_hat<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wald statistic: </span><span class="sc">{</span>W<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Conclusion: </span><span class="sc">{</span><span class="st">'Reject H₀'</span> <span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="st">'Fail to reject H₀'</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample proportion: 0.4805
Wald statistic: -1.712
p-value: 0.0869
Conclusion: Fail to reject H₀</code></pre>
</div>
</div>
</div>
</div>
</div>
<ol start="3" type="1">
<li><p><strong>Multiple Testing Correction</strong></p>
<p>You run 10 hypothesis tests and get these p-values:</p>
<pre><code>0.001, 0.004, 0.012, 0.025, 0.041, 0.053, 0.074, 0.135, 0.246, 0.531</code></pre>
<p>At α = 0.05, which hypotheses are rejected using:</p>
<ol type="a">
<li>No correction?</li>
<li>Bonferroni correction?</li>
<li>Benjamini-Hochberg correction?</li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="b7ef4bf8" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.array([<span class="fl">0.001</span>, <span class="fl">0.004</span>, <span class="fl">0.012</span>, <span class="fl">0.025</span>, <span class="fl">0.041</span>, <span class="fl">0.053</span>, <span class="fl">0.074</span>, <span class="fl">0.135</span>, <span class="fl">0.246</span>, <span class="fl">0.531</span>])</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(p_values)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># No correction</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>no_correction <span class="op">=</span> p_values <span class="op">&lt;=</span> alpha</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"No correction: Reject hypotheses </span><span class="sc">{</span>np<span class="sc">.</span>where(no_correction)[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  (</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(no_correction)<span class="sc">}</span><span class="ss"> rejections)"</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Bonferroni</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>bonferroni_threshold <span class="op">=</span> alpha <span class="op">/</span> m</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>bonferroni <span class="op">=</span> p_values <span class="op">&lt;=</span> bonferroni_threshold</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Bonferroni (threshold = </span><span class="sc">{</span>bonferroni_threshold<span class="sc">:.4f}</span><span class="ss">):"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Reject hypotheses </span><span class="sc">{</span>np<span class="sc">.</span>where(bonferroni)[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  (</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(bonferroni)<span class="sc">}</span><span class="ss"> rejections)"</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Benjamini-Hochberg</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>sorted_idx <span class="op">=</span> np.argsort(p_values)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>sorted_p <span class="op">=</span> p_values[sorted_idx]</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>bh_threshold <span class="op">=</span> (np.arange(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>) <span class="op">/</span> m) <span class="op">*</span> alpha</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>bh_reject_sorted <span class="op">=</span> sorted_p <span class="op">&lt;=</span> bh_threshold</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> np.<span class="bu">any</span>(bh_reject_sorted):</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    k_max <span class="op">=</span> np.<span class="bu">max</span>(np.where(bh_reject_sorted)[<span class="dv">0</span>])</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    bh_reject <span class="op">=</span> np.zeros(m, dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    bh_reject[sorted_idx[:k_max<span class="op">+</span><span class="dv">1</span>]] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    bh_reject <span class="op">=</span> np.zeros(m, dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Benjamini-Hochberg:"</span>)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Reject hypotheses </span><span class="sc">{</span>np<span class="sc">.</span>where(bh_reject)[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  (</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(bh_reject)<span class="sc">}</span><span class="ss"> rejections)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>No correction: Reject hypotheses [1 2 3 4 5]
  (5 rejections)

Bonferroni (threshold = 0.0050):
  Reject hypotheses [1 2]
  (2 rejections)

Benjamini-Hochberg:
  Reject hypotheses [1 2 3]
  (3 rejections)</code></pre>
</div>
</div>
</div>
</div>
</div>
<ol start="4" type="1">
<li><p><strong>Permutation Test vs Parametric Test</strong></p>
<p>When would you prefer a permutation test over a Wald test? Give at least three scenarios.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Prefer permutation tests when:</p>
<ol type="1">
<li><strong>Small sample size</strong>: Asymptotic approximations may not hold</li>
<li><strong>Non-standard distributions</strong>: Data is heavily skewed or has outliers</li>
<li><strong>Complex test statistics</strong>: No known distribution for the statistic</li>
<li><strong>Exact p-values needed</strong>: Critical decisions requiring exact inference</li>
<li><strong>Assumptions violated</strong>: Independence or normality assumptions fail</li>
</ol>
</div>
</div>
</div>
</section>
<section id="python-and-r-reference" class="level3" data-number="7.8.5">
<h3 data-number="7.8.5" class="anchored" data-anchor-id="python-and-r-reference"><span class="header-section-number">7.8.5</span> Python and R Reference</h3>
<div class="tabset-margin-container"></div><div class="tabset-margin-container"></div><div class="panel-tabset"><ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1757255603-333-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-333-1" role="tab" aria-controls="tabset-1757255603-333-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1757255603-333-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1757255603-333-2" role="tab" aria-controls="tabset-1757255603-333-2" aria-selected="false" href="">R</a></li></ul><div class="tab-content"><div id="tabset-1757255603-333-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1757255603-333-1-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.stats.multitest <span class="im">as</span> multitest</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test for single proportion</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wald_test_proportion(x, n, p0):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Test H0: p = p0"""</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    p_hat <span class="op">=</span> x <span class="op">/</span> n</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> np.sqrt(p0 <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p0) <span class="op">/</span> n)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> (p_hat <span class="op">-</span> p0) <span class="op">/</span> se</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W, p_value</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-sample t-test (Wald for means)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> stats.ttest_ind(group1, group2)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Paired t-test</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> stats.ttest_rel(sample1, sample2)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test for 2x2 tables</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>odds_ratio, p_value <span class="op">=</span> stats.fisher_exact([[a, b], [c, d]])</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Permutation test</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> permutation_test(x, y, n_permutations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    observed <span class="op">=</span> np.<span class="bu">abs</span>(np.mean(x) <span class="op">-</span> np.mean(y))</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    combined <span class="op">=</span> np.concatenate([x, y])</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_permutations):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        np.random.shuffle(combined)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        x_perm <span class="op">=</span> combined[:<span class="bu">len</span>(x)]</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        y_perm <span class="op">=</span> combined[<span class="bu">len</span>(x):]</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">abs</span>(np.mean(x_perm) <span class="op">-</span> np.mean(y_perm)) <span class="op">&gt;=</span> observed:</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (count <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (n_permutations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple testing corrections</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multitest <span class="im">import</span> multipletests</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Bonferroni and BH corrections</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>reject_bonf, pvals_bonf, _, _ <span class="op">=</span> multipletests(p_values, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'bonferroni'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>reject_bh, pvals_bh, _, _ <span class="op">=</span> multipletests(p_values, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'fdr_bh'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative: Using scipy (newer versions)</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> false_discovery_control</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># This adjusts p-values using Benjamini-Hochberg</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>adjusted_pvals <span class="op">=</span> false_discovery_control(p_values, method<span class="op">=</span><span class="st">'bh'</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Reject hypotheses where adjusted p-value &lt;= alpha</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>reject_scipy_bh <span class="op">=</span> adjusted_pvals <span class="op">&lt;=</span> <span class="fl">0.05</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div><div id="tabset-1757255603-333-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1757255603-333-2-tab"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test for single proportion</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(<span class="at">x =</span> <span class="dv">50</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">p =</span> <span class="fl">0.5</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-sample t-test</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(group1, group2)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Paired t-test</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(sample1, sample2, <span class="at">paired =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">fisher.test</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(a, b, c, d), <span class="at">nrow =</span> <span class="dv">2</span>))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Permutation test (using coin package)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(coin)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">independence_test</span>(outcome <span class="sc">~</span> group, <span class="at">data =</span> df, </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>                  <span class="at">distribution =</span> <span class="fu">approximate</span>(<span class="at">nresample =</span> <span class="dv">10000</span>))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple testing corrections</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">p.adjust</span>(p_values, <span class="at">method =</span> <span class="st">"bonferroni"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">p.adjust</span>(p_values, <span class="at">method =</span> <span class="st">"BH"</span>)  <span class="co"># Benjamini-Hochberg</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Or using multtest package</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(multtest)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">mt.rawp2adjp</span>(p_values, <span class="at">proc =</span> <span class="fu">c</span>(<span class="st">"Bonferroni"</span>, <span class="st">"BH"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></div></div></div>
</section>
<section id="connections-to-source-material" class="level3" data-number="7.8.6">
<h3 data-number="7.8.6" class="anchored" data-anchor-id="connections-to-source-material"><span class="header-section-number">7.8.6</span> Connections to Source Material</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mapping to “All of Statistics”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Lecture Note Section</th>
<th style="text-align: left;">Corresponding Source(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Introduction</strong></td>
<td style="text-align: left;">Lecture slides; drug trial example from slides</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Framework of Hypothesis Testing</strong></td>
<td style="text-align: left;">AoS Ch 10 Intro</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Null and Alternative Hypotheses</td>
<td style="text-align: left;">AoS Ch 10 Intro</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Type I and Type II Errors</td>
<td style="text-align: left;">AoS Table 10.1, Definition 10.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Power and Size</td>
<td style="text-align: left;">AoS Definition 10.1</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>The p-value</strong></td>
<td style="text-align: left;">AoS §10.2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Understanding the p-value</td>
<td style="text-align: left;">AoS §10.2 (Definition 10.11, Theorem 10.12)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Interpretation and Misinterpretation</td>
<td style="text-align: left;">AoS §10.2, expanded with modern critiques</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Hypothesis Tests</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ The Wald Test</td>
<td style="text-align: left;">AoS §10.1 (Definition 10.3, Theorem 10.4)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Statistical and Scientific Significance</td>
<td style="text-align: left;">AoS §10.1, Theorem 10.10 (CI duality)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Comparing Proportions/Means</td>
<td style="text-align: left;">AoS §10.1 (Examples 10.7, 10.8)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Permutation Test</td>
<td style="text-align: left;">AoS §10.5 (Example 10.19)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Fisher’s Exact Test</td>
<td style="text-align: left;">Expanded from slides</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ The Likelihood Ratio Test</td>
<td style="text-align: left;">AoS §10.6</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Multiple Testing Problem</strong></td>
<td style="text-align: left;">AoS §10.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">↳ Bonferroni Correction</td>
<td style="text-align: left;">AoS §10.7 (Theorem 10.24)</td>
</tr>
<tr class="even">
<td style="text-align: left;">↳ Benjamini-Hochberg</td>
<td style="text-align: left;">AoS §10.7 (Theorem 10.26)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>NHST in Practice: A Critical View</strong></td>
<td style="text-align: left;">Expanded from slides</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Self-Test Problems</strong></td>
<td style="text-align: left;">Based on AoS Exercise 10.6 and similar examples</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="further-reading" class="level3" data-number="7.8.7">
<h3 data-number="7.8.7" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">7.8.7</span> Further Reading</h3>
<ul>
<li><strong>Modern perspective</strong>: Wasserstein &amp; Lazar (2016), “The ASA Statement on p-Values”</li>
<li><strong>Critical view</strong>: Ioannidis (2005), “Why Most Published Research Findings Are False”</li>
</ul>
<hr>
<p><em>Remember: Hypothesis testing is a tool for making decisions under uncertainty. Use it wisely – report effect sizes and confidence intervals, correct for multiple testing, and never forget that statistical significance is not the same as practical importance!</em></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-wasserman2013all" class="csl-entry" role="listitem">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>NHST has been central to scientific research for decades, but has also been subject to criticism regarding its misuse and misinterpretation. We’ll explore both its strengths and limitations throughout this chapter, particularly in the sections on interpreting p-values and the critical view of NHST in practice.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In classical hypothesis testing, we <strong>never</strong> “accept” the null hypothesis – we can only keep it because we haven’t found enough evidence to reject it.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/06-parametric-inference-II.html" class="pagination-link" aria-label="Parametric Inference II: Properties of Estimators">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Inference II: Properties of Estimators</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/08-bayesian-inference-decision-theory.html" class="pagination-link" aria-label="Bayesian Inference and Statistical Decision Theory">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference and Statistical Decision Theory</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb18" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hypothesis Testing and p-values</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>After completing this chapter, you will be able to:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain the core framework of null-hypothesis significance testing (NHST)**, including the roles of null/alternative hypotheses, Type I/II errors, and statistical power.</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define the p-value and correctly interpret its meaning**, recognizing its limitations and common misinterpretations.</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply and interpret common hypothesis tests**, such as the Wald test and permutation test, and understand the connection between tests and confidence intervals.</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain the multiple testing problem** and apply standard correction methods like the Bonferroni and Benjamini-Hochberg procedures.</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Critically evaluate the use of NHST and p-values** in data analysis and scientific research.</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>This chapter covers null-hypothesis significance testing (NHST) and p-values, fundamental concepts in statistical inference. The material is adapted from Chapter 10 of @wasserman2013all, supplemented with computational examples and critical perspectives on the use of NHST in modern data science.</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction: Is an Observed Effect Real or Just Random Chance?</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>A drug company is running a clinical trial of a new drug. Patients are randomly assigned to either a treatment group that receives the new drug (100 subjects), or a control group that receives a placebo (other 100 subjects). After a suitable period of observation, the results are tallied:</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>| | Better | Not Better |</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>| :--- | :---: | :---: |</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>| Treated | 50 | 50 |</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>| Control | 40 | 60 |</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>Looking at these numbers, we see that 50% of the treated patients improved, compared to only 40% of the control patients. But is this 10 percentage point difference large enough to represent a *real* effect of the drug, or could it simply be due to random chance in how we happened to assign individuals to groups?</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>Similarly, an online retailer might run an A/B test comparing two different user interfaces. Users are randomly shown either the old system or a new design, and we track their purchase behavior:</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>| | Purchase | No Purchase |</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>| :--- | :---: | :---: |</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>| New System | 850 | 150 |</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>| Old System | 800 | 200 |</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>The new system appears to have a higher conversion rate (85% vs 80%), but again we face the fundamental question: is this difference statistically meaningful, or just random variation?</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>These questions lie at the heart of **Null-Hypothesis Significance Testing (NHST)**, one of the most widely used -- and most debated<span class="ot">[^nhst-debate]</span> -- frameworks in statistics. The core idea is to start by assuming there is *no effect* (the "null hypothesis") and then ask how surprising our observed data would be under that assumption. If the data would be very surprising under the null, we have evidence against it.</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a><span class="ot">[^nhst-debate]: </span>NHST has been central to scientific research for decades, but has also been subject to criticism regarding its misuse and misinterpretation. We'll explore both its strengths and limitations throughout this chapter, particularly in the sections on interpreting p-values and the critical view of NHST in practice.</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>This chapter builds the NHST framework from the ground up, introducing key concepts like p-values, statistical power, and the critical issue of multiple testing. We'll see how these tools help us distinguish real effects from random noise, while also understanding their limitations and why they're often misused in practice.</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a><span class="fu">## Finnish Terminology Reference</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>For Finnish-speaking students, here's a reference table of key terms in this chapter:</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>| English | Finnish | Context |</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>|---------|---------|---------|</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>| Null Hypothesis | Nollahypoteesi | The default assumption of no effect |</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>| Alternative Hypothesis | Vastahypoteesi, vaihtoehtoinen hypoteesi | What we hope to find evidence for |</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>| Simple Hypothesis | Yksinkertainen hypoteesi, pistehypoteesi | Hypothesis that completely specifies the distribution |</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>| Composite Hypothesis | Yhdistetty hypoteesi | Hypothesis that specifies a range of values |</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>| Two-sided Test | Kaksisuuntainen testi | Test detecting differences in either direction |</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>| One-sided Test | Yksitahoinen testi, yksisuuntainen testi | Test detecting differences in one direction |</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>| Rejection Region | Hylkäysalue | Set of outcomes leading to rejection |</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>| Test Statistic | Testisuure | Summary of evidence against null |</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>| Critical Value | Kriittinen arvo | Threshold for rejection |</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>| Type I Error | Hylkäysvirhe | False positive rejection |</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>| Type II Error | Hyväksymisvirhe | False negative (failure to detect) |</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>| Power | Voima | Probability of detecting true effect |</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>| Power function | Voimafunktio | Power as function of parameter |</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>| Size of a test | Testin koko | Maximum Type I error rate |</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>| Statistically significant | Tilastollisesti merkitsevä | Result unlikely under null |</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>| Wald Test | Waldin testi | Test based on asymptotic normality |</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>| Paired test | Parittainen testi | Test for dependent samples |</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>| Permutation test | Permutaatiotesti, satunnaistamistesti | Non-parametric test |</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>| Likelihood ratio statistic | Uskottavuusosamääräsuure | Ratio of likelihoods |</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>| Multiple testing | Monitestaus | Running many tests simultaneously |</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>| False discovery rate (FDR) | Väärien löydösten osuus | Expected proportion of false positives |</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Framework of Hypothesis Testing</span></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a><span class="fu">### Null and Alternative Hypotheses</span></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>When we observe a difference between two groups or a pattern in data, we need a systematic way to determine whether this observation represents a genuine phenomenon or could simply be due to chance. Hypothesis testing provides this framework by setting up two competing explanations and evaluating the evidence against one of them.</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>**Null Hypothesis ($H_0$)**: A statement of "no effect" or "no difference." It's the default assumption we seek to find evidence against.</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>**Alternative Hypothesis ($H_1$)**: The statement we hope to find evidence for, typically representing the presence of an effect or difference.</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>For example, in our drug trial:</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$H_0$: The drug has the same efficacy as the placebo (no effect)</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$H_1$: The drug's efficacy differs from the placebo</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>In the parametric framework we studied in Chapters 5-6, we can often formalize this by partitioning the parameter space $\Theta$ into two disjoint sets $\Theta_0$ and $\Theta_1$, and testing:</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a>$$H_0: \theta \in \Theta_0 \quad \text{versus} \quad H_1: \theta \in \Theta_1$$</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>The nature of the hypotheses determines the type of test:</span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>**Simple hypothesis**: A hypothesis that completely specifies the distribution, such as $\theta = \theta_0$.</span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>**Composite hypothesis**: A hypothesis that specifies a range of values, such as $\theta &gt; \theta_0$ or $\theta &lt; \theta_0$.</span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a>**Two-sided test**: Tests $H_0: \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$ (detects differences in either direction).</span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>**One-sided test**: Tests $H_0: \theta \leq \theta_0$ versus $H_1: \theta &gt; \theta_0$ (or the reverse), detecting differences in a specific direction.</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a>Most scientific applications use two-sided tests, as we're typically interested in detecting any difference from the null, not just differences in a predetermined direction.</span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Machinery of a Test</span></span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a>Once we've specified our hypotheses, we need a systematic procedure for deciding between them based on the observed data. This involves defining what outcomes would lead us to reject the null hypothesis.</span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a>Let $X$ be a random variable with range $\mathcal{X}$.</span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>**Rejection Region** ($R \subset \mathcal{X}$): The subset of outcomes for which we will reject $H_0$. If $X \in R$, we reject the null hypothesis; otherwise, we retain it.^[In classical hypothesis testing, we **never** "accept" the null hypothesis -- we can only keep it because we haven't found enough evidence to reject it.]</span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>**Test Statistic** ($T(X^n)$): A function of the data that summarizes the evidence against $H_0$. Common examples include differences in sample means, ratios of variances, or correlation coefficients.</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a>**Critical Value** ($c$): A threshold used to define the rejection region, often in terms of a test statistic, such as </span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>$$ R = <span class="sc">\{</span>x: T(x) &gt; c<span class="sc">\}</span>  \quad \text{ or } \quad  R = <span class="sc">\{</span>x: |T(x)| &gt; c<span class="sc">\}</span> $$</span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Testing Equality of Means</span></span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a>Suppose we have samples $X_1, \ldots, X_n \sim F_X$ and $Y_1, \ldots, Y_m \sim F_Y$ and want to test:</span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a>$$H_0: \mathbb{E}(X) = \mathbb{E}(Y) \quad \text{versus} \quad H_1: \mathbb{E}(X) \neq \mathbb{E}(Y)$$</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a>The test statistic might be (a scaled version of) the difference in sample means:</span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a>$$T = \bar{X}_n - \bar{Y}_m$$</span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true" tabindex="-1"></a>If $|T|$ is large, we have evidence against $H_0$. The rejection region would be $R = <span class="sc">\{</span>(x_1,\ldots,x_n,y_1,\ldots,y_m): |T| &gt; c<span class="sc">\}</span>$ for some critical value $c$ chosen to control the error rates, as explained in the next section.</span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true" tabindex="-1"></a><span class="fu">### Two Ways to Be Wrong: Type I and Type II Errors</span></span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true" tabindex="-1"></a>When we make a decision based on data, we can make two types of errors. Understanding these errors is crucial for properly designing and interpreting hypothesis tests.</span>
<span id="cb18-146"><a href="#cb18-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-147"><a href="#cb18-147" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-148"><a href="#cb18-148" aria-hidden="true" tabindex="-1"></a>**Type I Error**: Rejecting $H_0$ when $H_0$ is true (false positive). The probability of Type I error is denoted $\alpha$.</span>
<span id="cb18-149"><a href="#cb18-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-150"><a href="#cb18-150" aria-hidden="true" tabindex="-1"></a>**Type II Error**: Failing to reject $H_0$ when $H_1$ is true (false negative). The probability of Type II error is denoted $\beta$.</span>
<span id="cb18-151"><a href="#cb18-151" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-152"><a href="#cb18-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-153"><a href="#cb18-153" aria-hidden="true" tabindex="-1"></a>The possible outcomes of a hypothesis test can be summarized as:</span>
<span id="cb18-154"><a href="#cb18-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-155"><a href="#cb18-155" aria-hidden="true" tabindex="-1"></a>| | $H_0$ True | $H_0$ False |</span>
<span id="cb18-156"><a href="#cb18-156" aria-hidden="true" tabindex="-1"></a>| :--- | :---: | :---: |</span>
<span id="cb18-157"><a href="#cb18-157" aria-hidden="true" tabindex="-1"></a>| **$H_0$ Retained** | ✓ Correct (True Negative) | ✗ Type II Error (False Negative) |</span>
<span id="cb18-158"><a href="#cb18-158" aria-hidden="true" tabindex="-1"></a>| **$H_0$ Rejected** | ✗ Type I Error (False Positive) | ✓ Correct (True Positive) |</span>
<span id="cb18-159"><a href="#cb18-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-160"><a href="#cb18-160" aria-hidden="true" tabindex="-1"></a>Key quantities for characterizing test performance:</span>
<span id="cb18-161"><a href="#cb18-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-162"><a href="#cb18-162" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-163"><a href="#cb18-163" aria-hidden="true" tabindex="-1"></a>**Power Function**: For a test with rejection region $R$, the power function is:</span>
<span id="cb18-164"><a href="#cb18-164" aria-hidden="true" tabindex="-1"></a>$$\beta(\theta) = \mathbb{P}_\theta(X \in R)$$</span>
<span id="cb18-165"><a href="#cb18-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-166"><a href="#cb18-166" aria-hidden="true" tabindex="-1"></a>**Size of a Test**: The maximum probability of Type I error:</span>
<span id="cb18-167"><a href="#cb18-167" aria-hidden="true" tabindex="-1"></a>$$\alpha = \sup_{\theta \in \Theta_0} \beta(\theta)$$</span>
<span id="cb18-168"><a href="#cb18-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-169"><a href="#cb18-169" aria-hidden="true" tabindex="-1"></a>**Level of a Test**: A test has level $\alpha$ if its size is at most $\alpha$.</span>
<span id="cb18-170"><a href="#cb18-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-171"><a href="#cb18-171" aria-hidden="true" tabindex="-1"></a>**Power of a Test**: The probability of correctly rejecting $H_0$ when it's false:</span>
<span id="cb18-172"><a href="#cb18-172" aria-hidden="true" tabindex="-1"></a>$$\text{Power} = 1 - \beta = \mathbb{P}_\theta(\text{Reject } H_0 \mid \theta \in \Theta_1)$$</span>
<span id="cb18-173"><a href="#cb18-173" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-174"><a href="#cb18-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-175"><a href="#cb18-175" aria-hidden="true" tabindex="-1"></a>There's an inherent trade-off between Type I and Type II errors: making it harder to commit a Type I error (lowering $\alpha$) makes it easier to commit a Type II error (increasing $\beta$), thus reducing power. Standard practice is to fix the Type I error rate at a conventional level (typically $\alpha = 0.05$) and design the study to have adequate power (typically 80% or higher).</span>
<span id="cb18-176"><a href="#cb18-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-177"><a href="#cb18-177" aria-hidden="true" tabindex="-1"></a>The relationship between these errors explains why we use asymmetric language: we "reject" or "fail to reject" $H_0$, never "accept" it. Failing to reject doesn't prove $H_0$ is true -- we might simply lack power to detect the effect!</span>
<span id="cb18-178"><a href="#cb18-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-179"><a href="#cb18-179" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb18-180"><a href="#cb18-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-181"><a href="#cb18-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb18-182"><a href="#cb18-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-183"><a href="#cb18-183" aria-hidden="true" tabindex="-1"></a>Think of hypothesis testing like a criminal trial. The null hypothesis is "the defendant is innocent" -- our default assumption until proven otherwise. We only reject this assumption (convict) if the evidence against innocence is very strong.</span>
<span id="cb18-184"><a href="#cb18-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-185"><a href="#cb18-185" aria-hidden="true" tabindex="-1"></a>Just as a trial can go wrong in two ways, so can a hypothesis test:</span>
<span id="cb18-186"><a href="#cb18-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-187"><a href="#cb18-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Type I Error**: Convicting an innocent person (false positive)</span>
<span id="cb18-188"><a href="#cb18-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Type II Error**: Acquitting a guilty person (false negative)</span>
<span id="cb18-189"><a href="#cb18-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-190"><a href="#cb18-190" aria-hidden="true" tabindex="-1"></a>A common illustration uses the <span class="co">[</span><span class="ot">Voight-Kampff test</span><span class="co">](https://en.wikipedia.org/wiki/Blade_Runner#Voight-Kampff_machine)</span>, which detects replicants (human-looking androids):</span>
<span id="cb18-191"><a href="#cb18-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-192"><a href="#cb18-192" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Type I and Type II Errors illustrated with replicant detection. Credits: [gpt-4o](https://openai.com/index/introducing-4o-image-generation/).</span><span class="co">](../images/07_errors_blade_runner.png)</span>{width=70%}</span>
<span id="cb18-193"><a href="#cb18-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-194"><a href="#cb18-194" aria-hidden="true" tabindex="-1"></a>With $H_0$: "The subject is human":</span>
<span id="cb18-195"><a href="#cb18-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-196"><a href="#cb18-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Type I Error**: Test says <span class="in">`REPLICANT`</span> but subject is actually human (wrongly flagged as android).</span>
<span id="cb18-197"><a href="#cb18-197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Type II Error**: Test says <span class="in">`HUMAN`</span> but subject is actually an android (missed detection).</span>
<span id="cb18-198"><a href="#cb18-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-199"><a href="#cb18-199" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb18-200"><a href="#cb18-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-201"><a href="#cb18-201" aria-hidden="true" tabindex="-1"></a>Let's derive the power function for a concrete example. Consider testing the mean of a normal distribution:</span>
<span id="cb18-202"><a href="#cb18-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-203"><a href="#cb18-203" aria-hidden="true" tabindex="-1"></a>**Setup**: We have $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ with known variance $\sigma^2$, testing:</span>
<span id="cb18-204"><a href="#cb18-204" aria-hidden="true" tabindex="-1"></a>$$H_0: \mu = 0 \quad \text{versus} \quad H_1: \mu \neq 0$$</span>
<span id="cb18-205"><a href="#cb18-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-206"><a href="#cb18-206" aria-hidden="true" tabindex="-1"></a>**Test statistic**: The sample mean $\bar{X} \sim \mathcal{N}(\mu, \sigma^2/n)$. Under $H_0$ where $\mu = 0$:</span>
<span id="cb18-207"><a href="#cb18-207" aria-hidden="true" tabindex="-1"></a>$$Z = \frac{\sqrt{n}\bar{X}}{\sigma} \sim \mathcal{N}(0, 1)$$</span>
<span id="cb18-208"><a href="#cb18-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-209"><a href="#cb18-209" aria-hidden="true" tabindex="-1"></a>**Decision rule**: For a level $\alpha$ test, we reject $H_0$ when $|Z| &gt; z_{\alpha/2}$, where $z_{\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution.</span>
<span id="cb18-210"><a href="#cb18-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-211"><a href="#cb18-211" aria-hidden="true" tabindex="-1"></a>**Power function**: For any true value $\mu$, the power is:</span>
<span id="cb18-212"><a href="#cb18-212" aria-hidden="true" tabindex="-1"></a>$$\beta(\mu) = \mathbb{P}_\mu(\text{Reject } H_0) = \mathbb{P}_\mu(|Z| &gt; z_{\alpha/2})$$</span>
<span id="cb18-213"><a href="#cb18-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-214"><a href="#cb18-214" aria-hidden="true" tabindex="-1"></a>Under the true $\mu$, the test statistic follows $Z \sim \mathcal{N}(\sqrt{n}\mu/\sigma, 1)$. Therefore:</span>
<span id="cb18-215"><a href="#cb18-215" aria-hidden="true" tabindex="-1"></a>$$\beta(\mu) = \mathbb{P}\left(\left|\mathcal{N}\left(\frac{\sqrt{n}\mu}{\sigma}, 1\right)\right| &gt; z_{\alpha/2}\right)$$</span>
<span id="cb18-216"><a href="#cb18-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-217"><a href="#cb18-217" aria-hidden="true" tabindex="-1"></a>This probability depends on three key factors:</span>
<span id="cb18-218"><a href="#cb18-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-219"><a href="#cb18-219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Effect size**: $\delta = \mu/\sigma$ (standardized distance from null)  </span>
<span id="cb18-220"><a href="#cb18-220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sample size**: Larger $n$ → higher power</span>
<span id="cb18-221"><a href="#cb18-221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Significance level**: Larger $\alpha$ → higher power (but more Type I errors)</span>
<span id="cb18-222"><a href="#cb18-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-223"><a href="#cb18-223" aria-hidden="true" tabindex="-1"></a>Let's visualize this power function for $H_0: \mu = 0$, $\sigma = 1$, $n = 25$, $\alpha = 0.05$:</span>
<span id="cb18-224"><a href="#cb18-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-227"><a href="#cb18-227" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-228"><a href="#cb18-228" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb18-229"><a href="#cb18-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-230"><a href="#cb18-230" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-231"><a href="#cb18-231" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-232"><a href="#cb18-232" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-233"><a href="#cb18-233" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-234"><a href="#cb18-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-235"><a href="#cb18-235" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing H0: μ = 0 vs H1: μ ≠ 0</span></span>
<span id="cb18-236"><a href="#cb18-236" aria-hidden="true" tabindex="-1"></a><span class="co"># Known variance σ² = 1, sample size n = 25, α = 0.05</span></span>
<span id="cb18-237"><a href="#cb18-237" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb18-238"><a href="#cb18-238" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-239"><a href="#cb18-239" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb18-240"><a href="#cb18-240" aria-hidden="true" tabindex="-1"></a>z_alpha_2 <span class="op">=</span> stats.norm.ppf(<span class="dv">1</span> <span class="op">-</span> alpha<span class="op">/</span><span class="dv">2</span>)  <span class="co"># Critical value for two-sided test</span></span>
<span id="cb18-241"><a href="#cb18-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-242"><a href="#cb18-242" aria-hidden="true" tabindex="-1"></a><span class="co"># Range of true μ values</span></span>
<span id="cb18-243"><a href="#cb18-243" aria-hidden="true" tabindex="-1"></a>mu_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">200</span>)</span>
<span id="cb18-244"><a href="#cb18-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-245"><a href="#cb18-245" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate power using the mathematical formula</span></span>
<span id="cb18-246"><a href="#cb18-246" aria-hidden="true" tabindex="-1"></a>effect_sizes <span class="op">=</span> mu_range <span class="op">/</span> sigma  <span class="co"># δ = (μ - μ₀)/σ</span></span>
<span id="cb18-247"><a href="#cb18-247" aria-hidden="true" tabindex="-1"></a>power <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> stats.norm.cdf(z_alpha_2 <span class="op">-</span> np.sqrt(n) <span class="op">*</span> effect_sizes) <span class="op">+</span> </span>
<span id="cb18-248"><a href="#cb18-248" aria-hidden="true" tabindex="-1"></a>         stats.norm.cdf(<span class="op">-</span>z_alpha_2 <span class="op">-</span> np.sqrt(n) <span class="op">*</span> effect_sizes))</span>
<span id="cb18-249"><a href="#cb18-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-250"><a href="#cb18-250" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-251"><a href="#cb18-251" aria-hidden="true" tabindex="-1"></a>plt.plot(mu_range, power, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Power function β(μ)'</span>)</span>
<span id="cb18-252"><a href="#cb18-252" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>alpha, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="ss">f'Size α = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-253"><a href="#cb18-253" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-254"><a href="#cb18-254" aria-hidden="true" tabindex="-1"></a>plt.fill_between(mu_range, <span class="dv">0</span>, power, alpha<span class="op">=</span><span class="fl">0.15</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb18-255"><a href="#cb18-255" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True μ'</span>)</span>
<span id="cb18-256"><a href="#cb18-256" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'β(μ) = P(Reject H₀)'</span>)</span>
<span id="cb18-257"><a href="#cb18-257" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Power Function: n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, σ=</span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">, α=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-258"><a href="#cb18-258" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb18-259"><a href="#cb18-259" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-260"><a href="#cb18-260" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb18-261"><a href="#cb18-261" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-262"><a href="#cb18-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-263"><a href="#cb18-263" aria-hidden="true" tabindex="-1"></a><span class="co"># Add annotations for key points</span></span>
<span id="cb18-264"><a href="#cb18-264" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'β(0) = α = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="dv">0</span>, alpha), xytext<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.15</span>),</span>
<span id="cb18-265"><a href="#cb18-265" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>))</span>
<span id="cb18-266"><a href="#cb18-266" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Higher power</span><span class="ch">\n</span><span class="st">for μ far from 0'</span>, xy<span class="op">=</span>(<span class="fl">1.5</span>, <span class="fl">0.95</span>), xytext<span class="op">=</span>(<span class="fl">1.2</span>, <span class="fl">0.7</span>),</span>
<span id="cb18-267"><a href="#cb18-267" aria-hidden="true" tabindex="-1"></a>            arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">'-&gt;'</span>, color<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>))</span>
<span id="cb18-268"><a href="#cb18-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-269"><a href="#cb18-269" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-270"><a href="#cb18-270" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-271"><a href="#cb18-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-272"><a href="#cb18-272" aria-hidden="true" tabindex="-1"></a>The mirrored S-shaped curve demonstrates that power is minimized at the boundary of $H_0$ (here, $\mu = 0$) and increases monotonically as the true parameter moves away from the null value, making detecting the difference easier.</span>
<span id="cb18-273"><a href="#cb18-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-274"><a href="#cb18-274" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb18-275"><a href="#cb18-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-276"><a href="#cb18-276" aria-hidden="true" tabindex="-1"></a>Let's simulate hypothesis testing to see Type I and Type II errors in action. We'll test $H_0: \mu = 0$ using one-sample t-tests on data from normal distributions. By generating many datasets under different true means, we can observe the empirical error rates:</span>
<span id="cb18-277"><a href="#cb18-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-280"><a href="#cb18-280" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-281"><a href="#cb18-281" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-282"><a href="#cb18-282" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb18-283"><a href="#cb18-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show simulation code"</span></span>
<span id="cb18-284"><a href="#cb18-284" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-285"><a href="#cb18-285" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-286"><a href="#cb18-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-287"><a href="#cb18-287" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate multiple hypothesis tests</span></span>
<span id="cb18-288"><a href="#cb18-288" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb18-289"><a href="#cb18-289" aria-hidden="true" tabindex="-1"></a>n_tests <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb18-290"><a href="#cb18-290" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb18-291"><a href="#cb18-291" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb18-292"><a href="#cb18-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-293"><a href="#cb18-293" aria-hidden="true" tabindex="-1"></a><span class="co"># Scenario 1: H0 is true (μ = 0)</span></span>
<span id="cb18-294"><a href="#cb18-294" aria-hidden="true" tabindex="-1"></a><span class="co"># We should reject H0 about 5% of the time (Type I error rate)</span></span>
<span id="cb18-295"><a href="#cb18-295" aria-hidden="true" tabindex="-1"></a>type_i_errors <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-296"><a href="#cb18-296" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_tests):</span>
<span id="cb18-297"><a href="#cb18-297" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n_samples)  <span class="co"># H0 true: μ = 0</span></span>
<span id="cb18-298"><a href="#cb18-298" aria-hidden="true" tabindex="-1"></a>    t_stat, p_value <span class="op">=</span> stats.ttest_1samp(sample, <span class="dv">0</span>)</span>
<span id="cb18-299"><a href="#cb18-299" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p_value <span class="op">&lt;</span> alpha:</span>
<span id="cb18-300"><a href="#cb18-300" aria-hidden="true" tabindex="-1"></a>        type_i_errors <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-301"><a href="#cb18-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-302"><a href="#cb18-302" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type I Error Rate (H₀ true, μ=0):"</span>)</span>
<span id="cb18-303"><a href="#cb18-303" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Theoretical: </span><span class="sc">{</span>alpha<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-304"><a href="#cb18-304" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Observed:    </span><span class="sc">{</span>type_i_errors<span class="op">/</span>n_tests<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-305"><a href="#cb18-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-306"><a href="#cb18-306" aria-hidden="true" tabindex="-1"></a><span class="co"># Scenario 2: H0 is false (μ = 0.5)</span></span>
<span id="cb18-307"><a href="#cb18-307" aria-hidden="true" tabindex="-1"></a><span class="co"># Power = probability of correctly rejecting H0</span></span>
<span id="cb18-308"><a href="#cb18-308" aria-hidden="true" tabindex="-1"></a>true_mu <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb18-309"><a href="#cb18-309" aria-hidden="true" tabindex="-1"></a>rejections <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-310"><a href="#cb18-310" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_tests):</span>
<span id="cb18-311"><a href="#cb18-311" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> np.random.normal(true_mu, <span class="dv">1</span>, n_samples)  <span class="co"># H0 false: μ = 0.5</span></span>
<span id="cb18-312"><a href="#cb18-312" aria-hidden="true" tabindex="-1"></a>    t_stat, p_value <span class="op">=</span> stats.ttest_1samp(sample, <span class="dv">0</span>)</span>
<span id="cb18-313"><a href="#cb18-313" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p_value <span class="op">&lt;</span> alpha:</span>
<span id="cb18-314"><a href="#cb18-314" aria-hidden="true" tabindex="-1"></a>        rejections <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-315"><a href="#cb18-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-316"><a href="#cb18-316" aria-hidden="true" tabindex="-1"></a>power <span class="op">=</span> rejections <span class="op">/</span> n_tests</span>
<span id="cb18-317"><a href="#cb18-317" aria-hidden="true" tabindex="-1"></a>type_ii_error_rate <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> power</span>
<span id="cb18-318"><a href="#cb18-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-319"><a href="#cb18-319" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">When H₀ is false (true μ=</span><span class="sc">{</span>true_mu<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb18-320"><a href="#cb18-320" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Power (correct rejection):     </span><span class="sc">{</span>power<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-321"><a href="#cb18-321" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Type II Error Rate (miss):     </span><span class="sc">{</span>type_ii_error_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-322"><a href="#cb18-322" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-323"><a href="#cb18-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-324"><a href="#cb18-324" aria-hidden="true" tabindex="-1"></a>Factors affecting power:</span>
<span id="cb18-325"><a href="#cb18-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-326"><a href="#cb18-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Effect size**: Larger true differences from $H_0$ → higher power</span>
<span id="cb18-327"><a href="#cb18-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sample size**: More data → higher power</span>
<span id="cb18-328"><a href="#cb18-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Variability**: Lower variance → higher power</span>
<span id="cb18-329"><a href="#cb18-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Significance level**: Higher $\alpha$ → higher power (but more Type I errors)</span>
<span id="cb18-330"><a href="#cb18-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-331"><a href="#cb18-331" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-332"><a href="#cb18-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-333"><a href="#cb18-333" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Wald Test</span></span>
<span id="cb18-334"><a href="#cb18-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-335"><a href="#cb18-335" aria-hidden="true" tabindex="-1"></a>The Wald test is one of the most widely used hypothesis tests in statistics. It leverages the asymptotic normality of estimators that we studied in Chapters 5-6, providing a general framework for testing hypotheses about parameters.</span>
<span id="cb18-336"><a href="#cb18-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-337"><a href="#cb18-337" aria-hidden="true" tabindex="-1"></a>The key insight: If our estimator $\hat{\theta}$ is approximately normal (which many estimators are for large samples), we can use this normality to construct a test statistic with a known distribution under the null hypothesis.</span>
<span id="cb18-338"><a href="#cb18-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-339"><a href="#cb18-339" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-340"><a href="#cb18-340" aria-hidden="true" tabindex="-1"></a>**The Wald Test**: For testing $H_0: \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$:</span>
<span id="cb18-341"><a href="#cb18-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-342"><a href="#cb18-342" aria-hidden="true" tabindex="-1"></a>Assume that $\hat{\theta}$ is asymptotically normal:</span>
<span id="cb18-343"><a href="#cb18-343" aria-hidden="true" tabindex="-1"></a>$$\frac{(\hat{\theta} - \theta_0)}{\widehat{\text{se}}} \rightsquigarrow \mathcal{N}(0,1)$$</span>
<span id="cb18-344"><a href="#cb18-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-345"><a href="#cb18-345" aria-hidden="true" tabindex="-1"></a>The size $\alpha$ Wald test rejects $H_0$ when $|W| &gt; z_{\alpha/2}$, where:</span>
<span id="cb18-346"><a href="#cb18-346" aria-hidden="true" tabindex="-1"></a>$$W = \frac{\hat{\theta} - \theta_0}{\widehat{\text{se}}}$$</span>
<span id="cb18-347"><a href="#cb18-347" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-348"><a href="#cb18-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-349"><a href="#cb18-349" aria-hidden="true" tabindex="-1"></a>**Key Properties of the Wald Test:**</span>
<span id="cb18-350"><a href="#cb18-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-351"><a href="#cb18-351" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Correct error rate**: For large samples, the test has (approximately) the desired Type I error rate $\alpha$</span>
<span id="cb18-352"><a href="#cb18-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-353"><a href="#cb18-353" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Power increases with**:</span>
<span id="cb18-354"><a href="#cb18-354" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Larger effect size (bigger difference from $\theta_0$)</span>
<span id="cb18-355"><a href="#cb18-355" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Larger sample size (more data)</span>
<span id="cb18-356"><a href="#cb18-356" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Smaller variance (less noise)</span>
<span id="cb18-357"><a href="#cb18-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-358"><a href="#cb18-358" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Duality with confidence intervals**: The Wald test rejects $H_0: \theta = \theta_0$ if and only if $\theta_0$ is outside the $(1-\alpha)$ confidence interval</span>
<span id="cb18-359"><a href="#cb18-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-360"><a href="#cb18-360" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-361"><a href="#cb18-361" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical Details</span></span>
<span id="cb18-362"><a href="#cb18-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-363"><a href="#cb18-363" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Formal Properties of the Wald Test"}</span>
<span id="cb18-364"><a href="#cb18-364" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Asymptotic size**: As $n \to \infty$, $\mathbb{P}_{\theta_0}(|W| &gt; z_{\alpha/2}) \to \alpha$</span>
<span id="cb18-365"><a href="#cb18-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-366"><a href="#cb18-366" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Power function**: When the true parameter is $\theta_* \neq \theta_0$, the power is:</span>
<span id="cb18-367"><a href="#cb18-367" aria-hidden="true" tabindex="-1"></a>   $$\text{Power}(\theta_*) = 1 - \Phi\left(\frac{\theta_0 - \theta_*}{\widehat{\text{se}}} + z_{\alpha/2}\right) + \Phi\left(\frac{\theta_0 - \theta_*}{\widehat{\text{se}}} - z_{\alpha/2}\right)$$</span>
<span id="cb18-368"><a href="#cb18-368" aria-hidden="true" tabindex="-1"></a>   where $\Phi$ is the standard normal CDF.</span>
<span id="cb18-369"><a href="#cb18-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-370"><a href="#cb18-370" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Confidence interval duality**: Reject $H_0$ if and only if $\theta_0 \notin [\hat{\theta} - z_{\alpha/2} \cdot \widehat{\text{se}}, \hat{\theta} + z_{\alpha/2} \cdot \widehat{\text{se}}]$</span>
<span id="cb18-371"><a href="#cb18-371" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-372"><a href="#cb18-372" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-373"><a href="#cb18-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-374"><a href="#cb18-374" aria-hidden="true" tabindex="-1"></a>This last property provides a powerful duality: every confidence interval can be inverted to give a hypothesis test, and vice versa. This is why confidence intervals are often more informative than p-values alone -- they show all parameter values that wouldn't be rejected.</span>
<span id="cb18-375"><a href="#cb18-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-376"><a href="#cb18-376" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb18-377"><a href="#cb18-377" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Comparing Two Means</span></span>
<span id="cb18-378"><a href="#cb18-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-379"><a href="#cb18-379" aria-hidden="true" tabindex="-1"></a>The Wald test extends naturally to comparing means from two populations. For independent samples $X_1, \ldots, X_m$ and $Y_1, \ldots, Y_n$ with means $\mu_1$ and $\mu_2$:</span>
<span id="cb18-380"><a href="#cb18-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-381"><a href="#cb18-381" aria-hidden="true" tabindex="-1"></a>Testing $H_0: \mu_1 = \mu_2$ (or $\delta = \mu_1 - \mu_2 = 0$):</span>
<span id="cb18-382"><a href="#cb18-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-383"><a href="#cb18-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimator: $\hat{\delta} = \bar{X} - \bar{Y}$</span>
<span id="cb18-384"><a href="#cb18-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standard error: $\widehat{\text{se}} = \sqrt{s_1^2/m + s_2^2/n}$</span>
<span id="cb18-385"><a href="#cb18-385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Test statistic: $W = \hat{\delta} / \widehat{\text{se}}$</span>
<span id="cb18-386"><a href="#cb18-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-387"><a href="#cb18-387" aria-hidden="true" tabindex="-1"></a>This is the basis for the famous two-sample t-test (which uses a t-distribution for small samples instead of the normal approximation).</span>
<span id="cb18-388"><a href="#cb18-388" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-389"><a href="#cb18-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-390"><a href="#cb18-390" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistical and Scientific Significance</span></span>
<span id="cb18-391"><a href="#cb18-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-392"><a href="#cb18-392" aria-hidden="true" tabindex="-1"></a>When $H_0$ is rejected, the result is called **statistically significant**. This does **not** mean that the result has any practical or scientific relevance! The effect found could be very tiny and negligible for all practical purpose.</span>
<span id="cb18-393"><a href="#cb18-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-394"><a href="#cb18-394" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb18-395"><a href="#cb18-395" aria-hidden="true" tabindex="-1"></a><span class="fu">## Statistical vs. Scientific Significance</span></span>
<span id="cb18-396"><a href="#cb18-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-397"><a href="#cb18-397" aria-hidden="true" tabindex="-1"></a>**Statistical significance** ≠ **Scientific significance**</span>
<span id="cb18-398"><a href="#cb18-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-399"><a href="#cb18-399" aria-hidden="true" tabindex="-1"></a>Let's assume $\alpha = 0.05$ and $\theta_0 = 0$. In this case, we reject the null if the 95 \% confidence interval (CI) excludes 0. A result can be:</span>
<span id="cb18-400"><a href="#cb18-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-401"><a href="#cb18-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistically significant but scientifically trivial**: The CI excludes 0 but is very close to it (where "very close" depends on the domain).</span>
<span id="cb18-402"><a href="#cb18-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistically non-significant but scientifically important**: The CI includes 0 but also includes large, meaningful effects.</span>
<span id="cb18-403"><a href="#cb18-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-404"><a href="#cb18-404" aria-hidden="true" tabindex="-1"></a>Always report confidence intervals alongside p-values (described in the next section) to show both statistical and practical significance!</span>
<span id="cb18-405"><a href="#cb18-405" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-406"><a href="#cb18-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-407"><a href="#cb18-407" aria-hidden="true" tabindex="-1"></a>Let's visualize this distinction:</span>
<span id="cb18-408"><a href="#cb18-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-411"><a href="#cb18-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-412"><a href="#cb18-412" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-413"><a href="#cb18-413" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-414"><a href="#cb18-414" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-415"><a href="#cb18-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-416"><a href="#cb18-416" aria-hidden="true" tabindex="-1"></a><span class="co"># Two example confidence intervals</span></span>
<span id="cb18-417"><a href="#cb18-417" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-418"><a href="#cb18-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-419"><a href="#cb18-419" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 1: Statistically significant but small effect</span></span>
<span id="cb18-420"><a href="#cb18-420" aria-hidden="true" tabindex="-1"></a>theta0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-421"><a href="#cb18-421" aria-hidden="true" tabindex="-1"></a>ci1_lower, ci1_upper <span class="op">=</span> <span class="fl">0.001</span>, <span class="fl">0.003</span></span>
<span id="cb18-422"><a href="#cb18-422" aria-hidden="true" tabindex="-1"></a>ci1_center <span class="op">=</span> (ci1_lower <span class="op">+</span> ci1_upper) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb18-423"><a href="#cb18-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-424"><a href="#cb18-424" aria-hidden="true" tabindex="-1"></a>ax1.axvline(theta0, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'H₀: θ = 0'</span>)</span>
<span id="cb18-425"><a href="#cb18-425" aria-hidden="true" tabindex="-1"></a>ax1.barh(<span class="dv">0</span>, ci1_upper <span class="op">-</span> ci1_lower, left<span class="op">=</span>ci1_lower, height<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb18-426"><a href="#cb18-426" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb18-427"><a href="#cb18-427" aria-hidden="true" tabindex="-1"></a>ax1.plot(ci1_center, <span class="dv">0</span>, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'Estimate'</span>)</span>
<span id="cb18-428"><a href="#cb18-428" aria-hidden="true" tabindex="-1"></a>ax1.set_xlim(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">0.01</span>)</span>
<span id="cb18-429"><a href="#cb18-429" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb18-430"><a href="#cb18-430" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb18-431"><a href="#cb18-431" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Statistically Significant</span><span class="ch">\n</span><span class="st">but Tiny Effect'</span>)</span>
<span id="cb18-432"><a href="#cb18-432" aria-hidden="true" tabindex="-1"></a>ax1.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb18-433"><a href="#cb18-433" aria-hidden="true" tabindex="-1"></a>ax1.set_yticks([])</span>
<span id="cb18-434"><a href="#cb18-434" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-435"><a href="#cb18-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-436"><a href="#cb18-436" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 2: Not significant but large potential effect</span></span>
<span id="cb18-437"><a href="#cb18-437" aria-hidden="true" tabindex="-1"></a>ci2_lower, ci2_upper <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span></span>
<span id="cb18-438"><a href="#cb18-438" aria-hidden="true" tabindex="-1"></a>ci2_center <span class="op">=</span> (ci2_lower <span class="op">+</span> ci2_upper) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb18-439"><a href="#cb18-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-440"><a href="#cb18-440" aria-hidden="true" tabindex="-1"></a>ax2.axvline(theta0, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'H₀: θ = 0'</span>)</span>
<span id="cb18-441"><a href="#cb18-441" aria-hidden="true" tabindex="-1"></a>ax2.barh(<span class="dv">0</span>, ci2_upper <span class="op">-</span> ci2_lower, left<span class="op">=</span>ci2_lower, height<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb18-442"><a href="#cb18-442" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb18-443"><a href="#cb18-443" aria-hidden="true" tabindex="-1"></a>ax2.plot(ci2_center, <span class="dv">0</span>, <span class="st">'ko'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'Estimate'</span>)</span>
<span id="cb18-444"><a href="#cb18-444" aria-hidden="true" tabindex="-1"></a>ax2.set_xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb18-445"><a href="#cb18-445" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb18-446"><a href="#cb18-446" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Parameter value'</span>)</span>
<span id="cb18-447"><a href="#cb18-447" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Not Statistically Significant</span><span class="ch">\n</span><span class="st">but Large Potential Effect'</span>)</span>
<span id="cb18-448"><a href="#cb18-448" aria-hidden="true" tabindex="-1"></a>ax2.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb18-449"><a href="#cb18-449" aria-hidden="true" tabindex="-1"></a>ax2.set_yticks([])</span>
<span id="cb18-450"><a href="#cb18-450" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-451"><a href="#cb18-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-452"><a href="#cb18-452" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-453"><a href="#cb18-453" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-454"><a href="#cb18-454" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-455"><a href="#cb18-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-456"><a href="#cb18-456" aria-hidden="true" tabindex="-1"></a>The visualization demonstrates the CI-test duality: using significance level $\alpha = 0.05$, we construct a 95% CI (since $1 - \alpha = 0.95$). We reject $H_0: \theta = 0$ if and only if 0 lies outside this interval.</span>
<span id="cb18-457"><a href="#cb18-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-458"><a href="#cb18-458" aria-hidden="true" tabindex="-1"></a>**Left panel**: The 95% CI excludes 0, so we reject $H_0$ at the 5% level. But the effect is tiny (0.001 to 0.003) -- statistically significant but practically negligible.</span>
<span id="cb18-459"><a href="#cb18-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-460"><a href="#cb18-460" aria-hidden="true" tabindex="-1"></a>**Right panel**: The 95% CI includes 0, so we fail to reject $H_0$. However, the interval also includes large values (up to 2.5) -- we simply lack precision to determine if there's an effect.</span>
<span id="cb18-461"><a href="#cb18-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-462"><a href="#cb18-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-463"><a href="#cb18-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-464"><a href="#cb18-464" aria-hidden="true" tabindex="-1"></a><span class="fu">## The p-value: A Continuous Measure of Evidence</span></span>
<span id="cb18-465"><a href="#cb18-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-466"><a href="#cb18-466" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding the p-value</span></span>
<span id="cb18-467"><a href="#cb18-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-468"><a href="#cb18-468" aria-hidden="true" tabindex="-1"></a>Simply reporting "reject $H_0$" or "retain $H_0$" isn't very informative -- the result depends entirely on the chosen significance level $\alpha$. What if we could provide a continuous measure of the strength of evidence against the null hypothesis? This is exactly what the p-value provides.</span>
<span id="cb18-469"><a href="#cb18-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-470"><a href="#cb18-470" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-471"><a href="#cb18-471" aria-hidden="true" tabindex="-1"></a>The **p-value** is the probability, calculated under $H_0$, of observing a test statistic value at least as extreme as the one actually observed:</span>
<span id="cb18-472"><a href="#cb18-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-473"><a href="#cb18-473" aria-hidden="true" tabindex="-1"></a>$$\text{p-value} = \mathbb{P}_{H_0}(T \text{ as extreme or more extreme than } T_\text{obs})$$</span>
<span id="cb18-474"><a href="#cb18-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-475"><a href="#cb18-475" aria-hidden="true" tabindex="-1"></a>Equivalently, it's the smallest significance level $\alpha$ at which we would reject $H_0$:</span>
<span id="cb18-476"><a href="#cb18-476" aria-hidden="true" tabindex="-1"></a>$$\text{p-value} = \inf<span class="sc">\{</span>\alpha : T(X^n) \in R_\alpha<span class="sc">\}</span>$$</span>
<span id="cb18-477"><a href="#cb18-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-478"><a href="#cb18-478" aria-hidden="true" tabindex="-1"></a>where $R_\alpha$ is the rejection region for a size $\alpha$ test.</span>
<span id="cb18-479"><a href="#cb18-479" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-480"><a href="#cb18-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-481"><a href="#cb18-481" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-482"><a href="#cb18-482" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why are these two definitions equivalent?</span></span>
<span id="cb18-483"><a href="#cb18-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-484"><a href="#cb18-484" aria-hidden="true" tabindex="-1"></a>The rejection region $R_\alpha$ contains the most extreme $\alpha$ proportion of possible test statistics. If our observed statistic is "more extreme than 2.1% of possible values" (p = 0.021), then:</span>
<span id="cb18-485"><a href="#cb18-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-486"><a href="#cb18-486" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We'd reject for any $\alpha \geq 0.021$ (our observation is extreme enough)</span>
<span id="cb18-487"><a href="#cb18-487" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We'd fail to reject for any $\alpha &lt; 0.021$ (our observation isn't extreme enough)  </span>
<span id="cb18-488"><a href="#cb18-488" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Thus 0.021 is exactly the boundary -- the smallest $\alpha$ for which we'd reject</span>
<span id="cb18-489"><a href="#cb18-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-490"><a href="#cb18-490" aria-hidden="true" tabindex="-1"></a>The p-value acts as a threshold: it tells us both how extreme our observation is AND the critical significance level where our decision switches.</span>
<span id="cb18-491"><a href="#cb18-491" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-492"><a href="#cb18-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-493"><a href="#cb18-493" aria-hidden="true" tabindex="-1"></a>**Key properties of p-values:**</span>
<span id="cb18-494"><a href="#cb18-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-495"><a href="#cb18-495" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Range**: p-values lie between 0 and 1</span>
<span id="cb18-496"><a href="#cb18-496" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Interpretation**: Small p-value = strong evidence against $H_0$</span>
<span id="cb18-497"><a href="#cb18-497" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Decision rule**: Reject $H_0$ if p-value $&lt; \alpha$</span>
<span id="cb18-498"><a href="#cb18-498" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Under $H_0$**: The p-value follows a $\text{Uniform}(0,1)$ distribution</span>
<span id="cb18-499"><a href="#cb18-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-500"><a href="#cb18-500" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb18-501"><a href="#cb18-501" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: p-value for the Wald Test</span></span>
<span id="cb18-502"><a href="#cb18-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-503"><a href="#cb18-503" aria-hidden="true" tabindex="-1"></a>For the Wald test we studied earlier, if $w = (\hat{\theta} - \theta_0)/\widehat{\text{se}}$ is the observed Wald statistic, then:</span>
<span id="cb18-504"><a href="#cb18-504" aria-hidden="true" tabindex="-1"></a>$$\text{p-value} = \mathbb{P}(|Z| &gt; |w|) = 2\Phi(-|w|)$$</span>
<span id="cb18-505"><a href="#cb18-505" aria-hidden="true" tabindex="-1"></a>where $Z \sim \mathcal{N}(0,1)$ and $\Phi$ is the standard normal CDF.</span>
<span id="cb18-506"><a href="#cb18-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-507"><a href="#cb18-507" aria-hidden="true" tabindex="-1"></a>This formula works because under $H_0$, the Wald statistic $W \sim \mathcal{N}(0,1)$ asymptotically, so we calculate the probability of seeing a value as extreme as our observed $w$ from a standard normal distribution.</span>
<span id="cb18-508"><a href="#cb18-508" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-509"><a href="#cb18-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-510"><a href="#cb18-510" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb18-511"><a href="#cb18-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-512"><a href="#cb18-512" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb18-513"><a href="#cb18-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-514"><a href="#cb18-514" aria-hidden="true" tabindex="-1"></a>The p-value is often called a "surprise index." It answers the question: "If there were truly no effect (if $H_0$ were true), how likely would we be to see a result at least as extreme as the one we actually observed?"</span>
<span id="cb18-515"><a href="#cb18-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-516"><a href="#cb18-516" aria-hidden="true" tabindex="-1"></a>Think of it this way: Imagine you suspect a coin is biased. You flip it 10 times and get 9 heads. The p-value asks: "If this were actually a fair coin, what's the probability of getting 9 or more heads in 10 flips?" If that probability is very small, you have strong evidence the coin isn't fair.</span>
<span id="cb18-517"><a href="#cb18-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-518"><a href="#cb18-518" aria-hidden="true" tabindex="-1"></a>A small p-value means our data would be very surprising under the null hypothesis, providing evidence against it. A large p-value means our data is consistent with the null hypothesis (though this doesn't prove the null is true! Maybe we simply didn't collect enough data).</span>
<span id="cb18-519"><a href="#cb18-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-520"><a href="#cb18-520" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb18-521"><a href="#cb18-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-522"><a href="#cb18-522" aria-hidden="true" tabindex="-1"></a>Let $T(X^n)$ be the test statistic and $t_\text{obs} = T(x^n)$ be its observed value from the data. The p-value formalizes the "surprise index" under the null hypothesis $H_0$.</span>
<span id="cb18-523"><a href="#cb18-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-524"><a href="#cb18-524" aria-hidden="true" tabindex="-1"></a>**1. Simple Null Hypothesis ($H_0: \theta = \theta_0$)**</span>
<span id="cb18-525"><a href="#cb18-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-526"><a href="#cb18-526" aria-hidden="true" tabindex="-1"></a>For a simple null, the p-value is the probability of observing a test statistic at least as extreme as $t_\text{obs}$. The definition of "extreme" depends on the alternative hypothesis $H_1$:</span>
<span id="cb18-527"><a href="#cb18-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-528"><a href="#cb18-528" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Right-tailed test** ($H_1: \theta &gt; \theta_0$): Extreme means large values of $T$.</span>
<span id="cb18-529"><a href="#cb18-529" aria-hidden="true" tabindex="-1"></a>  $$ \text{p-value} = \mathbb{P}_{\theta_0}(T(X^n) \geq t_\text{obs}) $$</span>
<span id="cb18-530"><a href="#cb18-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Left-tailed test** ($H_1: \theta &lt; \theta_0$): Extreme means small values of $T$.</span>
<span id="cb18-531"><a href="#cb18-531" aria-hidden="true" tabindex="-1"></a>  $$ \text{p-value} = \mathbb{P}_{\theta_0}(T(X^n) \leq t_\text{obs}) $$</span>
<span id="cb18-532"><a href="#cb18-532" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Two-tailed test** ($H_1: \theta \neq \theta_0$): Extreme means large values of $|T|$ (or similar symmetric measure).</span>
<span id="cb18-533"><a href="#cb18-533" aria-hidden="true" tabindex="-1"></a>  $$ \text{p-value} = \mathbb{P}_{\theta_0}(|T(X^n)| \geq |t_\text{obs}|) $$</span>
<span id="cb18-534"><a href="#cb18-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-535"><a href="#cb18-535" aria-hidden="true" tabindex="-1"></a>**2. Composite Null Hypothesis ($H_0: \theta \in \Theta_0$)**</span>
<span id="cb18-536"><a href="#cb18-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-537"><a href="#cb18-537" aria-hidden="true" tabindex="-1"></a>When the null hypothesis is composite (e.g., $H_0: \mu \le 0$), there isn't a single distribution under $H_0$. We need to find the probability of an extreme result under the "worst-case" scenario within $\Theta_0$ -- the one that makes our data look least surprising. This is achieved by taking the supremum (least upper bound) of the probability over all possible parameter values in $\Theta_0$.</span>
<span id="cb18-538"><a href="#cb18-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-539"><a href="#cb18-539" aria-hidden="true" tabindex="-1"></a>For a right-tailed test, the formula is:</span>
<span id="cb18-540"><a href="#cb18-540" aria-hidden="true" tabindex="-1"></a>$$ \text{p-value} = \sup_{\theta \in \Theta_0} \mathbb{P}_\theta(T(X^n) \geq t_\text{obs}) $$</span>
<span id="cb18-541"><a href="#cb18-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-542"><a href="#cb18-542" aria-hidden="true" tabindex="-1"></a>This ensures that if we reject when p-value $&lt; \alpha$, the Type I error rate is controlled and does not exceed $\alpha$ for any $\theta \in \Theta_0$. For many standard tests, this supremum occurs at the boundary of $\Theta_0$ (e.g., at $\mu=0$ for $H_0: \mu \le 0$).</span>
<span id="cb18-543"><a href="#cb18-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-544"><a href="#cb18-544" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb18-545"><a href="#cb18-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-546"><a href="#cb18-546" aria-hidden="true" tabindex="-1"></a>Let's visualize how the p-value is calculated for a two-sided test. We'll use the Wald test as our example, where the test statistic follows a standard normal distribution under $H_0$.</span>
<span id="cb18-547"><a href="#cb18-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-548"><a href="#cb18-548" aria-hidden="true" tabindex="-1"></a>For a two-sided test (e.g., $H_0: \theta = \theta_0$ vs $H_1: \theta \neq \theta_0$), "extreme" means far from zero in either direction. The p-value is the total probability in both tails beyond our observed test statistic:</span>
<span id="cb18-549"><a href="#cb18-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-552"><a href="#cb18-552" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-553"><a href="#cb18-553" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-554"><a href="#cb18-554" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-555"><a href="#cb18-555" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-556"><a href="#cb18-556" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-557"><a href="#cb18-557" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-558"><a href="#cb18-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-559"><a href="#cb18-559" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Observed test statistic value</span></span>
<span id="cb18-560"><a href="#cb18-560" aria-hidden="true" tabindex="-1"></a>w_obs <span class="op">=</span> <span class="fl">2.3</span>  <span class="co"># Our observed Wald statistic</span></span>
<span id="cb18-561"><a href="#cb18-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-562"><a href="#cb18-562" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the standard normal distribution</span></span>
<span id="cb18-563"><a href="#cb18-563" aria-hidden="true" tabindex="-1"></a>z_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">300</span>)</span>
<span id="cb18-564"><a href="#cb18-564" aria-hidden="true" tabindex="-1"></a>pdf_values <span class="op">=</span> stats.norm.pdf(z_values)</span>
<span id="cb18-565"><a href="#cb18-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-566"><a href="#cb18-566" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate p-value</span></span>
<span id="cb18-567"><a href="#cb18-567" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(w_obs))</span>
<span id="cb18-568"><a href="#cb18-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-569"><a href="#cb18-569" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-570"><a href="#cb18-570" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution</span></span>
<span id="cb18-571"><a href="#cb18-571" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, pdf_values, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Standard Normal'</span>)</span>
<span id="cb18-572"><a href="#cb18-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-573"><a href="#cb18-573" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade the rejection regions (tails)</span></span>
<span id="cb18-574"><a href="#cb18-574" aria-hidden="true" tabindex="-1"></a>tail_right <span class="op">=</span> z_values[z_values <span class="op">&gt;=</span> <span class="bu">abs</span>(w_obs)]</span>
<span id="cb18-575"><a href="#cb18-575" aria-hidden="true" tabindex="-1"></a>tail_left <span class="op">=</span> z_values[z_values <span class="op">&lt;=</span> <span class="op">-</span><span class="bu">abs</span>(w_obs)]</span>
<span id="cb18-576"><a href="#cb18-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-577"><a href="#cb18-577" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tail_right, <span class="dv">0</span>, stats.norm.pdf(tail_right), </span>
<span id="cb18-578"><a href="#cb18-578" aria-hidden="true" tabindex="-1"></a>                  color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="ss">f'Right tail'</span>)</span>
<span id="cb18-579"><a href="#cb18-579" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tail_left, <span class="dv">0</span>, stats.norm.pdf(tail_left), </span>
<span id="cb18-580"><a href="#cb18-580" aria-hidden="true" tabindex="-1"></a>                  color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="ss">f'Left tail'</span>)</span>
<span id="cb18-581"><a href="#cb18-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-582"><a href="#cb18-582" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the observed values</span></span>
<span id="cb18-583"><a href="#cb18-583" aria-hidden="true" tabindex="-1"></a>plt.axvline(w_obs, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Observed: w = </span><span class="sc">{</span>w_obs<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-584"><a href="#cb18-584" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="op">-</span>w_obs, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-585"><a href="#cb18-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-586"><a href="#cb18-586" aria-hidden="true" tabindex="-1"></a><span class="co"># Add annotations</span></span>
<span id="cb18-587"><a href="#cb18-587" aria-hidden="true" tabindex="-1"></a>plt.text(w_obs <span class="op">+</span> <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="ss">f'α/2 = </span><span class="sc">{</span>p_value<span class="op">/</span><span class="dv">2</span><span class="sc">:.4f}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-588"><a href="#cb18-588" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="op">-</span>w_obs <span class="op">-</span> <span class="fl">0.8</span>, <span class="fl">0.05</span>, <span class="ss">f'α/2 = </span><span class="sc">{</span>p_value<span class="op">/</span><span class="dv">2</span><span class="sc">:.4f}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-589"><a href="#cb18-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-590"><a href="#cb18-590" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Test Statistic Value'</span>)</span>
<span id="cb18-591"><a href="#cb18-591" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb18-592"><a href="#cb18-592" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'p-value Calculation: p = </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb18-593"><a href="#cb18-593" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb18-594"><a href="#cb18-594" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-595"><a href="#cb18-595" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.45</span>)</span>
<span id="cb18-596"><a href="#cb18-596" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-597"><a href="#cb18-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-598"><a href="#cb18-598" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For observed test statistic w = </span><span class="sc">{</span>w_obs<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb18-599"><a href="#cb18-599" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value = 2 * P(Z &gt; |</span><span class="sc">{</span>w_obs<span class="sc">}</span><span class="ss">|) = </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-600"><a href="#cb18-600" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation: If H₀ were true, we'd see a test statistic"</span>)</span>
<span id="cb18-601"><a href="#cb18-601" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"this extreme or more extreme only </span><span class="sc">{</span>p_value<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% of the time."</span>)</span>
<span id="cb18-602"><a href="#cb18-602" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-603"><a href="#cb18-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-604"><a href="#cb18-604" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-605"><a href="#cb18-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-606"><a href="#cb18-606" aria-hidden="true" tabindex="-1"></a><span class="fu">### How to Interpret p-values (and How Not To)</span></span>
<span id="cb18-607"><a href="#cb18-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-608"><a href="#cb18-608" aria-hidden="true" tabindex="-1"></a>The p-value is one of the most misunderstood concepts in statistics. Let's clarify what it is and isn't.</span>
<span id="cb18-609"><a href="#cb18-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-610"><a href="#cb18-610" aria-hidden="true" tabindex="-1"></a>**What the p-value IS:**</span>
<span id="cb18-611"><a href="#cb18-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-612"><a href="#cb18-612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The p-value IS the probability**, computed under $H_0$, of observing data as extreme or more extreme than what we actually observed</span>
<span id="cb18-613"><a href="#cb18-613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The p-value IS a measure of evidence** against $H_0$ -- smaller values indicate stronger evidence</span>
<span id="cb18-614"><a href="#cb18-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The p-value IS the answer** to: "If $H_0$ were true, how surprising would our data be?"</span>
<span id="cb18-615"><a href="#cb18-615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The p-value IS useful** for deciding whether to reject $H_0$ at any given significance level</span>
<span id="cb18-616"><a href="#cb18-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-617"><a href="#cb18-617" aria-hidden="true" tabindex="-1"></a>A p-value of 0.02 means: If the null hypothesis were true, we'd see data this extreme or more extreme only 2% of the time. That's fairly surprising, suggesting the null might be false.</span>
<span id="cb18-618"><a href="#cb18-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-619"><a href="#cb18-619" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb18-620"><a href="#cb18-620" aria-hidden="true" tabindex="-1"></a><span class="fu">## Common p-value Misinterpretations</span></span>
<span id="cb18-621"><a href="#cb18-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-622"><a href="#cb18-622" aria-hidden="true" tabindex="-1"></a>**A p-value is NOT the probability that the null hypothesis is true.** </span>
<span id="cb18-623"><a href="#cb18-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-624"><a href="#cb18-624" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wrong: "p = 0.03 means there's a 3% chance the null hypothesis is true"</span>
<span id="cb18-625"><a href="#cb18-625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The p-value is $\mathbb{P}(\text{data} | H_0)$, not $\mathbb{P}(H_0 | \text{data})$</span>
<span id="cb18-626"><a href="#cb18-626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computing $\mathbb{P}(H_0 | \text{data})$ requires Bayesian methods (Chapter 8)</span>
<span id="cb18-627"><a href="#cb18-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-628"><a href="#cb18-628" aria-hidden="true" tabindex="-1"></a>**A large p-value is NOT strong evidence that the null hypothesis is true.**</span>
<span id="cb18-629"><a href="#cb18-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-630"><a href="#cb18-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A large p-value could mean:</span>
<span id="cb18-631"><a href="#cb18-631" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>$H_0$ is true, or</span>
<span id="cb18-632"><a href="#cb18-632" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>$H_0$ is false but our test has low power to detect the effect</span>
<span id="cb18-633"><a href="#cb18-633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Never conclude "we accept $H_0$" based on a large p-value</span>
<span id="cb18-634"><a href="#cb18-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-635"><a href="#cb18-635" aria-hidden="true" tabindex="-1"></a>**Statistical significance is NOT the same as practical significance.**</span>
<span id="cb18-636"><a href="#cb18-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-637"><a href="#cb18-637" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With enough data, tiny meaningless effects can become "statistically significant"</span>
<span id="cb18-638"><a href="#cb18-638" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Always examine the effect size (e.g., via confidence intervals) to judge practical importance</span>
<span id="cb18-639"><a href="#cb18-639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: A drug that lowers blood pressure by 0.1 mmHg might be statistically significant with n=10,000 but clinically meaningless</span>
<span id="cb18-640"><a href="#cb18-640" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-641"><a href="#cb18-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-642"><a href="#cb18-642" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-643"><a href="#cb18-643" aria-hidden="true" tabindex="-1"></a><span class="fu">## Standard p-value Interpretation Scales</span></span>
<span id="cb18-644"><a href="#cb18-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-645"><a href="#cb18-645" aria-hidden="true" tabindex="-1"></a>The interpretation of p-values varies significantly by field and context. Here's a common scale used in many fields:</span>
<span id="cb18-646"><a href="#cb18-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-647"><a href="#cb18-647" aria-hidden="true" tabindex="-1"></a>| p-value | Evidence against $H_0$ |</span>
<span id="cb18-648"><a href="#cb18-648" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb18-649"><a href="#cb18-649" aria-hidden="true" tabindex="-1"></a>| &lt; 0.01 | Strong evidence |</span>
<span id="cb18-650"><a href="#cb18-650" aria-hidden="true" tabindex="-1"></a>| 0.01 - 0.05 | Positive evidence |</span>
<span id="cb18-651"><a href="#cb18-651" aria-hidden="true" tabindex="-1"></a>| &gt; 0.05 | Little or no evidence |</span>
<span id="cb18-652"><a href="#cb18-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-653"><a href="#cb18-653" aria-hidden="true" tabindex="-1"></a>**Field-specific standards:**</span>
<span id="cb18-654"><a href="#cb18-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-655"><a href="#cb18-655" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Medicine/Psychology**: Often use $\alpha = 0.05$ as the standard.</span>
<span id="cb18-656"><a href="#cb18-656" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Genomics**: Use much stricter thresholds (e.g., $5 × 10^{-8}$) due to multiple testing, as we will see below.</span>
<span id="cb18-657"><a href="#cb18-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Particle Physics**: Extremely strict standards for discoveries:</span>
<span id="cb18-658"><a href="#cb18-658" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The <span class="co">[</span><span class="ot">Higgs boson discovery</span><span class="co">](https://en.wikipedia.org/wiki/Higgs_boson#Discovery_of_candidate_boson_at_CERN)</span> required a "5-sigma" result ($p &lt; 3 × 10^{-7}$).</span>
<span id="cb18-659"><a href="#cb18-659" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>This corresponds to less than 1 in 3.5 million chance of a false positive.</span>
<span id="cb18-660"><a href="#cb18-660" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb18-661"><a href="#cb18-661" aria-hidden="true" tabindex="-1"></a>These thresholds are conventions, not laws of nature. The appropriate threshold depends on the consequences of Type I and Type II errors in your specific context.</span>
<span id="cb18-662"><a href="#cb18-662" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-663"><a href="#cb18-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-664"><a href="#cb18-664" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb18-665"><a href="#cb18-665" aria-hidden="true" tabindex="-1"></a>If the test statistic has a continuous distribution, then under $H_0: \theta = \theta_0$, the p-value has a Uniform(0,1) distribution. Therefore, if we reject $H_0$ when the p-value is less than $\alpha$, the probability of a Type I error is exactly $\alpha$.</span>
<span id="cb18-666"><a href="#cb18-666" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-667"><a href="#cb18-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-668"><a href="#cb18-668" aria-hidden="true" tabindex="-1"></a>This property means that p-values "work correctly" -- under the null hypothesis, you'll get a p-value less than 0.05 exactly 5% of the time, a p-value less than 0.01 exactly 1% of the time, and so on.</span>
<span id="cb18-669"><a href="#cb18-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-670"><a href="#cb18-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-671"><a href="#cb18-671" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applying p-values: Classification Algorithm Comparison</span></span>
<span id="cb18-672"><a href="#cb18-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-673"><a href="#cb18-673" aria-hidden="true" tabindex="-1"></a>Let's apply the Wald test and p-value concepts to a practical problem that appears frequently in machine learning and data science: comparing the performance of two classification algorithms. This example (from AoS Example 10.7) illustrates both independent and paired testing scenarios.</span>
<span id="cb18-674"><a href="#cb18-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-675"><a href="#cb18-675" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb18-676"><a href="#cb18-676" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Comparing Algorithms with Independent Test Sets</span></span>
<span id="cb18-677"><a href="#cb18-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-678"><a href="#cb18-678" aria-hidden="true" tabindex="-1"></a>Suppose we test two classification algorithms on different, independent test sets:</span>
<span id="cb18-679"><a href="#cb18-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-680"><a href="#cb18-680" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Algorithm 1: Test set of size $m$, makes $X$ errors</span>
<span id="cb18-681"><a href="#cb18-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Algorithm 2: Test set of size $n$, makes $Y$ errors</span>
<span id="cb18-682"><a href="#cb18-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-683"><a href="#cb18-683" aria-hidden="true" tabindex="-1"></a>Then $X \sim \text{Binomial}(m, p_1)$ and $Y \sim \text{Binomial}(n, p_2)$, where $p_1$ and $p_2$ are the true error rates.</span>
<span id="cb18-684"><a href="#cb18-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-685"><a href="#cb18-685" aria-hidden="true" tabindex="-1"></a>We want to test:</span>
<span id="cb18-686"><a href="#cb18-686" aria-hidden="true" tabindex="-1"></a>$$H_0: p_1 = p_2 \quad \text{versus} \quad H_1: p_1 \neq p_2$$</span>
<span id="cb18-687"><a href="#cb18-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-688"><a href="#cb18-688" aria-hidden="true" tabindex="-1"></a>Or equivalently, $H_0: \delta = 0$ versus $H_1: \delta \neq 0$, where $\delta = p_1 - p_2$.</span>
<span id="cb18-689"><a href="#cb18-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-690"><a href="#cb18-690" aria-hidden="true" tabindex="-1"></a>**The Wald Test Approach:**</span>
<span id="cb18-691"><a href="#cb18-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-692"><a href="#cb18-692" aria-hidden="true" tabindex="-1"></a>The MLE is $\hat{\delta} = \hat{p}_1 - \hat{p}_2$ where $\hat{p}_1 = X/m$ and $\hat{p}_2 = Y/n$.</span>
<span id="cb18-693"><a href="#cb18-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-694"><a href="#cb18-694" aria-hidden="true" tabindex="-1"></a>The estimated standard error is:</span>
<span id="cb18-695"><a href="#cb18-695" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}} = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{m} + \frac{\hat{p}_2(1-\hat{p}_2)}{n}}$$</span>
<span id="cb18-696"><a href="#cb18-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-697"><a href="#cb18-697" aria-hidden="true" tabindex="-1"></a>The Wald test statistic is:</span>
<span id="cb18-698"><a href="#cb18-698" aria-hidden="true" tabindex="-1"></a>$$W = \frac{\hat{\delta} - 0}{\widehat{\text{se}}} = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{m} + \frac{\hat{p}_2(1-\hat{p}_2)}{n}}}$$</span>
<span id="cb18-699"><a href="#cb18-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-700"><a href="#cb18-700" aria-hidden="true" tabindex="-1"></a>For a size $\alpha$ test, we reject $H_0$ when $|W| &gt; z_{\alpha/2}$, and the p-value is $2\Phi(-|W|)$.</span>
<span id="cb18-701"><a href="#cb18-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-702"><a href="#cb18-702" aria-hidden="true" tabindex="-1"></a>**Numerical Example:** Let $m = n = 500$, with Algorithm 1 making 75 errors and Algorithm 2 making 100 errors:</span>
<span id="cb18-703"><a href="#cb18-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-706"><a href="#cb18-706" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-707"><a href="#cb18-707" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-708"><a href="#cb18-708" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-709"><a href="#cb18-709" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-710"><a href="#cb18-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-711"><a href="#cb18-711" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent test sets</span></span>
<span id="cb18-712"><a href="#cb18-712" aria-hidden="true" tabindex="-1"></a>m, n <span class="op">=</span> <span class="dv">500</span>, <span class="dv">500</span></span>
<span id="cb18-713"><a href="#cb18-713" aria-hidden="true" tabindex="-1"></a>p1_hat, p2_hat <span class="op">=</span> <span class="fl">0.15</span>, <span class="fl">0.20</span></span>
<span id="cb18-714"><a href="#cb18-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-715"><a href="#cb18-715" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard error</span></span>
<span id="cb18-716"><a href="#cb18-716" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p1_hat<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p1_hat)<span class="op">/</span>m <span class="op">+</span> p2_hat<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p2_hat)<span class="op">/</span>n)</span>
<span id="cb18-717"><a href="#cb18-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-718"><a href="#cb18-718" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test statistic</span></span>
<span id="cb18-719"><a href="#cb18-719" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> (p1_hat <span class="op">-</span> p2_hat) <span class="op">/</span> se</span>
<span id="cb18-720"><a href="#cb18-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-721"><a href="#cb18-721" aria-hidden="true" tabindex="-1"></a><span class="co"># p-value (two-sided)</span></span>
<span id="cb18-722"><a href="#cb18-722" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb18-723"><a href="#cb18-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-724"><a href="#cb18-724" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Error rates: Algorithm 1 = </span><span class="sc">{</span>p1_hat<span class="sc">:.2%}</span><span class="ss">, Algorithm 2 = </span><span class="sc">{</span>p2_hat<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb18-725"><a href="#cb18-725" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Difference: </span><span class="sc">{</span>p1_hat <span class="op">-</span> p2_hat<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb18-726"><a href="#cb18-726" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard error: </span><span class="sc">{</span>se<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-727"><a href="#cb18-727" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wald statistic: </span><span class="sc">{</span>W<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-728"><a href="#cb18-728" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-729"><a href="#cb18-729" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Conclusion: </span><span class="sc">{</span><span class="st">'Significant difference'</span> <span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="st">'No significant difference'</span><span class="sc">}</span><span class="ss"> at α = 0.05"</span>)</span>
<span id="cb18-730"><a href="#cb18-730" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-731"><a href="#cb18-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-732"><a href="#cb18-732" aria-hidden="true" tabindex="-1"></a>This 5 percentage point difference is statistically significant at α = 0.05. But the significance depends critically on sample size. As a numerical illustration:</span>
<span id="cb18-733"><a href="#cb18-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-734"><a href="#cb18-734" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With $m = n = 100$: Same 5% difference gives $|W| \approx 0.93$, p-value ≈ 0.35 (not significant)</span>
<span id="cb18-735"><a href="#cb18-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With $m = n = 500$: $|W| \approx 2.09$, p-value ≈ 0.037 (significant)  </span>
<span id="cb18-736"><a href="#cb18-736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With $m = n = 1000$: $|W| \approx 2.95$, p-value ≈ 0.003 (highly significant)</span>
<span id="cb18-737"><a href="#cb18-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-738"><a href="#cb18-738" aria-hidden="true" tabindex="-1"></a>Let's visualize how sample size affects our ability to detect this difference:</span>
<span id="cb18-739"><a href="#cb18-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-742"><a href="#cb18-742" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-743"><a href="#cb18-743" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb18-744"><a href="#cb18-744" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-745"><a href="#cb18-745" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-746"><a href="#cb18-746" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-747"><a href="#cb18-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-748"><a href="#cb18-748" aria-hidden="true" tabindex="-1"></a><span class="co"># Show how p-value changes with sample size</span></span>
<span id="cb18-749"><a href="#cb18-749" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">2000</span>]</span>
<span id="cb18-750"><a href="#cb18-750" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> []</span>
<span id="cb18-751"><a href="#cb18-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-752"><a href="#cb18-752" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> size <span class="kw">in</span> sample_sizes:</span>
<span id="cb18-753"><a href="#cb18-753" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> np.sqrt(<span class="fl">0.15</span><span class="op">*</span><span class="fl">0.85</span><span class="op">/</span>size <span class="op">+</span> <span class="fl">0.20</span><span class="op">*</span><span class="fl">0.80</span><span class="op">/</span>size)</span>
<span id="cb18-754"><a href="#cb18-754" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> (<span class="fl">0.15</span> <span class="op">-</span> <span class="fl">0.20</span>) <span class="op">/</span> se</span>
<span id="cb18-755"><a href="#cb18-755" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb18-756"><a href="#cb18-756" aria-hidden="true" tabindex="-1"></a>    p_values.append(p)</span>
<span id="cb18-757"><a href="#cb18-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-758"><a href="#cb18-758" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-759"><a href="#cb18-759" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-760"><a href="#cb18-760" aria-hidden="true" tabindex="-1"></a>plt.plot(sample_sizes, p_values, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb18-761"><a href="#cb18-761" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.05</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'α = 0.05'</span>)</span>
<span id="cb18-762"><a href="#cb18-762" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sample Size (per algorithm)'</span>)</span>
<span id="cb18-763"><a href="#cb18-763" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'p-value'</span>)</span>
<span id="cb18-764"><a href="#cb18-764" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'p-value vs Sample Size</span><span class="ch">\n</span><span class="st">(5</span><span class="sc">% d</span><span class="st">ifference in error rates)'</span>)</span>
<span id="cb18-765"><a href="#cb18-765" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb18-766"><a href="#cb18-766" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb18-767"><a href="#cb18-767" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-768"><a href="#cb18-768" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-769"><a href="#cb18-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-770"><a href="#cb18-770" aria-hidden="true" tabindex="-1"></a><span class="co"># Show power curve</span></span>
<span id="cb18-771"><a href="#cb18-771" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-772"><a href="#cb18-772" aria-hidden="true" tabindex="-1"></a>power <span class="op">=</span> [<span class="dv">1</span> <span class="op">-</span> p <span class="cf">if</span> p <span class="op">&lt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> p <span class="kw">in</span> p_values]</span>
<span id="cb18-773"><a href="#cb18-773" aria-hidden="true" tabindex="-1"></a>plt.plot(sample_sizes, [<span class="dv">1</span> <span class="op">-</span> p <span class="cf">for</span> p <span class="kw">in</span> p_values], <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">'s'</span>)</span>
<span id="cb18-774"><a href="#cb18-774" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.80</span>, color<span class="op">=</span><span class="st">'orange'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'80% power'</span>)</span>
<span id="cb18-775"><a href="#cb18-775" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sample Size (per algorithm)'</span>)</span>
<span id="cb18-776"><a href="#cb18-776" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Power'</span>)</span>
<span id="cb18-777"><a href="#cb18-777" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Power to Detect 5% Difference'</span>)</span>
<span id="cb18-778"><a href="#cb18-778" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb18-779"><a href="#cb18-779" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-780"><a href="#cb18-780" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-781"><a href="#cb18-781" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb18-782"><a href="#cb18-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-783"><a href="#cb18-783" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-784"><a href="#cb18-784" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-785"><a href="#cb18-785" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-786"><a href="#cb18-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-787"><a href="#cb18-787" aria-hidden="true" tabindex="-1"></a>**Key Insight:** The same 5% difference in error rates can be:</span>
<span id="cb18-788"><a href="#cb18-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-789"><a href="#cb18-789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Non-significant with small samples (low power)</span>
<span id="cb18-790"><a href="#cb18-790" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Highly significant with large samples (high power)</span>
<span id="cb18-791"><a href="#cb18-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-792"><a href="#cb18-792" aria-hidden="true" tabindex="-1"></a>This illustrates why reporting effect sizes (the actual difference) alongside p-values is crucial!</span>
<span id="cb18-793"><a href="#cb18-793" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-794"><a href="#cb18-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-795"><a href="#cb18-795" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb18-796"><a href="#cb18-796" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Comparing Algorithms with Paired Test Sets</span></span>
<span id="cb18-797"><a href="#cb18-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-798"><a href="#cb18-798" aria-hidden="true" tabindex="-1"></a>Often we test both algorithms on the **same** test set. This is more efficient but requires a different analysis because the results are no longer independent.</span>
<span id="cb18-799"><a href="#cb18-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-800"><a href="#cb18-800" aria-hidden="true" tabindex="-1"></a>**Data Structure:** For each test instance $i = 1, \ldots, n$:</span>
<span id="cb18-801"><a href="#cb18-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-802"><a href="#cb18-802" aria-hidden="true" tabindex="-1"></a>| Test Case | Algorithm 1 ($X_i$) | Algorithm 2 ($Y_i$) | Difference ($D_i = X_i - Y_i$) |</span>
<span id="cb18-803"><a href="#cb18-803" aria-hidden="true" tabindex="-1"></a>|:---------:|:------------------:|:------------------:|:-----------------------------:|</span>
<span id="cb18-804"><a href="#cb18-804" aria-hidden="true" tabindex="-1"></a>| 1 | 1 (correct) | 0 (incorrect) | 1 |</span>
<span id="cb18-805"><a href="#cb18-805" aria-hidden="true" tabindex="-1"></a>| 2 | 1 (correct) | 1 (correct) | 0 |</span>
<span id="cb18-806"><a href="#cb18-806" aria-hidden="true" tabindex="-1"></a>| 3 | 0 (incorrect) | 1 (correct) | -1 |</span>
<span id="cb18-807"><a href="#cb18-807" aria-hidden="true" tabindex="-1"></a>| ... | ... | ... | ... |</span>
<span id="cb18-808"><a href="#cb18-808" aria-hidden="true" tabindex="-1"></a>| n | $X_n$ | $Y_n$ | $D_n$ |</span>
<span id="cb18-809"><a href="#cb18-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-810"><a href="#cb18-810" aria-hidden="true" tabindex="-1"></a>The key insight: We can no longer treat $X$ and $Y$ as independent because they're tested on the same instances.</span>
<span id="cb18-811"><a href="#cb18-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-812"><a href="#cb18-812" aria-hidden="true" tabindex="-1"></a>**The Paired Test Approach:**</span>
<span id="cb18-813"><a href="#cb18-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-814"><a href="#cb18-814" aria-hidden="true" tabindex="-1"></a>Define $D_i = X_i - Y_i$ for each test instance. Then:</span>
<span id="cb18-815"><a href="#cb18-815" aria-hidden="true" tabindex="-1"></a>$$\delta = \mathbb{E}(D_i) = \mathbb{E}(X_i) - \mathbb{E}(Y_i) = \mathbb{P}(X_i = 1) - \mathbb{P}(Y_i = 1)$$</span>
<span id="cb18-816"><a href="#cb18-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-817"><a href="#cb18-817" aria-hidden="true" tabindex="-1"></a>We test $H_0: \delta = 0$ versus $H_1: \delta \neq 0$.</span>
<span id="cb18-818"><a href="#cb18-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-819"><a href="#cb18-819" aria-hidden="true" tabindex="-1"></a>The nonparametric plug-in estimate is $\hat{\delta} = \bar{D} = \frac{1}{n}\sum_{i=1}^n D_i$.</span>
<span id="cb18-820"><a href="#cb18-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-821"><a href="#cb18-821" aria-hidden="true" tabindex="-1"></a>The standard error is $\widehat{\text{se}}(\hat{\delta}) = S/\sqrt{n}$, where $S^2 = \frac{1}{n}\sum_{i=1}^n (D_i - \bar{D})^2$.</span>
<span id="cb18-822"><a href="#cb18-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-823"><a href="#cb18-823" aria-hidden="true" tabindex="-1"></a>The Wald test statistic is $W = \hat{\delta}/\widehat{\text{se}}$ and we reject $H_0$ if $|W| &gt; z_{\alpha/2}$.</span>
<span id="cb18-824"><a href="#cb18-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-825"><a href="#cb18-825" aria-hidden="true" tabindex="-1"></a>This is called a **paired comparison** or **paired test**.</span>
<span id="cb18-826"><a href="#cb18-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-829"><a href="#cb18-829" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-830"><a href="#cb18-830" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-831"><a href="#cb18-831" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate paired comparison</span></span>
<span id="cb18-832"><a href="#cb18-832" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb18-833"><a href="#cb18-833" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># test set size</span></span>
<span id="cb18-834"><a href="#cb18-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-835"><a href="#cb18-835" aria-hidden="true" tabindex="-1"></a><span class="co"># True probabilities of correct classification</span></span>
<span id="cb18-836"><a href="#cb18-836" aria-hidden="true" tabindex="-1"></a>p1_true <span class="op">=</span> <span class="fl">0.85</span>  <span class="co"># Algorithm 1</span></span>
<span id="cb18-837"><a href="#cb18-837" aria-hidden="true" tabindex="-1"></a>p2_true <span class="op">=</span> <span class="fl">0.80</span>  <span class="co"># Algorithm 2</span></span>
<span id="cb18-838"><a href="#cb18-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-839"><a href="#cb18-839" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate correlated outcomes (algorithms often agree)</span></span>
<span id="cb18-840"><a href="#cb18-840" aria-hidden="true" tabindex="-1"></a>correlation <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb18-841"><a href="#cb18-841" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb18-842"><a href="#cb18-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-843"><a href="#cb18-843" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate correlated binary outcomes</span></span>
<span id="cb18-844"><a href="#cb18-844" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, correlation], [correlation, <span class="dv">1</span>]]</span>
<span id="cb18-845"><a href="#cb18-845" aria-hidden="true" tabindex="-1"></a>latent <span class="op">=</span> multivariate_normal.rvs([<span class="dv">0</span>, <span class="dv">0</span>], cov, size<span class="op">=</span>n)</span>
<span id="cb18-846"><a href="#cb18-846" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> (latent[:, <span class="dv">0</span>] <span class="op">&lt;</span> stats.norm.ppf(p1_true)).astype(<span class="bu">int</span>)</span>
<span id="cb18-847"><a href="#cb18-847" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> (latent[:, <span class="dv">1</span>] <span class="op">&lt;</span> stats.norm.ppf(p2_true)).astype(<span class="bu">int</span>)</span>
<span id="cb18-848"><a href="#cb18-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-849"><a href="#cb18-849" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute differences</span></span>
<span id="cb18-850"><a href="#cb18-850" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> X <span class="op">-</span> Y</span>
<span id="cb18-851"><a href="#cb18-851" aria-hidden="true" tabindex="-1"></a>D_bar <span class="op">=</span> np.mean(D)</span>
<span id="cb18-852"><a href="#cb18-852" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.std(D, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-853"><a href="#cb18-853" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> S <span class="op">/</span> np.sqrt(n)</span>
<span id="cb18-854"><a href="#cb18-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-855"><a href="#cb18-855" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test</span></span>
<span id="cb18-856"><a href="#cb18-856" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> D_bar <span class="op">/</span> se</span>
<span id="cb18-857"><a href="#cb18-857" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb18-858"><a href="#cb18-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-859"><a href="#cb18-859" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample mean difference: </span><span class="sc">{</span>D_bar<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-860"><a href="#cb18-860" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard error: </span><span class="sc">{</span>se<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-861"><a href="#cb18-861" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wald statistic: </span><span class="sc">{</span>W<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-862"><a href="#cb18-862" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-863"><a href="#cb18-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-864"><a href="#cb18-864" aria-hidden="true" tabindex="-1"></a><span class="co"># Show contingency table</span></span>
<span id="cb18-865"><a href="#cb18-865" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> crosstab, DataFrame</span>
<span id="cb18-866"><a href="#cb18-866" aria-hidden="true" tabindex="-1"></a>ct <span class="op">=</span> crosstab(X, Y, rownames<span class="op">=</span>[<span class="st">'Alg1'</span>], colnames<span class="op">=</span>[<span class="st">'Alg2'</span>])</span>
<span id="cb18-867"><a href="#cb18-867" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Contingency table:"</span>)</span>
<span id="cb18-868"><a href="#cb18-868" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ct)</span>
<span id="cb18-869"><a href="#cb18-869" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Algorithms agree on </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(X <span class="op">==</span> Y)<span class="op">/</span>n<span class="sc">:.1%}</span><span class="ss"> of instances"</span>)</span>
<span id="cb18-870"><a href="#cb18-870" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-871"><a href="#cb18-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-872"><a href="#cb18-872" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-873"><a href="#cb18-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-874"><a href="#cb18-874" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb18-875"><a href="#cb18-875" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paired vs Independent Tests</span></span>
<span id="cb18-876"><a href="#cb18-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-877"><a href="#cb18-877" aria-hidden="true" tabindex="-1"></a>Paired tests are generally **more powerful** than independent tests when:</span>
<span id="cb18-878"><a href="#cb18-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-879"><a href="#cb18-879" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The same subjects/instances are measured twice</span>
<span id="cb18-880"><a href="#cb18-880" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>There's positive correlation between measurements</span>
<span id="cb18-881"><a href="#cb18-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-882"><a href="#cb18-882" aria-hidden="true" tabindex="-1"></a>The paired test removes the between-subject variability, focusing only on within-subject differences.</span>
<span id="cb18-883"><a href="#cb18-883" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-884"><a href="#cb18-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-885"><a href="#cb18-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-886"><a href="#cb18-886" aria-hidden="true" tabindex="-1"></a><span class="fu">## Constructing Statistical Tests</span></span>
<span id="cb18-887"><a href="#cb18-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-888"><a href="#cb18-888" aria-hidden="true" tabindex="-1"></a>Now that we understand p-values and have seen the Wald test in action, let's step back and consider the general problem: How do we construct a statistical test?</span>
<span id="cb18-889"><a href="#cb18-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-890"><a href="#cb18-890" aria-hidden="true" tabindex="-1"></a>Given a test statistic $T(X^n)$ and null hypothesis $H_0$, we need to determine the distribution of $T$ under $H_0$ to calculate p-values and make decisions. There are three main approaches:</span>
<span id="cb18-891"><a href="#cb18-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-892"><a href="#cb18-892" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Exact Distribution**: Sometimes we can calculate the exact distribution of $T$ under $H_0$</span>
<span id="cb18-893"><a href="#cb18-893" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Example: Fisher's exact test for 2×2 tables</span>
<span id="cb18-894"><a href="#cb18-894" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Advantage: Exact p-values, valid for any sample size</span>
<span id="cb18-895"><a href="#cb18-895" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Disadvantage: Only possible for simple cases</span>
<span id="cb18-896"><a href="#cb18-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-897"><a href="#cb18-897" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Asymptotic Approximation**: Use limiting distributions as $n \to \infty$</span>
<span id="cb18-898"><a href="#cb18-898" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Examples: Wald test (normal), likelihood ratio test (chi-squared)</span>
<span id="cb18-899"><a href="#cb18-899" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Advantage: Widely applicable, computationally simple</span>
<span id="cb18-900"><a href="#cb18-900" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Disadvantage: May be inaccurate for small samples</span>
<span id="cb18-901"><a href="#cb18-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-902"><a href="#cb18-902" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Simulation/Resampling**: Simulate the distribution by resampling</span>
<span id="cb18-903"><a href="#cb18-903" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Examples: Permutation test, bootstrap test</span>
<span id="cb18-904"><a href="#cb18-904" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Advantage: Minimal assumptions, works for any test statistic</span>
<span id="cb18-905"><a href="#cb18-905" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Disadvantage: Computationally intensive</span>
<span id="cb18-906"><a href="#cb18-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-907"><a href="#cb18-907" aria-hidden="true" tabindex="-1"></a>The choice depends on:</span>
<span id="cb18-908"><a href="#cb18-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-909"><a href="#cb18-909" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample size (small samples → avoid asymptotics)</span>
<span id="cb18-910"><a href="#cb18-910" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Distributional assumptions (violated → use resampling)</span>
<span id="cb18-911"><a href="#cb18-911" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Test statistic complexity (complex → simulation may be only option)</span>
<span id="cb18-912"><a href="#cb18-912" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computational resources (limited → prefer analytical methods)</span>
<span id="cb18-913"><a href="#cb18-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-914"><a href="#cb18-914" aria-hidden="true" tabindex="-1"></a>Let's now explore specific tests that exemplify each approach: the permutation test (simulation), Fisher's exact test (exact distribution), and the likelihood ratio test (asymptotic).</span>
<span id="cb18-915"><a href="#cb18-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-916"><a href="#cb18-916" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Permutation Test: A Simulation Approach</span></span>
<span id="cb18-917"><a href="#cb18-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-918"><a href="#cb18-918" aria-hidden="true" tabindex="-1"></a>The permutation test is a powerful non-parametric method for testing whether two distributions are the same. It exemplifies the simulation approach to test construction, making minimal assumptions -- only requiring that the distributions are identical under the null hypothesis.</span>
<span id="cb18-919"><a href="#cb18-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-920"><a href="#cb18-920" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-921"><a href="#cb18-921" aria-hidden="true" tabindex="-1"></a>**Permutation Test**: A nonparametric method for testing whether two distributions are the same.</span>
<span id="cb18-922"><a href="#cb18-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-923"><a href="#cb18-923" aria-hidden="true" tabindex="-1"></a>Let $X_1, \ldots, X_m \sim F_X$ and $Y_1, \ldots, Y_n \sim F_Y$ be independent samples.</span>
<span id="cb18-924"><a href="#cb18-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-925"><a href="#cb18-925" aria-hidden="true" tabindex="-1"></a>**Hypotheses**: $H_0: F_X = F_Y$ versus $H_1: F_X \neq F_Y$</span>
<span id="cb18-926"><a href="#cb18-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-927"><a href="#cb18-927" aria-hidden="true" tabindex="-1"></a>**Test statistic**: Choose any statistic $T(x_1, \ldots, x_m, y_1, \ldots, y_n)$, such as</span>
<span id="cb18-928"><a href="#cb18-928" aria-hidden="true" tabindex="-1"></a>$$T = |\bar{X}_m - \bar{Y}_n|$$</span>
<span id="cb18-929"><a href="#cb18-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-930"><a href="#cb18-930" aria-hidden="true" tabindex="-1"></a>**Key principle**: Under $H_0$, all $N! = (m+n)!$ permutations of the combined data are equally likely.</span>
<span id="cb18-931"><a href="#cb18-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-932"><a href="#cb18-932" aria-hidden="true" tabindex="-1"></a>**Permutation distribution**: The distribution that puts mass $1/N!$ on each value $T_j$ obtained from the $N!$ permutations.</span>
<span id="cb18-933"><a href="#cb18-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-934"><a href="#cb18-934" aria-hidden="true" tabindex="-1"></a>**Permutation p-value**: </span>
<span id="cb18-935"><a href="#cb18-935" aria-hidden="true" tabindex="-1"></a>$$\text{p-value} = \mathbb{P}_0(T \geq t_\text{obs}) = \frac{1}{N!} \sum_{j=1}^{N!} I(T_j \geq t_\text{obs})$$</span>
<span id="cb18-936"><a href="#cb18-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-937"><a href="#cb18-937" aria-hidden="true" tabindex="-1"></a>where $t_\text{obs}$ is the observed test statistic and $T_j$ is the statistic for permutation $j$.</span>
<span id="cb18-938"><a href="#cb18-938" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-939"><a href="#cb18-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-940"><a href="#cb18-940" aria-hidden="true" tabindex="-1"></a>The key insight: If the null hypothesis is true (both groups come from the same distribution), then the specific assignment of observations to groups was just one random possibility among many. We can simulate the null distribution by considering all possible reassignments.</span>
<span id="cb18-941"><a href="#cb18-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-942"><a href="#cb18-942" aria-hidden="true" tabindex="-1"></a>**When to use permutation tests:**</span>
<span id="cb18-943"><a href="#cb18-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-944"><a href="#cb18-944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small sample sizes where asymptotic approximations may fail</span>
<span id="cb18-945"><a href="#cb18-945" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Non-standard or unknown distributions</span>
<span id="cb18-946"><a href="#cb18-946" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex test statistics without known distributions</span>
<span id="cb18-947"><a href="#cb18-947" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When exact p-values are needed for critical decisions</span>
<span id="cb18-948"><a href="#cb18-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-949"><a href="#cb18-949" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb18-950"><a href="#cb18-950" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Permutation Test Algorithm</span></span>
<span id="cb18-951"><a href="#cb18-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-952"><a href="#cb18-952" aria-hidden="true" tabindex="-1"></a>Since evaluating all $(m+n)!$ permutations is computationally infeasible for realistic sample sizes, we use Monte Carlo sampling:</span>
<span id="cb18-953"><a href="#cb18-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-954"><a href="#cb18-954" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Compute observed test statistic**: $t_\text{obs} = T(X_1, \ldots, X_m, Y_1, \ldots, Y_n)$</span>
<span id="cb18-955"><a href="#cb18-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-956"><a href="#cb18-956" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Generate permutation distribution**: For $B$ iterations (typically 10,000+):</span>
<span id="cb18-957"><a href="#cb18-957" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Randomly permute the combined data</span>
<span id="cb18-958"><a href="#cb18-958" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Split into groups of original sizes $m$ and $n$</span>
<span id="cb18-959"><a href="#cb18-959" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Recompute the test statistic $T_i$</span>
<span id="cb18-960"><a href="#cb18-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-961"><a href="#cb18-961" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Calculate p-value**: </span>
<span id="cb18-962"><a href="#cb18-962" aria-hidden="true" tabindex="-1"></a>   $$\text{p-value} \approx \frac{1 + \sum_{i=1}^B I(T_i \geq t_\text{obs})}{B + 1}$$</span>
<span id="cb18-963"><a href="#cb18-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-964"><a href="#cb18-964" aria-hidden="true" tabindex="-1"></a>The "+1" corrections in both numerator and denominator:</span>
<span id="cb18-965"><a href="#cb18-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-966"><a href="#cb18-966" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prevent p-values of exactly 0 (which would be misleading)</span>
<span id="cb18-967"><a href="#cb18-967" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Make the estimate slightly conservative but consistent (as $B \rightarrow \infty$, it converges to the true p-value)</span>
<span id="cb18-968"><a href="#cb18-968" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can be interpreted as treating the observed data arrangement as one of B+1 total permutations (since it's a valid permutation that always has $T = t_\text{obs}$)</span>
<span id="cb18-969"><a href="#cb18-969" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb18-970"><a href="#cb18-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-971"><a href="#cb18-971" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb18-972"><a href="#cb18-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-973"><a href="#cb18-973" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuitive</span></span>
<span id="cb18-974"><a href="#cb18-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-975"><a href="#cb18-975" aria-hidden="true" tabindex="-1"></a>The core idea of the permutation test is quite simple: if the group labels (e.g., "Treatment" vs "Control") don't actually matter -- that is, if the null hypothesis is true -- then shuffling these labels randomly shouldn't change the distribution of our test statistic.</span>
<span id="cb18-976"><a href="#cb18-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-977"><a href="#cb18-977" aria-hidden="true" tabindex="-1"></a>Here's the logic:</span>
<span id="cb18-978"><a href="#cb18-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-979"><a href="#cb18-979" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Under $H_0$, the treatment and control groups come from the same distribution</span>
<span id="cb18-980"><a href="#cb18-980" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>So the specific assignment of units to groups was just one random possibility</span>
<span id="cb18-981"><a href="#cb18-981" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We can simulate other equally likely assignments by shuffling the labels</span>
<span id="cb18-982"><a href="#cb18-982" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>If our observed difference is unusual compared to these shuffled differences, we have evidence against $H_0$</span>
<span id="cb18-983"><a href="#cb18-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-984"><a href="#cb18-984" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical</span></span>
<span id="cb18-985"><a href="#cb18-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-986"><a href="#cb18-986" aria-hidden="true" tabindex="-1"></a>The mathematical foundation of the permutation test is the principle of **exchangeability**. Under the null hypothesis $H_0: F_X = F_Y$, all observations $(X_1, \ldots, X_m, Y_1, \ldots, Y_n)$ are independent and identically distributed (IID) from the same underlying distribution. This implies that the group labels ("Treatment", "Control") are arbitrary; any permutation of the combined data is equally likely to have occurred.</span>
<span id="cb18-987"><a href="#cb18-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-988"><a href="#cb18-988" aria-hidden="true" tabindex="-1"></a>The test leverages this property to construct an **exact, non-parametric reference distribution** for a chosen test statistic $T$ under $H_0$:</span>
<span id="cb18-989"><a href="#cb18-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-990"><a href="#cb18-990" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**The Permutation Distribution**: Conceptually, we consider the set of all $N! = (m+n)!$ possible permutations of the combined data. For each permutation, we compute the test statistic. The set of these $N!$ values forms the *exact* distribution of $T$ under the null hypothesis, conditional on the observed data values.</span>
<span id="cb18-991"><a href="#cb18-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-992"><a href="#cb18-992" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>**Exactness**: Because this distribution is derived directly from the data without asymptotic approximations, the permutation test is an **exact test**. This means that for a chosen significance level $\alpha$, the Type I error rate is controlled at exactly $\alpha$ (with proper handling of discrete data). This is a significant advantage over asymptotic tests like the Wald test, which are only guaranteed to have the correct size as $n \to \infty$.</span>
<span id="cb18-993"><a href="#cb18-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-994"><a href="#cb18-994" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>**Formal p-value**: The exact p-value is the proportion of permutations that yield a test statistic value as extreme or more extreme than the one observed with the original data labelling:</span>
<span id="cb18-995"><a href="#cb18-995" aria-hidden="true" tabindex="-1"></a>    $$ \text{p-value} = \frac{<span class="sc">\#\{</span>\text{permutations } \pi : T(\pi(\text{data})) \geq t_\text{obs}<span class="sc">\}</span>}{N!} $$</span>
<span id="cb18-996"><a href="#cb18-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-997"><a href="#cb18-997" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>**Monte Carlo Approximation**: Since calculating all $N!$ statistics is computationally infeasible, the algorithm in the "Computational" tab uses Monte Carlo sampling to approximate this exact p-value. By drawing a large number of random permutations ($B$), we create an empirical distribution that converges to the true permutation distribution as $B \to \infty$.</span>
<span id="cb18-998"><a href="#cb18-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-999"><a href="#cb18-999" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational</span></span>
<span id="cb18-1000"><a href="#cb18-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1001"><a href="#cb18-1001" aria-hidden="true" tabindex="-1"></a>Let's implement a permutation test and see it in action. We'll test whether two groups have different means, comparing our permutation test results with the standard parametric t-test. This demonstrates both how the test works and when it differs from classical approaches.</span>
<span id="cb18-1002"><a href="#cb18-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1003"><a href="#cb18-1003" aria-hidden="true" tabindex="-1"></a>Since evaluating all $N!$ permutations is computationally infeasible for realistic sample sizes (recall that 20! ≈ 2.4 × 10^18), we'll use Monte Carlo sampling to approximate the permutation distribution with $B = 10,000$ random permutations.</span>
<span id="cb18-1004"><a href="#cb18-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1007"><a href="#cb18-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1008"><a href="#cb18-1008" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-1009"><a href="#cb18-1009" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-1010"><a href="#cb18-1010" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-1011"><a href="#cb18-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1012"><a href="#cb18-1012" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> permutation_test(x, y, n_permutations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb18-1013"><a href="#cb18-1013" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-1014"><a href="#cb18-1014" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform a two-sample permutation test.</span></span>
<span id="cb18-1015"><a href="#cb18-1015" aria-hidden="true" tabindex="-1"></a><span class="co">    H0: The two samples come from the same distribution.</span></span>
<span id="cb18-1016"><a href="#cb18-1016" aria-hidden="true" tabindex="-1"></a><span class="co">    Test statistic: Absolute difference in means.</span></span>
<span id="cb18-1017"><a href="#cb18-1017" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-1018"><a href="#cb18-1018" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Observed test statistic</span></span>
<span id="cb18-1019"><a href="#cb18-1019" aria-hidden="true" tabindex="-1"></a>    t_obs <span class="op">=</span> <span class="bu">abs</span>(np.mean(x) <span class="op">-</span> np.mean(y))</span>
<span id="cb18-1020"><a href="#cb18-1020" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-1021"><a href="#cb18-1021" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine the data</span></span>
<span id="cb18-1022"><a href="#cb18-1022" aria-hidden="true" tabindex="-1"></a>    combined <span class="op">=</span> np.concatenate([x, y])</span>
<span id="cb18-1023"><a href="#cb18-1023" aria-hidden="true" tabindex="-1"></a>    n_x <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb18-1024"><a href="#cb18-1024" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-1025"><a href="#cb18-1025" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate permutations and compute test statistics</span></span>
<span id="cb18-1026"><a href="#cb18-1026" aria-hidden="true" tabindex="-1"></a>    t_perm <span class="op">=</span> []</span>
<span id="cb18-1027"><a href="#cb18-1027" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">42</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb18-1028"><a href="#cb18-1028" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-1029"><a href="#cb18-1029" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_permutations):</span>
<span id="cb18-1030"><a href="#cb18-1030" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Randomly permute the combined data</span></span>
<span id="cb18-1031"><a href="#cb18-1031" aria-hidden="true" tabindex="-1"></a>        permuted <span class="op">=</span> np.random.permutation(combined)</span>
<span id="cb18-1032"><a href="#cb18-1032" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split into two groups of original sizes</span></span>
<span id="cb18-1033"><a href="#cb18-1033" aria-hidden="true" tabindex="-1"></a>        x_perm <span class="op">=</span> permuted[:n_x]</span>
<span id="cb18-1034"><a href="#cb18-1034" aria-hidden="true" tabindex="-1"></a>        y_perm <span class="op">=</span> permuted[n_x:]</span>
<span id="cb18-1035"><a href="#cb18-1035" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute test statistic for this permutation</span></span>
<span id="cb18-1036"><a href="#cb18-1036" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="bu">abs</span>(np.mean(x_perm) <span class="op">-</span> np.mean(y_perm))</span>
<span id="cb18-1037"><a href="#cb18-1037" aria-hidden="true" tabindex="-1"></a>        t_perm.append(t)</span>
<span id="cb18-1038"><a href="#cb18-1038" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-1039"><a href="#cb18-1039" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate p-value (with +1 correction)</span></span>
<span id="cb18-1040"><a href="#cb18-1040" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding 1 to numerator and denominator for unbiased estimate</span></span>
<span id="cb18-1041"><a href="#cb18-1041" aria-hidden="true" tabindex="-1"></a>    p_value <span class="op">=</span> (np.<span class="bu">sum</span>(np.array(t_perm) <span class="op">&gt;=</span> t_obs) <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (n_permutations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb18-1042"><a href="#cb18-1042" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-1043"><a href="#cb18-1043" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t_obs, t_perm, p_value</span>
<span id="cb18-1044"><a href="#cb18-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1045"><a href="#cb18-1045" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Compare two small samples</span></span>
<span id="cb18-1046"><a href="#cb18-1046" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb18-1047"><a href="#cb18-1047" aria-hidden="true" tabindex="-1"></a>group1 <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">15</span>, <span class="dv">12</span>)  <span class="co"># Mean 100</span></span>
<span id="cb18-1048"><a href="#cb18-1048" aria-hidden="true" tabindex="-1"></a>group2 <span class="op">=</span> np.random.normal(<span class="dv">110</span>, <span class="dv">15</span>, <span class="dv">10</span>)  <span class="co"># Mean 110</span></span>
<span id="cb18-1049"><a href="#cb18-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1050"><a href="#cb18-1050" aria-hidden="true" tabindex="-1"></a>t_obs, t_perm, p_value <span class="op">=</span> permutation_test(group1, group2)</span>
<span id="cb18-1051"><a href="#cb18-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1052"><a href="#cb18-1052" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Observed difference in means: </span><span class="sc">{</span>t_obs<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-1053"><a href="#cb18-1053" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Permutation p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1054"><a href="#cb18-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1055"><a href="#cb18-1055" aria-hidden="true" tabindex="-1"></a><span class="co"># For comparison, also run parametric t-test</span></span>
<span id="cb18-1056"><a href="#cb18-1056" aria-hidden="true" tabindex="-1"></a>t_stat, p_parametric <span class="op">=</span> stats.ttest_ind(group1, group2)</span>
<span id="cb18-1057"><a href="#cb18-1057" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parametric t-test p-value: </span><span class="sc">{</span>p_parametric<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1058"><a href="#cb18-1058" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1059"><a href="#cb18-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1060"><a href="#cb18-1060" aria-hidden="true" tabindex="-1"></a>Let's visualize the permutation distribution to see what we've created:</span>
<span id="cb18-1061"><a href="#cb18-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1064"><a href="#cb18-1064" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1065"><a href="#cb18-1065" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-1066"><a href="#cb18-1066" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb18-1067"><a href="#cb18-1067" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-1068"><a href="#cb18-1068" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-1069"><a href="#cb18-1069" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show visualization code"</span></span>
<span id="cb18-1070"><a href="#cb18-1070" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-1071"><a href="#cb18-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1072"><a href="#cb18-1072" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-1073"><a href="#cb18-1073" aria-hidden="true" tabindex="-1"></a>plt.hist(t_perm, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, </span>
<span id="cb18-1074"><a href="#cb18-1074" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'blue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">'Permutation distribution'</span>)</span>
<span id="cb18-1075"><a href="#cb18-1075" aria-hidden="true" tabindex="-1"></a>plt.axvline(t_obs, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb18-1076"><a href="#cb18-1076" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Observed statistic = </span><span class="sc">{</span>t_obs<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb18-1077"><a href="#cb18-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1078"><a href="#cb18-1078" aria-hidden="true" tabindex="-1"></a><span class="co"># Add text annotation for p-value</span></span>
<span id="cb18-1079"><a href="#cb18-1079" aria-hidden="true" tabindex="-1"></a>plt.text(t_obs <span class="op">+</span> <span class="dv">1</span>, plt.ylim()[<span class="dv">1</span>] <span class="op">*</span> <span class="fl">0.8</span>, </span>
<span id="cb18-1080"><a href="#cb18-1080" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f'p-value = </span><span class="sc">{</span>p_value<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb18-1081"><a href="#cb18-1081" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb18-1082"><a href="#cb18-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1083"><a href="#cb18-1083" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Test Statistic (|difference in means|)'</span>)</span>
<span id="cb18-1084"><a href="#cb18-1084" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb18-1085"><a href="#cb18-1085" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Permutation Test: Distribution Under H₀'</span>)</span>
<span id="cb18-1086"><a href="#cb18-1086" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-1087"><a href="#cb18-1087" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-1088"><a href="#cb18-1088" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-1089"><a href="#cb18-1089" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1090"><a href="#cb18-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1091"><a href="#cb18-1091" aria-hidden="true" tabindex="-1"></a>**What this demonstration shows:**</span>
<span id="cb18-1092"><a href="#cb18-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1093"><a href="#cb18-1093" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**The permutation distribution shows typical chance differences**: Under $H_0$, we see what absolute differences in means we'd expect purely by chance when randomly assigning labels. Since we use the absolute difference, all values are positive, with most falling between 0 and 20.</span>
<span id="cb18-1094"><a href="#cb18-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1095"><a href="#cb18-1095" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Our observed statistic falls in the far right tail**: The red dashed line shows our actual observed difference is larger than what we'd typically see by chance. The p-value annotation shows that only about 0.2% of permutations produce a difference this large or larger, providing evidence against $H_0$.</span>
<span id="cb18-1096"><a href="#cb18-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1097"><a href="#cb18-1097" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Agreement with the parametric test**: Both the permutation test (p ≈ 0.016) and the t-test (p ≈ 0.017) reach similar conclusions. This is reassuring when assumptions hold, but the permutation test would still be valid even if normality assumptions were violated.</span>
<span id="cb18-1098"><a href="#cb18-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1099"><a href="#cb18-1099" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**The Monte Carlo approximation works well**: With 10,000 permutations, we get a smooth estimate of the true permutation distribution, making the test both computationally feasible and statistically reliable. The distribution has the characteristic shape for an absolute difference statistic -- starting near zero and skewing right -- but the permutation test makes no assumptions about this shape.</span>
<span id="cb18-1100"><a href="#cb18-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1101"><a href="#cb18-1101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1102"><a href="#cb18-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1103"><a href="#cb18-1103" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb18-1104"><a href="#cb18-1104" aria-hidden="true" tabindex="-1"></a><span class="fu">## Use $\geq$ not $&gt;$ for discrete distributions in permutation tests</span></span>
<span id="cb18-1105"><a href="#cb18-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1106"><a href="#cb18-1106" aria-hidden="true" tabindex="-1"></a>@wasserman2013all uses $I(T_j &gt; t_\text{obs})$ but this is incorrect for discrete distributions. The definition of p-value includes equality: the probability of observing a value "as extreme or more extreme." For continuous distributions this makes no difference, but for discrete cases (including permutation tests) we must use $\geq$.</span>
<span id="cb18-1107"><a href="#cb18-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1108"><a href="#cb18-1108" aria-hidden="true" tabindex="-1"></a>**Example illustrating why this matters:**</span>
<span id="cb18-1109"><a href="#cb18-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1110"><a href="#cb18-1110" aria-hidden="true" tabindex="-1"></a>Consider a tiny dataset: $X = (1, 9)$ and $Y = (3)$. We want to test if the means are different using $T = |\bar{X} - \bar{Y}|$ as our test statistic.</span>
<span id="cb18-1111"><a href="#cb18-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1112"><a href="#cb18-1112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Observed: $\bar{X} = 5$, $\bar{Y} = 3$, so $t_\text{obs} = |5 - 3| = 2$</span>
<span id="cb18-1113"><a href="#cb18-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1114"><a href="#cb18-1114" aria-hidden="true" tabindex="-1"></a>Now let's enumerate all 6 possible permutations:</span>
<span id="cb18-1115"><a href="#cb18-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1116"><a href="#cb18-1116" aria-hidden="true" tabindex="-1"></a>| Permutation | X values | Y value | $\bar{X}$ | $\bar{Y}$ | $T = |\bar{X} - \bar{Y}|$ |</span>
<span id="cb18-1117"><a href="#cb18-1117" aria-hidden="true" tabindex="-1"></a>|:-----------:|:--------:|:-------:|:---------:|:---------:|:-------------------------:|</span>
<span id="cb18-1118"><a href="#cb18-1118" aria-hidden="true" tabindex="-1"></a>| Original | (1, 9) | (3) | 5 | 3 | 2 |</span>
<span id="cb18-1119"><a href="#cb18-1119" aria-hidden="true" tabindex="-1"></a>| 2 | (9, 1) | (3) | 5 | 3 | 2 |</span>
<span id="cb18-1120"><a href="#cb18-1120" aria-hidden="true" tabindex="-1"></a>| 3 | (1, 3) | (9) | 2 | 9 | 7 |</span>
<span id="cb18-1121"><a href="#cb18-1121" aria-hidden="true" tabindex="-1"></a>| 4 | (3, 1) | (9) | 2 | 9 | 7 |</span>
<span id="cb18-1122"><a href="#cb18-1122" aria-hidden="true" tabindex="-1"></a>| 5 | (3, 9) | (1) | 6 | 1 | 5 |</span>
<span id="cb18-1123"><a href="#cb18-1123" aria-hidden="true" tabindex="-1"></a>| 6 | (9, 3) | (1) | 6 | 1 | 5 |</span>
<span id="cb18-1124"><a href="#cb18-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1125"><a href="#cb18-1125" aria-hidden="true" tabindex="-1"></a>Computing the p-value:</span>
<span id="cb18-1126"><a href="#cb18-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1127"><a href="#cb18-1127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Correct** (using $\geq$): $\mathbb{P}(T \geq 2) = 6/6 = 1.0$ (all permutations have $T \geq 2$)</span>
<span id="cb18-1128"><a href="#cb18-1128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Incorrect** (using $&gt;$): $\mathbb{P}(T &gt; 2) = 4/6 = 0.67$ (only 4 permutations have $T &gt; 2$)</span>
<span id="cb18-1129"><a href="#cb18-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1130"><a href="#cb18-1130" aria-hidden="true" tabindex="-1"></a>The correct p-value of 1.0 tells us our observed difference is the **smallest possible** -- not evidence against $H_0$ at all! The incorrect formula would suggest some evidence against the null, which is completely wrong.</span>
<span id="cb18-1131"><a href="#cb18-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1132"><a href="#cb18-1132" aria-hidden="true" tabindex="-1"></a>This example shows why the correct formula must use $I(T_j \geq t_\text{obs})$.</span>
<span id="cb18-1133"><a href="#cb18-1133" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1134"><a href="#cb18-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1135"><a href="#cb18-1135" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fisher's Exact Test: An Exact Approach</span></span>
<span id="cb18-1136"><a href="#cb18-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1137"><a href="#cb18-1137" aria-hidden="true" tabindex="-1"></a>Fisher's exact test exemplifies the exact distribution approach. It provides the exact probability of observing data as extreme or more extreme than what we observed, given fixed marginal totals in a 2×2 contingency table. This is particularly valuable for small sample sizes where asymptotic approximations may fail.</span>
<span id="cb18-1138"><a href="#cb18-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1139"><a href="#cb18-1139" aria-hidden="true" tabindex="-1"></a>To illustrate, let's return to the motivating drug trial problem from the chapter introduction:</span>
<span id="cb18-1140"><a href="#cb18-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1141"><a href="#cb18-1141" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb18-1142"><a href="#cb18-1142" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application: The Drug Trial</span></span>
<span id="cb18-1143"><a href="#cb18-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1144"><a href="#cb18-1144" aria-hidden="true" tabindex="-1"></a>Recall our 2×2 table of outcomes:</span>
<span id="cb18-1145"><a href="#cb18-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1146"><a href="#cb18-1146" aria-hidden="true" tabindex="-1"></a>| | Better | Not Better |</span>
<span id="cb18-1147"><a href="#cb18-1147" aria-hidden="true" tabindex="-1"></a>| :--- | :---: | :---: |</span>
<span id="cb18-1148"><a href="#cb18-1148" aria-hidden="true" tabindex="-1"></a>| Treated | 50 | 50 |</span>
<span id="cb18-1149"><a href="#cb18-1149" aria-hidden="true" tabindex="-1"></a>| Control | 40 | 60 |</span>
<span id="cb18-1150"><a href="#cb18-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1151"><a href="#cb18-1151" aria-hidden="true" tabindex="-1"></a>Many different statistical tests could be applied in this setting -- we could use a two-sample test of proportions (Wald test), a chi-squared test, or a permutation test. However, for 2×2 contingency tables with modest sample sizes, **Fisher's Exact Test** is one of the more attractive alternatives. It calculates the exact probability of observing a table as extreme or more extreme than this, given fixed marginal totals.</span>
<span id="cb18-1152"><a href="#cb18-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1155"><a href="#cb18-1155" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1156"><a href="#cb18-1156" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-1157"><a href="#cb18-1157" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb18-1158"><a href="#cb18-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1159"><a href="#cb18-1159" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the contingency table</span></span>
<span id="cb18-1160"><a href="#cb18-1160" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> [[<span class="dv">50</span>, <span class="dv">50</span>],  <span class="co"># Treated: 50 better, 50 not better</span></span>
<span id="cb18-1161"><a href="#cb18-1161" aria-hidden="true" tabindex="-1"></a>         [<span class="dv">40</span>, <span class="dv">60</span>]]  <span class="co"># Control: 40 better, 60 not better</span></span>
<span id="cb18-1162"><a href="#cb18-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1163"><a href="#cb18-1163" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test</span></span>
<span id="cb18-1164"><a href="#cb18-1164" aria-hidden="true" tabindex="-1"></a><span class="co"># alternative="greater" tests if treatment has higher "better" rate</span></span>
<span id="cb18-1165"><a href="#cb18-1165" aria-hidden="true" tabindex="-1"></a>odds_ratio, p_value <span class="op">=</span> stats.fisher_exact(table, alternative<span class="op">=</span><span class="st">"greater"</span>)</span>
<span id="cb18-1166"><a href="#cb18-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1167"><a href="#cb18-1167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Contingency table:"</span>)</span>
<span id="cb18-1168"><a href="#cb18-1168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"         Better  Not Better"</span>)</span>
<span id="cb18-1169"><a href="#cb18-1169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Treated:   50        50"</span>)</span>
<span id="cb18-1170"><a href="#cb18-1170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Control:   40        60"</span>)</span>
<span id="cb18-1171"><a href="#cb18-1171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Odds ratio: </span><span class="sc">{</span>odds_ratio<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-1172"><a href="#cb18-1172" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"One-sided p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1173"><a href="#cb18-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1174"><a href="#cb18-1174" aria-hidden="true" tabindex="-1"></a><span class="co"># Also try two-sided test</span></span>
<span id="cb18-1175"><a href="#cb18-1175" aria-hidden="true" tabindex="-1"></a>_, p_two_sided <span class="op">=</span> stats.fisher_exact(table, alternative<span class="op">=</span><span class="st">"two-sided"</span>)</span>
<span id="cb18-1176"><a href="#cb18-1176" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Two-sided p-value: </span><span class="sc">{</span>p_two_sided<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1177"><a href="#cb18-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1178"><a href="#cb18-1178" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb18-1179"><a href="#cb18-1179" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conclusion: Significant evidence of treatment effect (p &lt; 0.05)"</span>)</span>
<span id="cb18-1180"><a href="#cb18-1180" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-1181"><a href="#cb18-1181" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conclusion: No significant evidence of treatment effect at α = 0.05"</span>)</span>
<span id="cb18-1182"><a href="#cb18-1182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1183"><a href="#cb18-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1184"><a href="#cb18-1184" aria-hidden="true" tabindex="-1"></a>With p = 0.10, we don't have strong evidence to reject the null hypothesis at the conventional α = 0.05 level. The observed 10 percentage point difference could plausibly arise by chance.</span>
<span id="cb18-1185"><a href="#cb18-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1186"><a href="#cb18-1186" aria-hidden="true" tabindex="-1"></a>This example illustrates a key point: even seemingly large differences (50% vs 40% success rate) may not be statistically significant with modest sample sizes. Power analysis before conducting studies is crucial!</span>
<span id="cb18-1187"><a href="#cb18-1187" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb18-1188"><a href="#cb18-1188" aria-hidden="true" tabindex="-1"></a>**Many other applications share this same 2×2 structure:**</span>
<span id="cb18-1189"><a href="#cb18-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1190"><a href="#cb18-1190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**A/B testing**: Comparing conversion rates between website designs</span>
<span id="cb18-1191"><a href="#cb18-1191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Demographics**: Testing differences in proportions between groups  </span>
<span id="cb18-1192"><a href="#cb18-1192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Medical screening**: Comparing test accuracy between methods</span>
<span id="cb18-1193"><a href="#cb18-1193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Quality control**: Comparing defect rates between processes</span>
<span id="cb18-1194"><a href="#cb18-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1195"><a href="#cb18-1195" aria-hidden="true" tabindex="-1"></a>Whenever you're comparing binary outcomes between two groups, you face the same statistical question: is the observed difference real or just chance?</span>
<span id="cb18-1196"><a href="#cb18-1196" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1197"><a href="#cb18-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1198"><a href="#cb18-1198" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Likelihood Ratio Test: A General Asymptotic Approach</span></span>
<span id="cb18-1199"><a href="#cb18-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1200"><a href="#cb18-1200" aria-hidden="true" tabindex="-1"></a>The Wald test is useful for testing a scalar parameter. The likelihood ratio test is more general and can be used for testing a vector-valued parameter, making it one of the most important tools in statistical inference.</span>
<span id="cb18-1201"><a href="#cb18-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1202"><a href="#cb18-1202" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-1203"><a href="#cb18-1203" aria-hidden="true" tabindex="-1"></a>**Likelihood Ratio Test**: For testing $H_0: \theta \in \Theta_0$ versus $H_1: \theta \notin \Theta_0$:</span>
<span id="cb18-1204"><a href="#cb18-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1205"><a href="#cb18-1205" aria-hidden="true" tabindex="-1"></a>The **likelihood ratio statistic** is:</span>
<span id="cb18-1206"><a href="#cb18-1206" aria-hidden="true" tabindex="-1"></a>$$\lambda = 2 \log \left( \frac{\sup_{\theta \in \Theta} \mathcal{L}(\theta)}{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)} \right) = 2<span class="co">[</span><span class="ot">\ell(\hat{\theta}) - \ell(\hat{\theta}_0)</span><span class="co">]</span>$$</span>
<span id="cb18-1207"><a href="#cb18-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1208"><a href="#cb18-1208" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb18-1209"><a href="#cb18-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1210"><a href="#cb18-1210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\theta}$ is the MLE over the entire parameter space $\Theta$</span>
<span id="cb18-1211"><a href="#cb18-1211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\theta}_0$ is the MLE under the constraint $\theta \in \Theta_0$</span>
<span id="cb18-1212"><a href="#cb18-1212" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1213"><a href="#cb18-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1214"><a href="#cb18-1214" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb18-1215"><a href="#cb18-1215" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why use $\Theta$ instead of $\Theta_0^c$ in the numerator?</span></span>
<span id="cb18-1216"><a href="#cb18-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1217"><a href="#cb18-1217" aria-hidden="true" tabindex="-1"></a>You might expect to maximize over $\Theta_0^c$ (the alternative hypothesis space) in the numerator. However:</span>
<span id="cb18-1218"><a href="#cb18-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1219"><a href="#cb18-1219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using $\Theta$ has little practical effect on the test statistic</span>
<span id="cb18-1220"><a href="#cb18-1220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The theoretical properties are much simpler with this definition</span>
<span id="cb18-1221"><a href="#cb18-1221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It ensures the statistic is always non-negative</span>
<span id="cb18-1222"><a href="#cb18-1222" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1223"><a href="#cb18-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1224"><a href="#cb18-1224" aria-hidden="true" tabindex="-1"></a>The LRT is most useful when $\Theta_0$ is defined by constraining some parameters to fixed values. For example, if $\theta = (\theta_1, \ldots, \theta_r)$ and we want to test that the last $r-q$ components equal specific values:</span>
<span id="cb18-1225"><a href="#cb18-1225" aria-hidden="true" tabindex="-1"></a>$$\Theta_0 = <span class="sc">\{</span>\theta: (\theta_{q+1}, \ldots, \theta_r) = (\theta_{0,q+1}, \ldots, \theta_{0,r})<span class="sc">\}</span>$$</span>
<span id="cb18-1226"><a href="#cb18-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1227"><a href="#cb18-1227" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb18-1228"><a href="#cb18-1228" aria-hidden="true" tabindex="-1"></a>Under $H_0: \theta \in \Theta_0$, the likelihood ratio statistic has an asymptotic <span class="co">[</span><span class="ot">chi-squared distribution</span><span class="co">](https://en.wikipedia.org/wiki/Chi-squared_distribution)</span>:</span>
<span id="cb18-1229"><a href="#cb18-1229" aria-hidden="true" tabindex="-1"></a>$$\lambda \rightsquigarrow \chi^2_{r-q}$$</span>
<span id="cb18-1230"><a href="#cb18-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1231"><a href="#cb18-1231" aria-hidden="true" tabindex="-1"></a>where $r-q$ is the difference in dimensionality between $\Theta$ and $\Theta_0$ (the number of constraints imposed by $H_0$).</span>
<span id="cb18-1232"><a href="#cb18-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1233"><a href="#cb18-1233" aria-hidden="true" tabindex="-1"></a>**The p-value** is: $\mathbb{P}(\chi^2_{r-q} \geq \lambda)$</span>
<span id="cb18-1234"><a href="#cb18-1234" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1235"><a href="#cb18-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1236"><a href="#cb18-1236" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip icon=false}</span>
<span id="cb18-1237"><a href="#cb18-1237" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Counting Degrees of Freedom</span></span>
<span id="cb18-1238"><a href="#cb18-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1239"><a href="#cb18-1239" aria-hidden="true" tabindex="-1"></a>If $\theta = (\theta_1, \theta_2, \theta_3, \theta_4, \theta_5)$ and we test:</span>
<span id="cb18-1240"><a href="#cb18-1240" aria-hidden="true" tabindex="-1"></a>$$H_0: \theta_4 = \theta_5 = 0$$</span>
<span id="cb18-1241"><a href="#cb18-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1242"><a href="#cb18-1242" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb18-1243"><a href="#cb18-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1244"><a href="#cb18-1244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dimension of $\Theta$ = 5 (all parameters free)</span>
<span id="cb18-1245"><a href="#cb18-1245" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dimension of $\Theta_0$ = 3 (only $\theta_1, \theta_2, \theta_3$ free)</span>
<span id="cb18-1246"><a href="#cb18-1246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Degrees of freedom = 5 - 3 = 2</span>
<span id="cb18-1247"><a href="#cb18-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1248"><a href="#cb18-1248" aria-hidden="true" tabindex="-1"></a>The test statistic $\lambda \rightsquigarrow \chi^2_2$ under $H_0$.</span>
<span id="cb18-1249"><a href="#cb18-1249" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1250"><a href="#cb18-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1251"><a href="#cb18-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1252"><a href="#cb18-1252" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Multiple Testing Problem: The Peril of Many Tests</span></span>
<span id="cb18-1253"><a href="#cb18-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1254"><a href="#cb18-1254" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Problem</span></span>
<span id="cb18-1255"><a href="#cb18-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1256"><a href="#cb18-1256" aria-hidden="true" tabindex="-1"></a>Modern data science often involves testing many hypotheses simultaneously. Consider these scenarios:</span>
<span id="cb18-1257"><a href="#cb18-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1258"><a href="#cb18-1258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Genomics**: Testing thousands of genes for association with disease</span>
<span id="cb18-1259"><a href="#cb18-1259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**A/B testing**: Running dozens of experiments across a website</span>
<span id="cb18-1260"><a href="#cb18-1260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Neuroscience**: Testing brain activity at thousands of voxels</span>
<span id="cb18-1261"><a href="#cb18-1261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature selection**: Testing which of hundreds of features predict an outcome</span>
<span id="cb18-1262"><a href="#cb18-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1263"><a href="#cb18-1263" aria-hidden="true" tabindex="-1"></a>The problem is simple but severe: If you perform many tests, you're virtually guaranteed to get false positives by chance alone.</span>
<span id="cb18-1264"><a href="#cb18-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1265"><a href="#cb18-1265" aria-hidden="true" tabindex="-1"></a>Let's quantify this:</span>
<span id="cb18-1266"><a href="#cb18-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1267"><a href="#cb18-1267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One test** at α = 0.05: 5% chance of a Type I error</span>
<span id="cb18-1268"><a href="#cb18-1268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**20 independent tests** at α = 0.05 each: </span>
<span id="cb18-1269"><a href="#cb18-1269" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Probability of at least one Type I error = $1 - (0.95)^{20} \approx 0.64$</span>
<span id="cb18-1270"><a href="#cb18-1270" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>That's a 64% chance of at least one false positive!</span>
<span id="cb18-1271"><a href="#cb18-1271" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1000 tests**: Virtually certain to get many false positives</span>
<span id="cb18-1272"><a href="#cb18-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1275"><a href="#cb18-1275" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1276"><a href="#cb18-1276" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-1277"><a href="#cb18-1277" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-1278"><a href="#cb18-1278" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-1279"><a href="#cb18-1279" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-1280"><a href="#cb18-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1281"><a href="#cb18-1281" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the multiple testing problem</span></span>
<span id="cb18-1282"><a href="#cb18-1282" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb18-1283"><a href="#cb18-1283" aria-hidden="true" tabindex="-1"></a>n_tests <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>]</span>
<span id="cb18-1284"><a href="#cb18-1284" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb18-1285"><a href="#cb18-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1286"><a href="#cb18-1286" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate probability of at least one false positive</span></span>
<span id="cb18-1287"><a href="#cb18-1287" aria-hidden="true" tabindex="-1"></a>prob_no_false_positive <span class="op">=</span> [(<span class="dv">1</span> <span class="op">-</span> alpha)<span class="op">**</span>m <span class="cf">for</span> m <span class="kw">in</span> n_tests]</span>
<span id="cb18-1288"><a href="#cb18-1288" aria-hidden="true" tabindex="-1"></a>prob_at_least_one_fp <span class="op">=</span> [<span class="dv">1</span> <span class="op">-</span> p <span class="cf">for</span> p <span class="kw">in</span> prob_no_false_positive]</span>
<span id="cb18-1289"><a href="#cb18-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1290"><a href="#cb18-1290" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected number of false positives</span></span>
<span id="cb18-1291"><a href="#cb18-1291" aria-hidden="true" tabindex="-1"></a>expected_fp <span class="op">=</span> [m <span class="op">*</span> alpha <span class="cf">for</span> m <span class="kw">in</span> n_tests]</span>
<span id="cb18-1292"><a href="#cb18-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1293"><a href="#cb18-1293" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-1294"><a href="#cb18-1294" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-1295"><a href="#cb18-1295" aria-hidden="true" tabindex="-1"></a>plt.plot(n_tests, prob_at_least_one_fp, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb18-1296"><a href="#cb18-1296" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>alpha, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="ss">f'α = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-1297"><a href="#cb18-1297" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Tests'</span>)</span>
<span id="cb18-1298"><a href="#cb18-1298" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'P(At least one false positive)'</span>)</span>
<span id="cb18-1299"><a href="#cb18-1299" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Probability of False Discoveries'</span>)</span>
<span id="cb18-1300"><a href="#cb18-1300" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb18-1301"><a href="#cb18-1301" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-1302"><a href="#cb18-1302" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-1303"><a href="#cb18-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1304"><a href="#cb18-1304" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-1305"><a href="#cb18-1305" aria-hidden="true" tabindex="-1"></a>plt.plot(n_tests, expected_fp, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">'s'</span>)</span>
<span id="cb18-1306"><a href="#cb18-1306" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Tests'</span>)</span>
<span id="cb18-1307"><a href="#cb18-1307" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Expected # of False Positives'</span>)</span>
<span id="cb18-1308"><a href="#cb18-1308" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Expected False Positives'</span>)</span>
<span id="cb18-1309"><a href="#cb18-1309" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb18-1310"><a href="#cb18-1310" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb18-1311"><a href="#cb18-1311" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-1312"><a href="#cb18-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1313"><a href="#cb18-1313" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-1314"><a href="#cb18-1314" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-1315"><a href="#cb18-1315" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1316"><a href="#cb18-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1317"><a href="#cb18-1317" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts for Multiple Testing</span></span>
<span id="cb18-1318"><a href="#cb18-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1319"><a href="#cb18-1319" aria-hidden="true" tabindex="-1"></a>When conducting $m$ hypothesis tests, we need to understand what types of errors we want to control. Consider this classification table of test outcomes:</span>
<span id="cb18-1320"><a href="#cb18-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1321"><a href="#cb18-1321" aria-hidden="true" tabindex="-1"></a>| | $H_0$ Not Rejected | $H_0$ Rejected | Total |</span>
<span id="cb18-1322"><a href="#cb18-1322" aria-hidden="true" tabindex="-1"></a>| :--- | :---: | :---: | :---: |</span>
<span id="cb18-1323"><a href="#cb18-1323" aria-hidden="true" tabindex="-1"></a>| $H_0$ True | $U$ | $V$ | $m_0$ |</span>
<span id="cb18-1324"><a href="#cb18-1324" aria-hidden="true" tabindex="-1"></a>| $H_0$ False | $T$ | $S$ | $m_1$ |</span>
<span id="cb18-1325"><a href="#cb18-1325" aria-hidden="true" tabindex="-1"></a>| Total | $m-R$ | $R$ | $m$ |</span>
<span id="cb18-1326"><a href="#cb18-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1327"><a href="#cb18-1327" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb18-1328"><a href="#cb18-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1329"><a href="#cb18-1329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$V$ = Number of false positives (Type I errors)  </span>
<span id="cb18-1330"><a href="#cb18-1330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$S$ = Number of true positives (correct rejections)</span>
<span id="cb18-1331"><a href="#cb18-1331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$R = V + S$ = Total rejections</span>
<span id="cb18-1332"><a href="#cb18-1332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$m_0$ = Number of true null hypotheses</span>
<span id="cb18-1333"><a href="#cb18-1333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1334"><a href="#cb18-1334" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-1335"><a href="#cb18-1335" aria-hidden="true" tabindex="-1"></a>**Family-Wise Error Rate (FWER)**: The probability of making at least one Type I error among all tests:</span>
<span id="cb18-1336"><a href="#cb18-1336" aria-hidden="true" tabindex="-1"></a>$$\text{FWER} = \mathbb{P}(V \geq 1)$$</span>
<span id="cb18-1337"><a href="#cb18-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1338"><a href="#cb18-1338" aria-hidden="true" tabindex="-1"></a>**False Discovery Proportion (FDP)**: The proportion of rejections that are false:</span>
<span id="cb18-1339"><a href="#cb18-1339" aria-hidden="true" tabindex="-1"></a>$$\text{FDP} = \begin{cases}</span>
<span id="cb18-1340"><a href="#cb18-1340" aria-hidden="true" tabindex="-1"></a>\frac{V}{R} &amp; \text{if } R &gt; 0 <span class="sc">\\</span></span>
<span id="cb18-1341"><a href="#cb18-1341" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{if } R = 0</span>
<span id="cb18-1342"><a href="#cb18-1342" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb18-1343"><a href="#cb18-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1344"><a href="#cb18-1344" aria-hidden="true" tabindex="-1"></a>**False Discovery Rate (FDR)**: The expected false discovery proportion:</span>
<span id="cb18-1345"><a href="#cb18-1345" aria-hidden="true" tabindex="-1"></a>$$\text{FDR} = \mathbb{E}<span class="co">[</span><span class="ot">\text{FDP}</span><span class="co">]</span>$$</span>
<span id="cb18-1346"><a href="#cb18-1346" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1347"><a href="#cb18-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1348"><a href="#cb18-1348" aria-hidden="true" tabindex="-1"></a>These quantities represent different philosophies for error control:</span>
<span id="cb18-1349"><a href="#cb18-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1350"><a href="#cb18-1350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FWER control** is very conservative, aiming to avoid any false positives.</span>
<span id="cb18-1351"><a href="#cb18-1351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FDR control** is more liberal and often more sensible in practical applications, accepting some false positives but controlling their overall proportion.</span>
<span id="cb18-1352"><a href="#cb18-1352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1353"><a href="#cb18-1353" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Bonferroni Method: Controlling FWER</span></span>
<span id="cb18-1354"><a href="#cb18-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1355"><a href="#cb18-1355" aria-hidden="true" tabindex="-1"></a>The Bonferroni correction provides the simplest and most conservative approach to multiple testing.</span>
<span id="cb18-1356"><a href="#cb18-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1357"><a href="#cb18-1357" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-1358"><a href="#cb18-1358" aria-hidden="true" tabindex="-1"></a>**Bonferroni Method**: Given p-values $P_1, \ldots, P_m$, reject null hypothesis $H_{0i}$ if:</span>
<span id="cb18-1359"><a href="#cb18-1359" aria-hidden="true" tabindex="-1"></a>$$P_i &lt; \frac{\alpha}{m}$$</span>
<span id="cb18-1360"><a href="#cb18-1360" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1361"><a href="#cb18-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1362"><a href="#cb18-1362" aria-hidden="true" tabindex="-1"></a>**Intuition**: We divide our total error budget $\alpha$ equally among all $m$ tests. If each test has Type I error rate $\alpha/m$, the total error rate can't exceed $\alpha$.</span>
<span id="cb18-1363"><a href="#cb18-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1364"><a href="#cb18-1364" aria-hidden="true" tabindex="-1"></a>::: {.theorem}</span>
<span id="cb18-1365"><a href="#cb18-1365" aria-hidden="true" tabindex="-1"></a>Using the Bonferroni method, the probability of falsely rejecting any null hypothesis is at most $\alpha$:</span>
<span id="cb18-1366"><a href="#cb18-1366" aria-hidden="true" tabindex="-1"></a>$$\text{FWER} \leq \alpha$$</span>
<span id="cb18-1367"><a href="#cb18-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1368"><a href="#cb18-1368" aria-hidden="true" tabindex="-1"></a>**Proof**: Let $A_i$ be the event that test $i$ rejects when $H_{0i}$ is true. Then:</span>
<span id="cb18-1369"><a href="#cb18-1369" aria-hidden="true" tabindex="-1"></a>$$\text{FWER} = \mathbb{P}(\bigcup_{i \in I_0} A_i) \leq \sum_{i \in I_0} \mathbb{P}(A_i) \leq \sum_{i \in I_0} \frac{\alpha}{m} \leq \alpha$$</span>
<span id="cb18-1370"><a href="#cb18-1370" aria-hidden="true" tabindex="-1"></a>where $I_0$ is the set of true null hypotheses.</span>
<span id="cb18-1371"><a href="#cb18-1371" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1372"><a href="#cb18-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1373"><a href="#cb18-1373" aria-hidden="true" tabindex="-1"></a>**Pros**:</span>
<span id="cb18-1374"><a href="#cb18-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1375"><a href="#cb18-1375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple to implement  </span>
<span id="cb18-1376"><a href="#cb18-1376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provides strong control of Type I errors</span>
<span id="cb18-1377"><a href="#cb18-1377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works for any dependency structure among tests</span>
<span id="cb18-1378"><a href="#cb18-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1379"><a href="#cb18-1379" aria-hidden="true" tabindex="-1"></a>**Cons**:</span>
<span id="cb18-1380"><a href="#cb18-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1381"><a href="#cb18-1381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extremely conservative**: Dramatically reduces power</span>
<span id="cb18-1382"><a href="#cb18-1382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often fails to detect true effects  </span>
<span id="cb18-1383"><a href="#cb18-1383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Becomes nearly useless with thousands of tests</span>
<span id="cb18-1384"><a href="#cb18-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1385"><a href="#cb18-1385" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Benjamini-Hochberg Method: Controlling FDR</span></span>
<span id="cb18-1386"><a href="#cb18-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1387"><a href="#cb18-1387" aria-hidden="true" tabindex="-1"></a>The Benjamini-Hochberg (BH) procedure offers a more powerful alternative by controlling the proportion of false discoveries rather than the probability of any false discovery.</span>
<span id="cb18-1388"><a href="#cb18-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1389"><a href="#cb18-1389" aria-hidden="true" tabindex="-1"></a>::: {.definition}</span>
<span id="cb18-1390"><a href="#cb18-1390" aria-hidden="true" tabindex="-1"></a>**Benjamini-Hochberg (BH) Procedure**:</span>
<span id="cb18-1391"><a href="#cb18-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1392"><a href="#cb18-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Let $P_{(1)} \leq P_{(2)} \leq \ldots \leq P_{(m)}$ denote the ordered p-values</span>
<span id="cb18-1393"><a href="#cb18-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Define:</span>
<span id="cb18-1394"><a href="#cb18-1394" aria-hidden="true" tabindex="-1"></a>   $$\ell_i = \frac{i \alpha}{C_m m} \quad \text{and} \quad R = \max\{i: P_{(i)} \leq \ell_i<span class="sc">\}</span>$$</span>
<span id="cb18-1395"><a href="#cb18-1395" aria-hidden="true" tabindex="-1"></a>   where $C_m$ is defined as:</span>
<span id="cb18-1396"><a href="#cb18-1396" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$C_m = 1$ if the p-values are independent</span>
<span id="cb18-1397"><a href="#cb18-1397" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$C_m = \sum_{i=1}^m \frac{1}{i}$ otherwise (for dependent tests)</span>
<span id="cb18-1398"><a href="#cb18-1398" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Let $T = P_{(R)}$ be the **BH rejection threshold**</span>
<span id="cb18-1399"><a href="#cb18-1399" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Reject all null hypotheses $H_{0i}$ for which $P_i \leq T$</span>
<span id="cb18-1400"><a href="#cb18-1400" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1401"><a href="#cb18-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1402"><a href="#cb18-1402" aria-hidden="true" tabindex="-1"></a>**Intuition**: The procedure finds the largest set of rejections such that the expected proportion of false discoveries is controlled. In practice, $C_m = 1$ is almost always used (assuming independence), so the threshold for the $i$-th smallest p-value is $\ell_i = i\alpha/m$. The procedure looks for the rightmost p-value that falls below this sloped line.</span>
<span id="cb18-1403"><a href="#cb18-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1404"><a href="#cb18-1404" aria-hidden="true" tabindex="-1"></a>The procedure can be visualized by plotting ordered p-values against their threshold line:</span>
<span id="cb18-1405"><a href="#cb18-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1408"><a href="#cb18-1408" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1409"><a href="#cb18-1409" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb18-1410"><a href="#cb18-1410" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb18-1411"><a href="#cb18-1411" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize BH procedure with a clearer example</span></span>
<span id="cb18-1412"><a href="#cb18-1412" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb18-1413"><a href="#cb18-1413" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">20</span>  <span class="co"># Number of tests</span></span>
<span id="cb18-1414"><a href="#cb18-1414" aria-hidden="true" tabindex="-1"></a>alpha_fdr <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb18-1415"><a href="#cb18-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1416"><a href="#cb18-1416" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate p-values designed to show BH advantage</span></span>
<span id="cb18-1417"><a href="#cb18-1417" aria-hidden="true" tabindex="-1"></a><span class="co"># With α=0.05 and m=20: Bonferroni threshold = 0.0025, BH thresholds = i*0.0025</span></span>
<span id="cb18-1418"><a href="#cb18-1418" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.array([</span>
<span id="cb18-1419"><a href="#cb18-1419" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.0008</span>, <span class="fl">0.0045</span>, <span class="fl">0.0068</span>,  <span class="co"># BH will reject these 3 (below 0.0025, 0.005, 0.0075)</span></span>
<span id="cb18-1420"><a href="#cb18-1420" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.012</span>, <span class="fl">0.024</span>, <span class="fl">0.041</span>, <span class="fl">0.063</span>, <span class="fl">0.089</span>,  <span class="co"># Won't be rejected</span></span>
<span id="cb18-1421"><a href="#cb18-1421" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.115</span>, <span class="fl">0.181</span>, <span class="fl">0.224</span>, <span class="fl">0.301</span>,  <span class="co"># Medium p-values</span></span>
<span id="cb18-1422"><a href="#cb18-1422" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.412</span>, <span class="fl">0.501</span>, <span class="fl">0.578</span>, <span class="fl">0.656</span>,  <span class="co"># Larger p-values  </span></span>
<span id="cb18-1423"><a href="#cb18-1423" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.721</span>, <span class="fl">0.812</span>, <span class="fl">0.888</span>, <span class="fl">0.951</span>  <span class="co"># Very large p-values</span></span>
<span id="cb18-1424"><a href="#cb18-1424" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb18-1425"><a href="#cb18-1425" aria-hidden="true" tabindex="-1"></a>p_sorted <span class="op">=</span> np.sort(p_values)</span>
<span id="cb18-1426"><a href="#cb18-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1427"><a href="#cb18-1427" aria-hidden="true" tabindex="-1"></a><span class="co"># BH threshold line (using C_m = 1 for independent tests)</span></span>
<span id="cb18-1428"><a href="#cb18-1428" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.arange(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb18-1429"><a href="#cb18-1429" aria-hidden="true" tabindex="-1"></a>bh_threshold <span class="op">=</span> k_values <span class="op">*</span> alpha_fdr <span class="op">/</span> m  <span class="co"># This is ℓ_i with C_m = 1</span></span>
<span id="cb18-1430"><a href="#cb18-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1431"><a href="#cb18-1431" aria-hidden="true" tabindex="-1"></a><span class="co"># Find BH cutoff using the standard definition</span></span>
<span id="cb18-1432"><a href="#cb18-1432" aria-hidden="true" tabindex="-1"></a><span class="co"># R = max{i: P_(i) ≤ ℓ_i}</span></span>
<span id="cb18-1433"><a href="#cb18-1433" aria-hidden="true" tabindex="-1"></a>bh_check <span class="op">=</span> p_sorted <span class="op">&lt;=</span> bh_threshold</span>
<span id="cb18-1434"><a href="#cb18-1434" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> np.<span class="bu">any</span>(bh_check):</span>
<span id="cb18-1435"><a href="#cb18-1435" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.<span class="bu">max</span>(np.where(bh_check)[<span class="dv">0</span>]) <span class="op">+</span> <span class="dv">1</span>  <span class="co"># +1 because Python uses 0-indexing</span></span>
<span id="cb18-1436"><a href="#cb18-1436" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> p_sorted[R<span class="op">-</span><span class="dv">1</span>]  <span class="co"># BH rejection threshold</span></span>
<span id="cb18-1437"><a href="#cb18-1437" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-1438"><a href="#cb18-1438" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-1439"><a href="#cb18-1439" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-1440"><a href="#cb18-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1441"><a href="#cb18-1441" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb18-1442"><a href="#cb18-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1443"><a href="#cb18-1443" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot p-values and thresholds</span></span>
<span id="cb18-1444"><a href="#cb18-1444" aria-hidden="true" tabindex="-1"></a>plt.scatter(k_values, p_sorted, color<span class="op">=</span><span class="st">'blue'</span>, s<span class="op">=</span><span class="dv">60</span>, label<span class="op">=</span><span class="st">'Sorted p-values'</span>, zorder<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb18-1445"><a href="#cb18-1445" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, bh_threshold, <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'BH threshold (α=</span><span class="sc">{</span>alpha_fdr<span class="sc">}</span><span class="ss">)'</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-1446"><a href="#cb18-1446" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, [alpha_fdr<span class="op">/</span>m]<span class="op">*</span>m, <span class="st">'g--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="ss">f'Bonferroni threshold'</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-1447"><a href="#cb18-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1448"><a href="#cb18-1448" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> R <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb18-1449"><a href="#cb18-1449" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Highlight rejected p-values</span></span>
<span id="cb18-1450"><a href="#cb18-1450" aria-hidden="true" tabindex="-1"></a>    plt.scatter(k_values[:R], p_sorted[:R], </span>
<span id="cb18-1451"><a href="#cb18-1451" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">100</span>, marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'Rejected'</span>, zorder<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb18-1452"><a href="#cb18-1452" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark the cutoff point R</span></span>
<span id="cb18-1453"><a href="#cb18-1453" aria-hidden="true" tabindex="-1"></a>    plt.axvline(R <span class="op">+</span> <span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-1454"><a href="#cb18-1454" aria-hidden="true" tabindex="-1"></a>    plt.text(R <span class="op">+</span> <span class="fl">0.7</span>, <span class="fl">0.15</span>, <span class="ss">f'R = </span><span class="sc">{</span>R<span class="sc">}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb18-1455"><a href="#cb18-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1456"><a href="#cb18-1456" aria-hidden="true" tabindex="-1"></a><span class="co"># Formatting</span></span>
<span id="cb18-1457"><a href="#cb18-1457" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Rank i'</span>)</span>
<span id="cb18-1458"><a href="#cb18-1458" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'p-value (log scale)'</span>)</span>
<span id="cb18-1459"><a href="#cb18-1459" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Benjamini-Hochberg Procedure'</span>)</span>
<span id="cb18-1460"><a href="#cb18-1460" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>, framealpha<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb18-1461"><a href="#cb18-1461" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>, which<span class="op">=</span><span class="st">'both'</span>)</span>
<span id="cb18-1462"><a href="#cb18-1462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1463"><a href="#cb18-1463" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix axes</span></span>
<span id="cb18-1464"><a href="#cb18-1464" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>, m<span class="op">+</span><span class="dv">1</span>, <span class="dv">5</span>))  <span class="co"># Show every 5th rank</span></span>
<span id="cb18-1465"><a href="#cb18-1465" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, m<span class="op">+</span><span class="dv">1</span>)  <span class="co"># Proper bounds for discrete ranks</span></span>
<span id="cb18-1466"><a href="#cb18-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1467"><a href="#cb18-1467" aria-hidden="true" tabindex="-1"></a><span class="co"># Use log scale for y-axis</span></span>
<span id="cb18-1468"><a href="#cb18-1468" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb18-1469"><a href="#cb18-1469" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.0005</span>, <span class="fl">1.0</span>)  <span class="co"># Start just below smallest p-value</span></span>
<span id="cb18-1470"><a href="#cb18-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1471"><a href="#cb18-1471" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-1472"><a href="#cb18-1472" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-1473"><a href="#cb18-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1474"><a href="#cb18-1474" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> R <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb18-1475"><a href="#cb18-1475" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"BH procedure:"</span>)</span>
<span id="cb18-1476"><a href="#cb18-1476" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  R = </span><span class="sc">{</span>R<span class="sc">}</span><span class="ss"> (largest i where P_(i) &lt; i·α/m)"</span>)</span>
<span id="cb18-1477"><a href="#cb18-1477" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Rejection threshold T = P_(</span><span class="sc">{</span>R<span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span>T<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1478"><a href="#cb18-1478" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Rejects </span><span class="sc">{</span>R<span class="sc">}</span><span class="ss"> hypotheses (p-values ≤ </span><span class="sc">{</span>T<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb18-1479"><a href="#cb18-1479" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Bonferroni would reject </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(p_sorted <span class="op">&lt;</span> alpha_fdr<span class="op">/</span>m)<span class="sc">}</span><span class="ss"> hypotheses"</span>)</span>
<span id="cb18-1480"><a href="#cb18-1480" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  (only those with p &lt; </span><span class="sc">{</span>alpha_fdr<span class="op">/</span>m<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb18-1481"><a href="#cb18-1481" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-1482"><a href="#cb18-1482" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"No hypotheses rejected by either method"</span>)</span>
<span id="cb18-1483"><a href="#cb18-1483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1484"><a href="#cb18-1484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1485"><a href="#cb18-1485" aria-hidden="true" tabindex="-1"></a>The visualization shows the key difference between methods:</span>
<span id="cb18-1486"><a href="#cb18-1486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1487"><a href="#cb18-1487" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bonferroni** (flat green line): Same strict threshold for all tests</span>
<span id="cb18-1488"><a href="#cb18-1488" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BH** (sloped red line): More lenient threshold for higher-ranked p-values</span>
<span id="cb18-1489"><a href="#cb18-1489" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The BH procedure finds the rightmost crossing of p-values below the sloped line</span>
<span id="cb18-1490"><a href="#cb18-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1491"><a href="#cb18-1491" aria-hidden="true" tabindex="-1"></a>::: {.theorem name="Benjamini-Hochberg Theorem"}</span>
<span id="cb18-1492"><a href="#cb18-1492" aria-hidden="true" tabindex="-1"></a>When the BH procedure is applied with the appropriate $C_m$:</span>
<span id="cb18-1493"><a href="#cb18-1493" aria-hidden="true" tabindex="-1"></a>$$\text{FDR} = \mathbb{E}<span class="co">[</span><span class="ot">\text{FDP}</span><span class="co">]</span> \leq \frac{m_0}{m}\alpha \leq \alpha$$</span>
<span id="cb18-1494"><a href="#cb18-1494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1495"><a href="#cb18-1495" aria-hidden="true" tabindex="-1"></a>where $m_0$ is the number of true null hypotheses.</span>
<span id="cb18-1496"><a href="#cb18-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1497"><a href="#cb18-1497" aria-hidden="true" tabindex="-1"></a>This guarantee holds:</span>
<span id="cb18-1498"><a href="#cb18-1498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1499"><a href="#cb18-1499" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With $C_m = 1$ when the test statistics are independent</span>
<span id="cb18-1500"><a href="#cb18-1500" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With $C_m = \sum_{i=1}^m \frac{1}{i}$ for arbitrary dependence structures</span>
<span id="cb18-1501"><a href="#cb18-1501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1502"><a href="#cb18-1502" aria-hidden="true" tabindex="-1"></a>In practice, $C_m = 1$ is almost always used as the procedure is remarkably robust to many forms of dependence.</span>
<span id="cb18-1503"><a href="#cb18-1503" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1504"><a href="#cb18-1504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1505"><a href="#cb18-1505" aria-hidden="true" tabindex="-1"></a>**Comparison of Methods**:</span>
<span id="cb18-1506"><a href="#cb18-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1507"><a href="#cb18-1507" aria-hidden="true" tabindex="-1"></a>| Method | Controls | Decision Rule | Power | Use Case |</span>
<span id="cb18-1508"><a href="#cb18-1508" aria-hidden="true" tabindex="-1"></a>|:-------|:---------|:--------------|:------|:---------|</span>
<span id="cb18-1509"><a href="#cb18-1509" aria-hidden="true" tabindex="-1"></a>| Bonferroni | FWER $\leq \alpha$ | Reject if $P_i &lt; \alpha/m$ | Low | Critical applications |</span>
<span id="cb18-1510"><a href="#cb18-1510" aria-hidden="true" tabindex="-1"></a>| Benjamini-Hochberg | FDR $\leq \alpha$ | Reject if $P_i \leq T = P_{(R)}$* | Higher | Large-scale testing |</span>
<span id="cb18-1511"><a href="#cb18-1511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1512"><a href="#cb18-1512" aria-hidden="true" tabindex="-1"></a>*Where $R = \max<span class="sc">\{</span>i: P_{(i)} \leq i\alpha/(C_m m)<span class="sc">\}</span>$ with $C_m = 1$ for independent tests</span>
<span id="cb18-1513"><a href="#cb18-1513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1514"><a href="#cb18-1514" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-1515"><a href="#cb18-1515" aria-hidden="true" tabindex="-1"></a><span class="fu">## Adjusted p-values: An Alternative Presentation</span></span>
<span id="cb18-1516"><a href="#cb18-1516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1517"><a href="#cb18-1517" aria-hidden="true" tabindex="-1"></a>The BH procedure can also be presented using **adjusted p-values**, which is how many software packages (including <span class="in">`scipy.stats.false_discovery_control`</span>) implement it. This might seem like a completely different method, but it's just a reformulation of the same procedure.</span>
<span id="cb18-1518"><a href="#cb18-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1519"><a href="#cb18-1519" aria-hidden="true" tabindex="-1"></a>**Adjusted p-values**: For the BH method, the adjusted p-value is computed in two steps:</span>
<span id="cb18-1520"><a href="#cb18-1520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1521"><a href="#cb18-1521" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>First, scale each p-value: $\tilde{P}'_{(i)} = \min\left(1, \frac{m}{i} P_{(i)}\right)$</span>
<span id="cb18-1522"><a href="#cb18-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1523"><a href="#cb18-1523" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Then enforce monotonicity (working from largest to smallest):</span>
<span id="cb18-1524"><a href="#cb18-1524" aria-hidden="true" tabindex="-1"></a>   $$\tilde{P}_{(i)} = \min_{j \geq i} \tilde{P}'_{(j)}$$</span>
<span id="cb18-1525"><a href="#cb18-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1526"><a href="#cb18-1526" aria-hidden="true" tabindex="-1"></a>This ensures adjusted p-values are non-decreasing: $\tilde{P}_{(1)} \leq \tilde{P}_{(2)} \leq \ldots \leq \tilde{P}_{(m)}$.</span>
<span id="cb18-1527"><a href="#cb18-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1528"><a href="#cb18-1528" aria-hidden="true" tabindex="-1"></a>After computing adjusted p-values, reject all hypotheses where $\tilde{P}_i \leq \alpha$.</span>
<span id="cb18-1529"><a href="#cb18-1529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1530"><a href="#cb18-1530" aria-hidden="true" tabindex="-1"></a>This is **mathematically equivalent** to the threshold approach but ensures the adjusted p-values maintain proper ordering.</span>
<span id="cb18-1531"><a href="#cb18-1531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1534"><a href="#cb18-1534" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1535"><a href="#cb18-1535" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-1536"><a href="#cb18-1536" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb18-1537"><a href="#cb18-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1538"><a href="#cb18-1538" aria-hidden="true" tabindex="-1"></a><span class="co"># Example p-values</span></span>
<span id="cb18-1539"><a href="#cb18-1539" aria-hidden="true" tabindex="-1"></a>pvals <span class="op">=</span> [<span class="fl">0.003</span>, <span class="fl">0.02</span>, <span class="fl">0.04</span>, <span class="fl">0.08</span>, <span class="fl">0.15</span>, <span class="fl">0.25</span>]</span>
<span id="cb18-1540"><a href="#cb18-1540" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(pvals)</span>
<span id="cb18-1541"><a href="#cb18-1541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1542"><a href="#cb18-1542" aria-hidden="true" tabindex="-1"></a><span class="co"># Using scipy's function (returns adjusted p-values)</span></span>
<span id="cb18-1543"><a href="#cb18-1543" aria-hidden="true" tabindex="-1"></a>adjusted <span class="op">=</span> stats.false_discovery_control(pvals, method<span class="op">=</span><span class="st">'bh'</span>)</span>
<span id="cb18-1544"><a href="#cb18-1544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1545"><a href="#cb18-1545" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original vs Adjusted p-values:"</span>)</span>
<span id="cb18-1546"><a href="#cb18-1546" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb18-1547"><a href="#cb18-1547" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (p, adj) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(pvals, adjusted)):</span>
<span id="cb18-1548"><a href="#cb18-1548" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Scale the p-value</span></span>
<span id="cb18-1549"><a href="#cb18-1549" aria-hidden="true" tabindex="-1"></a>    scaled <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, (m<span class="op">/</span>(i<span class="op">+</span><span class="dv">1</span>)) <span class="op">*</span> p)</span>
<span id="cb18-1550"><a href="#cb18-1550" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"p[</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">] = </span><span class="sc">{</span>p<span class="sc">:.3f}</span><span class="ss"> → scaled = </span><span class="sc">{</span>scaled<span class="sc">:.3f}</span><span class="ss"> → adjusted = </span><span class="sc">{</span>adj<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-1551"><a href="#cb18-1551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1552"><a href="#cb18-1552" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Note: adjusted values enforce monotonicity"</span>)</span>
<span id="cb18-1553"><a href="#cb18-1553" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"(each adjusted p-value ≥ previous one)"</span>)</span>
<span id="cb18-1554"><a href="#cb18-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1555"><a href="#cb18-1555" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">At α = 0.05:"</span>)</span>
<span id="cb18-1556"><a href="#cb18-1556" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reject hypotheses where adjusted p-value ≤ 0.05"</span>)</span>
<span id="cb18-1557"><a href="#cb18-1557" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Result: Reject first </span><span class="sc">{</span><span class="bu">sum</span>(adjusted <span class="op">&lt;=</span> <span class="fl">0.05</span>)<span class="sc">}</span><span class="ss"> hypotheses"</span>)</span>
<span id="cb18-1558"><a href="#cb18-1558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1559"><a href="#cb18-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1560"><a href="#cb18-1560" aria-hidden="true" tabindex="-1"></a>The adjusted p-values tell you: "This is the smallest α level at which this hypothesis would be rejected by the BH procedure."</span>
<span id="cb18-1561"><a href="#cb18-1561" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1562"><a href="#cb18-1562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1563"><a href="#cb18-1563" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Recommendations</span></span>
<span id="cb18-1564"><a href="#cb18-1564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1565"><a href="#cb18-1565" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb18-1566"><a href="#cb18-1566" aria-hidden="true" tabindex="-1"></a><span class="fu">## When to Use Which Method</span></span>
<span id="cb18-1567"><a href="#cb18-1567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1568"><a href="#cb18-1568" aria-hidden="true" tabindex="-1"></a>**Use Bonferroni when**:</span>
<span id="cb18-1569"><a href="#cb18-1569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1570"><a href="#cb18-1570" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You have a small number of tests (&lt; 20)</span>
<span id="cb18-1571"><a href="#cb18-1571" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The cost of any false positive is very high</span>
<span id="cb18-1572"><a href="#cb18-1572" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You need to convince skeptical reviewers</span>
<span id="cb18-1573"><a href="#cb18-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1574"><a href="#cb18-1574" aria-hidden="true" tabindex="-1"></a>**Use Benjamini-Hochberg when**:</span>
<span id="cb18-1575"><a href="#cb18-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1576"><a href="#cb18-1576" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You have many tests (genomics, neuroimaging, etc.)</span>
<span id="cb18-1577"><a href="#cb18-1577" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some false positives are acceptable</span>
<span id="cb18-1578"><a href="#cb18-1578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You want to generate hypotheses for follow-up</span>
<span id="cb18-1579"><a href="#cb18-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1580"><a href="#cb18-1580" aria-hidden="true" tabindex="-1"></a>**Use no correction when**:</span>
<span id="cb18-1581"><a href="#cb18-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1582"><a href="#cb18-1582" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tests are explicitly exploratory</span>
<span id="cb18-1583"><a href="#cb18-1583" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You're doing hypothesis generation, not confirmation</span>
<span id="cb18-1584"><a href="#cb18-1584" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>But always report that no correction was applied!</span>
<span id="cb18-1585"><a href="#cb18-1585" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1586"><a href="#cb18-1586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1587"><a href="#cb18-1587" aria-hidden="true" tabindex="-1"></a><span class="fu">## NHST in Practice: A Critical View</span></span>
<span id="cb18-1588"><a href="#cb18-1588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1589"><a href="#cb18-1589" aria-hidden="true" tabindex="-1"></a>Despite its ubiquity in scientific research, NHST has fundamental limitations that have contributed to what many call the **replication crisis** -- the inability to reproduce many published scientific findings.</span>
<span id="cb18-1590"><a href="#cb18-1590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1591"><a href="#cb18-1591" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fundamental Problems with NHST</span></span>
<span id="cb18-1592"><a href="#cb18-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1593"><a href="#cb18-1593" aria-hidden="true" tabindex="-1"></a>**The Focus on the Null**:</span>
<span id="cb18-1594"><a href="#cb18-1594" aria-hidden="true" tabindex="-1"></a>NHST tells us whether to reject $H_0$, but doesn't require us to specify a realistic alternative. We test against "no effect" without having to say what effect we actually expect. This leads to vague research questions and post-hoc rationalization of any significant finding.</span>
<span id="cb18-1595"><a href="#cb18-1595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1596"><a href="#cb18-1596" aria-hidden="true" tabindex="-1"></a>**Ignoring Prior Plausibility**:</span>
<span id="cb18-1597"><a href="#cb18-1597" aria-hidden="true" tabindex="-1"></a>NHST treats all hypotheses equally -- the same p &lt; 0.05 is required whether testing a well-established biological mechanism or claiming telepathy exists. As the saying goes, "extraordinary claims require extraordinary evidence," but NHST doesn't account for this. This is a key limitation addressed by Bayesian methods (Chapter 8).</span>
<span id="cb18-1598"><a href="#cb18-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1599"><a href="#cb18-1599" aria-hidden="true" tabindex="-1"></a>**The Dichotomy Problem**:</span>
<span id="cb18-1600"><a href="#cb18-1600" aria-hidden="true" tabindex="-1"></a>The division into "significant" and "non-significant" at α = 0.05 is entirely arbitrary. As statistician Andrew Gelman notes:</span>
<span id="cb18-1601"><a href="#cb18-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1602"><a href="#cb18-1602" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "The difference between 'significant' and 'not significant' is not itself statistically significant!"</span></span>
<span id="cb18-1603"><a href="#cb18-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1604"><a href="#cb18-1604" aria-hidden="true" tabindex="-1"></a>A result with p = 0.049 is treated fundamentally differently from p = 0.051, despite being practically identical.</span>
<span id="cb18-1605"><a href="#cb18-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1606"><a href="#cb18-1606" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Low Power Trap</span></span>
<span id="cb18-1607"><a href="#cb18-1607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1608"><a href="#cb18-1608" aria-hidden="true" tabindex="-1"></a>Low-powered studies create a vicious cycle:</span>
<span id="cb18-1609"><a href="#cb18-1609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1610"><a href="#cb18-1610" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They often fail to detect true effects (high Type II error)</span>
<span id="cb18-1611"><a href="#cb18-1611" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When they do find "significant" results, these are more likely to be false positives</span>
<span id="cb18-1612"><a href="#cb18-1612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Crucially**: In low-power settings, most published "significant" findings are false positives</span>
<span id="cb18-1613"><a href="#cb18-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1614"><a href="#cb18-1614" aria-hidden="true" tabindex="-1"></a>This counterintuitive result occurs because with low power, the few significant results that emerge are disproportionately likely to be statistical flukes rather than real effects.</span>
<span id="cb18-1615"><a href="#cb18-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1616"><a href="#cb18-1616" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Misuses</span></span>
<span id="cb18-1617"><a href="#cb18-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1618"><a href="#cb18-1618" aria-hidden="true" tabindex="-1"></a>**p-hacking (Data Dredging)**:</span>
<span id="cb18-1619"><a href="#cb18-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1620"><a href="#cb18-1620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Testing multiple hypotheses until finding p &lt; 0.05</span>
<span id="cb18-1621"><a href="#cb18-1621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stopping data collection when significance is reached</span>
<span id="cb18-1622"><a href="#cb18-1622" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Trying different analyses until one "works"</span>
<span id="cb18-1623"><a href="#cb18-1623" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Excluding "outliers" post-hoc to achieve significance</span>
<span id="cb18-1624"><a href="#cb18-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1625"><a href="#cb18-1625" aria-hidden="true" tabindex="-1"></a>**HARKing** (Hypothesizing After Results are Known):</span>
<span id="cb18-1626"><a href="#cb18-1626" aria-hidden="true" tabindex="-1"></a>Looking at the data first, then pretending the observed pattern was the hypothesis all along.</span>
<span id="cb18-1627"><a href="#cb18-1627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1628"><a href="#cb18-1628" aria-hidden="true" tabindex="-1"></a>**Publication Bias**:</span>
<span id="cb18-1629"><a href="#cb18-1629" aria-hidden="true" tabindex="-1"></a>Journals preferentially publish "significant" results, creating a distorted scientific literature where negative results disappear. This file-drawer problem means the published record overestimates effect sizes and underestimates uncertainty.</span>
<span id="cb18-1630"><a href="#cb18-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1631"><a href="#cb18-1631" aria-hidden="true" tabindex="-1"></a><span class="fu">### Moving Forward: Better Practices</span></span>
<span id="cb18-1632"><a href="#cb18-1632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1633"><a href="#cb18-1633" aria-hidden="true" tabindex="-1"></a>**Practical Recommendations**:</span>
<span id="cb18-1634"><a href="#cb18-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1635"><a href="#cb18-1635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Always report effect sizes and confidence intervals**, not just p-values</span>
<span id="cb18-1636"><a href="#cb18-1636" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Conduct power analyses** before collecting data</span>
<span id="cb18-1637"><a href="#cb18-1637" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pre-register hypotheses and analysis plans** to prevent p-hacking</span>
<span id="cb18-1638"><a href="#cb18-1638" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Consider Bayesian methods** when prior information exists</span>
<span id="cb18-1639"><a href="#cb18-1639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Use appropriate multiple testing corrections** when testing many hypotheses</span>
<span id="cb18-1640"><a href="#cb18-1640" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Report all analyses attempted**, not just the "significant" ones</span>
<span id="cb18-1641"><a href="#cb18-1641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1642"><a href="#cb18-1642" aria-hidden="true" tabindex="-1"></a>**Alternative Approaches**:</span>
<span id="cb18-1643"><a href="#cb18-1643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1644"><a href="#cb18-1644" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bayesian inference** (Chapter 8): Incorporates prior knowledge and provides probability statements about hypotheses</span>
<span id="cb18-1645"><a href="#cb18-1645" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Estimation-focused analysis**: Emphasize effect sizes and uncertainty rather than binary decisions</span>
<span id="cb18-1646"><a href="#cb18-1646" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Replication studies**: The ultimate test of a finding's validity</span>
<span id="cb18-1647"><a href="#cb18-1647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1648"><a href="#cb18-1648" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter Summary</span></span>
<span id="cb18-1649"><a href="#cb18-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1650"><a href="#cb18-1650" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Concepts Review</span></span>
<span id="cb18-1651"><a href="#cb18-1651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1652"><a href="#cb18-1652" aria-hidden="true" tabindex="-1"></a>We've explored the foundations of null-hypothesis significance testing (NHST):</span>
<span id="cb18-1653"><a href="#cb18-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1654"><a href="#cb18-1654" aria-hidden="true" tabindex="-1"></a>**The Framework**:</span>
<span id="cb18-1655"><a href="#cb18-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1656"><a href="#cb18-1656" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Null and alternative hypotheses**: $H_0$ (no effect) vs $H_1$ (effect exists)</span>
<span id="cb18-1657"><a href="#cb18-1657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Type I and Type II errors**: False positives vs false negatives</span>
<span id="cb18-1658"><a href="#cb18-1658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Power and size**: Probability of detecting true effects vs controlling false positives</span>
<span id="cb18-1659"><a href="#cb18-1659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Test statistic and rejection region**: Summarizing evidence and decision rules</span>
<span id="cb18-1660"><a href="#cb18-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1661"><a href="#cb18-1661" aria-hidden="true" tabindex="-1"></a>**The p-value**:</span>
<span id="cb18-1662"><a href="#cb18-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1663"><a href="#cb18-1663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measures how surprising data would be under $H_0$</span>
<span id="cb18-1664"><a href="#cb18-1664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>NOT the probability that $H_0$ is true</span>
<span id="cb18-1665"><a href="#cb18-1665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small p-value = evidence against $H_0$, not proof</span>
<span id="cb18-1666"><a href="#cb18-1666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Statistical significance ≠ practical significance</span>
<span id="cb18-1667"><a href="#cb18-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1668"><a href="#cb18-1668" aria-hidden="true" tabindex="-1"></a>**Key Tests**:</span>
<span id="cb18-1669"><a href="#cb18-1669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1670"><a href="#cb18-1670" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wald test**: Uses asymptotic normality of estimators</span>
<span id="cb18-1671"><a href="#cb18-1671" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Permutation test**: Non-parametric alternative requiring minimal assumptions</span>
<span id="cb18-1672"><a href="#cb18-1672" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fisher's exact test**: Exact test for contingency tables</span>
<span id="cb18-1673"><a href="#cb18-1673" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Likelihood ratio test**: General framework for testing constraints on parameters</span>
<span id="cb18-1674"><a href="#cb18-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1675"><a href="#cb18-1675" aria-hidden="true" tabindex="-1"></a>**Multiple Testing**:</span>
<span id="cb18-1676"><a href="#cb18-1676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1677"><a href="#cb18-1677" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Running many tests inflates Type I error rate</span>
<span id="cb18-1678"><a href="#cb18-1678" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bonferroni**: Controls FWER (conservative but safe)</span>
<span id="cb18-1679"><a href="#cb18-1679" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Benjamini-Hochberg**: Controls FDR (more powerful, modern standard)</span>
<span id="cb18-1680"><a href="#cb18-1680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1681"><a href="#cb18-1681" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pitfalls to Avoid</span></span>
<span id="cb18-1682"><a href="#cb18-1682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1683"><a href="#cb18-1683" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Misinterpreting p-values**: Remember, p-value ≠ P($H_0$ is true)</span>
<span id="cb18-1684"><a href="#cb18-1684" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Multiple testing without correction**: Always consider how many tests you're running</span>
<span id="cb18-1685"><a href="#cb18-1685" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Confusing statistical and practical significance**: A tiny effect can be "significant" with enough data</span>
<span id="cb18-1686"><a href="#cb18-1686" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Ignoring assumptions**: Tests like the Wald test require large samples</span>
<span id="cb18-1687"><a href="#cb18-1687" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Post-hoc hypothesis formulation**: Don't look at data, then formulate hypotheses to test</span>
<span id="cb18-1688"><a href="#cb18-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1689"><a href="#cb18-1689" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter Connections</span></span>
<span id="cb18-1690"><a href="#cb18-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1691"><a href="#cb18-1691" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Previous chapters**: </span>
<span id="cb18-1692"><a href="#cb18-1692" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Chapters 5-6 gave us estimators and their properties; now we test hypotheses about them</span>
<span id="cb18-1693"><a href="#cb18-1693" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Chapter 4 (Bootstrap) provides an alternative to asymptotic tests</span>
<span id="cb18-1694"><a href="#cb18-1694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**This chapter**: Core framework for statistical inference and decision-making</span>
<span id="cb18-1695"><a href="#cb18-1695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next chapter**: Bayesian inference offers an alternative paradigm that:</span>
<span id="cb18-1696"><a href="#cb18-1696" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Provides probabilities for hypotheses</span>
<span id="cb18-1697"><a href="#cb18-1697" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Incorporates prior information</span>
<span id="cb18-1698"><a href="#cb18-1698" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Avoids some NHST pitfalls</span>
<span id="cb18-1699"><a href="#cb18-1699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1700"><a href="#cb18-1700" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Test Problems</span></span>
<span id="cb18-1701"><a href="#cb18-1701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1702"><a href="#cb18-1702" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Understanding Type I and Type II Errors**</span>
<span id="cb18-1703"><a href="#cb18-1703" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1704"><a href="#cb18-1704" aria-hidden="true" tabindex="-1"></a>   A medical test for a disease has the following properties:</span>
<span id="cb18-1705"><a href="#cb18-1705" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1706"><a href="#cb18-1706" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>If a person has the disease, the test is positive 95% of the time</span>
<span id="cb18-1707"><a href="#cb18-1707" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>If a person doesn't have the disease, the test is negative 98% of the time</span>
<span id="cb18-1708"><a href="#cb18-1708" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1709"><a href="#cb18-1709" aria-hidden="true" tabindex="-1"></a>   In hypothesis testing terms (where $H_0$: person is healthy):</span>
<span id="cb18-1710"><a href="#cb18-1710" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1711"><a href="#cb18-1711" aria-hidden="true" tabindex="-1"></a>   a) What is the Type I error rate?</span>
<span id="cb18-1712"><a href="#cb18-1712" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1713"><a href="#cb18-1713" aria-hidden="true" tabindex="-1"></a>   b) What is the Type II error rate?</span>
<span id="cb18-1714"><a href="#cb18-1714" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1715"><a href="#cb18-1715" aria-hidden="true" tabindex="-1"></a>   c) What is the power of the test?</span>
<span id="cb18-1716"><a href="#cb18-1716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1717"><a href="#cb18-1717" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-1718"><a href="#cb18-1718" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb18-1719"><a href="#cb18-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1720"><a href="#cb18-1720" aria-hidden="true" tabindex="-1"></a>a) Type I error = rejecting $H_0$ when true = false positive = 1 - 0.98 = 0.02</span>
<span id="cb18-1721"><a href="#cb18-1721" aria-hidden="true" tabindex="-1"></a>b) Type II error = failing to reject $H_0$ when false = false negative = 1 - 0.95 = 0.05</span>
<span id="cb18-1722"><a href="#cb18-1722" aria-hidden="true" tabindex="-1"></a>c) Power = 1 - Type II error = 0.95</span>
<span id="cb18-1723"><a href="#cb18-1723" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1724"><a href="#cb18-1724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1725"><a href="#cb18-1725" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Wald Test Calculation**</span>
<span id="cb18-1726"><a href="#cb18-1726" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1727"><a href="#cb18-1727" aria-hidden="true" tabindex="-1"></a>   Death times around Passover (from AoS Exercise 10.6): Of 1919 deaths, 922 occurred the week before Passover and 997 the week after. Test $H_0: p = 0.5$ where $p$ is the probability of death in the week before.</span>
<span id="cb18-1728"><a href="#cb18-1728" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1729"><a href="#cb18-1729" aria-hidden="true" tabindex="-1"></a>   Calculate the Wald test statistic and p-value.</span>
<span id="cb18-1730"><a href="#cb18-1730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1731"><a href="#cb18-1731" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-1732"><a href="#cb18-1732" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb18-1733"><a href="#cb18-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1736"><a href="#cb18-1736" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1737"><a href="#cb18-1737" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-1738"><a href="#cb18-1738" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-1739"><a href="#cb18-1739" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-1740"><a href="#cb18-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1741"><a href="#cb18-1741" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1919</span></span>
<span id="cb18-1742"><a href="#cb18-1742" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">922</span></span>
<span id="cb18-1743"><a href="#cb18-1743" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> x <span class="op">/</span> n</span>
<span id="cb18-1744"><a href="#cb18-1744" aria-hidden="true" tabindex="-1"></a>p_0 <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb18-1745"><a href="#cb18-1745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1746"><a href="#cb18-1746" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test statistic</span></span>
<span id="cb18-1747"><a href="#cb18-1747" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p_0 <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_0) <span class="op">/</span> n)</span>
<span id="cb18-1748"><a href="#cb18-1748" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> (p_hat <span class="op">-</span> p_0) <span class="op">/</span> se</span>
<span id="cb18-1749"><a href="#cb18-1749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1750"><a href="#cb18-1750" aria-hidden="true" tabindex="-1"></a><span class="co"># p-value (two-sided)</span></span>
<span id="cb18-1751"><a href="#cb18-1751" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb18-1752"><a href="#cb18-1752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1753"><a href="#cb18-1753" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample proportion: </span><span class="sc">{</span>p_hat<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1754"><a href="#cb18-1754" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wald statistic: </span><span class="sc">{</span>W<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-1755"><a href="#cb18-1755" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-1756"><a href="#cb18-1756" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Conclusion: </span><span class="sc">{</span><span class="st">'Reject H₀'</span> <span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="st">'Fail to reject H₀'</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-1757"><a href="#cb18-1757" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1758"><a href="#cb18-1758" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1759"><a href="#cb18-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1760"><a href="#cb18-1760" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Multiple Testing Correction**</span>
<span id="cb18-1761"><a href="#cb18-1761" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1762"><a href="#cb18-1762" aria-hidden="true" tabindex="-1"></a>   You run 10 hypothesis tests and get these p-values:</span>
<span id="cb18-1763"><a href="#cb18-1763" aria-hidden="true" tabindex="-1"></a>   <span class="in">```</span></span>
<span id="cb18-1764"><a href="#cb18-1764" aria-hidden="true" tabindex="-1"></a><span class="in">   0.001, 0.004, 0.012, 0.025, 0.041, 0.053, 0.074, 0.135, 0.246, 0.531</span></span>
<span id="cb18-1765"><a href="#cb18-1765" aria-hidden="true" tabindex="-1"></a><span class="in">   ```</span></span>
<span id="cb18-1766"><a href="#cb18-1766" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1767"><a href="#cb18-1767" aria-hidden="true" tabindex="-1"></a>   At α = 0.05, which hypotheses are rejected using:</span>
<span id="cb18-1768"><a href="#cb18-1768" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1769"><a href="#cb18-1769" aria-hidden="true" tabindex="-1"></a>   a) No correction?</span>
<span id="cb18-1770"><a href="#cb18-1770" aria-hidden="true" tabindex="-1"></a>   b) Bonferroni correction?</span>
<span id="cb18-1771"><a href="#cb18-1771" aria-hidden="true" tabindex="-1"></a>   c) Benjamini-Hochberg correction?</span>
<span id="cb18-1772"><a href="#cb18-1772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1773"><a href="#cb18-1773" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-1774"><a href="#cb18-1774" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb18-1775"><a href="#cb18-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1778"><a href="#cb18-1778" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-1779"><a href="#cb18-1779" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb18-1780"><a href="#cb18-1780" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> np.array([<span class="fl">0.001</span>, <span class="fl">0.004</span>, <span class="fl">0.012</span>, <span class="fl">0.025</span>, <span class="fl">0.041</span>, <span class="fl">0.053</span>, <span class="fl">0.074</span>, <span class="fl">0.135</span>, <span class="fl">0.246</span>, <span class="fl">0.531</span>])</span>
<span id="cb18-1781"><a href="#cb18-1781" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb18-1782"><a href="#cb18-1782" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(p_values)</span>
<span id="cb18-1783"><a href="#cb18-1783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1784"><a href="#cb18-1784" aria-hidden="true" tabindex="-1"></a><span class="co"># No correction</span></span>
<span id="cb18-1785"><a href="#cb18-1785" aria-hidden="true" tabindex="-1"></a>no_correction <span class="op">=</span> p_values <span class="op">&lt;=</span> alpha</span>
<span id="cb18-1786"><a href="#cb18-1786" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"No correction: Reject hypotheses </span><span class="sc">{</span>np<span class="sc">.</span>where(no_correction)[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-1787"><a href="#cb18-1787" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  (</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(no_correction)<span class="sc">}</span><span class="ss"> rejections)"</span>)</span>
<span id="cb18-1788"><a href="#cb18-1788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1789"><a href="#cb18-1789" aria-hidden="true" tabindex="-1"></a><span class="co"># Bonferroni</span></span>
<span id="cb18-1790"><a href="#cb18-1790" aria-hidden="true" tabindex="-1"></a>bonferroni_threshold <span class="op">=</span> alpha <span class="op">/</span> m</span>
<span id="cb18-1791"><a href="#cb18-1791" aria-hidden="true" tabindex="-1"></a>bonferroni <span class="op">=</span> p_values <span class="op">&lt;=</span> bonferroni_threshold</span>
<span id="cb18-1792"><a href="#cb18-1792" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Bonferroni (threshold = </span><span class="sc">{</span>bonferroni_threshold<span class="sc">:.4f}</span><span class="ss">):"</span>)</span>
<span id="cb18-1793"><a href="#cb18-1793" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Reject hypotheses </span><span class="sc">{</span>np<span class="sc">.</span>where(bonferroni)[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-1794"><a href="#cb18-1794" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  (</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(bonferroni)<span class="sc">}</span><span class="ss"> rejections)"</span>)</span>
<span id="cb18-1795"><a href="#cb18-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1796"><a href="#cb18-1796" aria-hidden="true" tabindex="-1"></a><span class="co"># Benjamini-Hochberg</span></span>
<span id="cb18-1797"><a href="#cb18-1797" aria-hidden="true" tabindex="-1"></a>sorted_idx <span class="op">=</span> np.argsort(p_values)</span>
<span id="cb18-1798"><a href="#cb18-1798" aria-hidden="true" tabindex="-1"></a>sorted_p <span class="op">=</span> p_values[sorted_idx]</span>
<span id="cb18-1799"><a href="#cb18-1799" aria-hidden="true" tabindex="-1"></a>bh_threshold <span class="op">=</span> (np.arange(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>) <span class="op">/</span> m) <span class="op">*</span> alpha</span>
<span id="cb18-1800"><a href="#cb18-1800" aria-hidden="true" tabindex="-1"></a>bh_reject_sorted <span class="op">=</span> sorted_p <span class="op">&lt;=</span> bh_threshold</span>
<span id="cb18-1801"><a href="#cb18-1801" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> np.<span class="bu">any</span>(bh_reject_sorted):</span>
<span id="cb18-1802"><a href="#cb18-1802" aria-hidden="true" tabindex="-1"></a>    k_max <span class="op">=</span> np.<span class="bu">max</span>(np.where(bh_reject_sorted)[<span class="dv">0</span>])</span>
<span id="cb18-1803"><a href="#cb18-1803" aria-hidden="true" tabindex="-1"></a>    bh_reject <span class="op">=</span> np.zeros(m, dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb18-1804"><a href="#cb18-1804" aria-hidden="true" tabindex="-1"></a>    bh_reject[sorted_idx[:k_max<span class="op">+</span><span class="dv">1</span>]] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb18-1805"><a href="#cb18-1805" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-1806"><a href="#cb18-1806" aria-hidden="true" tabindex="-1"></a>    bh_reject <span class="op">=</span> np.zeros(m, dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb18-1807"><a href="#cb18-1807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1808"><a href="#cb18-1808" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Benjamini-Hochberg:"</span>)</span>
<span id="cb18-1809"><a href="#cb18-1809" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Reject hypotheses </span><span class="sc">{</span>np<span class="sc">.</span>where(bh_reject)[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-1810"><a href="#cb18-1810" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  (</span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(bh_reject)<span class="sc">}</span><span class="ss"> rejections)"</span>)</span>
<span id="cb18-1811"><a href="#cb18-1811" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1812"><a href="#cb18-1812" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1813"><a href="#cb18-1813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1814"><a href="#cb18-1814" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Permutation Test vs Parametric Test**</span>
<span id="cb18-1815"><a href="#cb18-1815" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-1816"><a href="#cb18-1816" aria-hidden="true" tabindex="-1"></a>   When would you prefer a permutation test over a Wald test? Give at least three scenarios.</span>
<span id="cb18-1817"><a href="#cb18-1817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1818"><a href="#cb18-1818" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-1819"><a href="#cb18-1819" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution</span></span>
<span id="cb18-1820"><a href="#cb18-1820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1821"><a href="#cb18-1821" aria-hidden="true" tabindex="-1"></a>Prefer permutation tests when:</span>
<span id="cb18-1822"><a href="#cb18-1822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1823"><a href="#cb18-1823" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Small sample size**: Asymptotic approximations may not hold</span>
<span id="cb18-1824"><a href="#cb18-1824" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Non-standard distributions**: Data is heavily skewed or has outliers</span>
<span id="cb18-1825"><a href="#cb18-1825" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Complex test statistics**: No known distribution for the statistic</span>
<span id="cb18-1826"><a href="#cb18-1826" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Exact p-values needed**: Critical decisions requiring exact inference</span>
<span id="cb18-1827"><a href="#cb18-1827" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Assumptions violated**: Independence or normality assumptions fail</span>
<span id="cb18-1828"><a href="#cb18-1828" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1829"><a href="#cb18-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1830"><a href="#cb18-1830" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python and R Reference</span></span>
<span id="cb18-1831"><a href="#cb18-1831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1832"><a href="#cb18-1832" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb18-1833"><a href="#cb18-1833" aria-hidden="true" tabindex="-1"></a>::: {.tabbed-content}</span>
<span id="cb18-1834"><a href="#cb18-1834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1835"><a href="#cb18-1835" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python</span></span>
<span id="cb18-1836"><a href="#cb18-1836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1837"><a href="#cb18-1837" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb18-1838"><a href="#cb18-1838" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb18-1839"><a href="#cb18-1839" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-1840"><a href="#cb18-1840" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb18-1841"><a href="#cb18-1841" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.stats.multitest <span class="im">as</span> multitest</span>
<span id="cb18-1842"><a href="#cb18-1842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1843"><a href="#cb18-1843" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test for single proportion</span></span>
<span id="cb18-1844"><a href="#cb18-1844" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wald_test_proportion(x, n, p0):</span>
<span id="cb18-1845"><a href="#cb18-1845" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Test H0: p = p0"""</span></span>
<span id="cb18-1846"><a href="#cb18-1846" aria-hidden="true" tabindex="-1"></a>    p_hat <span class="op">=</span> x <span class="op">/</span> n</span>
<span id="cb18-1847"><a href="#cb18-1847" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> np.sqrt(p0 <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p0) <span class="op">/</span> n)</span>
<span id="cb18-1848"><a href="#cb18-1848" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> (p_hat <span class="op">-</span> p0) <span class="op">/</span> se</span>
<span id="cb18-1849"><a href="#cb18-1849" aria-hidden="true" tabindex="-1"></a>    p_value <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> stats.norm.cdf(<span class="op">-</span><span class="bu">abs</span>(W))</span>
<span id="cb18-1850"><a href="#cb18-1850" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W, p_value</span>
<span id="cb18-1851"><a href="#cb18-1851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1852"><a href="#cb18-1852" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-sample t-test (Wald for means)</span></span>
<span id="cb18-1853"><a href="#cb18-1853" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> stats.ttest_ind(group1, group2)</span>
<span id="cb18-1854"><a href="#cb18-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1855"><a href="#cb18-1855" aria-hidden="true" tabindex="-1"></a><span class="co"># Paired t-test</span></span>
<span id="cb18-1856"><a href="#cb18-1856" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> stats.ttest_rel(sample1, sample2)</span>
<span id="cb18-1857"><a href="#cb18-1857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1858"><a href="#cb18-1858" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test for 2x2 tables</span></span>
<span id="cb18-1859"><a href="#cb18-1859" aria-hidden="true" tabindex="-1"></a>odds_ratio, p_value <span class="op">=</span> stats.fisher_exact([[a, b], [c, d]])</span>
<span id="cb18-1860"><a href="#cb18-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1861"><a href="#cb18-1861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1862"><a href="#cb18-1862" aria-hidden="true" tabindex="-1"></a><span class="co"># Permutation test</span></span>
<span id="cb18-1863"><a href="#cb18-1863" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> permutation_test(x, y, n_permutations<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb18-1864"><a href="#cb18-1864" aria-hidden="true" tabindex="-1"></a>    observed <span class="op">=</span> np.<span class="bu">abs</span>(np.mean(x) <span class="op">-</span> np.mean(y))</span>
<span id="cb18-1865"><a href="#cb18-1865" aria-hidden="true" tabindex="-1"></a>    combined <span class="op">=</span> np.concatenate([x, y])</span>
<span id="cb18-1866"><a href="#cb18-1866" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-1867"><a href="#cb18-1867" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_permutations):</span>
<span id="cb18-1868"><a href="#cb18-1868" aria-hidden="true" tabindex="-1"></a>        np.random.shuffle(combined)</span>
<span id="cb18-1869"><a href="#cb18-1869" aria-hidden="true" tabindex="-1"></a>        x_perm <span class="op">=</span> combined[:<span class="bu">len</span>(x)]</span>
<span id="cb18-1870"><a href="#cb18-1870" aria-hidden="true" tabindex="-1"></a>        y_perm <span class="op">=</span> combined[<span class="bu">len</span>(x):]</span>
<span id="cb18-1871"><a href="#cb18-1871" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">abs</span>(np.mean(x_perm) <span class="op">-</span> np.mean(y_perm)) <span class="op">&gt;=</span> observed:</span>
<span id="cb18-1872"><a href="#cb18-1872" aria-hidden="true" tabindex="-1"></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-1873"><a href="#cb18-1873" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (count <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (n_permutations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb18-1874"><a href="#cb18-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1875"><a href="#cb18-1875" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple testing corrections</span></span>
<span id="cb18-1876"><a href="#cb18-1876" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multitest <span class="im">import</span> multipletests</span>
<span id="cb18-1877"><a href="#cb18-1877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1878"><a href="#cb18-1878" aria-hidden="true" tabindex="-1"></a><span class="co"># Bonferroni and BH corrections</span></span>
<span id="cb18-1879"><a href="#cb18-1879" aria-hidden="true" tabindex="-1"></a>reject_bonf, pvals_bonf, _, _ <span class="op">=</span> multipletests(p_values, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'bonferroni'</span>)</span>
<span id="cb18-1880"><a href="#cb18-1880" aria-hidden="true" tabindex="-1"></a>reject_bh, pvals_bh, _, _ <span class="op">=</span> multipletests(p_values, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'fdr_bh'</span>)</span>
<span id="cb18-1881"><a href="#cb18-1881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1882"><a href="#cb18-1882" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative: Using scipy (newer versions)</span></span>
<span id="cb18-1883"><a href="#cb18-1883" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> false_discovery_control</span>
<span id="cb18-1884"><a href="#cb18-1884" aria-hidden="true" tabindex="-1"></a><span class="co"># This adjusts p-values using Benjamini-Hochberg</span></span>
<span id="cb18-1885"><a href="#cb18-1885" aria-hidden="true" tabindex="-1"></a>adjusted_pvals <span class="op">=</span> false_discovery_control(p_values, method<span class="op">=</span><span class="st">'bh'</span>)</span>
<span id="cb18-1886"><a href="#cb18-1886" aria-hidden="true" tabindex="-1"></a><span class="co"># Reject hypotheses where adjusted p-value &lt;= alpha</span></span>
<span id="cb18-1887"><a href="#cb18-1887" aria-hidden="true" tabindex="-1"></a>reject_scipy_bh <span class="op">=</span> adjusted_pvals <span class="op">&lt;=</span> <span class="fl">0.05</span></span>
<span id="cb18-1888"><a href="#cb18-1888" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1889"><a href="#cb18-1889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1890"><a href="#cb18-1890" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb18-1891"><a href="#cb18-1891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1892"><a href="#cb18-1892" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb18-1893"><a href="#cb18-1893" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb18-1894"><a href="#cb18-1894" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test for single proportion</span></span>
<span id="cb18-1895"><a href="#cb18-1895" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(<span class="at">x =</span> <span class="dv">50</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">p =</span> <span class="fl">0.5</span>)</span>
<span id="cb18-1896"><a href="#cb18-1896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1897"><a href="#cb18-1897" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-sample t-test</span></span>
<span id="cb18-1898"><a href="#cb18-1898" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(group1, group2)</span>
<span id="cb18-1899"><a href="#cb18-1899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1900"><a href="#cb18-1900" aria-hidden="true" tabindex="-1"></a><span class="co"># Paired t-test</span></span>
<span id="cb18-1901"><a href="#cb18-1901" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(sample1, sample2, <span class="at">paired =</span> <span class="cn">TRUE</span>)</span>
<span id="cb18-1902"><a href="#cb18-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1903"><a href="#cb18-1903" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test</span></span>
<span id="cb18-1904"><a href="#cb18-1904" aria-hidden="true" tabindex="-1"></a><span class="fu">fisher.test</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(a, b, c, d), <span class="at">nrow =</span> <span class="dv">2</span>))</span>
<span id="cb18-1905"><a href="#cb18-1905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1906"><a href="#cb18-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1907"><a href="#cb18-1907" aria-hidden="true" tabindex="-1"></a><span class="co"># Permutation test (using coin package)</span></span>
<span id="cb18-1908"><a href="#cb18-1908" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(coin)</span>
<span id="cb18-1909"><a href="#cb18-1909" aria-hidden="true" tabindex="-1"></a><span class="fu">independence_test</span>(outcome <span class="sc">~</span> group, <span class="at">data =</span> df, </span>
<span id="cb18-1910"><a href="#cb18-1910" aria-hidden="true" tabindex="-1"></a>                  <span class="at">distribution =</span> <span class="fu">approximate</span>(<span class="at">nresample =</span> <span class="dv">10000</span>))</span>
<span id="cb18-1911"><a href="#cb18-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1912"><a href="#cb18-1912" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple testing corrections</span></span>
<span id="cb18-1913"><a href="#cb18-1913" aria-hidden="true" tabindex="-1"></a><span class="fu">p.adjust</span>(p_values, <span class="at">method =</span> <span class="st">"bonferroni"</span>)</span>
<span id="cb18-1914"><a href="#cb18-1914" aria-hidden="true" tabindex="-1"></a><span class="fu">p.adjust</span>(p_values, <span class="at">method =</span> <span class="st">"BH"</span>)  <span class="co"># Benjamini-Hochberg</span></span>
<span id="cb18-1915"><a href="#cb18-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1916"><a href="#cb18-1916" aria-hidden="true" tabindex="-1"></a><span class="co"># Or using multtest package</span></span>
<span id="cb18-1917"><a href="#cb18-1917" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(multtest)</span>
<span id="cb18-1918"><a href="#cb18-1918" aria-hidden="true" tabindex="-1"></a><span class="fu">mt.rawp2adjp</span>(p_values, <span class="at">proc =</span> <span class="fu">c</span>(<span class="st">"Bonferroni"</span>, <span class="st">"BH"</span>))</span>
<span id="cb18-1919"><a href="#cb18-1919" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-1920"><a href="#cb18-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1921"><a href="#cb18-1921" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1922"><a href="#cb18-1922" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1923"><a href="#cb18-1923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1924"><a href="#cb18-1924" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb18-1925"><a href="#cb18-1925" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb18-1926"><a href="#cb18-1926" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python and R Reference Code</span></span>
<span id="cb18-1927"><a href="#cb18-1927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1928"><a href="#cb18-1928" aria-hidden="true" tabindex="-1"></a>Python and R code examples for this chapter can be found in the HTML version of these notes.</span>
<span id="cb18-1929"><a href="#cb18-1929" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1930"><a href="#cb18-1930" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1931"><a href="#cb18-1931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1932"><a href="#cb18-1932" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connections to Source Material</span></span>
<span id="cb18-1933"><a href="#cb18-1933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1934"><a href="#cb18-1934" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb18-1935"><a href="#cb18-1935" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping to "All of Statistics"</span></span>
<span id="cb18-1936"><a href="#cb18-1936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1937"><a href="#cb18-1937" aria-hidden="true" tabindex="-1"></a>| Lecture Note Section | Corresponding Source(s) |</span>
<span id="cb18-1938"><a href="#cb18-1938" aria-hidden="true" tabindex="-1"></a>| :--- | :--- |</span>
<span id="cb18-1939"><a href="#cb18-1939" aria-hidden="true" tabindex="-1"></a>| **Introduction** | Lecture slides; drug trial example from slides |</span>
<span id="cb18-1940"><a href="#cb18-1940" aria-hidden="true" tabindex="-1"></a>| **Framework of Hypothesis Testing** | AoS Ch 10 Intro |</span>
<span id="cb18-1941"><a href="#cb18-1941" aria-hidden="true" tabindex="-1"></a>| ↳ Null and Alternative Hypotheses | AoS Ch 10 Intro |</span>
<span id="cb18-1942"><a href="#cb18-1942" aria-hidden="true" tabindex="-1"></a>| ↳ Type I and Type II Errors | AoS Table 10.1, Definition 10.1 |</span>
<span id="cb18-1943"><a href="#cb18-1943" aria-hidden="true" tabindex="-1"></a>| ↳ Power and Size | AoS Definition 10.1 |</span>
<span id="cb18-1944"><a href="#cb18-1944" aria-hidden="true" tabindex="-1"></a>| **The p-value** | AoS §10.2 |</span>
<span id="cb18-1945"><a href="#cb18-1945" aria-hidden="true" tabindex="-1"></a>| ↳ Understanding the p-value | AoS §10.2 (Definition 10.11, Theorem 10.12) |</span>
<span id="cb18-1946"><a href="#cb18-1946" aria-hidden="true" tabindex="-1"></a>| ↳ Interpretation and Misinterpretation | AoS §10.2, expanded with modern critiques |</span>
<span id="cb18-1947"><a href="#cb18-1947" aria-hidden="true" tabindex="-1"></a>| **Hypothesis Tests** | |</span>
<span id="cb18-1948"><a href="#cb18-1948" aria-hidden="true" tabindex="-1"></a>| ↳ The Wald Test | AoS §10.1 (Definition 10.3, Theorem 10.4) |</span>
<span id="cb18-1949"><a href="#cb18-1949" aria-hidden="true" tabindex="-1"></a>| ↳ Statistical and Scientific Significance | AoS §10.1, Theorem 10.10 (CI duality) |</span>
<span id="cb18-1950"><a href="#cb18-1950" aria-hidden="true" tabindex="-1"></a>| ↳ Comparing Proportions/Means | AoS §10.1 (Examples 10.7, 10.8) |</span>
<span id="cb18-1951"><a href="#cb18-1951" aria-hidden="true" tabindex="-1"></a>| ↳ The Permutation Test | AoS §10.5 (Example 10.19) |</span>
<span id="cb18-1952"><a href="#cb18-1952" aria-hidden="true" tabindex="-1"></a>| ↳ Fisher's Exact Test | Expanded from slides |</span>
<span id="cb18-1953"><a href="#cb18-1953" aria-hidden="true" tabindex="-1"></a>| ↳ The Likelihood Ratio Test | AoS §10.6 |</span>
<span id="cb18-1954"><a href="#cb18-1954" aria-hidden="true" tabindex="-1"></a>| **Multiple Testing Problem** | AoS §10.7 |</span>
<span id="cb18-1955"><a href="#cb18-1955" aria-hidden="true" tabindex="-1"></a>| ↳ Bonferroni Correction | AoS §10.7 (Theorem 10.24) |</span>
<span id="cb18-1956"><a href="#cb18-1956" aria-hidden="true" tabindex="-1"></a>| ↳ Benjamini-Hochberg | AoS §10.7 (Theorem 10.26) |</span>
<span id="cb18-1957"><a href="#cb18-1957" aria-hidden="true" tabindex="-1"></a>| **NHST in Practice: A Critical View** | Expanded from slides |</span>
<span id="cb18-1958"><a href="#cb18-1958" aria-hidden="true" tabindex="-1"></a>| **Self-Test Problems** | Based on AoS Exercise 10.6 and similar examples |</span>
<span id="cb18-1959"><a href="#cb18-1959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1960"><a href="#cb18-1960" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-1961"><a href="#cb18-1961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1962"><a href="#cb18-1962" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Reading</span></span>
<span id="cb18-1963"><a href="#cb18-1963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1964"><a href="#cb18-1964" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Modern perspective**: Wasserstein &amp; Lazar (2016), "The ASA Statement on p-Values"</span>
<span id="cb18-1965"><a href="#cb18-1965" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Critical view**: Ioannidis (2005), "Why Most Published Research Findings Are False"</span>
<span id="cb18-1966"><a href="#cb18-1966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1967"><a href="#cb18-1967" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb18-1968"><a href="#cb18-1968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-1969"><a href="#cb18-1969" aria-hidden="true" tabindex="-1"></a>*Remember: Hypothesis testing is a tool for making decisions under uncertainty. Use it wisely -- report effect sizes and confidence intervals, correct for multiple testing, and never forget that statistical significance is not the same as practical importance!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
// Function to render math in an element
function renderMath(element) {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(element, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\[', right: '\\]', display: true},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
}

// Wait for page to fully load
window.addEventListener('load', function() {
  // Render math in all tabs initially
  document.querySelectorAll('.tab-pane').forEach(pane => renderMath(pane));
  
  // Re-render when tabs are shown
  document.addEventListener('shown.bs.tab', function(e) {
    const tabPane = document.querySelector(e.target.getAttribute('data-bs-target'));
    if (tabPane) renderMath(tabPane);
  });
});
</script>




</body></html>